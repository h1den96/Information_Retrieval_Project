[
  {
    "id": 1,
    "title": "Information retrieval",
    "content": "Information retrieval  ( IR ) in  computing  and  information science  is the task of identifying and retrieving  information system  resources that are relevant to an  information need .  The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on  full-text  or other content-based indexing. Information retrieval is the  science [ 1 ]  of searching for information in a document, searching for documents themselves, and also searching for the  metadata  that describes data, and for  databases  of texts, images or sounds. Automated information retrieval systems are used to reduce what has been called  information overload . An IR system is a software system that provides access to books, journals and other documents; it also stores and manages those documents.  Web search engines  are the most visible IR applications. An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval, a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of  relevance . An object is an entity that is represented by information in a content collection or  database . User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This  ranking  of results is a key difference of information retrieval searching compared to database searching. [ 2 ] Depending on the  application  the data objects may be, for example, text documents, images, [ 3 ]  audio, [ 4 ]   mind maps [ 5 ]  or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or  metadata . Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query. [ 6 ] there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute The idea of using computers to search for relevant pieces of information was popularized in the article  As We May Think  by  Vannevar Bush  in 1945. [ 7 ]  It would appear that Bush was inspired by patents for a 'statistical machine' – filed by  Emanuel Goldberg  in the 1920s and 1930s – that searched for documents stored on film. [ 8 ]  The first description of a computer searching for information was described by Holmstrom in 1948, [ 9 ]  detailing an early mention of the  Univac  computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy,  Desk Set . In the 1960s, the first large information retrieval research group was formed by  Gerard Salton  at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small  text corpora  such as the Cranfield collection (several thousand documents). [ 7 ]  Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s. In 1992, the US Department of Defense along with the  National Institute of Standards and Technology  (NIST), cosponsored the  Text Retrieval Conference  (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that  scale  to huge corpora. The introduction of  web search engines  has boosted the need for very large scale retrieval systems even further. Areas where information retrieval techniques are employed include (the entries are in alphabetical order within each category): Methods/Techniques in which information retrieval techniques are employed include: In order to effectively retrieve relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model. The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for  Boolean retrieval [ clarification needed ]  or top-k retrieval, include  precision and recall . All measures assume a  ground truth  notion of relevance: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be  ill-posed  and there may be different shades of relevance."
  },
  {
    "id": 2,
    "title": "Information science",
    "content": "Information science [ 1 ]   [ 2 ] [ 3 ]  is an academic field which is primarily concerned with  analysis , collection,  classification , manipulation, storage,  retrieval , movement, dissemination, and protection of  information . [ 4 ]  Practitioners within and outside the field study the  application and the usage of knowledge in  organizations  in addition to the interaction between people, organizations, and any existing  information systems  with the aim of creating, replacing, improving, or understanding the information systems. Historically, information science is associated with informatics,  computer science ,  data science ,  psychology ,  technology ,  documentation science ,  library science ,  healthcare , and  intelligence agencies . [ 5 ]  However, information science also incorporates aspects of diverse fields such as  archival science ,  cognitive science ,  commerce ,  law ,  linguistics ,  museology ,  management ,  mathematics ,  philosophy ,  public policy , and  social sciences . Information science focuses on understanding  problems  from the perspective of the stakeholders involved and then applying information and other technologies as needed. In other words, it tackles systemic problems first rather than individual pieces of  technology  within that system. In this respect, one can see information science as a response to  technological determinism , the belief that technology \"develops by its own laws, that it realizes its own potential, limited only by the material resources available and the creativity of its developers. It must therefore be regarded as an autonomous system controlling and ultimately permeating all other subsystems of society.\" [ 6 ] Many universities have entire colleges, departments or schools devoted to the study of information science, while numerous information-science scholars work in disciplines such as  communication ,  healthcare ,  computer science ,  law , and  sociology . Several institutions have formed an I-School Caucus (see  List of I-Schools ), but numerous others besides these also have comprehensive information foci. Within information science, current issues as of 2013 [update]  include: The first known usage of the term \"information science\" was in 1955. [ 7 ]  An early definition of Information science (going back to 1968, the year when the  American Documentation Institute  renamed itself as the  American Society for Information Science and Technology ) states: Some authors use  informatics  as a synonym for  information science . This is especially true when related to the concept developed by  A. I. Mikhailov  and other Soviet authors in the mid-1960s. The Mikhailov school saw informatics as a discipline related to the study of scientific information. [ 9 ] \nInformatics is difficult to precisely define because of the rapidly evolving and  interdisciplinary  nature of the field. Definitions reliant on the nature of the tools used for deriving meaningful information from data are emerging in Informatics academic programs. [ 10 ] Regional differences and international terminology complicate the problem. Some people [ which? ]  note that much of what is called \"Informatics\" today was once called \"Information Science\" – at least in fields such as  Medical Informatics . For example, when library scientists began also to use the phrase \"Information Science\" to refer to their work, the term \"informatics\" emerged: Another term discussed as a synonym for \"information studies\" is \" information systems \".  Brian Campbell Vickery 's  Information Systems  (1973) placed information systems within IS. [ 11 ]   Ellis, Allen & Wilson (1999) , on the other hand, provided a bibliometric investigation describing the relation between two  different  fields: \"information science\" and \"information systems\". [ 12 ] Philosophy of information studies conceptual issues arising at the intersection of  psychology ,  computer science ,  information technology , and  philosophy . It includes the investigation of the conceptual nature and basic principles of  information , including its dynamics, utilisation and sciences, as well as the elaboration and application of information-theoretic and computational methodologies to its philosophical problems. [ 13 ] In science and information science, an ontology formally represents knowledge as a set of concepts within a  domain , and the relationships between those concepts.  It can be used to  reason  about the entities within that domain and may be used to describe the domain. More specifically, an ontology is a model for describing the world that consists of a set of types, properties, and relationship types. Exactly what is provided around these varies, but they are the essentials of an ontology. There is also generally an expectation that there be a close resemblance between the real world and the features of the model in an ontology. [ 14 ] In theory, an ontology is a \"formal, explicit specification of a shared conceptualisation\". [ 15 ]  An ontology renders shared  vocabulary  and  taxonomy  which models a domain with the definition of objects and/or concepts and their properties and relations. [ 16 ] Ontologies are the structural frameworks for organizing information and are used in  artificial intelligence , the  Semantic Web ,  systems engineering ,  software engineering ,  biomedical informatics ,  library science ,  enterprise bookmarking , and  information architecture  as a form of  knowledge representation  about the world or some part of it. The creation of domain ontologies is also essential to the definition and use of an  enterprise architecture framework . Authors such as Ingwersen [ 17 ]  argue that informatology has problems defining its own boundaries with other disciplines. According to Popper \"Information science operates busily on an ocean of commonsense practical applications, which increasingly involve the computer ... and on commonsense views of language, of communication, of knowledge and Information, computer science is in little better state\". [ 18 ]  Other authors, such as Furner, deny that information science is a true science. [ 19 ] An information scientist is an individual, usually with a relevant subject degree or high level of subject knowledge, who provides focused information to scientific and technical research staff in industry or to subject faculty and students in academia. The industry *information specialist/scientist*  and the academic information subject specialist/librarian have, in general, similar subject background training, but the academic position holder will be required to hold a second advanced degree (MLS/MI/MA in IS, e.g.) in information and library studies in addition to a subject master's.  The title also applies to an individual carrying out research in information science. A systems analyst works on creating, designing, and improving information systems for a specific need. Often systems analysts work with one or more  businesses  to evaluate and implement organizational processes and techniques for accessing information in order to improve  efficiency  and  productivity  within the organization (s). An information professional is an individual who preserves, organizes, and disseminates information. Information professionals are skilled in the organization and retrieval of recorded knowledge. Traditionally, their work has been with print materials, but these skills are being increasingly used with electronic, visual, audio, and digital materials. Information professionals work in a variety of public, private, non-profit, and academic institutions. Information professionals can also be found within organisational and industrial contexts. Performing roles that include system design and development and system analysis. Information science, in studying the collection,  classification , manipulation, storage,  retrieval  and dissemination of  information  has origins in the common stock of human knowledge. Information analysis has been carried out by scholars at least as early as the time of the  Assyrian Empire  with the emergence of cultural depositories, what is today known as libraries and archives. [ 20 ]  Institutionally, information science emerged in the 19th century along with many other social science disciplines. As a science, however, it finds its institutional roots in the  history of science , beginning with publication of the first issues of  Philosophical Transactions ,  generally considered the first scientific journal, in 1665 by the Royal Society (London). The  institutionalization  of science occurred throughout the 18th century. In 1731,  Benjamin Franklin  established the  Library Company of Philadelphia , the first library owned by a group of public citizens, which quickly expanded beyond the realm of books and became a center of  scientific experimentation , and which hosted public exhibitions of scientific experiments. [ 21 ]  Benjamin Franklin invested a town in  Massachusetts  with a collection of books that the town voted to make available to all free of charge, forming the first  public library  of the  United States . [ 22 ]  Academie de Chirurgia ( Paris ) published  Memoires pour les Chirurgiens , generally considered to be the first  medical journal , in 1736. The  American Philosophical Society , patterned on the  Royal Society  ( London ), was founded in Philadelphia in 1743. As numerous other scientific journals and societies were founded,  Alois Senefelder  developed the concept of  lithography  for use in mass printing work in  Germany  in 1796. By the 19th century the first signs of information science emerged as separate and distinct from other sciences and social sciences but in conjunction with communication and computation. In 1801,  Joseph Marie Jacquard  invented a punched card system to control operations of the cloth weaving loom in France. It was the first use of \"memory storage of patterns\" system. [ 23 ]  As chemistry journals emerged throughout the 1820s and 1830s, [ 24 ]   Charles Babbage  developed his \"difference engine\", the first step towards the modern computer, in 1822 and his \"analytical engine\" by 1834. By 1843  Richard Hoe  developed the rotary press, and in 1844  Samuel Morse  sent the first public telegraph message. By 1848 William F. Poole begins the  Index to Periodical Literature,  the first general periodical literature index in the US. In 1854  George Boole  published  An Investigation into Laws of Thought...,  which lays the foundations for  Boolean algebra , which is later used in  information retrieval . [ 25 ]  In 1860 a congress was held at Karlsruhe Technische Hochschule to discuss the feasibility of establishing a systematic and rational nomenclature for chemistry. The congress did not reach any conclusive results, but several key participants returned home with  Stanislao Cannizzaro 's outline (1858), which ultimately convinces them of the validity of his scheme for calculating atomic weights. [ 26 ] By 1865, the  Smithsonian Institution  began a catalog of current scientific papers, which became the  International Catalogue of Scientific Papers  in 1902. [ 27 ]  The following year the Royal Society began publication of its  Catalogue of Papers  in London. In 1868, Christopher Sholes, Carlos Glidden, and S. W. Soule produced the  first practical typewriter . By 1872 Lord Kelvin devised an analogue computer to predict the tides, and by 1875  Frank Stephen Baldwin  was granted the first US patent for a practical calculating machine that performs four arithmetic functions. [ 24 ]   Alexander Graham Bell  and  Thomas Edison  invented the telephone and phonograph in 1876 and 1877 respectively, and the  American Library Association  was founded in Philadelphia. In 1879  Index Medicus  was first issued by the Library of the Surgeon General, U.S. Army, with  John Shaw Billings  as librarian, and later the library issues  Index Catalogue,  which achieved an international reputation as the most complete catalog of medical literature. [ 28 ] The discipline of  documentation science , which marks the earliest theoretical foundations of modern information science, emerged in the late part of the 19th century in Europe together with several more scientific indexes whose purpose was to organize scholarly literature. Many information science historians cite  Paul Otlet  and  Henri La Fontaine  as the fathers of information science with the founding of the International Institute of Bibliography (IIB) in 1895. [ 29 ]  A second generation of European Documentalists emerged after the  Second World War , most notably  Suzanne Briet . [ 30 ]  However, \"information science\" as a term is not popularly used in academia until sometime in the latter part of the 20th century. [ 31 ] Documentalists emphasized the utilitarian integration of technology and technique toward specific social goals. According to Ronald Day, \"As an organized system of techniques and technologies, documentation was understood as a player in the historical development of global organization in modernity – indeed, a major player inasmuch as that organization was dependent on the organization and transmission of information.\" [ 31 ]  Otlet and Lafontaine (who won the  Nobel Prize  in 1913) not only envisioned later technical innovations but also projected a global vision for information and  information technologies  that speaks directly to postwar visions of a global \"information society\". Otlet and Lafontaine established numerous organizations dedicated to standardization, bibliography, international associations, and consequently, international cooperation. These organizations were fundamental for ensuring international production in commerce, information, communication and modern economic development, and they later found their global form in such institutions as the  League of Nations  and the  United Nations . Otlet designed the  Universal Decimal Classification , based on  Melville Dewey 's decimal classification system. [ 31 ] Although he lived decades before computers and networks emerged, what he discussed prefigured what ultimately became the  World Wide Web . His vision of a great network of  knowledge  focused on  documents  and included the notions of  hyperlinks ,  search engines , remote access, and  social networks . Otlet not only imagined that all the world's knowledge should be interlinked and made available remotely to anyone, but he also proceeded to build a structured document collection. This collection involved standardized paper sheets and cards filed in custom-designed cabinets according to a hierarchical index (which culled information worldwide from diverse sources) and a commercial information retrieval service (which answered written requests by copying relevant information from index cards). Users of this service were even warned if their query was likely to produce more than 50 results per search. [ 31 ] \nBy 1937 documentation had formally been institutionalized, as evidenced by the founding of the American Documentation Institute (ADI), later called the  American Society for Information Science and Technology . With the 1950s came increasing awareness of the potential of automatic devices for literature searching and information storage and retrieval. As these concepts grew in magnitude and potential, so did the variety of information science interests. By the 1960s and 70s, there was a move from batch processing to online modes, from mainframe to mini and microcomputers. Additionally, traditional boundaries among disciplines began to fade and many information science scholars joined with other programs. They further made themselves multidisciplinary by incorporating disciplines in the sciences, humanities and social sciences, as well as other professional programs, such as  law  and  medicine  in their curriculum. Among the individuals who had distinct opportunities to facilitate interdisciplinary activity targeted at scientific communication was  Foster E. Mohrhardt , director of the  National Agricultural Library  from 1954 to 1968. [ 32 ] By the 1980s, large databases, such as Grateful Med at the  National Library of Medicine , and user-oriented services such as  Dialog  and  Compuserve , were for the first time accessible by individuals from their personal computers. The 1980s also saw the emergence of numerous  special interest groups  to respond to the changes. By the end of the decade, special interest groups were available involving non-print media, social sciences, energy and the environment, and community information systems. Today, information science largely examines technical bases, social consequences, and theoretical understanding of online databases, widespread use of databases in government, industry, and education, and the development of the Internet and World Wide Web. [ 33 ] Dissemination  has historically been interpreted as unilateral communication of information. With the advent of the  internet , and the explosion in popularity of  online communities ,  social media  has changed the information landscape in many respects, and creates both new modes of communication and new types of information\", [ 34 ]  changing the interpretation of the definition of dissemination. The nature of social networks allows for faster diffusion of information than through organizational sources. [ 35 ]  The internet has changed the way we view, use, create, and store information; now it is time to re-evaluate the way we share and spread it. Social media networks provide an open information environment for the mass of people who have limited time or access to traditional outlets of information diffusion, [ 35 ]  this is an \"increasingly mobile and social world [that] demands...new types of information skills\". [ 34 ]  Social media integration as an access point is a very useful and mutually beneficial tool for users and providers. All major news providers have visibility and an access point through networks such as  Facebook  and  Twitter  maximizing their breadth of audience. Through social media people are directed to, or provided with, information by people they know. The ability to \"share, like, and comment on...content\" [ 36 ]  increases the reach farther and wider than traditional methods. People like to interact with information, they enjoy including the people they know in their circle of knowledge. Sharing through social media has become so influential that publishers must \"play nice\" if they desire to succeed. Although, it is often mutually beneficial for publishers and Facebook to \"share, promote and uncover new content\" [ 36 ]  to improve both user base experiences. The impact of popular opinion can spread in unimaginable ways. Social media allows interaction through simple to learn and access tools;  The Wall Street Journal  offers an app through Facebook, and  The Washington Post  goes a step further and offers an independent social app that was downloaded by 19.5 million users in six months, [ 36 ]  proving how interested people are in the new way of being provided information. The connections and networks sustained through social media help information providers learn what is important to people. The connections people have throughout the world enable the exchange of information at an unprecedented rate. It is for this reason that these networks have been realized for the potential they provide. \"Most news media monitor Twitter for breaking news\", [ 35 ]  as well as news anchors frequently request the audience to tweet pictures of events. [ 36 ]  The users and viewers of the shared information have earned \"opinion-making and agenda-setting power\" [ 35 ]  This channel has been recognized for the usefulness of providing targeted information based on public demand. The following areas are some of those that information science investigates and develops. Information access is an area of research at the intersection of  Informatics , Information Science,  Information Security ,  Language Technology , and  Computer Science . The objectives of information access research are to automate the processing of large and unwieldy amounts of information and to simplify users' access to it. What about assigning privileges and restricting access to unauthorized users? The extent of access should be defined in the level of clearance granted for the information. Applicable technologies include  information retrieval ,  text mining ,  text editing ,  machine translation , and  text categorisation . In discussion, information access is often defined as concerning the insurance of free and closed or public access to information and is brought up in discussions on  copyright ,  patent law , and  public domain . Public libraries need resources to provide knowledge of information assurance. Information architecture (IA) is the art and science of organizing and labelling  websites ,  intranets ,  online communities  and software to support usability. [ 37 ]  It is an emerging discipline and  community of practice  focused on bringing together principles of  design  and  architecture  to the  digital landscape . [ 38 ]  Typically it involves a  model  or  concept  of  information  which is used and applied to activities that require explicit details of complex  information systems . These activities include  library  systems and  database  development. Information management (IM) is the collection and management of information from one or more sources and the distribution of that information to one or more audiences. This sometimes involves those who have a stake in, or a right to that information. Management means the organization of and control over the structure, processing and delivery of information. Throughout the 1970s this was largely limited to files, file maintenance, and the life cycle management of paper-based files, other media and records. With the proliferation of information technology starting in the 1970s, the job of information management took on a new light and also began to include the field of data maintenance. Information retrieval (IR) is the area of study concerned with searching for documents, for  information  within documents, and for  metadata  about documents, as well as that of searching  structured storage ,  relational databases , and the  World Wide Web . Automated information retrieval systems are used to reduce what has been called \" information overload \". Many universities and  public libraries  use IR systems to provide access to books, journals and other documents.  Web search engines  are the most visible  IR applications . An information retrieval process begins when a user enters a  query  into the system. Queries are formal statements of  information needs , for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of  relevancy . An object is an entity that is represented by information in a  database . User queries are matched against the database information. Depending on the  application  the data objects may be, for example, text documents, images, [ 39 ]  audio, [ 40 ]   mind maps [ 41 ]  or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata. Most IR systems compute a numeric score on how well each object in the database match the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query. [ 42 ] Information seeking is the process or activity of attempting to obtain information in both human and technological contexts. Information seeking is related to, but different from, information retrieval (IR). Much library and information science (LIS) research has focused on the information-seeking practices of practitioners within various fields of professional work. Studies have been carried out into the information-seeking behaviors of librarians, [ 43 ]  academics, [ 44 ]  medical professionals, [ 45 ]  engineers [ 46 ]  and lawyers [ 47 ]  (among others). Much of this research has drawn on the work done by Leckie, Pettigrew (now Fisher) and Sylvain, who in 1996 conducted an extensive review of the LIS literature (as well as the literature of other academic fields) on professionals' information seeking. The authors proposed an analytic model of professionals' information seeking behaviour, intended to be generalizable across the professions, thus providing a platform for future research in the area. The model was intended to \"prompt new insights... and give rise to more refined and applicable theories of information seeking\" ( Leckie, Pettigrew & Sylvain 1996 , p. 188). The model has been adapted by  Wilkinson (2001)  who proposes a model of the information seeking of lawyers. Recent studies in this topic address the concept of information-gathering that \"provides a broader perspective that adheres better to professionals' work-related reality and desired skills.\" [ 48 ]  ( Solomon & Bronstein 2021 ). An information society is a  society  where the creation, distribution, diffusion, uses, integration and manipulation of  information  is a significant economic, political, and cultural activity. The aim of an information society is to gain competitive advantage internationally, through using  IT  in a creative and productive way.  The  knowledge economy  is its economic counterpart, whereby wealth is created through the economic exploitation of understanding. People who have the means to partake in this form of society are sometimes called  digital citizens . Basically, an information society is the means of getting information from one place to another ( Wark 1997 , p. 22). As technology has become more advanced over time so too has the way we have adapted in sharing this information with each other. Information society theory discusses the role of information and information technology in society, the question of which key concepts should be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology. Knowledge representation (KR) is an area of  artificial intelligence  research aimed at representing knowledge in symbols to facilitate  inferencing  from those  knowledge  elements, creating new elements of knowledge. The KR can be made to be independent of the underlying knowledge model or knowledge base system (KBS) such as a  semantic network . [ 49 ] Knowledge Representation (KR) research involves analysis of how to reason accurately and effectively and how best to use a set of symbols to represent a set of facts within a knowledge domain. A symbol vocabulary and a system of logic are combined to enable  inferences  about elements in the KR to create new KR sentences. Logic is used to supply formal  semantics  of how reasoning functions should be applied to the symbols in the KR system. Logic is also used to define how operators can process and reshape the knowledge. Examples of operators and operations include, negation, conjunction, adverbs, adjectives, quantifiers and modal operators. The logic is interpretation theory. These elements—symbols, operators, and interpretation theory—are what give sequences of symbols meaning within a KR."
  },
  {
    "id": 3,
    "title": "Outline of information science",
    "content": "The following  outline  is provided as an overview of and topical guide to information science: Information science  –  interdisciplinary  field primarily concerned with the analysis, collection,  classification , manipulation, storage,  retrieval  and dissemination of  information . [ 1 ]  Practitioners within the field study the application and usage of knowledge in  organizations , along with the interaction between people, organizations and any existing  information systems , with the aim of creating, replacing, improving or understanding information systems. Information science can be described as all of the following: As an interdisciplinary field, information science draws upon and incorporates concepts and methodologies from: There are many fields which claim to be \"sciences\" or \"disciplines\" which are difficult to distinguish from each other and from information science. Some of them are:"
  },
  {
    "id": 4,
    "title": "Information access",
    "content": "Information access  is the freedom or ability to identify, obtain and make use of  database   or information effectively. [ 1 ] There are various research efforts in information access for which the objective is to simplify and make it more effective for human users to access and further process large and unwieldy amounts of data and  information . Several technologies applicable to the general area are  Information Retrieval ,  Text Mining ,  Machine Translation , and  Text Categorisation . [ 2 ] During discussions on free access to information as well as on  information policy , information access is understood as concerning the insurance of free and closed access to  information . Information access covers many issues including  copyright ,  open source ,  privacy , and  security . Groups such as the  American Library Association , the  American Association of Law Libraries ,  Ralph Nader 's  Taxpayers Assets Project  have advocated for free access to  legal information . The  vendor neutral citation  movement in the  legal  field is working to ensure that courts will accept  citations  from cases on the web which do not have the traditional (copyrighted) page numbers from the  West Publishing  company.  There is a worldwide  Free Access to Law Movement  which advocates free access to legal information. The Wired Magazine Article  Who Owns The Law  is an introduction to the access to legal information issue. Postsecondary organizations such as K-12 work to share information. They feel it is a legal and moral obligation to provide access (including to people with disabilities or impairments) to information through the services and programs they offer. [ 3 ] Some effects of charging for information access, such as literature searches for physicians, is studied in the article \"Fee or Free: The Effect of Charging on Information Demand\". In this study, a $5 charge resulted in a 77% decrease in searches. [ 4 ] This technology-related article is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 5,
    "title": "Information architecture",
    "content": "Information architecture  ( IA ) is the structural design of shared  information  environments; the  art  and  science  of organizing and labelling  websites ,  intranets ,  online communities  and  software  to support usability and findability; and an emerging  community of practice  focused on bringing principles of  design ,  architecture  and  information science  to the digital landscape. [ 1 ]   Typically, it involves a  model  or  concept  of  information  that is used and applied to activities which require explicit details of complex  information systems . These activities include  library  systems and  database  development. Information architecture  has somewhat different meanings in different branches of  information systems  or  information technology : The difficulty in establishing a common definition for \"information architecture\" arises partly from the term's existence in multiple fields.  In the field of  systems design , for example, information architecture is a component of  enterprise architecture  that deals with the information component when describing the structure of an enterprise. While the definition of information architecture is relatively well-established in the field of systems design, it is much more debatable within the context of online information (i.e., websites). Andrew Dillon refers to the latter as the \"big IA–little IA debate\". [ 7 ]  In the little IA view, information architecture is essentially the application of  information science  to  web design  which considers, for example, issues of classification and information retrieval. In the big IA view, information architecture involves more than just the organization of a website; it also factors in  user experience , thereby considering  usability  issues of  information design ."
  },
  {
    "id": 6,
    "title": "Information behavior",
    "content": "Information behavior  is a field of  information science  research that seeks to understand the way people search for and use information [ 1 ]  in various contexts. It can include  information seeking  and  information retrieval , but it also aims to understand why people seek information and how they use it. The term 'information behavior' was coined by  Thomas D. Wilson  in 1982 [ 2 ]  and sparked controversy upon its introduction. [ 3 ]  The term has now been adopted and  Wilson's model of information behavior  is widely cited in information behavior literature. [ 4 ]  In 2000, Wilson defined information behavior as \"the totality of human behavior in relation to sources and channels of information\". [ 5 ] A variety of theories of information behavior seek to understand the processes that surround information seeking. [ 6 ]  An analysis of the most cited publications on information behavior during the early 21st century shows its theoretical nature. [ 7 ]  Information behavior research can employ various research methodologies grounded in broader research paradigms from psychology, sociology and education. [ 8 ] In 2003, a framework for information-seeking studies was introduced that aims to guide the production of clear, structured descriptions of research objects and positions information-seeking as a concept within information behavior. [ 9 ] Information need is a concept introduced by Wilson. Understanding the information need of an individual involved three elements: Information-seeking behavior is a more specific concept of information behavior. It specifically focuses on searching, finding, and retrieving information. Information-seeking behavior research can focus on improving information systems or, if it includes information need, can also focus on why the user behaves the way they do. A review study on information search behavior of users highlighted that behavioral factors, personal factors, product/service factors and situational factors affect information search behavior. [ 10 ]  Information-seeking behavior can be more or less explicit on the part of users: users might seek to solve some task or to establish some piece of knowledge which can be found in the data in question, [ 11 ]  or alternatively the search process itself is part of the objective of the user, in use cases for exploring visual content or for familiarising oneself with the content of an information service. [ 12 ]  In the general case, information-seeking needs to be understood and analysed as a session rather than as a one-off transaction with a search engine,  and in a broader context which includes user high-level intentions in addition to the immediate information need. [ 13 ] User studies vs. usage studies Introduced by  Elfreda Chatman  in 1987, [ 14 ]  information poverty is informed by the understanding that information is not equally accessible to all people. Information poverty does not describe a lack of information, but rather a worldview in which one's own experiences inside their own small world may create a distrust in the information provided by those outside their own lived experiences. [ 14 ] In Library and Information Science (LIS), a  metatheory  is described \"a set of assumptions that orient and direct theorizing about a given phenomenon\". [ 15 ]   Library and information science  researchers have adopted a number of different metatheories in their research. A common concern among LIS researchers, and a prominent discussion in the field, is the broad spectrum of theories that inform the study of information behavior, information users, or information use. This variation has been noted as a cause of concern because it makes individual studies difficult to compare or synthesize if they are not guided by the same theory. This sentiment has been expressed in studies of information behavior literature from the early 1980s  [ 16 ]  and more recent literature reviews have declared it necessary to refine their reviews to specific contexts or situations due to the sheer breadth of information behavior research available. [ 17 ] Below are descriptions of some, but not all, metatheories that have guided LIS research. A cognitive approach to understanding information behavior is grounded in psychology. It holds the assumption that a person's thinking influences how they seek, retrieve, and use information. Researchers that approach information behavior with the assumption that it is influenced by cognition, seek to understand what someone is thinking while they engage in information behavior and how those thoughts influence their behavior. [ 18 ] Wilson's attempt to understand information-seeking behavior by defining information need includes a cognitive approach. Wilson theorizes that information behavior is influenced by the cognitive need of an individual. By understanding the cognitive information need of an individual, we may gain insight into their information behavior. [ 2 ] Nigel Ford takes a cognitive approach to information-seeking, focusing on the intellectual processes of information-seeking. In 2004, Ford proposed an information-seeking model using a cognitive approach that focuses on how to improve information retrieval systems and serves to establish information-seeking and information behavior as concepts in and of themselves, rather than synonymous terms .   [ 19 ] The constructionist approach to information behavior has roots in the humanities and social sciences. It relies on  social constructionism , which assumes that a person's information behavior is influenced by their experiences in society. [ 18 ]  In order to understand information behavior, constructionist researchers must first understand the social discourse that surrounds the behavior. The most popular thinker referenced in constructionist information behavior research is  Michel Foucault , who famously rejected the concept of a universal human nature. The constructionist approach to information behavior research creates space for contextualizing the behavior based on the social experiences of the individual. One study that approaches information behavior research through the social constructionist approach is a study of the information behavior of a public library knitting group. [ 20 ]  The authors use a collectivist theory to frame their research, which denies the universality of information behavior and focuses on \"understanding the ways that discourse communities collectively construct information needs, seeking, sources, and uses\". [ 20 ] The constructivist approach is born out of education and sociology in which, \"individuals are seen as actively constructing an understanding of their worlds, heavily influenced by the social world(s) in which they are operating\". [ 18 ]  Constructivist approaches to information behavior research generally treat the individual's reality as constructed within their own mind rather than built by the society in which they live. [ 21 ] The constructivist metatheory makes space for the influence of society and culture with social constructivism, \"which argues that, while the mind constructs reality in its relationship to the world, this mental process is significantly informed by influences received from societal conventions, history and interaction with significant others\". [ 21 ] A common concern among LIS researchers, and a prominent discussion in the field, is the broad spectrum of theories that inform LIS research. This variation has been noted as a cause of concern because it makes individual studies difficult to compare if they are not guided by the same theory. Recent studies have shown that the impact of these theories and theoretical models is very limited. [ 22 ]  LIS researchers have applied concepts and theories from many disciplines, including sociology, psychology, communication, organizational behavior, and computer science. [ 23 ] [ 24 ] The term was coined by  Thomas D. Wilson  in his 1981 paper, on the grounds that the current term, 'information needs' was unhelpful since 'need' could not be directly observed, while how people behaved in seeking information could be observed and investigated. [ 2 ]  However, there is increasing work in the information-searching field that is relating behaviors to underlying needs. [ 25 ]  In 2000,  Wilson  described information behavior as the totality of human behavior in relation to sources and channels of information, including both active and passive information-seeking, and information use. [ 5 ]  He described information-seeking behavior as purposive seeking of information as a consequence of a need to satisfy some goal.  Information-seeking behavior is the micro-level of behavior employed by the searcher in interacting with information systems of all kinds, be it between the seeker and the system, or the pure method of creating and following up on a search. Thomas Wilson proposed that information behavior covers all aspects of human information behavior, whether active or passive. Information-s eeking  behavior is the act of actively seeking information in order to answer a specific query. Information-s earching  behavior is the behavior which stems from the searcher interacting with the system in question. Information  use  behavior pertains to the searcher adopting the  knowledge  they sought. Elfreda Chatman  developed the theory of life in the round, which she defines as a world of tolerated approximation. It acknowledges reality at its most routine, predictable enough that unless an initial problem should arise, there is no point in seeking information. [ 26 ]  Chatman examined this principle within a small world: a world which imposes on its participants similar concerns and awareness of who is important; which ideas are relevant and whom to trust. Participants in this world are considered insiders. [ 26 ]  Chatman focused her study on women at a maximum security prison. She learned that over time, prisoner's private views were assimilated to a communal acceptance of life in the round: a small world perceived in accordance with agreed upon standards and communal perspective. Members who live in the round will not cross the boundaries of their world to seek information unless it is critical; there is a collective expectation that information is relevant; or life lived in the round no longer functions. The world outside prison has secondary importance to inmates who are absent from this reality which is changing with time. [ 26 ] This compares the internet search methods of experienced information seekers (navigators) and inexperienced information seekers (explorers). Navigators revisit domains; follow sequential searches and have few deviations or regressions within their search patterns and interactions. Explorers visit many domains; submit many questions and their search trails branch frequently. [ 27 ] Brenda Dervin  developed the concept of sensemaking.  Sensemaking considers how we (attempt to) make sense of uncertain situations. [ 28 ]   Her description of Sensemaking consisted of the definition of how we interpret information to use for our own information related decisions. Brenda Dervin described sensemaking as a method through which people make sense of their worlds in their own language. ASK was also developed by Nicholas J. Belkin. An anomalous state of knowledge is one in which the searcher recognises a gap in the state of knowledge. This, his or her further hypothesis, is influential in studying why people start to search. [ 29 ] McKenzie's model proposes that the information-seeking in everyday life of individuals occurs on a \"continuum of information practices... from actively seeking out a known source... to being given un-asked for advice.\" [ 30 ]  This model crosses the threshold in information-seeking studies from information behavior research to information practices research. Information practices research creates space for understanding encounters with information that may not be a result of the individual's behavior. [ citation needed ] McKenzie's two-dimensional model includes four modes of information practices (active seeking, active scanning, non-directed monitoring, by proxy) over two phases of the information process (connecting and interacting). [ 30 ] Mode (below) In  library and information science ,  Information search process  (ISP) is a model proposed by  Carol Kuhlthau  in 1991 that represents a tighter focus on information-seeking behavior. Kuhlthau's framework was based on research into high school students, [ 31 ]  but extended over time to include a diverse range of people, including those in the workplace. It examined the role of emotions, specifically uncertainty, in the information-seeking process, concluding that many searches are abandoned due to an overwhelmingly high level of uncertainty. [ 32 ] [ 33 ] [ 34 ] ISP is a 6-stage process, with each stage each encompassing 4 aspects: [ 35 ] [ 40 ] Kuhlthau's work is  constructivist  and explores information-seeking beyond the user's cognitive experience into their emotional experience while seeking information. She finds that the process of information-searching begins with feelings of uncertainty, navigates through feelings of anxiety, confusion, or doubt, and ultimately completes their information-seeking with feelings of relief or satisfaction, or disappointment. The consideration of an information-seeker's affect has been replicated more recently in Keilty and Leazer's study which focuses on physical affect and esthetics instead of emotional affect. [ 41 ] The usefulness of the model has been re-evaluated in 2008. [ 42 ] David Ellis investigated the behavior of researchers in the physical and social sciences, [ 43 ]   and engineers and research scientists [ 44 ]   through  semi-structured interviews  using a  grounded theory  approach, with a focus on describing the activities associated with information seeking rather than describing a process. Ellis' initial investigations produced six key activities within the information-seeking process: Later studies by Ellis (focusing on academic researchers in other disciplines) resulted in the addition of two more activities [ citation needed ] : Choo, Detlor and Turnbull elaborated on Ellis' model by applying it to information-searching on the web. Choo identified the key activities associated with Ellis in online searching episodes and connected them with four types of searching (undirected viewing, conditioned viewing, information search, and formal search). [ 45 ] Developed by  Stuart Card ,  Ed H. Chi  and  Peter Pirolli , this model is derived from anthropological theories and is comparable to foraging for food. Information seekers use clues (or information scents) such as links, summaries and images to estimate how close they are to target information. A scent must be obvious as users often browse aimlessly or look for specific information. Information foraging is descriptive of why people search in particular ways rather than how they search. [ 46 ] Foster and Urquhart provide a rich understanding of their model for nonlinear information behavior. This model takes into consideration varying contexts and personalities when researching information behavior. The authors of this article are themselves cautious of this new model since it still requires more development . [ 47 ] Reijo Savolainen published his ELIS model in 1995. It is based on three basic concepts: way of life, life domain and information search in everyday life (ELIS). [ 48 ]"
  },
  {
    "id": 7,
    "title": "Information management",
    "content": "Information management  ( IM ) is the appropriate and optimized capture, storage, retrieval, and use of  information . It may be  personal information management  or organizational. Information management for  organizations  concerns a cycle of organizational activity: the acquisition of information from one or more sources, the custodianship and the distribution of that information to those who need it, and its ultimate disposal through  archiving  or deletion and extraction. This cycle of information organisation involves a variety of  stakeholders , including those who are responsible for assuring the  quality ,  accessibility  and  utility  of acquired information; those who are responsible for its safe  storage  and  disposal ; and those who need it for  decision making . Stakeholders might have rights to originate, change, distribute or delete information according to organisational information management  policies . Information management embraces all the generic concepts of management, including the  planning ,  organizing , structuring,  processing ,  controlling ,  evaluation  and  reporting  of information activities, all of which is needed in order to meet the needs of those with organisational roles or functions that depend on information. These generic concepts allow the information to be presented to the audience or the correct group of people. After individuals are able to put that information to use, it then gains more value. Information management is closely related to, and overlaps with, the management of  data ,  systems ,  technology ,  processes  and – where the availability of information is critical to organisational success –  strategy . This broad view of the realm of information management contrasts with the earlier, more traditional view, that the  life cycle  of managing information is an operational matter that requires specific procedures, organisational capabilities and standards that deal with information as a  product  or a service. In the 1970s, the management of information largely concerned matters closer to what would now be called  data management :  punched cards ,  magnetic tapes  and other record-keeping  media , involving a life cycle of such formats requiring origination, distribution, backup, maintenance and disposal. At this time the huge potential of  information technology  began to be recognised: for example a single  chip  storing a whole  book , or  electronic mail  moving messages instantly around the world, remarkable ideas at the time. [ 1 ]  With the proliferation of information technology and the extending reach of information systems in the 1980s and 1990s, [ 2 ]  information management took on a new form. Progressive businesses such as  BP  transformed the vocabulary of what was then \" IT management \", so that \" systems analysts \" became \" business analysts \", \"monopoly supply\" became a mixture of \" insourcing \" and \" outsourcing \", and the large IT function was transformed into \"lean teams\" that began to allow some agility in the processes that harness information for  business benefit . [ 3 ]  The scope of  senior management  interest in information at BP extended from the creation of value through improved  business processes , based upon the effective management of information, permitting the implementation of appropriate information systems (or \" applications \") that were operated on  IT infrastructure  that was outsourced. [ 3 ]  In this way, information management was no longer a simple job that could be performed by anyone who had nothing else to do, it became highly strategic and a matter for  senior management  attention. An understanding of the technologies involved, an ability to manage information systems  projects  and  business change  well, and a willingness to align technology and business strategies all became necessary. [ 4 ] In the transitional period leading up to the strategic view of information management, Venkatraman, a strong advocate of this transition and transformation, [ 5 ]  proffered a simple arrangement of ideas that succinctly brought together the  management of data , information, and  knowledge  (see the figure) argued that: This is often referred to as the DIKAR model: Data, Information, Knowledge, Action and Result, [ 6 ]  it gives a strong clue as to the layers involved in aligning technology and organisational strategies, and it can be seen as a pivotal moment in changing attitudes to information management.  The recognition that information management is an investment that must deliver meaningful results is important to all modern organisations that depend on information and good decision-making for their success. [ 7 ] It is commonly believed that good information management is crucial to the smooth working of organisations, and although there is no commonly accepted  theory  of information management  per se , behavioural and organisational theories help. Following the  behavioural science  theory of management, mainly developed at  Carnegie Mellon University  and prominently supported by March and Simon, [ 8 ]  most of what goes on in modern organizations is actually information handling and decision making. One crucial factor in information handling and decision making is an individual's ability to process information and to make decisions under limitations that might derive from the context: a person's age, the situational complexity, or a lack of requisite quality in the information that is at hand – all of which is exacerbated by the rapid advance of technology and the new kinds of  system  that it enables, especially as the  social web  emerges as a phenomenon that business cannot ignore. And yet, well before there was any general recognition of the importance of information management in organisations, March and Simon  [ 8 ]  argued that organizations have to be considered as  cooperative systems , with a high level of information processing and a vast need for decision making at various levels. Instead of using the model of the \" economic man \", as advocated in classical theory  [ 9 ]  they proposed \" administrative man \" as an alternative, based on their argumentation about the cognitive limits of rationality. Additionally they proposed the notion of  satisficing , which entails searching through the available alternatives until an acceptability threshold is met - another idea that still has currency. [ 10 ] In addition to the organisational factors mentioned by March and Simon, there are other issues that stem from economic and environmental dynamics. There is the cost of collecting and evaluating the information needed to take a decision, including the time and effort required. [ 11 ]  The  transaction cost  associated with information processes can be high. In particular, established organizational rules and procedures can prevent the taking of the most appropriate decision, leading to sub-optimum outcomes. [ 12 ] [ 13 ]  This is an issue that has been presented as a major problem with bureaucratic organizations that lose the economies of strategic change because of entrenched attitudes. [ 14 ] According to the Carnegie Mellon School an organization's ability to process information is at the core of organizational and managerial  competency , and an organization's strategies must be designed to improve information processing capability  [ 15 ]  and as information systems that provide that capability became formalised and automated, competencies were severely tested at many levels. [ 16 ]  It was recognised that organisations needed to be able to learn and adapt in ways that were never so evident before  [ 17 ]  and academics began to organise and publish definitive works concerning the strategic management of information, and information systems. [ 4 ] [ 18 ]  Concurrently, the ideas of  business process  management [ 19 ]  and  knowledge management [ 20 ]  although much of the optimistic early thinking about  business process redesign  has since been discredited in the information management literature. [ 21 ]  In the strategic studies field, it is considered of the highest priority the understanding of the information environment, conceived as the aggregate of individuals, organizations, and systems that collect, process, disseminate, or act on information. This environment consists of three interrelated dimensions which continuously interact with individuals, organizations, and systems. These dimensions are the physical, informational, and cognitive. [ 22 ] Venkatraman has provided a simple view of the requisite capabilities of an organisation that wants to manage information well – the  DIKAR  model (see above). He also worked with others to understand how technology and business strategies could be appropriately aligned in order to identify specific capabilities that are needed. [ 23 ]  This work was paralleled by other writers in the world of consulting, [ 24 ]  practice, [ 25 ]  and academia. [ 26 ] Bytheway has collected and organised basic tools and techniques for information management in a single volume. [ 7 ]   At the heart of his view of information management is a portfolio model that takes account of the surging interest in external sources of information and the need to organise un-structured information external so as to make it useful (see the figure). Such an information portfolio as this shows how information can be gathered and usefully organised, in four stages: Stage 1 : Taking advantage of  public information : recognise and adopt well-structured external schemes of reference data, such as post codes, weather data, GPS positioning data and travel timetables, exemplified in the personal computing press. [ 27 ] Stage 2 : Tagging the noise on the  World Wide Web : use existing schemes such as  post codes  and  GPS  data or more typically by adding “tags”, or construct a formal  ontology  that provides structure. Shirky provides an overview of these two approaches. [ 28 ] Stage 3 : Sifting and analysing: in the wider world the generalised ontologies that are under development extend to hundreds of entities and hundreds of relations between them and provide the means to elicit meaning from large volumes of data. Structured data in databases works best when that structure reflects a higher-level information model – an ontology, or an  entity-relationship model . [ 29 ] Stage 4 : Structuring and archiving: with the large volume of data available from sources such as the  social web  and from the miniature  telemetry  systems used in personal  health management , new ways to archive and then trawl data for meaningful information.  Map-reduce  methods, originating from  functional programming , are a more recent way of eliciting information from large archival  datasets  that is becoming interesting to regular businesses that have very large data resources to work with, but it requires advanced multi-processor resources. [ 30 ] In 2004, the management system \" Information Management Body of Knowledge \" was first published on the World Wide Web [ 31 ]  \nand set out to show that the required management competencies to derive real benefits from an investment in information are complex and multi-layered. The framework model that is the basis for understanding competencies comprises six \"knowledge\" areas and four \"process\" areas: The IMBOK is based on the argument that there are six areas of required management competency, two of which (\"business process management\" and \"business information management\") are very closely related. [ 32 ] Even with full capability and competency within the six knowledge areas, it is argued that things can still go wrong. The problem lies in the migration of ideas and information management value from one area of competency to another. Summarising what Bytheway explains in some detail (and supported by selected secondary references): [ 37 ] There are always many ways to see a business, and the information management viewpoint is only one way. Other areas of business activity will also contribute to strategy – it is not only good information management that moves a business forwards.  Corporate governance ,  human resource management ,  product development  and  marketing  will all have an important role to play in strategic ways, and we must not see one domain of activity alone as the sole source of strategic success. On the other hand, corporate governance, human resource management, product development and marketing are all dependent on effective information management, and so in the final analysis our competency to manage information well, on the broad basis that is offered here, can be said to be predominant. Organizations are often confronted with many information management challenges and issues at the  operational level , especially when  organisational change  is engendered. The novelty of new  systems architectures  and a lack of experience with new styles of information management requires a level of organisational  change management  that is notoriously difficult to deliver. As a result of a general organisational reluctance to change, to enable new forms of information management, there might be (for example): a shortfall in the requisite resources, a failure to acknowledge new classes of information and the new procedures that use them, a lack of support from senior management leading to a loss of strategic vision, and even political manoeuvring that undermines the operation of the whole organisation. [ 41 ]  However, the implementation of new forms of information management should normally lead to operational benefits. In early work, taking an information processing view of organisation design, Jay  Galbraith  has identified five tactical areas to increase information processing capacity and reduce the need for information processing. [ 42 ] The lateral relations concept leads to an organizational form that is different from the simple hierarchy, the \" matrix organization \". This brings together the vertical (hierarchical) view of an organisation and the horizontal (product or project) view of the work that it does visible to the outside world. The creation of a matrix organization is one management response to a persistent fluidity of external demand, avoiding multifarious and spurious responses to episodic demands that tend to be dealt with individually."
  },
  {
    "id": 8,
    "title": "Information seeking",
    "content": "Information seeking  is the process or activity of attempting to obtain information in both human and technological contexts. Information seeking is related to, but different from,  information retrieval  (IR). Traditionally, IR tools have been designed for IR professionals to enable them to effectively and efficiently retrieve information from a source. It is assumed that the information exists in the source and that a well-formed query will retrieve it (and nothing else). It has been argued that  laypersons'  information seeking on the internet is very different from information retrieval as performed within the IR discourse. Yet, internet search engines are built on IR principles. Since the late 1990s a body of research on how casual users interact with internet search engines has been forming, but the topic is far from fully understood. IR can be said to be technology-oriented, focusing on  algorithms  and issues such as  precision  and  recall . Information seeking may be understood as a more human-oriented and open-ended process than information retrieval. In information seeking, one does not know whether there exists an answer to one's query, so the process of seeking may provide the learning required to satisfy one's  information need . Much library and information science (LIS) research has focused on the information-seeking practices of practitioners within various fields of professional work. Studies have been carried out into the information-seeking behaviors of librarians, [ 1 ]  academics, [ 2 ]  medical professionals, [ 3 ]  engineers, [ 4 ]  lawyers [ 5 ] [ 6 ]  and mini-publics [ 7 ] (among others). Much of this research has drawn on the work done by Leckie, Pettigrew (now Fisher) and Sylvain, who in 1996 conducted an extensive review of the LIS literature (as well as the literature of other academic fields) on professionals' information seeking. The authors proposed an analytic model of professionals' information seeking behaviour, intended to be generalizable across the professions, thus providing a platform for future research in the area. The model was intended to \"prompt new insights... and give rise to more refined and applicable theories of information seeking\" (1996, p. 188). The model has been adapted by Wilkinson (2001) who proposes a model of the information seeking of lawyers. Recent studies in this topic address the concept of information-gathering that \"provides a broader perspective that adheres better to professionals' work-related reality and desired skills.\" [ 8 ]  (Solomon & Bronstein, 2021). A variety of theories of information behavior – e.g.  Zipf 's  Principle of Least Effort ,  Brenda Dervin 's Sense Making,  Elfreda Chatman 's Life in the Round – seek to understand the processes that surround information seeking. In addition, many theories from other disciplines have been applied in investigating an aspect or whole process of information seeking behavior. [ 9 ] [ 10 ] A review of the literature on information seeking behavior shows that information seeking has generally been accepted as dynamic and non-linear (Foster, 2005; Kuhlthau 2006). People experience the information search process as an interplay of thoughts, feelings and actions ( Kuhlthau, 2006 ). Donald O. Case (2007) also wrote a good book that is a review of the literature. Information seeking has been found to be linked to a variety of interpersonal communication behaviors beyond question-asking, to include strategies such as candidate answers. Robinson's (2010) [ 11 ]  research suggests that when seeking information at work, people rely on both other people and information repositories (e.g., documents and databases), and spend similar amounts of time consulting each (7.8% and 6.4% of work time, respectively; 14.2% in total). However, the distribution of time among the constituent information seeking stages differs depending on the source. When consulting other people, people spend less time locating the information source and information within that source, similar time understanding the information, and more time problem solving and decision making, than when consulting information repositories. Furthermore, the research found that people spend substantially more time receiving information passively (i.e., information that they have not requested) than actively (i.e., information that they have requested), and this pattern is also reflected when they provide others with information. The concepts of information seeking, information retrieval, and information behaviour are objects of investigation of  information science . Within this scientific discipline a variety of studies has been undertaken analyzing the interaction of an individual with  information sources  in case of a specific  information need , task, and context. The research models developed in these studies vary in their level of scope.  Wilson  (1999) therefore developed a nested model of conceptual areas, which visualizes the interrelation of the here mentioned central concepts. Wilson defines models of information behavior to be \"statements, often in the form of diagrams, that attempt to describe an information-seeking activity, the causes and consequences of that activity, or the relationships among stages in information-seeking behaviour\" (1999: 250)."
  },
  {
    "id": 9,
    "title": "Information society",
    "content": "An  information society  is a  society  or  subculture  where the usage,  creation ,  distribution , manipulation and  integration  of  information  is a significant activity. [ 1 ]  Its main drivers are  information and communication technologies , which have resulted in rapid growth of a variety of forms of information. Proponents of this theory posit that these technologies are impacting most important forms of social organization, including  education ,  economy , [ 2 ]   health ,  government , [ 3 ]   warfare , and levels of  democracy . [ 4 ]  The people who are able to partake in this form of society are sometimes called either  computer users  or even  digital citizens , defined by K. Mossberger as “Those who use the Internet regularly and effectively”. This is one of many dozen internet terms that have been identified to suggest that humans are entering a new and different phase of society. [ 5 ] Some of the markers of this steady change may be technological, economic, occupational, spatial, cultural, or a combination of all of these. [ 6 ] \nInformation society is seen as a successor to  industrial society . Closely related concepts are the  post-industrial society  ( post-fordism ),  post-modern  society, computer society and  knowledge society , telematic society,   society of the spectacle  ( postmodernism ),  Information Revolution  and  Information Age ,  network society  ( Manuel Castells ) or even  liquid modernity . There is currently no universally accepted concept of what exactly can be defined as an information society and what shall not be included in the term. Most theoreticians agree that a transformation can be seen as started somewhere between the 1970s, the early 1990s transformations of the Socialist East [ clarification needed ]  and the 2000s period that formed most of today's net principles and currently as is changing the way societies work fundamentally. Information technology goes beyond the  internet , as the principles of internet design and usage influence other areas, and there are discussions about how big the influence of specific media or specific modes of production really is.  Frank Webster  notes five major types of information that can be used to define information society: technological, economic, occupational, spatial and cultural. [ 6 ]  According to Webster, the character of information has transformed the way that we live today. How we conduct ourselves centers around theoretical knowledge and information. [ 7 ] Kasiwulaya and Gomo (Makerere University) allude [ where? ] [ dubious  –  discuss ]  that information societies are those that have intensified their use of IT for economic, social, cultural and political transformation. In 2005, governments reaffirmed their dedication to the foundations of the Information\nSociety in the  Tunis Commitment  and outlined the basis for implementation and follow-up in the Tunis Agenda for the Information Society. In particular, the Tunis Agenda addresses the issues of financing of ICTs for development and Internet governance that could not be resolved in the first phase. Some people, such as  Antonio Negri , characterize the information society as one in which people do immaterial labour. [ 8 ]  By this, they appear to refer to the production of knowledge or cultural artifacts. One problem with this model is that it ignores the material and essentially industrial basis of the society. However it does point to a problem for workers, namely how many creative people does this society need to function?  For example, it may be that you only need a few star performers, rather than a plethora of non-celebrities, as the work of those performers can be easily distributed, forcing all secondary players to the bottom of the market. It  is  now common for publishers to promote only their best selling authors and to try to avoid the rest—even if they still sell steadily. Films are becoming more and more judged, in terms of distribution, by their first weekend's performance, in many cases cutting out opportunity for word-of-mouth development. Michael Buckland  characterizes information in society in his book  Information and Society.  Buckland expresses the idea that information can be interpreted differently from person to person based on that individual's experiences. [ 9 ] Considering that metaphors and technologies of information move forward in a reciprocal relationship, we can describe some societies (especially the  Japanese society ) as an information society because we think of it as such. [ 10 ] \n [ 11 ] The word information may be interpreted in many different ways. According to Buckland in  Information and Society , most of the meanings fall into three categories of human knowledge: information as knowledge, information as a process, and information as a thing. [ 12 ] Thus, the Information Society refers to the social importance given to communication and information in today's society, where social, economic and cultural relations are involved. [ 13 ] In the Information Society, the process of capturing, processing and communicating information is the main element that characterizes it. Thus, in this type of society, the vast majority of it will be dedicated to the provision of services and said services will consist of the processing, distribution or use of information. [ 13 ] The growth of the amount of technologically mediated information has been quantified in different ways, including society's technological capacity to store information, to communicate information, and to compute information. [ 16 ]  It is estimated that, the world's technological capacity to store information grew from 2.6 (optimally compressed)  exabytes  in 1986, which is the informational equivalent to less than one 730-MB  CD-ROM  per person in 1986 (539 MB per person), to 295 (optimally compressed)  exabytes  in 2007. [ 17 ]  This is the informational equivalent of 60  CD-ROM  per person in 2007 [ 18 ]  and represents a sustained annual growth rate of some 25%. The world's combined technological capacity to receive information through one-way  broadcast  networks was the informational equivalent of 174 newspapers per person per day in 2007. [ 17 ] The world's combined effective capacity to exchange information through two-way  telecommunications networks  was 281  petabytes  of (optimally compressed) information in 1986, 471  petabytes  in 1993, 2.2 (optimally compressed)  exabytes  in 2000, and 65 (optimally compressed)  exabytes  in 2007, which is the informational equivalent of 6 newspapers per person per day in 2007. [ 18 ]  The world's technological capacity to compute information with humanly guided general-purpose computers grew from 3.0 × 10^8 MIPS in 1986, to 6.4 x 10^12 MIPS in 2007, experiencing the fastest growth rate of over 60% per year during the last two decades. [ 17 ] James R. Beniger  describes the necessity of information in modern society in the following way: “The need for sharply increased control that resulted from the industrialization of material processes through application of inanimate sources of energy probably accounts for the rapid development of automatic feedback technology in the early industrial period (1740-1830)” (p. 174)\n“Even with enhanced feedback control, industry could not have developed without the enhanced means to process matter and energy, not only as inputs of the raw materials of production but also as outputs distributed to final consumption.”(p. 175) [ 5 ] One of the first people to develop the concept of the information society was the economist  Fritz Machlup . In 1933, Fritz Machlup began studying the effect of patents on research. His work culminated in the study  The production and distribution of knowledge in the United States  in 1962. This book was widely regarded [ 19 ]  and was eventually translated into  Russian  and  Japanese . The Japanese have also studied the information society (or  jōhōka shakai ,  情報化社会 ). The issue of technologies and their role in contemporary society have been discussed in the scientific literature using a range of labels and concepts. This section introduces some of them. Ideas of a knowledge or  information economy ,  post-industrial society ,  postmodern  society,  network society , the  information revolution , informational capitalism, network capitalism, and the like, have been debated over the last several decades. Fritz Machlup (1962) introduced the concept of the  knowledge industry . He began studying the effects of patents on research before distinguishing five sectors of the knowledge sector: education, research and development, mass media, information technologies, information services. Based on this categorization he calculated that in 1959 29% per cent of the GNP in the USA had been produced in knowledge industries. [ 20 ] [ 21 ] [ citation needed ] Peter Drucker  has argued that there is a transition from an economy based on material goods to one based on knowledge. [ 22 ]   Marc Porat  distinguishes a primary (information goods and services that are directly used in the production, distribution or processing of information) and a secondary sector (information services produced for internal consumption by government and non-information firms) of the information economy. [ 23 ] Porat uses the total value added by the primary and secondary information sector to the GNP as an indicator for the information economy. The  OECD  has employed Porat's definition for calculating the share of the information economy in the total economy (e.g. OECD 1981, 1986). Based on such indicators, the information society has been defined as a society where more than half of the GNP is produced and more than half of the employees are active in the information economy. [ 24 ] For  Daniel Bell  the number of employees producing services and information is an indicator for the informational character of a society. \"A post-industrial society is based on services. (…) What counts is not raw muscle power, or energy, but information. (…) A post industrial society is one in which the majority of those employed are not involved in the production of tangible goods\". [ 25 ] Alain Touraine  already spoke in 1971 of the post-industrial society. \"The passage to postindustrial society takes place when investment results in the production of symbolic goods that modify values, needs, representations, far more than in the production of material goods or even of 'services'. Industrial society had transformed the means of production: post-industrial society changes the ends of production, that is, culture. (…) The decisive point here is that in postindustrial society all of the economic system is the object of intervention of society upon itself. That is why we can call it the programmed society, because this phrase captures its capacity to create models of management, production, organization, distribution, and consumption, so that such a society appears, at all its functional levels, as the product of an action exercised by the society itself, and not as the outcome of natural laws or cultural specificities\" (Touraine 1988: 104). In the programmed society also the area of cultural reproduction including aspects such as information, consumption, health, research, education would be industrialized. That modern society is increasing its capacity to act upon itself means for Touraine that society is reinvesting ever larger parts of production and so produces and transforms itself. This makes Touraine's concept substantially different from that of Daniel Bell who focused on the capacity to process and generate information for efficient society functioning. Jean-François Lyotard [ 26 ]  has argued that \"knowledge has become the principle [ sic ] force of production over the last few decades\". Knowledge would be transformed into a commodity. Lyotard says that postindustrial society makes knowledge accessible to the layman because knowledge and information technologies would diffuse into society and break up Grand Narratives of centralized structures and groups. Lyotard denotes these changing circumstances as postmodern condition or postmodern society. Similarly to Bell, Peter Otto and Philipp Sonntag (1985) say that an information society is a society where the majority of employees work in information jobs, i.e. they have to deal more with information, signals, symbols, and images than with energy and matter.  Radovan Richta  (1977) argues that society has been transformed into a scientific civilization based on services, education, and creative activities. This transformation would be the result of a scientific-technological transformation based on technological progress and the increasing importance of computer technology. Science and technology would become immediate forces of production (Aristovnik 2014: 55). Nico Stehr  (1994, 2002a, b) says that in the knowledge society a majority of jobs involves working with knowledge. \"Contemporary society may be described as a knowledge society based on the extensive penetration of all its spheres of life and institutions by scientific and technological knowledge\" (Stehr 2002b: 18). For Stehr, knowledge is a capacity for social action. Science would become an immediate productive force, knowledge would no longer be primarily embodied in machines, but already appropriated nature that represents knowledge would be rearranged according to certain designs and programs (Ibid.: 41-46). For Stehr, the economy of a knowledge society is largely driven not by material inputs, but by symbolic or knowledge-based inputs (Ibid.: 67), there would be a large number of professions that involve working with knowledge, and a declining number of jobs that demand low cognitive skills as well as in manufacturing (Stehr 2002a). Also  Alvin Toffler  argues that knowledge is the central resource in the economy of the information society: \"In a Third Wave economy, the central resource – a single word broadly encompassing data, information, images, symbols, culture, ideology, and values –  is actionable knowledge\" (Dyson/Gilder/Keyworth/Toffler 1994). At the end of the twentieth century, the concept of the  network society  gained importance in information society theory. For  Manuel Castells , network logic is besides information, pervasiveness, flexibility, and convergence a central feature of the information technology paradigm (2000a: 69ff). \"One of the key features of informational society is the networking logic of its basic structure, which explains the use of the concept of 'network society'\" (Castells 2000: 21). \"As an historical trend, dominant functions and processes in the Information Age are increasingly organized around networks. Networks constitute the new social morphology of our societies, and the diffusion of networking logic substantially modifies the operation and outcomes in processes of production, experience, power, and culture\" (Castells 2000: 500). For Castells the network society is the result of informationalism, a new technological paradigm. Jan Van Dijk  (2006) defines the network society as a \"social formation with an infrastructure of social and media networks enabling its prime mode of organization at all levels (individual, group/organizational and societal). Increasingly, these networks link all units or parts of this formation (individuals, groups and organizations)\" (Van Dijk 2006: 20). For Van Dijk networks have become the nervous system of society, whereas Castells links the concept of the network society to capitalist transformation, Van Dijk sees it as the logical result of the increasing widening and thickening of networks in nature and society.  Darin Barney  uses the term for characterizing societies that exhibit two fundamental characteristics: \"The first is the presence in those societies of sophisticated – almost exclusively digital – technologies of networked communication and information management/distribution, technologies which form the basic infrastructure mediating an increasing array of social, political and economic practices. (…) The second, arguably more intriguing, characteristic of network societies is the reproduction and institutionalization throughout (and between) those societies of networks as the basic form of human organization and relationship across a wide range of social, political and economic configurations and associations\". [ 27 ] The major critique of concepts such as information society, postmodern society, knowledge society, network society,  postindustrial society, etc. that has mainly been voiced by critical scholars is that they create the impression that we have entered a completely new type of society. \"If there is just more information then it is hard to understand why anyone should suggest that we have before us something radically new\" (Webster 2002a: 259). Critics such as  Frank Webster  argue that these approaches stress discontinuity, as if contemporary society had nothing in common with society as it was 100 or 150 years ago. Such assumptions would have ideological character because they would fit with the view that we can do nothing about change and have to adapt to existing political realities (kasiwulaya 2002b: 267). These critics argue that contemporary society first of all is still a capitalist society oriented towards accumulating economic, political, and  cultural capital . They acknowledge that information society theories stress some important new qualities of society (notably globalization and informatization), but charge that they fail to show that these are attributes of overall capitalist structures. Critics such as Webster insist on the continuities that characterise change. In this way Webster distinguishes between different epochs of capitalism: laissez-faire capitalism of the 19th century,  corporate capitalism  in the 20th century, and informational capitalism for the 21st century (kasiwulaya 2006). For describing contemporary society based on a new dialectic of continuity and discontinuity, other critical scholars have suggested several terms like: Other scholars prefer to speak of information capitalism (Morris-Suzuki 1997) or informational capitalism ( Manuel Castells  2000,  Christian Fuchs  2005, Schmiede 2006a, b). Manuel Castells sees informationalism as a new technological paradigm (he speaks of a mode of development) characterized by \"information generation, processing, and transmission\" that have become \"the fundamental sources of productivity and power\" (Castells 2000: 21). The \"most decisive historical factor accelerating, channelling and shaping the information technology paradigm, and inducing its associated social forms, was/is the process of capitalist restructuring undertaken since the 1980s, so that the new techno-economic system can be adequately characterized as informational capitalism\" (Castells 2000: 18). Castells has added to theories of the information society the idea that in contemporary society dominant functions and processes are increasingly organized around networks that constitute the new social morphology of society (Castells 2000: 500).  Nicholas Garnham [ 30 ]  is critical of Castells and argues that the latter's account is technologically determinist because Castells points out that his approach is based on a dialectic of technology and society in which technology embodies society and society uses technology (Castells 2000: 5sqq). But Castells also makes clear that the rise of a new \"mode of development\" is shaped by capitalist production, i.e. by society, which implies that technology isn't the only driving force of society. Antonio Negri  and  Michael Hardt  argue that contemporary society is an Empire that is characterized by a singular global logic of capitalist domination that is based on immaterial labour. With the concept of immaterial labour Negri and Hardt introduce ideas of information society discourse into their Marxist account of contemporary capitalism. Immaterial labour would be labour \"that creates immaterial products, such as knowledge, information, communication, a relationship, or an emotional response\" (Hardt/Negri 2005: 108; cf. also 2000: 280-303), or services, cultural products, knowledge (Hardt/Negri 2000: 290). There would be two forms: intellectual labour that produces ideas, symbols, codes, texts, linguistic figures, images, etc.; and  affective labour  that produces and manipulates affects such as a feeling of ease, well-being, satisfaction, excitement, passion, joy, sadness, etc. (Ibid.). Overall, neo-Marxist accounts of the information society have in common that they stress that knowledge, information technologies, and computer networks have played a role in the restructuration and globalization of capitalism and the emergence of a flexible regime of accumulation ( David Harvey  1989). They warn that new technologies are embedded into societal antagonisms that cause  structural unemployment , rising poverty,  social exclusion , the  deregulation  of the  welfare state  and of  labour rights , the lowering of wages, welfare, etc. Concepts such as knowledge society, information society, network society, informational capitalism, postindustrial society, transnational network capitalism, postmodern society, etc. show that there is a vivid discussion in contemporary sociology on the character of contemporary society and the role that technologies, information, communication, and co-operation play in it. [ citation needed ]  Information society theory discusses the role of information and information technology in society, the question which key concepts shall be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology. Information society is the means of sending and receiving information from one place to another. [ 31 ]  As technology has advanced so too has the way people have adapted in sharing information with each other. \"Second nature\" refers a group of experiences that get made over by culture. [ 32 ]  They then get remade into something else that can then take on a new meaning. As a society we transform this process so it becomes something natural to us, i.e. second nature. So, by following a particular pattern created by culture we are able to recognise how we use and move information in different ways. From sharing information via different time zones (such as talking online) to information ending up in a different location (sending a letter overseas) this has all become a habitual process that we as a society take for granted. [ 33 ] However, through the process of sharing information vectors have enabled us to spread information even further. Through the use of these vectors information is able to move and then separate from the initial things that enabled them to move. [ 34 ]  From here, something called \"third nature\" has developed. An extension of second nature, third nature is in control of second nature. It expands on what second nature is limited by. It has the ability to mould information in new and different ways. So, third nature is able to ‘speed up, proliferate, divide, mutate, and beam in on us from elsewhere. [ 35 ]  It aims to create a balance between the boundaries of space and time (see second nature). This can be seen through the telegraph, it was the first successful technology that could send and receive information faster than a human being could move an object. [ 36 ]  As a result different vectors of people have the ability to not only shape culture but create new possibilities that will ultimately shape society. Therefore, through the use of second nature and third nature society is able to use and explore new vectors of possibility where information can be moulded to create new forms of interaction. [ 37 ] In  sociology ,  informational society  refers to a  post-modern  type of society. Theoreticians like  Ulrich Beck ,  Anthony Giddens  and  Manuel Castells  argue that since the 1970s a transformation from  industrial society  to informational society has happened on a global scale. [ 39 ] As  steam power  was the technology standing behind industrial society, so  information technology  is seen as the catalyst for the changes in work organisation, societal structure and politics occurring in the late 20th century. In the book  Future Shock ,  Alvin Toffler  used the phrase  super-industrial society  to describe this type of society.  Other writers and thinkers have used terms like \" post-industrial society \" and \"post-modern industrial society\" with a similar meaning. A number of terms in current use emphasize related but different aspects of the emerging global economic order. The Information Society intends to be the most encompassing in that an economy is a subset of a society. The  Information Age  is somewhat limiting, in that it refers to a 30-year period between the widespread use of computers and the  knowledge economy , rather than an emerging economic order. The knowledge era is about the nature of the content, not the socioeconomic processes by which it will be traded. The  computer revolution , and knowledge revolution refer to specific revolutionary transitions, rather than the end state towards which we are evolving. The  Information Revolution  relates with the well-known terms agricultural revolution and  Industrial Revolution . One of the central paradoxes of the information society is that it makes information easily reproducible, leading to a variety of freedom/control problems relating to  intellectual property . Essentially, business and capital, whose place becomes that of producing and selling information and knowledge, seems to require control over this new resource so that it can effectively be managed and sold as the basis of the information economy. However, such control can prove to be both technically and socially problematic. Technically because  copy protection  is often easily circumvented and socially  rejected  because the users and citizens of the information society can prove to be unwilling to accept such absolute  commodification  of the facts and information that compose their environment. Responses to this concern range from the  Digital Millennium Copyright Act  in the United States (and similar legislation elsewhere) which make  copy protection  (see  Digital rights management ) circumvention illegal, to the  free software ,  open source  and  copyleft  movements, which seek to encourage and disseminate the \"freedom\" of various information products (traditionally both as in \"gratis\" or free of cost, and liberty, as in freedom to use, explore and share). Caveat: Information society is often used by politicians meaning something like \"we all do internet now\"; the sociological term information society (or informational society) has some deeper implications about change of societal structure. Because we lack political control of intellectual property, we are lacking in a concrete map of issues, an analysis of costs and benefits, and functioning political groups that are unified by common interests representing different opinions of this diverse situation that are prominent in the information society. [ 41 ]"
  },
  {
    "id": 10,
    "title": "Knowledge organization",
    "content": "Knowledge organization  ( KO ),  organization of knowledge ,  organization of information , or  information organization  is an intellectual discipline concerned with activities such as  document description ,  indexing , and  classification  that serve to provide systems of representation and order for knowledge and information objects. According to  The Organization of Information  by Joudrey and Taylor, information organization: examines the activities carried out and tools used by people who work in places that accumulate  information resources  (e.g., books, maps, documents, datasets, images) for the use of humankind, both immediately and for posterity. It discusses the processes that are in place to make resources findable, whether someone is searching for a single known item or is browsing through hundreds of resources just hoping to discover something useful. Information organization supports a myriad of information-seeking scenarios. [ 1 ] Issues related to  knowledge sharing  can be said to have been an important part of knowledge management for a long time. Knowledge sharing has received a lot of attention in research and business practice both within and outside organizations and its different levels. [ 2 ] Sharing knowledge is not only about giving it to others, but it also includes searching, locating, and absorbing knowledge. Unawareness of the employees' work and duties tends to provoke the repetition of mistakes, the waste of resources, and duplication of the same projects. Motivating co-workers to share their knowledge is called knowledge enabling. It leads to trust among individuals and encourages a more open and proactive relationship that grants the exchange of information easily. [ 3 ] Knowledge sharing is part of the three-phase knowledge management process which is a continuous process model. The three parts are knowledge creation, knowledge implementation, and knowledge sharing. The process is continuous, which is why the parts cannot be fully separated. Knowledge creation is the consequence of individuals' minds, interactions, and activities. Developing new ideas and arrangements alludes to the process of knowledge creation. Using the knowledge which is present at the company in the most effective manner stands for the implementation of knowledge. Knowledge sharing, the most essential part of the process for our topic, takes place when two or more people benefit by learning from each other. [ 4 ] Traditional human-based approaches performed by librarians, archivists, and subject specialists are increasingly challenged by computational ( big data ) algorithmic techniques. KO as a field of study is concerned with the nature and quality of such knowledge-organizing processes (KOP) (such as  taxonomy  and  ontology ) as well as the resulting  knowledge organizing systems  (KOS). Among the major figures in the history of KO are  Melvil Dewey  (1851–1931) and  Henry Bliss  (1870–1955). Dewey's goal was an efficient way to manage library collections; not an optimal system to support users of libraries. His system was meant to be used in many libraries as a standardized way to manage collections. The first version of this system was created in 1876. [ 5 ] An important characteristic in Henry Bliss' (and many contemporary thinkers of KO) was that the sciences tend to reflect the order of Nature and that library classification should reflect the order of knowledge as uncovered by science: Natural order  →  Scientific classification  → Library classification (KO) The implication is that librarians, in order to classify books, should know about scientific developments. This should also be reflected in their education: Again from the standpoint of the higher education of librarians, the teaching of systems of classification ... would be perhaps better conducted by including courses in the systematic encyclopedia and methodology of all the sciences, that is to say, outlines which try to summarize the most recent results in the relation to one another in which they are now studied together. ... ( Ernest Cushing Richardson , quoted from Bliss, 1935, p. 2) Among the other principles, which may be attributed to the traditional approach to KO are: Today, after more than 100 years of research and development in LIS, the \"traditional\" approach still has a strong position in KO and in many ways its principles still dominate. The date of the foundation of this approach may be chosen as the publication of  S. R. Ranganathan 's  colon classification  in 1933. The approach has been further developed by, in particular, the British  Classification Research Group . The best way to explain this approach is probably to explain its analytico-synthetic methodology. The meaning of the term \"analysis\" is: breaking down each subject into its basic concepts. The meaning of the term synthesis is: combining the relevant units and concepts to describe the subject matter of the information package in hand. Given subjects (as they appear in, for example, book titles) are first analyzed into a few common categories, which are termed \"facets\". Ranganathan proposed his PMEST formula: Personality, Matter, Energy, Space and Time: Important in the IR-tradition have been, among others, the  Cranfield experiments , which were founded in the 1950s, and the TREC experiments ( Text Retrieval Conferences ) starting in 1992. It was the Cranfield experiments, which introduced the measures  \"recall\" and \"precision\"  as evaluation criteria for systems efficiency. The Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (\"UNITERM\"). The Cranfield I test found, according to Ellis (1996, 3–6) the following results: Although these results have been criticized and questioned, the IR-tradition became much more influential while library classification research lost influence. The dominant trend has been to regard only  statistical averages . What has largely been neglected is to ask: Are there certain kinds of questions in relation to which other kinds of representation, for example, controlled vocabularies, may improve recall and precision? The best way to define this approach is probably by method: Systems based upon user-oriented approaches must specify how the design of a system is made on the basis of empirical studies of users. User studies demonstrated very early that users prefer verbal search systems as opposed to systems based on classification notations. This is one example of a principle derived from empirical studies of users. Adherents of classification notations may, of course, still have an argument: That notations are well-defined and that users may miss important information by not considering them. Folksonomies  is a recent kind of KO based on users' rather than on librarians' or subject specialists' indexing. These approaches are primarily based on using bibliographical references to organize networks of papers, mainly by bibliographic coupling (introduced by Kessler 1963) or co-citation analysis ( independently suggested by Marshakova 1973 [ 8 ]  and Small 1973). In recent years it has become a popular activity to construe bibliometric maps as structures of research fields. Two considerations are important in considering bibliometric approaches to KO: Domain analysis is a  sociological - epistemological  standpoint that advocates that the indexing of a given document should reflect the needs of a given group of users or a given ideal purpose. In other words, any description or representation of a given document is more or less suited to the fulfillment of certain tasks. A description is never objective or neutral, and the goal is not to standardize descriptions or make one description once and for all for different target groups. The development of the Danish library \" KVINFO \" may serve as an example that explains the domain-analytic point of view. KVINFO was founded by the librarian and writer  Nynne Koch  and its history goes back to 1965. Nynne Koch was employed at the Royal Library in Copenhagen in a position without influence on book selection. She was interested in women's studies and began personally to collect printed catalog cards of books in the Royal Library, which were considered relevant for women's studies. She developed a classification system for this subject. Later she became the head of KVINFO and got a budget for buying books and journals, and still later, KVINFO became an independent library. The important theoretical point of view is that the Royal Library had an official systematic catalog of a high standard. Normally it is assumed that such a catalog is able to identify relevant books for users whatever their theoretical orientation. This example demonstrates, however, that for a specific user group (feminist scholars), an alternative way of organizing catalog cards was important. In other words: Different points of view need different systems of organization. Domain analysis has examined epistemological issues in the field, i.e. comparing the assumptions made in different approaches to KO and examining the questions regarding subjectivity and objectivity in KO. Subjectivity is not just about individual differences. Such differences are of minor interest because they cannot be used as guidelines for KO. What seems important are collective views shared by many users. A kind of subjectivity about many users is related to philosophical positions. In any field of knowledge different views are always at play. In arts, for example, different views of art are always present. Such views determine views on art works, writing on art works, how art works are organized in exhibitions and how writings on art are organized in libraries. In general it can be stated that different philosophical positions on any issue have implications for relevance criteria, information needs and for criteria of organizing knowledge. One widely used analysis of information-organizational principles, attributed to  Richard Saul Wurman , summarizes them as Location, Alphabet, Time, Category, Hierarchy (LATCH). [ 9 ] [ 10 ]"
  },
  {
    "id": 11,
    "title": "Ontology (information science)",
    "content": "In  information science , an  ontology  encompasses a representation, formal naming, and definitions of the categories, properties, and relations between the concepts, data, or entities that pertain to one, many, or all  domains of discourse . More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of terms and relational expressions that represent the entities in that subject area. The field which studies ontologies so conceived is sometimes referred to as  applied ontology . [ 1 ] Every  academic discipline  or field, in creating its terminology, thereby lays the groundwork for an ontology. Each uses ontological assumptions to frame explicit theories, research and applications. Improved ontologies may improve problem solving within that domain,  interoperability  of data systems, and discoverability of data. Translating research papers within every field is a problem made easier when experts from different countries maintain a  controlled vocabulary  of  jargon  between each of their languages. [ 2 ]  For instance, the  definition and ontology of economics  is a primary concern in  Marxist economics , [ 3 ]  but also in other  subfields of economics . [ 4 ]  An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what  capital assets  are at risk and by how much (see  risk management ). What ontologies in both  information science  and  philosophy  have in common is the attempt to represent entities, including both objects and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of  ontology engineering  (e.g.,  Quine  and  Kripke  in philosophy,  Sowa  and  Guarino  in information science), [ 5 ]  and debates concerning to what extent  normative  ontology is possible (e.g.,  foundationalism  and  coherentism  in philosophy,  BFO  and  Cyc  in artificial intelligence). Applied ontology  is considered by some as a successor to prior work in philosophy. However many current efforts are more concerned with establishing  controlled vocabularies  of narrow domains than with philosophical  first principles , or with questions such as the mode of existence of  fixed essences  or whether enduring objects (e.g.,  perdurantism  and  endurantism ) may be ontologically more primary than  processes .  Artificial intelligence  has retained considerable attention regarding  applied ontology  in subfields like  natural language processing  within  machine translation  and  knowledge representation , but ontology editors are being used often in a range of fields, including biomedical informatics, [ 6 ]  industry. [ 7 ]  Such efforts often use ontology editing tools such as  Protégé . [ 8 ] Ontology  is a branch of  philosophy  and intersects areas such as  metaphysics ,  epistemology , and  philosophy of language , as it considers how knowledge, language, and perception relate to the nature of reality.  Metaphysics  deals with questions like \"what exists?\" and \"what is the nature of reality?\". One of five traditional branches of philosophy, metaphysics is concerned with exploring existence through properties, entities and relations such as those between  particulars  and  universals ,  intrinsic and extrinsic properties , or  essence  and  existence . Metaphysics has been an ongoing topic of discussion since recorded history. The  compound  word  ontology  combines  onto - , from the  Greek   ὄν ,  on  ( gen.  ὄντος,  ontos ), i.e. \"being; that which is\", which is the  present   participle  of the  verb   εἰμί ,  eimí , i.e. \"to be, I am\", and  -λογία ,  -logia , i.e. \"logical discourse\", see  classical compounds  for this type of word formation. [ 9 ] [ 10 ] While the  etymology  is Greek, the oldest extant record of the word itself, the  Neo-Latin  form  ontologia , appeared in 1606 in the work  Ogdoas Scholastica  by  Jacob Lorhard  ( Lorhardus ) and in 1613 in the  Lexicon philosophicum  by  Rudolf Göckel  ( Goclenius ). [ 11 ] The first occurrence in English of  ontology  as recorded by the  OED  ( Oxford English Dictionary , online edition, 2008) came in  Archeologia Philosophica Nova  or  New Principles of Philosophy  by  Gideon Harvey . Since the mid-1970s, researchers in the field of  artificial intelligence  (AI) have recognized that  knowledge engineering  is the key to building large and powerful AI systems [ citation needed ] . AI researchers argued that they could create new ontologies as  computational models  that enable certain kinds of  automated reasoning , which was only  marginally successful . In the 1980s, the AI community began to use the term  ontology  to refer to both a theory of a modeled world and a component of  knowledge-based systems . In particular, David Powers introduced the word  ontology  to AI to refer to real world or robotic grounding, [ 12 ] [ 13 ] [ 14 ]  publishing in 1990 literature reviews emphasizing grounded ontology in association with the call for papers for a AAAI Summer Symposium Machine Learning of Natural Language and Ontology, with an expanded version published in SIGART Bulletin and included as a preface to the proceedings. [ 15 ]  Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy. [ 16 ] In 1993, the widely cited web page and paper \"Toward Principles for the Design of Ontologies Used for Knowledge Sharing\" by  Tom Gruber [ 17 ]  used  ontology  as a technical term in  computer science  closely related to earlier idea of  semantic networks  and  taxonomies . Gruber introduced the term as  a specification of a conceptualization : An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy. [ 18 ] Attempting to distance ontologies from taxonomies and similar efforts in  knowledge modeling  that rely on  classes  and  inheritance , Gruber stated (1993): Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the  subsumption relation , but ontologies need not be limited to these forms. Ontologies are also not limited to  conservative definitions  – that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world. [ 19 ]  To specify a conceptualization, one needs to state  axioms  that do constrain the possible interpretations for the defined terms. [ 20 ] As refinement of Gruber's definition Feilmayr and Wöß (2016) stated: \"An ontology is a formal, explicit specification of a shared conceptualization that is characterized by high semantic expressiveness required for increased complexity.\" [ 21 ] Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed. Most ontologies describe individuals (instances), classes (concepts), attributes and relations. A domain ontology (or domain-specific ontology) represents concepts which belong to a realm of the world, such as biology or politics. Each domain ontology typically models domain-specific definitions of terms. For example, the word  card  has many different meanings. An ontology about the domain of  poker  would model the \" playing card \" meaning of the word, while an ontology about the domain of  computer hardware  would model the \" punched card \" and \" video card \" meanings. Since domain ontologies are written by different people, they represent concepts in very specific and unique ways, and are often incompatible within the same project. As systems that rely on domain ontologies expand, they often need to merge domain ontologies by hand-tuning each entity or using a combination of software merging and hand-tuning. This presents a challenge to the ontology designer. Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.) [ citation needed ] . At present, merging ontologies that are not developed from a common  upper ontology  is a largely manual process and therefore time-consuming and expensive. Domain ontologies that use the same upper ontology to provide a set of basic elements with which to specify the meanings of the domain ontology entities can be merged with less effort. There are studies on generalized techniques for merging ontologies, [ 22 ]  but this area of research is still ongoing, and it is a recent event to see the issue sidestepped by having multiple domain ontologies using the same upper ontology like the  OBO Foundry . An upper ontology (or foundation ontology) is a model of the commonly shared relations and objects that are generally applicable across a wide range of domain ontologies. It usually employs a  core glossary  that overarches the terms and associated object descriptions as they are used in various relevant domain ontologies. Standardized upper ontologies available for use include  BFO ,  BORO method ,  Dublin Core ,  GFO ,  Cyc ,  SUMO ,  UMBEL , and  DOLCE . [ 23 ] [ 24 ]   WordNet  has been considered an upper ontology by some and has been used as a linguistic tool for learning domain ontologies. [ 25 ] The  Gellish  ontology is an example of a combination of an upper and a domain ontology. A survey of ontology visualization methods is presented by Katifori et al. [ 26 ]  An updated survey of ontology visualization methods and tools was published by Dudás et al. [ 27 ]  The most established ontology visualization methods, namely indented tree and graph visualization are evaluated by Fu et al. [ 28 ]  A visual language for ontologies represented in  OWL  is specified by the  Visual Notation for OWL Ontologies (VOWL) . [ 29 ] Ontology engineering (also called ontology building)  is a set of tasks related to the development of ontologies for a particular domain. [ 30 ]  It is a subfield of  knowledge engineering  that studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tools and languages that support them. [ 31 ] [ 32 ] Ontology engineering aims to make explicit the knowledge contained in software applications, and organizational procedures for a particular domain. Ontology engineering offers a direction for overcoming semantic obstacles, such as those related to the definitions of business terms and software classes. Known challenges with ontology engineering include: Ontology editors  are applications designed to assist in the creation or manipulation of ontologies. It is common for ontology editors to use one or more  ontology languages . Aspects of ontology editors include: visual navigation possibilities within the  knowledge model ,  inference engines  and  information extraction ; support for modules; the import and export of foreign  knowledge representation  languages for  ontology matching ; and the support of meta-ontologies such as  OWL-S ,  Dublin Core , etc. [ 33 ] Ontology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.  Information extraction and  text mining  have been explored to automatically link ontologies to documents, for example in the context of the BioCreative challenges. [ 34 ] Epistemological assumptions, which in research asks \"What do you know? or \"How do you know it?\", creates the foundation researchers use when approaching a certain topic or area for potential research. As epistemology is directly linked to knowledge and how we come about accepting certain truths, individuals conducting academic research must understand what allows them to begin theory building. Simply, epistemological assumptions force researchers to question how they arrive at the knowledge they have. [ citation needed ] An  ontology language  is a  formal language  used to encode an ontology. There are a number of such languages for ontologies, both proprietary and standards-based: The W3C  Linking Open Data community project  coordinates attempts to converge different ontologies into worldwide  Semantic Web . The development of ontologies has led to the emergence of services providing lists or directories of ontologies called ontology libraries. The following are libraries of human-selected ontologies. The following are both directories and search engines. In general, ontologies can be used beneficially in several fields."
  },
  {
    "id": 12,
    "title": "Philosophy of information",
    "content": "The  philosophy of information  ( PI ) is a branch of  philosophy  that studies topics relevant to  information processing , representational system and consciousness,  cognitive science ,  computer science ,  information science  and  information technology . It includes: The philosophy of information (PI) has evolved from the  philosophy of artificial intelligence ,  logic of information ,  cybernetics ,  social theory ,  ethics  and the study of language and information. The  logic of information , also known as the  logical theory of information , considers the information content of logical  signs  and expressions along the lines initially developed by  Charles Sanders Peirce . Later contributions to the field were made by  Fred Dretske ,  Jon Barwise ,  Brian Cantwell Smith , and others. The  Center for the Study of Language and Information (CSLI)  was founded at Stanford University in 1983 by philosophers, computer scientists, linguists, and psychologists, under the direction of  John Perry  and  Jon Barwise . More recently this field has become known as the philosophy of information. The expression was coined in the 1990s by  Luciano Floridi , who has published prolifically in this area with the intention of elaborating a unified and coherent, conceptual frame for the whole subject. [ 2 ] The concept  information  has been defined by several theorists. [ 3 ] Charles S. Peirce 's theory of information was embedded in his wider theory of symbolic communication he called the  semiotic , now a major part of  semiotics . For Peirce, information integrates the aspects of  signs  and  expressions  separately covered by the concepts of  denotation  and  extension , on the one hand, and by  connotation  and  comprehension  on the other. Donald M. MacKay  says that information is a distinction that makes a difference. [ 4 ] According to Luciano Floridi [ citation needed ] , four kinds of mutually compatible phenomena are commonly referred to as \"information\": Recent creative advances and efforts in  computing , such as  semantic web ,  ontology engineering ,  knowledge engineering , and modern  artificial intelligence  provide  philosophy  with fertile ideas, new and evolving subject matters, methodologies, and models for philosophical inquiry. While  computer science  brings new opportunities and challenges to traditional philosophical studies, and changes the ways philosophers understand foundational concepts in philosophy, further major progress in  computer science  would only be feasible when philosophy provides sound foundations for areas such as bioinformatics, software engineering, knowledge engineering, and ontologies. Classical topics in philosophy, namely,  mind ,  consciousness ,  experience ,  reasoning ,  knowledge ,  truth ,  morality  and  creativity  are rapidly becoming common concerns and foci of investigation in  computer science , e.g., in areas such as agent computing,  software agents , and intelligent mobile agent technologies. [ citation needed ] According to Luciano Floridi \" [ 5 ]  one can think of several ways for applying computational methods towards philosophical matters:"
  },
  {
    "id": 13,
    "title": "Science and technology studies",
    "content": "Science and technology studies  ( STS ) or  science, technology, and society  is an  interdisciplinary  field that examines the creation, development, and consequences of science and technology in their historical, cultural, and social contexts. [ 1 ] Like most  interdisciplinary  fields of study, STS emerged from the confluence of a variety of disciplines and disciplinary subfields, all of which had developed an interest—typically, during the 1960s or 1970s—in viewing science and technology as socially embedded enterprises. [ 2 ]   The key disciplinary components of STS took shape independently, beginning in the 1960s, and developed in isolation from each other well into the 1980s, although  Ludwik Fleck 's (1935) monograph  Genesis and Development of a Scientific Fact  anticipated many of STS's key themes. In the 1970s  Elting E. Morison  founded the STS program at the  Massachusetts Institute of Technology  (MIT), which served as a model. By 2011, 111 STS research centers and academic programs were counted worldwide. [ 3 ] During the 1970s and 1980s, universities in the US, UK, and Europe began drawing these various components together in new, interdisciplinary programs. For example, in the 1970s,  Cornell University  developed a new program that united  science studies  and policy-oriented scholars with historians and philosophers of science and technology. Each of these programs developed unique identities due to variations in the components that were drawn together, as well as their location within the various universities. For example, the University of Virginia's STS program united scholars drawn from a variety of fields (with particular strength in the history of technology); however, the program's teaching responsibilities—it is located within an engineering school and teaches ethics to undergraduate engineering students—means that all of its faculty share a strong interest in  engineering ethics . [ 5 ] A decisive moment in the development of STS was the mid-1980s addition of technology studies to the range of interests reflected in science. During that decade, two works appeared  en seriatim  that signaled what  Steve Woolgar  was to call the \"turn to technology\". [ 6 ]  In a seminal 1984 article,  Trevor Pinch  and  Wiebe Bijker  showed how the sociology of technology could proceed along the theoretical and methodological lines established by the sociology of scientific knowledge.  [ 7 ]  This was the intellectual foundation of the field they called the social construction of technology. Donald MacKenzie and  Judy Wajcman  primed the pump by publishing a collection of articles attesting to the influence of society on technological design ( Social Shaping of Technology , 1985). [ 8 ]  Social science research continued to interrogate STS research from this point onward as researchers moved from post-modern to post-structural frameworks of thought, Bijker and Pinch contributing to SCOT knowledge and Wajcman providing boundary work through a feminist lens. [ 9 ] The \"turn to technology\" helped to cement an already growing awareness of underlying unity among the various emerging STS programs. More recently, there has been an associated turn to ecology, nature, and materiality in general, whereby the socio-technical and natural/material co-produce each other. This is especially evident in work in STS analyses of biomedicine (such as  Carl May  and  Annemarie Mol ) and ecological interventions (such as  Bruno Latour ,  Sheila Jasanoff ,  Matthias Gross ,  Sara B. Pritchard , and  S. Lochlann Jain ). Social constructions are human-created ideas, objects, or events created by a series of choices and interactions. [ 10 ]  These interactions have consequences that change the perception that different groups of people have on these constructs. Some examples of social construction include class, race, money, and citizenship. The following also alludes to the notion that not everything is set, a circumstance or result could potentially be one way or the other. According to the article \"What is Social Construction?\" by Laura Flores, \"Social construction work is critical of the status quo. Social constructionists about X tend to hold that: Very often they go further, and urge that: In the past, there have been viewpoints that were widely regarded as fact until being called to question due to the introduction of new knowledge. Such viewpoints include the past concept of a correlation between intelligence and the nature of a human's ethnicity or race (X may not be at all as it is). [ 11 ] An example of the evolution and interaction of various social constructions within science and technology can be found in the development of both the high-wheel bicycle, or  velocipede , and then of the  bicycle .  The velocipede was widely used in the latter half of the 19th century. In the latter half of the 19th century, a social need was first recognized for a more efficient and rapid means of transportation. Consequently, the velocipede was first developed, which was able to reach higher translational velocities than the smaller non-geared bicycles of the day, by replacing the front wheel with a larger radius wheel. One notable trade-off was a certain decreased stability leading to a greater risk of falling. This trade-off resulted in many riders getting into accidents by losing balance while riding the bicycle or being thrown over the handlebars. The first \"social construction\" or progress of the velocipede caused the need for a newer \"social construction\" to be recognized and developed into a safer bicycle design.  Consequently, the velocipede was then developed into what is now commonly known as the \" bicycle \" to fit within society's newer \"social construction,\" the newer standards of higher vehicle safety. Thus the popularity of the modern geared bicycle design came as a response to the first social construction, the original need for greater speed, which had caused the high-wheel bicycle to be designed in the first place.  The popularity of the modern geared bicycle design ultimately ended the widespread use of the velocipede itself, as eventually it was found to best accomplish the social needs/social constructions of both greater speed and of greater safety. [ 12 ] With methodology from ANT, feminist STS theorists built upon SCOT's theory of co-construction to explore the relationship between gender and technology, proposing one cannot exist separately from the other. [ 13 ]  This approach suggests the material and social are not separate, reality being produced through interactions and studied through representations of those realities. [ 14 ]  Building on  Steve Woolgar 's boundary work on user configuration, [ 15 ]  feminist critiques shifted the focus away from users of technology and science towards whether technology and science represent a fixed, unified reality. [ 16 ]  According to this approach, identity could no longer be treated as causal in human interactions with technology as it cannot exist prior to that interaction, feminist STS researchers proposing a \"double-constructivist\" approach to account for this contradiction. [ 17 ]   John Law  credits feminist STS scholars for contributing material-semiotic approaches to the broader discipline of STS, stating that research not only attempts to describe reality, but enacts it through the research process. [ 18 ] Sociotechnical imaginaries are what certain communities, societies, and nations envision as achievable through the combination of scientific innovation and social changes. These visions can be based on what is possible to achieve for a certain society, and can also show what a certain state or nation desires. [ 19 ]  STIs are often bound with ideologies and ambitions of those who create and circulate them. Sociotechnical imaginaries can be created by states and policymakers, smaller groups within society, or can be a result of the interaction of both. [ 19 ] The term was coined in 2009 by  Sheila Jasanoff  and Sang-Hyun Kim who compared and contrasted sociotechnical imaginaries of nuclear energy in the  USA  with those of  South Korea  over the second half of the 20th century. [ 19 ]  Jasanoff and Kim analyzed the discourse of government representatives, national policies, and civil society organizations, looked at the technological and infrastructural developments, and social protests, and conducted interviews with experts. They concluded that in South Korea nuclear energy was imagined mostly as the means of national development, while in the US the dominant sociotechnical imaginary framed nuclear energy as risky and in need of containment. [ 19 ] The concept has been applied to several objects of study including biomedical research, [ 20 ] [ 21 ]  nanotechnology development [ 22 ]  and energy systems and climate change. [ 23 ] [ 24 ] [ 25 ] [ 26 ] [ 27 ] [ 19 ]  Within energy systems, research has focused on nuclear energy, [ 19 ]  fossil fuels, [ 24 ] [ 27 ]  renewables [ 23 ]  as well as broader topics of energy transitions, [ 25 ]  and the development of new technologies to address climate change. [ 26 ] Social technical systems are an interplay between technologies and humans, this is clearly expressed in the  sociotechnical systems theory . To expound on this interplay, humans fulfill and define tasks, then humans in companies use IT and IT supports people, and finally, IT processes tasks and new IT generates new tasks. This IT redefines work practices. This is what we call the sociotechnical systems. [ 28 ]  In socio-technical systems, there are two principles to internalize, that is joint optimization and complementarity. Joint optimization puts an emphasis on developing both systems in parallel and it is only in the interaction of both systems that the success of an organization arises. [ 28 ]  The principle of complementarity means that both systems have to be optimized. [ 28 ]  If you focus on one system and have bias over the other it will likely lead to the failure of the organization or jeopardize the success of a system. Although the above socio-technical system theory is focused on an organization, it is undoubtedly imperative to correlate this theory and its principles to society today and in science and technology studies. According to Barley and Bailey, there is a  tendency for AI designers and scholars of design studies to privilege the technical over the social, focusing more on taking \"humans out of the loop\" paradigm than the \"augmented intelligence\" paradigm. [ 29 ] Recent work on  artificial intelligence  considers large sociotechnical systems, such as  social networks  and  online marketplaces , as agents whose behavior can be purposeful and adaptive. The behavior of  recommender systems  can therefore be analyzed in the language and framework of sociotechnical systems, leading also to a new perspective for their legal regulation. [ 30 ] [ 31 ] Technoscience is a subset of Science, Technology, and Society studies that focuses on the inseparable connection between science and technology. It states that fields are linked and grow together, and scientific knowledge requires an infrastructure of technology in order to remain stationary or move forward. Both technological development and scientific discovery drive one another towards more advancement. Technoscience excels at shaping human thoughts and behavior by opening up new possibilities that gradually or quickly come to be perceived as necessities. [ 32 ] \"Technological action is a social process.\" [ 33 ]  Social factors and technology are intertwined so that they are dependent upon each other. This includes the aspect that social, political, and economic factors are inherent in technology and that social structure influences what technologies are pursued. In other words, \"technoscientific phenomena combined inextricably with social/political/economic/psychological phenomena, so 'technology' includes a spectrum of artifacts, techniques, organizations, and systems.\" [ 34 ]  Winner expands on this idea by saying \"in the late twentieth-century technology and society, technology and culture, technology and politics are by no means separate.\" [ 35 ] Deliberative democracy  is a reform of  representative  or  direct  democracies which mandates discussion and debate of popular topics which affect society.  Deliberative democracy is a tool for making decisions.  Deliberative democracy can be traced back all the way to  Aristotle's writings . More recently, the term was coined by Joseph Bessette in his 1980 work  Deliberative Democracy: The Majority Principle in Republican Government , where he uses the idea in opposition to the elitist interpretations of the  United States Constitution  with emphasis on public discussion. [ 37 ] Deliberative democracy can lead to more legitimate, credible, and trustworthy outcomes. Deliberative democracy allows for \"a wider range of public knowledge\", and it has been argued that this can lead to \"more socially intelligent and robust\" science. One major shortcoming of deliberative democracy is that many models insufficiently ensure critical interaction. [ 38 ] According to Ryfe, there are five mechanisms that stand out as critical to the successful design of deliberative democracy: Recently, [ when? ]  there has been a movement towards greater transparency in the fields of policy and technology. Jasanoff comes to the conclusion that there is no longer a question of if there needs to be increased public participation in making decisions about science and technology, but now there need to be ways to make a more meaningful conversation between the public and those developing the technology. [ 40 ] Bruce Ackerman  and  James S. Fishkin  offered an example of a reform in their paper \"Deliberation Day.\" The deliberation is to enhance public understanding of popular, complex and controversial issues through devices such as Fishkin's  deliberative polling , [ 41 ]  though implementation of these reforms is unlikely in a large government such as that of the United States. However, things similar to this have been implemented in small, local governments like  New England  towns and villages. New England town hall meetings are a good example of  deliberative democracy  in a realistic setting. [ 37 ] An ideal deliberative democracy balances the voice and influence of all participants. While the main aim is to reach consensus, deliberative democracy should encourage the voices of those with opposing viewpoints, concerns due to uncertainties, and questions about assumptions made by other participants. It should take its time and ensure that those participating understand the topics on which they debate. Independent managers of debates should also have a substantial grasp of the concepts discussed, but must \"[remain] independent and impartial as to the outcomes of the process.\" [ 38 ] In 1968,  Garrett Hardin  popularised the phrase \"tragedy of the commons.\" It is an economic theory where rational people act against the best interest of the group by consuming a common resource. Since then, the tragedy of the commons has been used to symbolize the degradation of the environment whenever many individuals use a common resource. Although Garrett Hardin was not an STS scholar, the concept of the tragedy of the commons still applies to science, technology, and society. [ 42 ] In a contemporary setting, the Internet acts as an example of the tragedy of the commons through the exploitation of digital resources and private information. Data and internet passwords can be stolen much more easily than physical documents. Virtual spying is almost free compared to the costs of physical spying. [ 43 ]  Additionally,  net neutrality  can be seen as an example of tragedy of the commons in an STS context. The movement for net neutrality argues that the Internet should not be a resource that is dominated by one particular group, specifically those with more money to spend on Internet access. A counterexample to the tragedy of the commons is offered by Andrew Kahrl. Privatization can be a way to deal with the tragedy of the commons. However, Kahrl suggests that the privatization of beaches on  Long Island , in an attempt to combat the overuse of Long Island beaches, made the residents of Long Island more susceptible to flood damage from  Hurricane Sandy . The privatization of these beaches took away from the protection offered by the natural landscape. Tidal lands that offer natural protection were drained and developed. This attempt to combat the tragedy of the commons by privatization was counter-productive. Privatization actually destroyed the public good of natural protection from the landscape. [ 44 ] Alternative  modernity [ 45 ] [ 46 ]  is a conceptual tool conventionally used to represent the state of present western society. Modernity represents the political and social structures of society, the sum of interpersonal discourse, and ultimately a snapshot of society's direction at a point in time. Unfortunately, conventional modernity is incapable of modeling alternative directions for further growth within our society. Also, this concept is ineffective at analyzing similar but unique modern societies such as those found in the diverse cultures of the developing world. Problems can be summarized into two elements: inward failure to analyze the growth potentials of a given society, and outward failure to model different cultures and social structures and predict their growth potentials. Previously, modernity carried a connotation of the current state of being modern, and its evolution through European colonialism. The process of becoming \"modern\" is believed to occur in a linear, pre-determined way, and is seen by Philip Brey as a way to interpret and evaluate social and cultural formations. This thought ties in with  modernization theory , the thought that societies progress from \"pre-modern\" to \"modern\" societies. Within the field of science and technology, there are two main lenses with which to view modernity. The first is as a way for society to quantify what it wants to move towards. In effect, we can discuss the notion of \"alternative modernity\" (as described by Andrew Feenberg) and which of these we would like to move towards. Alternatively, modernity can be used to analyze the differences in interactions between cultures and individuals. From this perspective, alternative modernities exist simultaneously, based on differing cultural and societal expectations of how a society (or an individual within society) should function. Because of different types of interactions across different cultures, each culture will have a different modernity. The pace of innovation is the speed at which technological innovation or advancement is occurring, with the most apparent instances being too slow or too rapid. Both these rates of innovation are extreme and therefore have effects on the people that get to use this technology. \"No innovation without representation\" is a democratic ideal of ensuring that everyone involved gets a chance to be represented fairly in technological developments. Legacy thinking is defined as an inherited method of thinking imposed from an external source without objection by the individual because it is already widely accepted by society. Legacy thinking can impair the ability to drive technology for the betterment of society by blinding people to innovations that do not fit into their accepted model of how society works.  By accepting ideas without questioning them, people often see all solutions that contradict these accepted ideas as impossible or impractical.  Legacy thinking tends to advantage the wealthy, who have the means to project their ideas on the public.  It may be used by the wealthy as a vehicle to drive technology in their favor rather than for the greater good.\nExamining the role of citizen participation and representation in politics provides an excellent example of legacy thinking in society. The belief that one can spend money freely to gain influence has been popularized, leading to public acceptance of corporate  lobbying . As a result, a self-established role in politics has been cemented where the public does not exercise the power ensured to them by the Constitution to the fullest extent. This can become a barrier to political progress as corporations who have the capital to spend have the potential to wield great influence over policy. [ 50 ]  Legacy thinking, however, keeps the population from acting to change this, despite polls from Harris Interactive that report over 80% of Americans to feel that big business holds too much power in government. [ 51 ]  Therefore, Americans are beginning to try to steer away from this line of thought, rejecting legacy thinking, and demanding less corporate, and more public, participation in political decision-making. Additionally, an examination of  net neutrality  functions as a separate example of legacy thinking. Starting with  dial-up , the internet has always been viewed as a private luxury good. [ 52 ] [ 53 ]  Internet today is a vital part of modern-day society members. They use it in and out of life every day. [ 54 ]  Corporations are able to mislabel and greatly overcharge for their internet resources. Since the American public is so dependent upon the internet there is little for them to do. Legacy thinking has kept this pattern on track despite growing movements arguing that the internet should be considered a utility. Legacy thinking prevents progress because it was widely accepted by others before us through advertising that the internet is a luxury and not a  utility. Due to pressure from grassroots movements the  Federal Communications Commission  (FCC) has redefined the requirements for broadband and internet in general as a utility. [ 54 ]  Now AT&T and other major internet providers are lobbying against this action and are in large able to delay the onset of this movement due to legacy thinking's grip on American [ specify ]  culture and politics. For example, those who cannot overcome the barrier of legacy thinking may not consider the  privatization of clean drinking water  as an issue. [ 55 ]  This is partial because access to water has become such a given fact of the matter to them. For a person living in such circumstances, it may be widely accepted to not concern themselves with drinking water because they have not needed to be concerned with it in the past. Additionally, a person living within an area that does not need to worry about their water supply or the sanitation of their water supply is less likely to be concerned with the privatization of water. This notion can be examined through the thought experiment of \" veil of ignorance \". [ 56 ]  Legacy thinking causes people to be particularly ignorant about the implications behind the \"you get what you pay for\" mentality applied to a life necessity. By utilizing the \"veil of ignorance\", one can overcome the barrier of legacy thinking as it requires a person to imagine that they are unaware of their own circumstances, allowing them to free themselves from externally imposed thoughts or widely accepted ideas. STS is taught in several countries. According to the STS wiki, STS programs can be found in twenty countries, including 45 programs in the United States, three programs in India, and eleven programs in the UK. [ 62 ]  STS programs can be found in  Canada , [ 63 ]  Germany, [ 64 ] [ 65 ]   Israel , [ 66 ]   Malaysia , [ 67 ]  and  Taiwan . [ 68 ]  Some examples of institutions offering STS programs are  Stanford University , [ 69 ]   University College London , [ 70 ]   Harvard University , [ 71 ]  the  University of Oxford , [ 72 ]   Mines ParisTech , [ 73 ]   Bar-Ilan University , [ 74 ]  and  York University . [ 63 ]  In Europe the European Inter-University Association on Society, Science and Technology ( ESST ) offers an MA degree in STS through study programs and student exchanges with over a dozen specializations. The field has professional associations in regions and countries around the world. Notable peer-reviewed journals in STS include: Student journals in STS include:"
  },
  {
    "id": 14,
    "title": "Taxonomy",
    "content": "Taxonomy  is a practice and science concerned with classification or categorization. Typically, there are two parts to it: the development of an underlying scheme of classes (a taxonomy) and the allocation of things to the classes ( classification ). Originally, taxonomy referred only to the  classification of organisms  on the basis of shared characteristics. Today it also has a more general sense. It may refer to the classification of things or concepts, as well as to the principles underlying such work. Thus a taxonomy can be used to organize species, documents, videos or anything else. A taxonomy organizes taxonomic units known as \"taxa\" (singular \"taxon\").\" Many are  hierarchies . One function of a taxonomy is to help users more easily find what they are searching for. This may be effected in ways that include  a  library classification system  and a  search engine taxonomy . The word was coined in 1813 by the Swiss botanist  A. P. de Candolle  and is irregularly compounded from the  Greek   τάξις ,  taxis  'order' and  νόμος ,  nomos  'law', connected by the French form  -o- ; the regular form would be  taxinomy , as used in the Greek  reborrowing   ταξινομία . [ 1 ] [ 2 ] Wikipedia categories form a taxonomy, [ 3 ]  which can be extracted by automatic means. [ 4 ]  As of 2009 [update] , it has been shown that a manually-constructed taxonomy, such as that of computational lexicons like  WordNet , can be used to improve and restructure the Wikipedia category taxonomy. [ 5 ] In a broader sense, taxonomy also applies to relationship schemes other than parent-child hierarchies, such as  network structures . Taxonomies may then include a single child with multi-parents, for example, \"Car\" might appear with both parents \"Vehicle\" and \"Steel Mechanisms\"; to some however, this merely means that 'car' is a part of several different taxonomies. [ 6 ]  A taxonomy might also simply be organization of kinds of things into groups, or an alphabetical list; here, however, the term vocabulary is more appropriate. In current usage within  knowledge management , taxonomies are considered narrower than  ontologies  since ontologies apply a larger variety of relation types. [ 7 ] Mathematically, a hierarchical taxonomy is a  tree structure  of classifications for a given set of objects. It is also named  containment hierarchy . At the top of this structure is a single classification, the root node, that applies to all objects. Nodes below this root are more specific classifications that apply to subsets of the total set of classified objects. The progress of reasoning proceeds from the general to the more specific. By contrast, in the context of legal terminology, an open-ended contextual taxonomy is employed—a taxonomy holding only with respect to a specific context. In scenarios taken from the legal domain, a formal account of the open-texture of legal terms is modeled, which suggests varying notions of the \"core\" and \"penumbra\" of the meanings of a concept. The progress of reasoning proceeds from the specific to the more general. [ 8 ] Anthropologists  have observed that taxonomies are generally embedded in local cultural and social systems, and serve various social functions. Perhaps the most well-known and influential study of  folk taxonomies  is  Émile Durkheim 's  The Elementary Forms of Religious Life . A more recent treatment of folk taxonomies (including the results of several decades of empirical research) and the discussion of their relation to the scientific taxonomy can be found in  Scott Atran 's  Cognitive Foundations of Natural History.  Folk taxonomies of organisms have been found in large part to agree with scientific classification, at least for the larger and more obvious species, which means that it is not the case that folk taxonomies are based purely on utilitarian characteristics. [ 9 ] In the seventeenth century the German mathematician and philosopher  Gottfried Leibniz , following the work of the thirteenth-century Majorcan philosopher  Ramon Llull  on his  Ars generalis ultima , a system for procedurally generating concepts by combining a fixed set of ideas, sought to develop an  alphabet of human thought . Leibniz intended his  characteristica universalis  to be an \"algebra\" capable of expressing all conceptual thought. The concept of creating such a \" universal language \" was frequently examined in the 17th century, also notably by the English philosopher  John Wilkins  in his work  An Essay towards a Real Character and a Philosophical Language  (1668), from which the classification scheme in  Roget 's  Thesaurus  ultimately derives. Taxonomy in biology encompasses the description, identification, nomenclature, and classification of organisms. Uses of taxonomy include: Uses of taxonomy in business and economics include: Vegas et al. [ 10 ]  make a compelling case to advance the knowledge in the field of software engineering through the use of taxonomies. Similarly, Ore et al. [ 11 ]  provide a systematic methodology to approach taxonomy building in software engineering related topics. Several taxonomies have been proposed in software testing research to classify techniques, tools, concepts and artifacts. The following are some example taxonomies: Engström et al. [ 14 ]  suggest and evaluate the use of a taxonomy to bridge the communication between researchers and practitioners engaged in the area of software testing. They have also developed a web-based tool [ 15 ]  to facilitate and encourage the use of the taxonomy. The tool and its source code are available for public use. [ 16 ] Uses of taxonomy in education include: Uses of taxonomy in safety include: Citing inadequacies with current practices in listing authors of papers in medical research journals, Drummond Rennie and co-authors called in a 1997 article in  JAMA , the  Journal of the American Medical Association  for a radical conceptual and systematic change, to reflect the realities of multiple authorship and to buttress accountability. We propose dropping the outmoded notion of author in favor of the more useful and realistic one of contributor. [ 17 ] : 152 In 2012, several major academic and scientific publishing bodies  mounted  Project CRediT  to develop a  controlled vocabulary  of contributor roles. [ 18 ]   Known as  CRediT ( Contributor Roles Taxonomy ) , this is an example of a flat, non-hierarchical taxonomy; however, it does include an optional, broad classification of the degree of contribution:  lead ,  equal  or  supporting .   Amy Brand  and co-authors summarise their intended outcome as: Identifying specific contributions to published research will lead to appropriate credit, fewer author disputes, and fewer disincentives to collaboration and the sharing of data and code. [ 17 ] : 151 CRediT comprises 14 specific contributor roles using the following defined terms: The taxonomy is an open standard conformiing to the  OpenStand  principles, [ 19 ]  and is published under a  Creative Commons  licence. [ 18 ] Websites with a well designed taxonomy or hierarchy are easily understood by users, due to the possibility of users developing a mental model of the site structure. [ 20 ] Guidelines for writing taxonomy for the web include: Frederick Suppe [ 21 ]  distinguished two senses of classification: a broad meaning, which he called \"conceptual classification\" and a narrow meaning, which he called \"systematic classification\". About conceptual classification Suppe wrote: [ 21 ] : 292   \"Classification is intrinsic to the use of language, hence to most if not all communication. Whenever we use nominative phrases we are classifying the designated subject as being importantly similar to other entities bearing the same designation; that is, we classify them together. Similarly the use of predicative phrases classifies actions or properties as being of a particular kind. We call this conceptual classification, since it refers to the classification involved in conceptualizing our experiences and surroundings\" About systematic classification Suppe wrote: [ 21 ] : 292   \"A second, narrower sense of classification is the systematic classification involved in the design and utilization of taxonomic schemes such as the biological classification of animals and plants by genus and species. Two of the predominant types of relationships in  knowledge-representation  systems are  predication  and the universally quantified  conditional . Predication relationships express the notion that an individual entity is an example of a certain type (for example,  John is a bachelor ), while universally quantified conditionals express the notion that a type is a subtype of another type (for example, \" A dog is a mammal\" , which means the same as \" All dogs are mammals\" ). [ 22 ] The \"has-a\" relationship is quite different: an elephant  has  a trunk; a trunk is a part, not a subtype of elephant. The study of part-whole relationships is  mereology . Taxonomies are often represented as  is-a  hierarchies  where each level is more specific than the level above it (in mathematical language is \"a subset of\" the level above). For example, a basic biology taxonomy would have concepts such as  mammal , which is a subset of  animal , and  dogs  and  cats , which are subsets of  mammal . This kind of taxonomy is called an is-a model because the specific objects are considered as instances of a concept. For example,  Fido  is-an instance of the concept  dog  and  Fluffy  is-a  cat . [ 23 ] In  linguistics , is-a relations are called  hyponymy . When one word describes a category, but another describe some subset of that category, the larger term is called a  hypernym  with respect to the smaller, and the smaller is called a \"hyponym\" with respect to the larger. Such a hyponym, in turn, may have further subcategories for which it is a hypernym. In the simple biology example,  dog  is a hypernym with respect to its subcategory  collie , which in turn is a hypernym with respect to  Fido  which is one of its hyponyms. Typically, however,  hypernym  is used to refer to subcategories rather than single individuals. Researchers reported that large populations consistently develop highly similar category systems. This may be relevant to lexical aspects of large communication networks and cultures such as  folksonomies  and  language  or human communication, and sense-making in general. [ 24 ] [ 25 ] Hull (1998) suggested \"The fundamental elements of any classification are its theoretical commitments, basic units and the criteria for ordering these basic units into a classification\". [ 26 ] There is a widespread opinion in knowledge organization and related fields that such classes corresponds to concepts. We can, for example, classify \"waterfowls\" into the classes \"ducks\", \"geese\", and \"swans\"; we can also say, however, that the concept “waterfowl” is a generic broader term in relation to the concepts \"ducks\", \"geese\", and \"swans\". This example demonstrates the close relationship between classification theory and concept theory. A main opponent of concepts as units is Barry Smith. [ 27 ]  Arp, Smith and Spear (2015) discuss ontologies and criticize the conceptualist understanding. [ 28 ] : 5ff   The book writes (7): “The code assigned to France, for example, is ISO 3166 – 2:FR and the code is assigned to France itself — to the country that is otherwise referred to as Frankreich or Ranska. It is not assigned to the concept of France (whatever that might be).” Smith's alternative to concepts as units is based on a realist orientation, when scientists make successful claims about the types of entities that exist in reality, they are referring to objectively existing entities which realist philosophers call universals or natural kinds. Smith's main argument - with which many followers of the concept theory agree - seems to be that classes cannot be determined by introspective methods, but must be based on scientific and scholarly research. Whether units are called concepts or universals, the problem is to decide when a thing (say a \"blackbird\") should be considered a natural class. In the case of blackbirds, for example, recent DNA analysis have reconsidered the concept (or universal) \"blackbird\" and found that what was formerly considered one species (with subspecies) are in reality many different species, which just have chosen similar characteristics to adopt to their ecological niches. [ 29 ] : 141 An important argument for considering concepts the basis of classification is that concepts are subject to change and that they changes when scientific revolutions occur. Our concepts of many birds, for example, have changed with recent development in DNA analysis and the influence of the cladistic paradigm - and have demanded new classifications. Smith's example of  France  demands an explanation. First,  France  is not a general concept, but an individual concept. Next, the legal definition of France is determined by the conventions that France has made with other countries. It is still a concept, however, as Leclercq (1978) demonstrates with the corresponding concept  Europe . [ 30 ] Hull (1998) continued: [ 26 ]  \"Two fundamentally different sorts of classification are those that reflect structural organization and those that are systematically related to historical development.\" What is referred to is that in biological classification the anatomical traits of organisms is one kind of classification, the classification in relation to the evolution of species is another (in the section below, we expand these two fundamental sorts of classification to four). Hull adds that in biological classification, evolution supplies the theoretical orientation. [ 26 ] Ereshefsky (2000) presented and discussed three general philosophical schools of classification: \"essentialism, cluster analysis, and historical classification. Essentialism sorts entities according to causal relations rather than their intrinsic qualitative features.\" [ 31 ] These three categories may, however, be considered parts of broader philosophies. Four main approaches to classification may be distinguished: (1) logical and rationalist approaches including \"essentialism\"; (2) empiricist approaches including cluster analysis (It is important to notice that empiricism is not the same as empirical study, but a certain ideal of doing empirical studies. With the exception of the logical approaches they all are based on empirical studies, but are basing their studies on different philosophical principles). (3) Historical and hermeneutical approaches including Ereshefsky's \"historical classification\" and (4) Pragmatic, functionalist and teleological approaches (not covered by Ereshefsky). In addition there are combined approaches (e.g., the so-called  evolutionary taxonomy \", which mixes historical and empiricist principles). Logical division [ 32 ]  (top-down classification or downward classification) is an approach that divides a class into subclasses and then divide subclasses into their subclasses, and so on, which finally forms a tree of classes. The root of the tree is the original class, and the leaves of the tree are the final classes. Plato advocated a method based on dichotomy, which was rejected by Aristotle and replaced by the method of definitions based on genus, species, and specific difference. [ 33 ]  The method of facet analysis (cf.,  faceted classification ) is primarily based on logical division. [ 34 ]  This approach tends to classify according to \"essential\" characteristics, a widely discussed and criticized concept (cf.,  essentialism ). These methods may overall be related to the rationalist theory of knowledge. \"Empiricism alone is not enough: a healthy advance in taxonomy depends on a sound theoretical foundation\" [ 35 ] : 548 Phenetics  or  numerical taxonomy [ 36 ]  is by contrast bottom-up classification, where the starting point is a set of items or individuals, which are classified by putting those with shared characteristics as members of a narrow class and proceeding upward. Numerical taxonomy is an approach based solely on observable, measurable similarities and differences of the things to be classified. Classification is based on overall similarity: The elements that are most alike in most attributes are classified together. But it is based on statistics, and therefore does not fulfill the criteria of logical division (e.g. to produce classes, that are mutually exclusive and jointly coextensive with the class they divide). Some people will argue that this is not classification/taxonomy at all, but such an argument must consider the definitions of classification (see above). These methods may overall be related to the empiricist theory of knowledge. Genealogical classification  is classification of items according to their common heritage. This must also be done on the basis of some empirical characteristics, but these characteristics are developed by the theory of evolution. Charles Darwin's [ 37 ]  main contribution to classification theory of not just his claim \"... all true classification is genealogical ...\" but that he provided operational guidance for classification. [ 38 ] : 90–92   Genealogical classification is not restricted to biology, but is also much used in, for example, classification of languages, and may be considered a general approach to classification.\" These methods may overall be related to the historicist theory of knowledge. One of the main schools of historical classification is  cladistics , which is today dominant in biological taxonomy, but also applied to other domains. The historical and hermeneutical approaches is not restricted to the development of the object of classification (e.g., animal species) but is also concerned with the subject of classification (the classifiers) and their embeddedness in scientific traditions and other human cultures. Pragmatic classification  (and functional [ 39 ]  and teleological classification) is the classification of items which emphasis the goals, purposes, consequences, [ 40 ]   interests, values and politics of classification. It is, for example, classifying animals into wild animals, pests, domesticated animals and pets. Also  kitchenware  (tools, utensils, appliances, dishes, and cookware used in food preparation, or the serving of food) is an example of a classification which is not based on any of the above-mentioned three methods, but clearly on pragmatic or functional criteria. Bonaccorsi, et al. (2019) is about the general theory of functional classification and applications of this approach for patent classification. [ 39 ]  Although the examples may suggest that pragmatic classifications are primitive compared to established scientific classifications, it must be considered in relation to the pragmatic and critical theory of knowledge, which consider all knowledge as influences by interests. [ 41 ] \nRidley (1986) wrote: [ 42 ] : 191   \"teleological classification. Classification of groups by their shared purposes, or functions, in life - where purpose can be identified with adaptation. An imperfectly worked-out, occasionally suggested, theoretically possible principle of classification that differs from the two main such principles,  phenetic  and  phylogenetic classification \". Natural classification is a concept closely related to the concept  natural kind .  Carl Linnaeus  is often recognized as the first scholar to clearly have differentiated \"artificial\" and \"natural\" classifications [ 43 ] [ 44 ]  A natural classification is one, using Plato's metaphor, that is “carving nature at its joints” [ 45 ]  Although Linnaeus considered natural classification the ideal, he recognized that his own system (at least partly) represented an artificial classification. John Stuart Mill explained the artificial nature of the Linnaean classification and suggested the following definition of a natural classification: \"The Linnæan arrangement answers the purpose of making us think together of all those kinds of plants, which possess the same number of stamens and pistils; but to think of them in that manner is of little use, since we seldom have anything to affirm in common of the plants which have a given number of stamens and pistils.\" [ 46 ] : 498  \"The ends of scientific classification are best answered, when the objects are formed into groups respecting which a greater number of general propositions can be made, and those propositions more important, than could be made respecting any other groups into which the same things could be distributed.\" [ 46 ] : 499   \"A classification thus formed is properly scientific or philosophical, and is commonly called a Natural, in contradistinction to a Technical or Artificial, classification or arrangement.\" [ 46 ] : 499 Ridley (1986) provided the following definitions: [ 42 ] Stamos (2004) [ 47 ] : 138   wrote: \"The fact is, modern scientists classify atoms into elements based on proton number rather than anything else because it alone is the causally privileged factor [gold is atomic number 79 in the periodic table because it has 79 protons in its nucleus]. Thus nature itself has supplied the causal monistic essentialism. Scientists in their turn simply discover and follow (where \"simply\" ≠ \"easily\").\" The  periodic table  is the classification of the chemical elements which is in particular associated with  Dmitri Mendeleev  (cf.,  History of the periodic table ). An authoritative work on this system is Scerri (2020). [ 48 ]  Hubert Feger (2001; numbered listing added) wrote about it: [ 49 ] : 1967–1968   \"A well-known, still used, and expanding classification is Mendeleev's Table of Elements. It can be viewed as a prototype of all taxonomies in that it satisfies the following evaluative criteria: Bursten (2020) wrote, however \"Hepler-Smith, a historian of chemistry, and I, a philosopher whose work often draws on chemistry, found common ground in a shared frustration with our disciplines’ emphases on the chemical elements as the stereotypical example of a natural kind. The frustration we shared was that while the elements did display many hallmarks of paradigmatic kindhood, elements were not the kinds of kinds that generated interesting challenges for classification in chemistry, nor even were they the kinds of kinds that occupied much contemporary critical chemical thought. Compounds, complexes, reaction pathways, substrates, solutions – these were the kinds of the chemistry laboratory, and rarely if ever did they slot neatly into taxonomies in the orderly manner of classification suggested by the Periodic Table of Elements. A focus on the rational and historical basis of the development of the Periodic Table had made the received view of chemical classification appear far more pristine, and far less interesting, than either of us believed it to be.\" [ 50 ] Linnaean taxonomy  is the particular form of biological classification ( taxonomy ) set up by  Carl Linnaeus , as set forth in his  Systema Naturae  (1735) and subsequent works. A major discussion in the scientific literature is whether a system that was constructed before Charles Darwin's theory of evolution can still be fruitful and reflect the development of life. [ 51 ] [ 52 ] Astronomy  is a fine example on how  Kuhn's  (1962) theory of scientific revolutions (or paradigm shifts) influences classification. [ 53 ]  For example: Hornbostel–Sachs  is a system of musical instrument classification devised by Erich Moritz von Hornbostel and Curt Sachs, and first published in 1914. [ 54 ]  In the original classification, the top categories are: A fifth top category, Each top category is subdivided and Hornbostel-Sachs is a very comprehensive classification of musical instruments with wide applications. In Wikipedia, for example, all musical instruments are organized according to this classification. In opposition to, for example, the astronomical and biological classifications presented above, the Hornbostel-Sachs classification seems very little influenced by research in  musicology  and  organology . It is based on huge collections of musical instruments, but seems rather as a system imposed upon the universe of instruments than as a system with organic connections to scholarly theory. It may therefore be interpreted as a system based on logical division and rationalist philosophy. Diagnostic and Statistical Manual of Mental Disorders  (DSM) is a classification of mental disorders published by the American Psychiatric Association (APA).The first edition of the DSM was published in 1952, [ 55 ]  and the newest, fifth edition was published in 2013. [ 56 ]  In contrast to, for example, the periodic table and the Hornbostel-Sachs classification, the principles for classification have changed much during its history. The first edition was influenced by psychodynamic theory, The DSM-III, published in 1980 [ 57 ]  adopted an atheoretical, “descriptive” approach to classification [ 58 ]   The system is very important for all people involved in psychiatry, whether as patients, researchers or therapists (in addition to insurance companies), but the systems is strongly criticized and has not the scientific status as many other classifications. [ 59 ]"
  },
  {
    "id": 15,
    "title": "Bibliometrics",
    "content": "Bibliometrics   is the application of statistical methods to the study of bibliographic data, especially in scientific and  library and information science  contexts, and is closely associated with  scientometrics  (the analysis of scientific metrics and indicators) to the point that both fields largely overlap. Bibliometrics studies first appeared in the late 19th century. They have known a significant development after the  Second World War  in a context of \"periodical crisis\" and new technical opportunities offered by computing tools. In the early 1960s, the  Science Citation Index  of  Eugene Garfield  and the citation network analysis of  Derek John de Solla Price  laid the fundamental basis of a structured research program on bibliometrics. Citation analysis  is a commonly used bibliometric method based on constructing the  citation graph , [ 1 ]  a network or graph representation of the citations shared by documents. Many research fields use bibliometric methods to explore the impact of their field, the impact of a set of researchers, the impact of a particular paper, or to identify particularly impactful papers within a specific field of research. Bibliometrics tools have been commonly integrated in  descriptive linguistics , the development of  thesauri , and evaluation of reader usage. Beyond specialized scientific use, popular web search engines, such as the  pagerank  algorithm implemented by Google have been largely shaped by bibliometrics methods and concepts. The emergence of the Web and the open science movement has gradually transformed the definition and the purpose of \"bibliometrics.\" In the 2010s historical proprietary infrastructures for citation data such as the  Web of Science  or  Scopus  have been challenged by new initiatives in favor of open citation data. The  Leiden Manifesto for Research Metrics  (2015) opened a wide debate on the use and transparency of metrics. The term  bibliométrie  was first used by  Paul Otlet  in 1934, [ 2 ] [ 3 ]  and defined as \"the measurement of all aspects related to the publication and reading of books and documents.\" [ 4 ]  The  anglicized  version  bibliometrics  was first used by Alan Pritchard in a paper published in 1969, titled \"Statistical Bibliography or Bibliometrics?\" [ 5 ]  He defined the term as \"the application of mathematics and statistical methods to books and other media of communication.\"  Bibliometrics  was conceived as a replacement for  statistical bibliography , the main label used by publications in the field until then: for Pritchard, statistical bibliography was too \"clumsy\" and did not make it very clear what was the main object of study. [ 6 ] The concept of bibliometrics \"stresses the material aspect of the undertaking: counting books, articles, publications, citations\". [ 7 ]  In theory, bibliometrics is a distinct field from  scientometrics  (from the Russian  naukometriya ), [ 8 ]  which relies on the analysis of non-bibliographic indicators of scientific activity. In practice, bibliometrics and scientometrics studies tend to use similar data sources and methods, as citation data has become the leading standard of quantitative scientific evaluation during the mid-20th century: \"insofar as bibliometric techniques are applied to scientific and technical literature, the two areas of scientometrics and bibliometrics overlap to a considerable degree.\" [ 7 ]  The development of the web and the expansion of bibliometrics approach to non-scientific production has entailed the introduction of broader labels in the 1990s and the 2000s: infometrics, webometrics or cybermetrics. [ 9 ]  These terms have not been extensively adopted, as they partly overlap with pre-existing research practices, such as information retrieval. Scientific works, studies and researches that have a bibliometric character can be identified, depending on the definition, already for the 12th century in the form of Jewish indexes. [ 10 ] Bibliometric analysis appeared at the turn of the 19th and the 20th century. [ 11 ] [ 12 ] [ 13 ] [ 14 ]  These developments predate the first occurrence of the concept of  bibiometrics  by several decades. Alternative label were commonly used:  bibliography statistics  became especially prevalent after 1920 and continued to remain in use until the end of the 1960s. [ 14 ]  Early statistical studies of scientific metadata were motivated by the significant expansion of scientific output and the parallel development of indexing services of databases that made this information more accessible in the first place. [ 15 ]  Citation index were first applied to case law in the 1860s and their most famous example,  Shepard's Citations  (first published in 1873) will serve as a direct inspiration for the  Science Citation Index  one century later. [ 16 ] The emergence of  social sciences  inspired new speculative research on the  science of science  and the possibility of studying science itself as a scientific object: \"The belief that social activities, including science, could be reduced to quantitative laws, just as the trajectory of a cannonball and the revolutions of the heavenly bodies, traces back to the positivist sociology of  Auguste Comte ,  William Ogburn , and  Herbert Spencer .\" [ 17 ]  Bibliometric analysis was not conceived as a separate body studies but one of the available methods for the quantitative analysis of scientific activity in different fields of research:  science history  ( Histoire des sciences et des savants depuis deux siècles  of  Alphonse de Candolle  in 1885,  The history of comparative anatomy, a statistical analysis of the literature  by  Francis Joseph Cole  and  Nellie B. Eales  in 1917),  bibliography  ( The Theory of National and International Bibliography  of  Francis Burburry Campbell  in 1896) or  sociology of science  ( Statistics of American Psychologists  of  James McKeen Cattell  in 1903). Early bibliometrics and scientometrics work were not simply descriptive but expressed normative views of what science should be and how it could progress. The measurement of the performance of individual researchers, scientific institutions or entire countries was a major objective. [ 15 ]  The statistical analysis of James McKeen Cattell acted as a preparatory work for a large scale evaluation of American researchers with  eugenicists  undertones:  American Men of Science  (1906), \"with its astoundingly simplistic rating system of asterisks attached to individual entries in proportion to the estimated eminence of the starred scholar.\" [ 11 ] After 1910, bibliometrics approach increasingly became the main focus in several study of scientific performance rather than one quantitative method among others. [ 18 ]  In 1917, Francis Joseph Cole and Nellie B. Eales argued in favor of the primary statistical value of publications as a publication \"is an isolated and definite piece of work, it is permanent, accessible, and may be judged, and in most cases it is not difficult to ascertain when, where, and by whom it was done, and to plot the results on squared paper.\" [ 19 ]  Five years later,  Edward Wyndham Hulme  expanded this argument to the point that publications could be considered as the standard measure of an entire civilization: \"If civilization is but the product of the human mind operating upon a shifting platform of its environment, we may claim for bibliography that it is not only a pillar in the structure of the edifice, but that it can function as a measure of the varying forces to which this structure is continuously subjected.\" [ 20 ]  This shift toward publication had a limited impact: well until the 1970s, national and international evaluation of scientific activities \"disdained bibliometric indicators\" which were deemed too simplistic, in favor of socological and economic measures. [ 21 ] Both the enhanced value attached to scientific publications as a measure of knowledge and the difficulties met by libraries to manage the growing flow of academic periodicals entailed the development of the first citation indexes. [ 22 ]  In 1927, P. Gross and E. M. Gross compiled the 3,633 references quoted by the  Journal of the American Chemical Society  during the year 1926 and ranked journals depending on their level of citation. The two authors created a set of tools and methods still commonly used by academic search engines, including attributing a bonus to recent citations since \"the  present trend  rather than the  past performance  of a journal should be considered first.\" [ 23 ]  Yet the academic environment measured was markedly different: German rather than English ranked by far the main  language of science  of chemistry with more than 50% of all references. [ 24 ] In the same period, fundamental algorithms, metrics and methods of bibliometrics were first identified in several unrelated projects, [ 25 ]  most of them being related to the structural inequalities of scientific production. In  Alfred Lotka  introduced its law of productivity from an analysis of the authored publications in the  Chemical Abstracts  and the  Geschichtstafeln der Physik : the number of authors producing an n number of contributions is equal to the 1/n^2 number of authors that only produced one publication. [ 26 ]  In, the chief librarian of the  London Science Museum ,  Samuel Bradford  derived a  law of scattering  from his experience in bibliographic indexing: there are exponentially diminishing returns of searching for references in science journals, as more and more work need to be consulted to find relevant work. Both the Lotka and Bradford law have been criticized as they are far from universal and rather uncovers a rough  power law  relationship rendered by deceivingly precise equations. [ 27 ] After the  Second World War , the growing challenge in managing and accessing scientific publications turned into a full-fledged \"periodical crisis\": existing journals could not keep up with the rapidly increasing scientific output spurred by the  big science  projects. [ 28 ] [ 8 ]  The issue became politically relevant after the successful launch of  Sputnik  in 1957: \"The Sputnik crisis turned the librarians' problem of bibliographic control into a national information crisis..\" [ 29 ]  In a context of rapid and dramatic change, the emerging field of bibliometrics was linked to large scale reforms of academic publishing and nearly utopian visions of the future of science. In 1934,  Paul Otlet  introduced under the concept of  bibliométrie  or  bibliology  an ambitious project of measuring the impact of texts on society. In contrast with the bounded definition of  bibliometrics  that will become prevalent after the 1960s, the vision of Otlet was not limited to scientific publication nor in fact to  publication  as a fundamental unit: it aimed for \"by the resolution of texts into atomic elements, or ideas, which he located in the single paragraphs (alinéa, verset, articulet) composing a book.\" [ 30 ]  In 1939  John Desmond Bernal  envisioned a network of scientific archives, which was briefly considered by the  Royal Society  in 1948: \"The scientific paper sent to the central publication office, upon approval by an editorial board of referees, would be microfilmed, and a sort of print-on-demand system set in action thereafter.\" [ 31 ]  While not using the concept of  bibliometrics , Bernal had a formative influence of leading figures of the field such as Derek John de Solla Price. The emerging computing technologies were immediately considered as a potential solution to make a larger amount of scientific output readable and searchable. During the 1950s and 1960s, an uncoordinated wave of experiments in indexing technologies resulted in the rapid development of key concepts of computing research retrieval. [ 32 ]  In 1957, IBM engineer  Hans Peter Luhn  introduced an influential paradigm of statistical-based analysis of word frequencies, as \"communication of ideas by means of words is carried out on the basis of statistical probability.\" [ 33 ]  Automated translation of non-English scientific work has also significantly contributed to fundamental research on natural language processing of bibliographic references, as in this period a significant amount of scientific publications  were not still available in English , especially the one coming from the Soviet block. Influent members of the  National Science Foundation  like  Joshua Ledeberg  advocated for the creation of a \"centralized information system\",  SCITEL , partly influenced by the ideas of John Desmond Bernal. This system would at first coexist with printed journals and gradually replace them altogether on account of its efficiency. [ 34 ]  In the plan laid out by Ledeberg to Eugen Garfield in November 1961, a centralized deposit would index as much as 1,000,000 scientific articles per year. Beyond full-text searching, the infrastructure would also ensure the indexation of citation and other metadata, as well as the automated translation of foreign language articles. [ 35 ] The first working prototype on an online retrieval system developed in 1963 by  Doug Engelbart  and Charles Bourne at the Stanford Research Institute proved the feasibility of these theoretical assumptions, although it was heavily constrained by memory issues: no more than 10,000 words of a few documents could be indexed. [ 36 ]  The early scientific computing infrastructures were focused on more specific research areas, such as  MEDLINE  for medicine, NASA/RECON for space engineering or OCLC Worldcat for library search: \"most of the earliest online retrieval system provided access to a bibliographic database and the rest used a file containing another sort of information—encyclopedia articles, inventory data, or chemical compounds.\" [ 37 ]  Exclusive focus on text analysis proved limitative as the digitized collections expanded: a query could yield a large number results and it was difficult to evaluate the relevancy and the accuracy of the results. [ 38 ] The  periodical crisis  and the limitations of index retrieval technologies motivated the development of bibliometric tools and large citation index like the  Science Citation Index  of  Eugene Garfield . Garfield's work was initially primarily concerned with the automated analysis of text work. In contrast with ongoing work largely focused on internal semantic relationship, Garfield highlighted \"the importance of metatext in discourse analysis\", such as introductory sentences and bibliographic references. [ 39 ]  Secondary forms of scientific production like literature reviews and bibliographic notes became central to Garfield's vision as they have already been to  John Desmond Bernal 's vision of scientific archives. [ 40 ]  By 1953, Garfield's attention was permanently shifted to citation analysis: in a private letter to  William C. Adair , the vice-president of the publisher of the  Shepard's Citation  index, \"he suggested a well tried solution to the problem of automatic indexing, namely to \"shepardize\" biomedical literature, to untangle the skein of its content by following the thread of citation links in the same way the legal citator did with court sentences.\" [ 41 ]  In 1955, Garfield published his seminal article \"Citation Indexes for Science\", that both laid out the outline of the Science Citation Index and had a large influence on the future development of bibliometrics. [ 41 ]  The general citation index envisioned by Garfield was originally one of the building block of the ambitious plan of Joshua Lederberg to computerize scientific literature. [ 42 ]  Due to lack of funding, the plan was never realized. [ 43 ]  In 1963, Eugene Garfield created the  Institute for Scientific Information  that aimed to transform the projects initially envisioned with Lederberg into a profitable business. The field of bibliometrics coalesced in parallel to the development of the Science Citation Index, that was to become its fundamental infrastructure and data resource: [ 44 ]  \"while the early twentieth century contributed methods that were necessary for measuring research, the mid-twentieth century was characterized by the development of institutions that motivated and facilitated research measurement.\" [ 45 ]  Significant influences of the nascent field included along with John Desmond Bernal, Paul Otlet the sociology of science of  Robert K. Merton , that was re-interpreted in a non-ethic manner: the  Matthew Effect , that is the increasing concentration of attention given to researchers that were already notable, was no longer considered  as a derive (?) but a feature of normal science. [ 46 ] A follower of Bernal, the British historian of science  Derek John de Solla Price  has had a major impact on the disciplinary formation of bibliometrics: with \"the publication of  Science Since Babylon  (1961),  Little Science, Big Science  (1963), and  Networks of Scientific Papers  (1965) by Derek Price, scientometrics already had a sound empirical and conceptual toolkit available.\" [ 44 ]  Price was a proponent of  bibliometric reductionism . [ 47 ]  As Francis Joseph Cole and Nellie B. Eales in 1917, he argued that a publication is the best possible standard to lay out a quantitative study of science: they \"resemble a pile of bricks (…) to remain in perpetuity as an intellectual edifice built by skill and artifice, resting on primitive foundation.\" [ 48 ]  Price doubled down on this reductionist approach by limiting in turn the large set of existing bibliographic data to citation data. Price's framework, like Garfield's, takes for granted the structural inequality of science production, as a minority of researchers creates a large share of publication and an even smaller share have a real measurable impact on subsequent research (with as few as 2% of papers having 4 citations or more at the time). [ 49 ]  Despite the unprecedented growth of post-war science, Price claimed for the continued existence of an  invisible college  of elite scientists that, as in the time of  Robert Boyle  undertook the most valuable work. [ 50 ]  While Price was aware of the power relationships that ensured the domination of such an elite, there was a fundamental ambiguity in the bibliometrics studies, that highlighted the concentration of academic publishing and prestige but also created tools, models and metrics that normalized pre-existing inequalities. [ 50 ]  The central position of the  Scientific Citation Index  amplified this performative effect. In the end of the 1960s Eugene Garfield formulated a  law of concentration  that was formally a reinterpretation of the  Samuel Bradford 's  law of scattering , with a major difference: while Bradford talked for the perspective of a specific research project, Garfield drew a generalization of the law to the entire set of scientific publishing: \"the core literature for all scientific disciplines involves a group of no more than 1000 journals, and may involve as few as 500.\" Such law was also a justification of the practical limitation of the citation index to a limited subset of  core  journals, with the underlying assumption that any expansion into second-tier journals would yield diminishing returns. [ 51 ]  Rather than simply observing structural trends and patterns, bibliometrics tend to amplify and stratify them even further: \"Garfield's citation indexes would have brought to a logical completion, the story of a stratified scientific literature produced by (…) a few, high-quality, \"must-buy\" international journals owned by a decreasing number of multinational corporations ruling the roost in the global information market.\" [ 52 ] Under the impulsion of Garfield and Price, bibliometrics became both a research field and a testing ground for quantitative policy evaluation of research. This second aspect was not a major focus of the Science Citation Index has been a progressive development: the famous  Impact Factor  was originally devised in the 1960s by Garfield and Irving Sher to select the core group of journals that were to be featured in  Current Contents  and the Science Citation Index and was only regularly published after 1975. [ 53 ]  The metric itself is a very simple ratio between the total count of citation received by the journal on the past year and its productivity on the past two years, to ponderate the prolificity of some publications. [ 54 ]  For example,  Nature  had an impact factor of 41.577 in 2017: [ 55 ] IF \n \n \n 2017 \n \n \n = \n \n \n \n \n Citations \n \n \n 2017 \n \n \n \n \n \n Publications \n \n \n 2016 \n \n \n + \n \n \n Publications \n \n \n 2015 \n \n \n \n \n \n = \n \n \n 74090 \n \n 880 \n + \n 902 \n \n \n \n = \n 41.577. \n \n \n {\\displaystyle {\\text{IF}}_{2017}={\\frac {{\\text{Citations}}_{2017}}{{\\text{Publications}}_{2016}+{\\text{Publications}}_{2015}}}={\\frac {74090}{880+902}}=41.577.} The simplicity of the impact factor has likely been a major factor in its wide adoption by scientific institutions, journals, funders or evaluators: \"none of the revised versions or substitutes of ISI IF has gained general acceptance beyond its proponents, probably because the alleged alternatives lack the degree of interpretability of the original measure.\" [ 56 ] Alongside these simplified measurements, Garfield continued to support and fund fundamental research in science history and sociology of science. First published 1964,  The Use of Citation Data in Writing the History of Science  compiles several experimental case studies relying on the citation network of the Science Citation Index, including a quantitative reconstruction of the discovery of the DNA. [ 57 ]  Interest in this area persisted well after the sell of the Index to Thomson Reuters: as late as 2001, Garfield unveiled  HistCite , a software for \"algorithmic historiography\" created in collaboration with Alexander Pudovkin, and Vladimir S. Istomin. [ 58 ] The development of the  World Wide Web  and the  Digital Revolution  had a complex impact on bibliometrics. The web itself and some of its key components (such as search engines) were partly a product of bibliometrics theory. In its original form, it was derived from a bibliographic scientific infrastructure commissioned to  Tim Berners-Lee  by the  CERN  for the specific needs of high energy physics,  ENQUIRE . The structure of ENQUIRE was closer to an internal web of data: it connected \"nodes\" that \"could refer to a person, a software module, etc. and that could be interlined with various relations such as made, include, describes and so forth.\" [ 59 ]  Sharing of data and data documentation was a major focus in the initial communication of the World Wide Web when the project was first unveiled in August 1991 : \"The WWW project was started to allow high energy physicists to share data, news, and documentation. We are very interested in spreading the web to other areas, and having gateway servers for other data.\" [ 60 ]  The web rapidly superseded pre-existing online infrastructure, even when they included more advanced computing features. [ 61 ]  The core value attached to hyperlinking in the design of the web seem to validate the intuitions of the funding figures of bibliometrics: \"The onset of the World Wide Web in the mid-1990s made Garfield's citationist dream more likely to come true. In the world network of hypertexts, not only is the bibliographic reference one of the possible forms taken by a hyperlink inside the electronic version of a scientific article, but the Web itself also exhibits a citation structure, links between web pages being formally similar to bibliographic citations.\" [ 62 ]  Consequently, bibliometrics concepts have been incorporated in major communication technologies the search algorithm of Google: \"the citation-driven concept of relevance applied to the network of hyperlinks between web pages would revolutionize the way Web search engines let users quickly pick useful materials out of the anarchical universe of digital information.\" [ 63 ] While the web expanded the intellectual influence of bibliometrics way beyond specialized scientific research, it also shattered the core tenets of the field. In contrast with the wide utopian visions of Bernal and Otlet that partly inspired it, the Science Citation Index was always conceived as a closed infrastructure, not only from the perspective of their users but also from the perspective of the collection index: the logical conclusion of Price's theory of  invisible college  and Garfield's law of concentration was to focus exclusively on a limited set of core scientific journals. With the rapid expansion of the Web, numerous forms of publications (notably preprints), scientific activities and communities suddenly became visible and highlighted by contrast the limitations of applied bibliometrics. [ 64 ]  The other fundamental aspect of bibliometric reductionism, the exclusive focus on citation, has also been increasingly fragilized by the multiplication of alternative data sources and the unprecedented access to full text corpus that made it possible to revive the large scale semantic analysis first envisioned by Garfield in the early 1950s: \"Links alone, then, just like bibliographic citations alone, do not seem sufficient to pin down critical communication patterns on the Web, and their statistical analysis will probably follow, in the years to come, the same path of citation analysis, establishing fruitful alliances with other emerging qualitative and quantitative outlooks over the web landscape.\" [ 65 ] The close relationship between bibliometrics and commercial vendors of citation data and indicators has become more strained since the 1990s. Leading scientific publishers have diversified their activities beyond publishing and moved \"from a content-provision to a data analytics business.\" [ 66 ]  By 2019, Elsevier has either acquired or built a large portofolio platforms, tools, databases and indicators covering all aspects and stages of scientific research: \"the largest supplier of academic journals is also in charge of evaluating and validating research quality and impact (e.g., Pure, Plum Analytics, Sci Val), identifying academic experts for potential employers (e.g., Expert Lookup5), managing the research networking platforms through which to collaborate (e.g., SSRN, Hivebench, Mendeley), managing the tools through which to find funding (e.g., Plum X, Mendeley, Sci Val), and controlling the platforms through which to analyze and store researchers' data (e.g., Hivebench, Mendeley).\" [ 67 ]  Metrics and indicators are key components of this vertical integration: \"Elsevier's further move to offering metrics-based decision making is simultaneously a move to gain further influence in the entirety of the knowledge production process, as well as to further monetize its disproportionate ownership of content.\" [ 68 ]  The new market for scientific publication and scientific data has been compared with the business models of social networks, search engines and other forms of  platform capitalism [ 69 ] [ 70 ] [ 71 ]  While content access is free, it is indirectly paid through data extraction and surveillance. [ 72 ]  In 2020, Rafael Ball envisioned a bleak future for bibliometricians where their research contribute to the emerge of a highly invasive form of \"surveillance capitalism\":scientists \"be given a whole series of scores which not only provide a more comprehensive picture of the academic performance, but also the perception, behaviour, demeanour, appearance and (subjective) credibility (…) In China, this kind of personal data analysis is already being implemented and used simultaneously as an incentive and penalty system.\" [ 73 ] The  Leiden manifesto for research metrics  (2015) highlighted the growing rift between the commercial providers of scientific metrics and bibliometric communities. The signatories stressed the potential social damage of uncontrolled metric-based evaluation and surveillance: \"as scientometricians, social scientists and research administrators, we have watched with increasing alarm the pervasive misapplication of indicators to the evaluation of scientific performance.\" [ 74 ]  Several structural reforms of bibliometric research and research evaluation are proposed, including a stronger reliance on qualitative assessment and the reliance on \"open, transparent and simple\"  data collection . [ 74 ]  The Leiden Manifesto has stirred an important debate in bibliometrics/scientometrics/infometrics with some critics arguing that the elaboration of quantitative metrics bears no responsibility on their misuse in commercial platforms and research evaluation. [ 75 ] Historically, bibliometric methods have been used to trace relationships amongst  academic journal   citations .  Citation analysis , which involves examining an item's referring documents, is used in searching for materials and analyzing their merit. [ 76 ]   Citation indices , such as  Institute for Scientific Information 's  Web of Science , allow users to search forward in time from a known article to more recent publications which cite the known item. [ 3 ] Data from citation indexes can be analyzed to determine the popularity and impact of specific articles, authors, and publications. [ 77 ] [ 78 ]  Using citation analysis to gauge the importance of one's work, for example, has been common in hiring practices of the late 20th century. [ 79 ] [ 80 ]  Information scientists also use citation analysis to quantitatively assess the core journal titles and watershed publications in particular disciplines; interrelationships between authors from different institutions and schools of thought; and related data about the sociology of academia. Some more pragmatic applications of this information includes the planning of retrospective  bibliographies , \"giving some indication both of the age of material used in a discipline, and of the extent to which more recent publications supersede the older ones\"; indicating through high frequency of citation which documents should be archived; comparing the coverage of secondary services which can help publishers gauge their achievements and competition, and can aid librarians in evaluating \"the effectiveness of their stock.\" [ 81 ]  There are also some limitations to the value of citation data. They are often incomplete or biased; data has been largely collected by hand (which is expensive), though citation indexes can also be used; incorrect citing of sources occurs continually; thus, further investigation is required to truly understand the rationale behind citing to allow it to be confidently applied. [ 82 ] Bibliometrics can be used for understanding the research hot topics, for example, in housing Bibliometrics, the results show that Keywords such as influencing factors of housing prices, supply and demand analysis, policy impact on housing prices, and regional city trends are commonly found in housing price research literature. Recent popular keywords include regression analysis and house price predictions. The USA has been a pioneer in housing price research, with well-established means and methods leading the way in this field. Developing countries, on the other hand, need to adopt innovative research approaches and focus more on sustainability in their housing price studies. Research indicates a strong correlation between housing prices and the economy, with keywords like gross domestic product, interest rates, and currency frequently appearing in economic-related cluster analyses. [ 83 ] Bibliometrics are now used in quantitative research assessment exercises of academic output which is starting to threaten practice based research. [ 84 ]  The UK government has considered using bibliometrics as a possible auxiliary tool in its  Research Excellence Framework , a process which will assess the quality of the research output of UK universities and on the basis of the assessment results, allocate research funding. [ 85 ]  This has met with significant skepticism and, after a pilot study, looks unlikely to replace the current peer review process. [ 86 ]  Furthermore, excessive usage of bibliometrics in assessment of value of academic research encourages  gaming the system  in various ways including publishing large quantity of works with low new content (see  least publishable unit ), publishing premature research to satisfy the numbers, focusing on popularity of the topic rather than scientific value and author's interest, often with detrimental role to research. [ 87 ]  Some of these phenomena are addressed in a number of recent initiatives, including the  San Francisco Declaration on Research Assessment . Guidelines have been written on the using of bibliometrics in academic research, in disciplines such as Management, [ 88 ]  Education, [ 89 ]  and Information Science. [ 90 ]  Other bibliometrics applications include: creating thesauri; measuring term frequencies; as metrics in  scientometric  analysis, exploring  grammatical  and  syntactical  structures of texts; measuring usage by readers; quantifying value of online media of communication; in the context of technological trend analyses; [ 91 ]  measuring Jaccard distance cluster analysis and text mining based on binary logistic regression. [ 92 ] [ 93 ] In the context of the  big deal  cancellations by several library systems in the world, [ 94 ]  data analysis tools like  Unpaywall Journals  are used by libraries to assist with big deal cancellations: libraries can avoid subscriptions for materials already served by instant  open access  via  open archives  like PubMed Central. [ 95 ] The  open science  movement has been acknowledged as the most important transformation faced by bibliometrics since the emergence of the field in the 1960s. [ 96 ] [ 97 ]  The free sharing of a wide variety of scientific outputs on the web affected the practice of bibliometrics at all levels: the definition and the collection of the data, infrastructure, and metrics. Before the crystallization of the field around the Science Citation Index and the reductionist theories of Derek de Solla Price, bibliometrics has been largely influenced by utopian projects of enhanced knowledge sharing beyond specialized academic communities. The scientific networks envisioned by Paul Otlet or John Desmond Bernal have gained a new relevancy with the development of the Web: \"The philosophical inspiration of the pioneers in pursuing the above lines of inquiry, however, faded gradually into the background (…) Whereas Bernal's input would eventually find an ideal continuation in the open access movement, the citation machine set into motion by Garfield and Small led to the proliferation of sectorial studies of a fundamentally empirical nature.\" [ 98 ] In the early developments, the open science movement partly co-opted the standard tools of bibliometrics and quantitative evaluation: \"the fact that no reference was made to metadata in the main OA declarations (Budapest, Berlin, Bethesda) has led to a paradoxical situation (…) it was through the use of the Web of Science that OA advocates were eager to show how much accessibility led to a citation advantage compared to paywalled articles.\" [ 99 ]  After 2000, an important bibliometric literature was devoted to the citation advantage of open access publications. [ 100 ] By the end of the 2000s, the impact factor and other metrics have increasingly held responsible a systemic  locked-in  of prestigious non-accessible sources. Key figures of the open science movement like  Stevan Harnad  called for the creation of \"open access scientometrics\" that would take \"advantage of the wealth of usage and impact metrics enabled by the multiplication of online, full-text, open access digital archives.\" [ 101 ]  As the public of open science expanded beyond academic circles, new metrics should aim for \"measuring the broader societal impacts of scientific research.\" [ 102 ] The concept of  alt-metrics  was introduced in 2009 by  Cameron Neylon  and  Shirly Wu  as  article-level metrics . [ 103 ]  In contrast with the focus of leading metrics on journals (impact factor) or, more recently, on individual researchers (h-index), the article-level metrics makes it possible to track the circulation of individual publications: \"(an) article that used to live on a shelf now lives in  Mendeley ,  CiteULike , or  Zotero  – where we can see and count it\" [ 104 ]  As such they are more compatible with the diversity of publication strategies that has characterized open science: preprints, reports or even non-textual outputs like dataset or software may also have associated metrics. [ 102 ]  In their original research proposition, Neylon and Wu favored the use of data from  reference management software  like Zotero or Mendeley. [ 103 ]  The concept of  altmetrics  evolved and came to encover data extracted \"from social media applications, like blogs, Twitter, ResearchGate and Mendeley.\". [ 102 ]  Social media sources proved especially to be more reliable on a long-term basis, as specialized academic tools like Mendeley came to be integrated into a proprietary ecosystem developed by leading scientific publishers. Major altmetrics indicators that emerged in the 2010s include  Altmetric.com ,  PLUMx  and  ImpactStory . As the meaning of altmetrics shifted, the debate over the positive impact of the metrics evolved toward their redefinition in an open science ecosystem: \"Discussions on the misuse of metrics and their interpretation put metrics themselves in the center of open science practices.\" [ 105 ]  While altmetrics were initially conceived for open science publications and their expanded circulation beyond academic circles, their compatibility with the emerging requirements for open metrics has been brought into question: social network data, in particular, is far from transparent and readily accessible. [ 106 ] [ 107 ]  In 2016, Ulrich Herb published a systematic assessment of the leading publications' metrics in regard to open science principles and concluded that \"neither citation-based impact metrics nor alternative metrics can be labeled open metrics. They all lack scientific foundation, transparency and verifiability.\" [ 108 ] Herb laid an alternative program for open metrics that have yet to be developed. [ 110 ] [ 111 ]  The main criteria included: This definition has been implemented in research programs, like ROSI ( Reference implementation for open scientometric indicators ). [ 112 ]  In 2017, the European Commission Expert Group on Altmetrics expanded the open metrics program of Ulrich Herb under a new concept, the  Next-generation metrics . These metrics should be managed by \"open, transparent and linked data infrastructure\". [ 113 ]  The expert group underline that not everything should be measured and not all metrics are relevants: \"Measure what matters: the next generation of metrics should begin with those qualities and impacts that European societies most value and need indices for, rather than those which are most easily collected and measure\". [ 113 ] Until the 2010s, the impact of open science movement was largely limited to scientific publications: it \"has tended to overlook the importance of social structures and systemic constraints in the design of new forms of knowledge infrastructures.\" [ 114 ]  In 1997, Robert D. Cameron called for the development of an open databases of citation that would completely alter the condition of science communication: \"Imagine a universal bibliographic and citation database linking every scholarly work ever written—no matter how published—to every work that cites and every work that cites it. Imagine that such a citation database was freely available over the Internet and was updated every day with all the new works published that day, including papers in traditional and electronic journals, conference papers, theses, technical reports, working papers, and preprints.\" [ 115 ]  Despite the development of specific indexes focused on open access works like  CiteSeer , a large open alternative to the Science Citation Index failed to materialize. The collection of citation data, remained dominated by large commercial structure such as the direct descendant of the Scientific Citation Index, the  Web of Science . This had the effect of maintaining the emerging ecosystem of open resources at the periphery of academic networks: \"common pool of resources is not governed or managed by the current scholarly commons initiative. There is no dedicated hard infrastructure and though there may be a nascent community, there is no formal membership.\" [ 116 ] Since 2015, open science infrastructures, platforms and journals have converged to the creation of digital academic commons, increasingly structured around a shared ecosystem of services and standards has emerged through the network of dependencies from one infrastructure to another. This movement stem from an increasingly critical stance toward leading proprietary databases. In 2012, the San Francisco Declaration on Research Assessment (DORA) called for \"ending the use of journal impact factors in funding, hiring and promotion decisions.\" [ 117 ]  The Leiden Manifesto for research metrics (2015) encouraged the development of \"open, transparent and simple\" data collection. [ 74 ] Collaborations between academic and non-academic actors collectively committed in the creation and maintenance of  knowledge commons  has been a determining factor in the creation of new infrastructure for open citation data. Since 2010, a dataset of open citation data, the  Open Citation Corpus , has been collected by several researchers from a variety of open access sources (including PLOS and Pubmed). [ 118 ]  This collection was the initial kernel of the  Initiative for OpenCitations , incepted in 2017 in response to issues of data accessibility faced by a  Wikimedia  project,  Wikidata . A conference, given by Dario Taraborelli, head of research at the  Wikimedia Foundation  showed that only 1% of papers in  Crossref  had citations metadata that were freely available and references stored on Wikidata were unable to include the very large segment of non-free data. This coverage expanded to more than half of the recorded papers, when Elsevier finally joined the initiative in January 2021. [ 119 ] Since 2021,  OpenAlex  has become a major  open infrastructure  for scientific metadata. Initially created as a replacement for the discontinued  Microsoft Academic Graph , OpenAlex indexed in 2022 209 millions of scholarly works from 213 millions authors as well as their associated institutions, venues and concepts in a knowledge graph integrated into the semantic web (and  Wikidata ). [ 120 ]  Due to its large coverage and large amount of data properly migrated from the Microsoft Academic Graph (MAG), OpenAlex \"seems to be at least as suited for bibliometric analyses as MAG for publication years before 2021.\" [ 121 ]  In 2023, a study on the coverage of  data journals  in scientific indexes found that OpenAlex, along with Dimensions, \"enjoy a strong advantage over the two more traditional databases, WoS and Scopus\"  [ 122 ]  and is overall especially suited for the indexation of non-journal publications like books [ 123 ]  or from researchers in non-western countries [ 124 ] The opening of science data has been a major topic of debate in the bibliometrics and scientometrics community and had wide range social and intellectual consequences. In 2019, the entire scientific board of the  Journal of Infometrics  resigned and created a new open access journals,  Quantitative Science Studies . The journal was published by  Elsevier  since 2007 and the members of the board were increasingly critical of the lack of progress in the open sharing of open citation data: \"Our field depends on high-quality scientific metadata. To make our science more robust and reproducible, these data must be as open as possible. Therefore, our editorial board was deeply concerned with the refusal of Elsevier to participate in the Initiative for Open Citations (I4OC).\" [ 125 ] The unprecedented availability of a wide range of scientific productions (publications, data, software, conference, reviews...) has entailed a more dramatic redefinition of the bibliometrics project. For new alternative works anchored in the open science landscape, the principles of bibliometrics as defined by Garfield and Price in the 1960s need to be rethought. The pre-selection of a limited corpus of important journals seem neither necessary nor appropriate. In 2019, the proponents of the Matilda project, \"do not want to just \"open\" the existing closed information, but wish to give back a fair place to the whole academic content that has been excluded from such tools, in a \"all texts are born equal\" fashion.\" [ 126 ]  They aim to \"redefine bibliometrics tools as a technology\" by focusing on the exploration and mapping of scientific corpus. [ 127 ] Issues of inclusivity and more critical approach of structural inequalities in science have become more prevalent in scientometrics and bibliometrics, especially in relation to gender imbalance. [ 128 ] [ 129 ] [ 130 ]  After 2020, one of the most heated debate in the field [ 131 ]  revolved around the reception of a study on the gender imbalance in fundamental physics. [ 132 ] The structural shift in the definition of bibliometrics, scientometrics or infometrics has entailed the need for alternative labels. The concept of  Quantitative Science Studies  was originally introduced in the late 2000s in the context of a renewed critical assessment of classic bibliometric findings. [ 133 ]  It has become more prevalent in the late 2010s. After leaving  Elsevier , the editors of the  Journal of Infometrics  opted for this new label and created a journal for  Quantitative Science Studies . The first editorial removed all references to metric and aimed for a wider inclusion of quantitative and qualitative research on the science of science: We hope that those who identify under labels such as scientometrics, science of science, and metascience will all find a home in QSS. We also recognize the diverse range of disciplines for whom science is an object of study: We welcome historians of science, philosophers of science, and sociologists of science to our journal. While we bear the moniker of quantitative, we are inclusive of a breadth of epistemological perspectives. Quantitative science studies cannot operate in isolation: Robust empirical work requires the integration of theories and insights from all metasciences. [ 134 ]"
  },
  {
    "id": 16,
    "title": "Classification",
    "content": "Classification  is the activity of assigning objects to some pre-existing classes or categories. This is distinct from the task of establishing the classes themselves (for example through  cluster analysis ). [ 1 ]  Examples include diagnostic tests, identifying spam emails and deciding whether to give someone a driving license. As well as 'category', synonyms or near-synonyms for 'class' include 'type', 'species', 'order', 'concept', 'taxon', 'group', 'identification' and 'division'. The meaning of the word 'classification' (and its synonyms) may take on one of several related meanings. It may encompass both classification and the creation of classes, as for example in 'the task of categorizing pages in Wikipedia'; this overall activity is listed under  Taxonomy . It may refer exclusively to the underlying scheme of classes (which otherwise may be called a taxonomy). Or it may refer to the label given to an object by the classifier. Classification is a part of many different kinds of activities and is studied from many different points of view including  medicine ,  philosophy ,  law ,  anthropology ,  biology ,  taxonomy ,  cognition ,  communications ,  knowledge organization ,  psychology ,  statistics ,  machine learning ,  economics  and  mathematics . Methodological work aimed at improving the accuracy of a classifier is commonly divided between cases where there are exactly two classes ( binary classification ) and cases where there are three or more classes ( multiclass classification ). Unlike in  decision theory , it is assumed that a classifier repeats the classification task over and over. And unlike a  lottery , it is assumed that each classification can be either right or wrong; in the theory of measurement, classification is understood as measurement against a  nominal  scale. Thus it is possible to try to measure the accuracy of a classifier. Measuring the accuracy of a classifier allows a choice to be made between two alternative classifiers. This is important both when developing a classifier and in choosing which classifier to deploy. There are however many different methods for evaluating the accuracy of a classifier and no general method for determining which method should be used in which circumstances. Different fields have taken different approaches, even in binary classification. In  pattern recognition , error rate is popular. The  Gini coefficient  and KS statistic are widely used in the credit scoring industry.  Sensitivity and specificity  are widely used in epidemiology and medicine.  Precision and recall  are widely used in information retrieval. [ 2 ] Classifier accuracy depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems (a phenomenon that may be explained by the  no-free-lunch theorem )."
  },
  {
    "id": 17,
    "title": "Censorship",
    "content": "Censorship  is the suppression of  speech , public communication, or other  information . This may be done on the basis that such material is considered objectionable, harmful, sensitive, or \"inconvenient\". [ 2 ] [ 3 ] [ 4 ]  Censorship can be conducted by  governments , [ 5 ]  private institutions. [ 6 ]  When an individual such as an author or other creator engages in censorship of their own works or speech, it is referred to as  self-censorship . General censorship occurs in a variety of different media, including speech, books, music, films, and other arts,  the press , radio, television, and the Internet for a variety of claimed reasons including  national security , to control  obscenity ,  pornography , and  hate speech , to protect children or other vulnerable groups, to promote or restrict political or religious views, and to prevent  slander  and  libel . Specific rules and regulations regarding censorship vary between  legal jurisdictions  and/or private organizations. In 399 BC, Greek philosopher,  Socrates , while defying attempts by the Athenian state to censor his philosophical teachings, was accused of collateral charges related to the corruption of Athenian youth and sentenced to death by drinking a poison,  hemlock . The details of Socrates's conviction are recorded by Plato as follows. In 399 BC, Socrates went on  trial [ 8 ]  and was subsequently found guilty of both corrupting the minds of the youth of Athens and of  impiety  ( asebeia , [ 9 ]  \"not believing in the gods of the state\"), [ 10 ]  and as a punishment sentenced to death, caused by the drinking of a mixture containing hemlock. [ 11 ] [ 12 ] [ 13 ] Socrates' student,  Plato , is said to have advocated censorship in his essay on  The Republic , which opposed the existence of democracy. In contrast to Plato, Greek playwright  Euripides  (480–406 BC) defended the true liberty of freeborn men, including the right to speak freely. In 1766,  Sweden became the first country to abolish censorship by law . [ 14 ] Censorship has been criticized throughout history for being unfair and hindering progress. [ citation needed ]  In a 1997 essay on Internet censorship, social commentator Michael Landier explains that censorship is counterproductive as it prevents the censored topic from being discussed. Landier expands his argument by claiming that those who impose censorship must consider what they censor to be true, as individuals believing themselves to be correct would welcome the opportunity to disprove those with opposing views. [ 15 ] Censorship is often used to impose moral values on society, as in the censorship of material considered obscene. English novelist  E. M. Forster  was a staunch opponent of censoring material on the grounds that it was obscene or immoral, raising the issue of moral subjectivity and the constant changing of moral values. When the 1928 novel  Lady Chatterley's Lover  was  put on trial in 1960 , Forster wrote: [ 16 ] Lady Chatterley's Lover  is a literary work of importance...I do not think that it could be held obscene, but am in a difficulty here, for the reason that I have never been able to follow the legal definition of obscenity. The law tells me that obscenity may deprave and corrupt, but as far as I know, it offers no definition of depravity or corruption. Proponents have sought to justify it using different rationales for various types of information censored: In wartime, explicit censorship is carried out with the intent of preventing the release of information that might be useful to an enemy. Typically it involves keeping times or locations secret, or delaying the release of information (e.g., an operational objective) until it is of no possible use to enemy forces. The moral issues here are often seen as somewhat different, as the proponents of this form of censorship argue that the release of tactical information usually presents a greater risk of casualties among one's own forces and could possibly lead to loss of the overall conflict. [ citation needed ] During  World War I  letters written by British soldiers would have to go through censorship. This consisted of officers going through letters with a black marker and crossing out anything which might compromise operational secrecy before the letter was sent. [ 22 ]  The  World War II  catchphrase \" Loose lips sink ships \" was used as a common justification to exercise official wartime censorship and encourage individual restraint when sharing potentially sensitive information. [ 23 ] An example of \" sanitization \" policies comes from the  USSR  under  Joseph Stalin , where publicly used photographs were often altered to remove people whom Stalin had condemned to execution. Though past photographs may have been remembered or kept, this deliberate and systematic alteration to all of history in the public mind is seen as one of the central themes of  Stalinism  and  totalitarianism . [ citation needed ] Censorship is occasionally carried out to aid authorities or to protect an individual, as with some kidnappings when attention and media coverage of the victim can sometimes be seen as unhelpful. [ 24 ] Censorship by religion is a form of censorship where  freedom of expression  is controlled or limited using religious authority or on the basis of the teachings of the  religion . [ 25 ]  This form of censorship has a long history and is practiced in many societies and by many religions. Examples include the  Galileo affair ,  Edict of Compiègne , the  Index Librorum Prohibitorum  (list of prohibited books) and the condemnation of  Salman Rushdie 's novel  The Satanic Verses  by  Iranian  leader  Ayatollah Ruhollah Khomeini . Images of the Islamic figure Muhammad are also regularly censored. In some secular countries, this is sometimes done to prevent hurting religious sentiments. [ 26 ] The content of school textbooks is often an issue of debate, since their target audiences are young people. The term  whitewashing  is commonly used to refer to revisionism aimed at glossing over difficult or questionable historical events, or a biased presentation thereof. The  reporting of military atrocities in history  is extremely controversial, as in the case of  the Holocaust  (or  Holocaust denial ),  Bombing of Dresden , the  Nanking Massacre  as found with  Japanese history textbook controversies , the  Armenian genocide , the  Tiananmen Square protests of 1989 , and the  Winter Soldier Investigation  of the  Vietnam War . In the context of secondary school education, the way facts and history are presented greatly influences the interpretation of contemporary thought, opinion and socialization. One argument for censoring the type of information disseminated is based on the inappropriate quality of such material for the younger public. The use of the \"inappropriate\" distinction is in itself controversial, as it changed heavily. A Ballantine Books version of the book  Fahrenheit 451  which is the version used by most school classes [ 27 ]  contained approximately 75 separate edits, omissions, and changes from the original Bradbury manuscript. In February 2006, a  National Geographic  cover was censored by the  Nashravaran Journalistic Institute . The offending cover was about the subject of  love  and a picture of an embracing couple was hidden beneath a white sticker. [ 28 ] Economic induced censorship is a type of censorship enacted by economic markets to favor, and disregard, types of information. Economic induced censorship is also caused by market forces which privatize and establish commodification of certain information that is not accessible by the general public, primarily because of the cost associated with commodified information such as academic journals, industry reports and pay to use repositories. [ 29 ] The concept was illustrated as a censorship pyramid [ 30 ]  that was conceptualized by primarily  Julian Assange , along with  Andy Müller-Maguhn ,  Jacob Appelbaum  and  Jérémie Zimmermann , in the  Cypherpunks (book) . Self-censorship is the act of censoring or  classifying  one's own discourse. This is done out of fear of, or deference to, the sensibilities or preferences (actual or perceived) of others and without overt pressure from any specific party or institution of authority. Self-censorship is often practiced by  film producers ,  film directors ,  publishers ,  news anchors ,  journalists ,  musicians , and other kinds of  authors  including individuals who use  social media . [ 32 ] According to a  Pew Research Center  and the  Columbia Journalism Review  survey, \"About one-quarter of the local and national journalists say they have purposely avoided newsworthy stories, while nearly as many acknowledge they have softened the tone of stories to benefit the interests of their news organizations. Fully four-in-ten (41%) admit they have engaged in either or both of these practices.\" [ 33 ] Threats to media freedom have shown a significant increase in Europe in recent years, according to a study published in April 2017 by the  Council of Europe .\nThis results in a fear of physical or psychological violence, and the ultimate result is self-censorship by journalists. [ 34 ] Copy approval is the right to read and amend an article, usually an interview, before publication. Many publications refuse to give copy approval but it is increasingly becoming common practice when dealing with publicity anxious celebrities. [ 35 ]  Picture approval is the right given to an individual to choose which photos will be published and which will not.  Robert Redford  is well known for insisting upon picture approval. [ 36 ]  Writer approval is when writers are chosen based on whether they will write flattering articles or not. Hollywood publicist Pat Kingsley is known for banning certain writers who wrote undesirably about one of her clients from interviewing any of her other clients. [ citation needed ] Flooding the public, often through online  social networks , with false or misleading information is sometimes called \"reverse censorship\". American legal scholar  Tim Wu  has explained that this type of information control, sometimes by  state actors , can \"distort or drown out disfavored speech through the creation and dissemination of  fake news , the payment of fake commentators, and the deployment of propaganda  robots .\" [ 37 ] Soft, or indirect, censorship is the practice of influencing news coverage by applying financial pressure on media companies that are deemed critical of a government or its policies and rewarding media outlets and individual journalists who are seen as friendly to the government. [ 38 ] Book censorship can be enacted at the national or sub-national level, and can carry legal penalties for their infraction. Books may also be challenged at a local, community level. As a result, books can be removed from schools or libraries, although these bans do not typically extend outside of that area. Aside from the usual justifications of pornography and  obscenity , some films are censored due to changing racial attitudes or  political correctness  in order to avoid  ethnic stereotyping  and/or ethnic offense despite its historical or artistic value. One example is the still withdrawn \" Censored Eleven \" series of animated cartoons, which may have been innocent then, but are \"incorrect\" now. [ 39 ] Film censorship is carried out by various countries. Film censorship is achieved by censoring the producer or restricting a state citizen. For example, in China the film industry censors  LGBT-related films . Filmmakers must resort to finding funds from international investors such as the \"Ford Foundations\" and or produce through an independent film company. [ 40 ] Music censorship has been implemented by states, religions, educational systems, families, retailers and lobbying groups – and in most cases they violate international conventions of human rights. [ 41 ] Censorship of maps is often employed for military purposes. For example, the technique was used in former  East Germany , especially for the areas near the border to  West Germany  in order to make attempts of defection more difficult. Censorship of maps is also applied by  Google Maps , where certain areas are grayed out or blacked or areas are purposely left outdated with old imagery. [ 42 ] Art is loved and feared because of its evocative power. Destroying or oppressing art can potentially justify its meaning even more. [ 43 ] British photographer and visual artist  Graham Ovenden 's photos and paintings were ordered to be destroyed by a London's magistrate court in 2015 for being \"indecent\" [ 44 ]  and their copies had been removed from the online  Tate gallery . [ 45 ] A 1980 Israeli law forbade banned  artwork  composed of the four colours of the  Palestinian flag , [ 46 ]  and Palestinians were arrested for displaying such artwork or even for carrying sliced melons with the same pattern. [ 47 ] [ 48 ] [ 49 ] Moath al-Alwi  is a Guantanamo Bay prisoner who creates  model ships  as an expression of art. Alwi does so with the few tools he has at his disposal such as dental floss and shampoo bottles, and he is also allowed to use a small pair of scissors with rounded edges. [ 50 ]  A few of Alwi's pieces are on display at John Jay College of Criminal Justice in New York. There are also other artworks on display at the College that were created by other inmates. The artwork that is being displayed might be the only way for some of the inmates to communicate with the outside. Recently things have changed though. The military has come up with a new policy that will not allow the artwork at Guantanamo Bay Military Prison to leave the prison. The artwork created by Alwi and other prisoners is now government property and can be destroyed or disposed of in whatever way the government choose, making it no longer the artist's property. [ 51 ] Around 300 artists in Cuba are fighting for their artistic freedom due to new censorship rules Cuba's government has in place for artists. In December 2018, following the introduction of new rules that would ban music performances and artwork not authorized by the state,  performance artist   Tania Bruguera  was detained upon arriving to Havana and released after four days. [ 52 ] An example of extreme state censorship was the Nazis' requirements of using art as propaganda. Art was only allowed to be used as a political instrument to control people and failure to act in accordance with the censors was punishable by law, even fatal. The  Degenerate Art Exhibition  was a historical instance of this, the goal of which was to advertise Nazi values and slander others. [ 53 ] Internet censorship is control or suppression of the publishing or accessing of information on  the Internet . It may be carried out by governments or by private organizations either at the behest of the government or on their own initiative. Individuals and organizations may engage in  self-censorship  on their own or due to intimidation and fear. The issues associated with Internet censorship are similar to those for offline censorship of more traditional media. One difference is that national borders are more permeable online: residents of a country that bans certain information can find it on websites hosted outside the country. Thus censors must work to prevent access to information even though they lack physical or legal control over the websites themselves. This in turn requires the use of technical censorship methods that are unique to the Internet, such as site blocking and content filtering. [ 59 ] Furthermore, the  Domain Name System  (DNS) a critical component of the Internet is dominated by centralized and few entities. The most widely used DNS root is administered by the  Internet Corporation for Assigned Names and Numbers  (ICANN). [ 60 ] [ 61 ]  As an administrator they have rights to shut down and seize  domain names  when they deem necessary to do so and at most times the direction is from governments. This has been the case with  Wikileaks  shutdowns [ 62 ]  and name seizure events such as the ones executed by the  National Intellectual Property Rights Coordination Center  (IPR Center) managed by the  Homeland Security Investigations  (HSI). [ 63 ]  This makes it easy for internet censorship by authorities as they have control over what should or should not be on the Internet. Some activists and researchers have started opting for  alternative DNS roots , though the Internet Architecture Board [ 64 ]  (IAB) does not support these DNS root providers. Unless the censor has total control over all Internet-connected computers, such as in  North Korea  or  Cuba , total censorship of information is very difficult or impossible to achieve due to the underlying distributed technology of the Internet.  Pseudonymity  and  data havens  (such as  Freenet ) protect  free speech  using technologies that guarantee material cannot be removed and prevents the identification of authors. Technologically savvy users can often find ways to  access blocked content . Nevertheless, blocking remains an effective means of limiting access to sensitive information for most users when censors, such as those in  China , are able to devote significant resources to building and maintaining a comprehensive censorship system. [ 59 ] Views about the feasibility and effectiveness of Internet censorship have evolved in parallel with the development of the Internet and censorship technologies: A  BBC World Service poll  of 27,973 adults in 26 countries, including 14,306 Internet users, [ 68 ]  was conducted between 30 November 2009 and 7 February 2010. The head of the polling organization felt, overall, that the poll showed that: The poll found that nearly four in five (78%) Internet users felt that the Internet had brought them greater freedom, that most Internet users (53%) felt that \"the internet should never be regulated by any level of government anywhere\", and almost four in five Internet users and non-users around the world felt that access to the Internet was a fundamental right (50% strongly agreed, 29% somewhat agreed, 9% somewhat disagreed, 6% strongly disagreed, and 6% gave no opinion). [ 70 ] The rising use of social media in many nations has led to the emergence of citizens organizing protests through social media, sometimes called \" Twitter Revolutions \". The most notable of these social media-led protests were the  Arab Spring uprisings , starting in 2010. In response to the use of social media in these protests, the Tunisian government began a hack of Tunisian citizens' Facebook accounts, and reports arose of accounts being deleted. [ 71 ] Automated systems can be used to censor  social media  posts, and therefore limit what citizens can say online. This most notably occurs in  China , where social media posts are automatically censored depending on content. In 2013, Harvard political science professor  Gary King  led a study to determine what caused social media posts to be censored and found that posts mentioning the government were not more or less likely to be deleted if they were supportive or critical of the government. Posts mentioning collective action were more likely to be deleted than those that had not mentioned collective action. [ 72 ]  Currently, social media censorship appears primarily as a way to restrict Internet users' ability to organize protests. For the Chinese government, seeing citizens unhappy with local governance is beneficial as state and national leaders can replace unpopular officials. King and his researchers were able to predict when certain officials would be removed based on the number of unfavorable social media posts. [ 73 ] Research has proved that criticism is tolerable on social media sites, therefore it is not censored unless it has a higher chance of collective action. It is not important whether the criticism is supportive or unsupportive of the states' leaders, the main priority of censoring certain social media posts is to make sure that no big actions are being made due to something that was said on the internet. Posts that challenge the Party's political leading role in the Chinese government are more likely to be censored due to the challenges it poses to the Chinese Communist Party. [ 74 ] In December 2022  Elon Musk , owner and CEO of  Twitter  released internal documents from the social media microblogging site to journalists  Matt Taibbi ,  Michael Shellenberger  and  Bari Weiss . The analysis of these files on Twitter, collectively called, the  Twitter Files , explored the content moderation and visibility filtering carried out in collaboration with the  Federal Bureau of Investigation  on the  Hunter Biden laptop controversy . On the platform TikTok, certain hashtags have been categorized by the platform's code and determines how viewers can or cannot interact with the content or hashtag specifically. Some shadowbanned tags include: #acab, #GayArab, #gej due to their referencing of certain social movements and LGBTQ identity. As TikTok guidelines are becoming more localized around the world, some experts believe that this could result in more censorship than before. [ 75 ] Since the early 1980s, advocates of video games have emphasized their use as an  expressive medium , arguing for their protection under the laws governing  freedom of speech  and also as an educational tool. Detractors argue that video games are  harmful  and therefore should be  subject to legislative oversight and restrictions . Many video games have certain elements removed or edited due to  regional rating standards . [ 76 ] [ 77 ] \nFor example, in the Japanese and PAL Versions of  No More Heroes , blood splatter and gore is removed from the gameplay. Decapitation scenes are implied, but not shown. Scenes of missing body parts after having been cut off, are replaced with the same scene, but showing the body parts fully intact. [ 78 ] Surveillance and censorship are different. Surveillance can be performed without censorship, but it is harder to engage in censorship without some form of surveillance. [ 79 ]  Even when surveillance does not lead directly to censorship, the widespread knowledge or belief that a person, their computer, or their use of the Internet is under surveillance can have a \" chilling effect \" and lead to self-censorship. [ 80 ] The former Soviet Union maintained a particularly extensive program of state-imposed censorship. The main organ for official censorship in the Soviet Union was the  Chief Agency for Protection of Military and State Secrets  generally known as the  Glavlit , its Russian acronym. The  Glavlit  handled censorship matters arising from domestic writings of just about any kind – even beer and vodka labels.  Glavlit  censorship personnel were present in every large Soviet publishing house or newspaper; the agency employed some 70,000 censors to review information before it was disseminated by publishing houses, editorial offices, and broadcasting studios. No mass medium escaped  Glavlit ' s control. All press agencies and radio and television stations had  Glavlit  representatives on their editorial staffs. [ 81 ] Sometimes, public knowledge of the existence of a specific document is subtly suppressed, a situation resembling censorship. The authorities taking such action will justify it by declaring the work to be \" subversive \" or \"inconvenient\". An example is  Michel Foucault 's 1978 text  Sexual Morality and the Law  (later republished as  The Danger of Child Sexuality ), originally published as  La loi de la pudeur  [literally, \"the law of decency\"]. This work defends the decriminalization of  statutory rape  and the  abolition of age of consent laws . [ citation needed ] When a publisher comes under pressure to suppress a book, but has already entered into a contract with the author, they will sometimes effectively censor the book by deliberately ordering a small print run and making minimal, if any, attempts to publicize it. This practice became known in the early 2000s as  privishing  ( priv ate publ ishing ). [ 82 ]  an  OpenNet Initiative  (ONI) classifications: [ 83 ] Censorship for individual countries is measured by  Freedom House (FH)  Freedom of the Press  report , [ 84 ]   Reporters Without Borders (RWB)  Press freedom index [ 85 ]  and  V-Dem  government censorship effort index. Censorship aspects are measured by Freedom on the Net [ 54 ]  and  OpenNet Initiative  (ONI) classifications. [ 83 ]   Censorship by country  collects information on censorship,  internet censorship ,  press freedom ,  freedom of speech , and  human rights  by country and presents it in a sortable table, together with links to articles with more information. In addition to countries, the table includes information on former countries, disputed countries, political sub-units within countries, and regional organizations. In  French-speaking Belgium , politicians considered  far-right  are  banned  from live media appearances such as interviews or debates. [ 86 ] [ 87 ] Very little is formally censored in Canada, aside from \" obscenity \" (as defined in the landmark criminal case of  R v Butler ) which is generally limited to  pornography  and  child pornography  depicting and/or advocating non-consensual sex, sexual violence, degradation, or dehumanization, in particular that which causes harm (as in  R v Labaye ). Most films are simply subject to classification by the  British Columbia Film Classification Office  under the non-profit  Crown corporation  by the name of  Consumer Protection BC , whose classifications are officially used by the provinces of  British Columbia ,  Saskatchewan ,  Ontario , and  Manitoba . [ 88 ] Cuban media used to be operated under the supervision of the  Communist Party's   Department of Revolutionary Orientation , which \"develops and coordinates propaganda strategies\". [ 89 ]  Connection to the Internet is restricted and censored. [ 90 ] The  People's Republic of China  employs sophisticated censorship mechanisms, referred to as the  Golden Shield Project , to monitor the internet. Popular search engines such as  Baidu  also remove politically sensitive search results. [ 91 ] [ 92 ] [ 93 ] Strict censorship existed in the Eastern Bloc. [ 94 ]  Throughout the bloc, the various ministries of culture held a tight rein on their writers. [ 95 ]  Cultural products there reflected the propaganda needs of the state. [ 95 ]  Party-approved censors exercised strict control in the early years. [ 96 ]  In the Stalinist period, even the weather forecasts were changed if they suggested that the sun might not shine on  May Day . [ 96 ]  Under  Nicolae Ceauşescu  in  Romania , weather reports were doctored so that the temperatures were not seen to rise above or fall below the levels which dictated that work must stop. [ 96 ] Possession and use of  copying machines  was tightly controlled in order to hinder the production and distribution of  samizdat , illegal  self-published  books and magazines.  Possession of even a single samizdat manuscript such as a book by  Andrei Sinyavsky  was a serious crime which might involve a visit from the  KGB . Another outlet for works which did not find favor with the authorities was publishing abroad. Amid declining car sales in 2020, France banned a television ad by a Dutch bike company, saying the ad \"unfairly discredited the automobile industry\". [ 97 ] The  Constitution of India  guarantees  freedom of expression , but places  certain restrictions  on content, with a view towards maintaining communal and religious harmony, given the history of communal tension in the nation. [ 98 ]  According to the Information Technology Rules 2011, objectionable content includes anything that \"threatens the unity, integrity, defence, security or sovereignty of India, friendly relations with foreign states or public order\". [ 99 ]  Notably many pornographic websites are blocked in India. Iraq under  Baathist   Saddam Hussein  had much the same techniques of press censorship as did Romania under Nicolae Ceauşescu but with greater potential violence. [ 100 ] During the  GHQ  occupation of Japan after WW2, any criticism of the Allies' pre-war policies, the SCAP, the Far East Military Tribunal, the inquiries against the United States and every direct and indirect references to the role played by the Allied High Command in drafting Japan's new constitution or to censorship of publications, movies, newspapers and magazines was subject to massive censorship,  purges ,  media blackout . [ 101 ] In the four years (September 1945–November 1949) since the  CCD  was active, 200 million pieces of mail and 136 million telegrams were opened, and telephones were tapped 800,000 times. Since no criticism of the occupying forces for crimes such as the dropping of the atomic bomb, rape and robbery by US soldiers was allowed, a strict check was carried out. Those who got caught were put on a blacklist called the watchlist, and the persons and the organizations to which they belonged were investigated in detail, which made it easier to dismiss or arrest the \"disturbing molecule\". [ 102 ] Under subsection 48(3) and (4) of the  Penang  Islamic Religious Administration Enactment 2004, non-Muslims in  Malaysia  are penalized for using the following words, or to write or publish them, in any form, version or translation in any language or for use in any publicity material in any medium:\n \"Allah\", \"Firman Allah\", \"Ulama\", \"Hadith\", \"Ibadah\", \"Kaabah\", \"Qadhi ' \", \"Illahi\", \"Wahyu\", \"Mubaligh\", \"Syariah\", \"Qiblat\", \"Haji\", \"Mufti\", \"Rasul\", \"Iman\", \"Dakwah\", \"Wali\", \"Fatwa\", \"Imam\", \"Nabi\", \"Sheikh\", \"Khutbah\", \"Tabligh\", \"Akhirat\", \"Azan\", \"Al Quran\", \"As Sunnah\", \"Auliya ' \", \"Karamah\", \"False Moon God\", \"Syahadah\", \"Baitullah\", \"Musolla\", \"Zakat Fitrah\", \"Hajjah\", \"Taqwa\" and \"Soleh\". [ 103 ] [ 104 ] [ 105 ] On 4 March 2022, Russian President  Vladimir Putin  signed into law a bill introducing  prison sentences of up to 15 years  for those who publish \"knowingly false information\" about the Russian military and its operations, leading to some media outlets in Russia to stop reporting on Ukraine or shutting their media outlet. [ 106 ] [ 107 ]  Although the  1993 Russian Constitution  has an article expressly prohibiting  censorship , [ 108 ]  the Russian censorship apparatus  Roskomnadzor  ordered the country's media to only use information from Russian state sources or face fines and blocks. [ 109 ]  As of December 2022, more than 4,000 people were prosecuted under \"fake news\" laws in connection with the  Russian invasion of Ukraine . [ 110 ] Novaya Gazeta 's  editor-in-chief  Dmitry Muratov  was awarded the  2021 Nobel Peace Prize  for his \"efforts to safeguard freedom of expression\". In March 2022,  Novaya Gazeta  suspended its print activities after receiving a second warning from  Roskomnadzor . [ 111 ] According to Christian Mihr, executive director of  Reporters Without Borders , \"censorship in Serbia is neither direct nor transparent, but is easy to prove.\" [ 112 ]  According to Mihr there are numerous examples of censorship and self-censorship in Serbia  [ 112 ]  According to Mihr, Serbian prime minister  Aleksandar Vučić  has proved \"very sensitive to criticism, even on critical questions,\" as was the case with Natalija Miletic, a correspondent for  Deutsche Welle Radio , who questioned him in Berlin about the media situation in Serbia and about allegations that some ministers in the Serbian government had plagiarized their diplomas, and who later received threats and offensive articles on the Serbian press. [ 112 ] Multiple news outlets have accused Vučić of anti-democratic strongman tendencies. [ 113 ] [ 114 ] [ 115 ] [ 116 ] [ 117 ]  In July 2014, journalists associations were concerned about the freedom of the media in Serbia, in which Vučić came under criticism. [ 118 ] [ 119 ] In September 2015 five members of United States Congress (Edie Bernice Johnson, Carlos Curbelo, Scott Perry, Adam Kinzinger, and  Zoe Lofgren ) have informed Vice President of the United States  Joseph Biden  that Aleksandar's brother, Andrej Vučić, is leading a group responsible for deteriorating media freedom in  Serbia . [ 120 ] In the  Republic of Singapore , Section 33 of the Films Act originally banned the making, distribution and exhibition of \"party political films\", at the pain of a fine not exceeding $100,000 or imprisonment for a term not exceeding two years. [ 121 ]  The Act further defines a \"party political film\" as any film or video In 2001, the short documentary called  A Vision of Persistence  on opposition politician  J. B. Jeyaretnam  was also banned for being a \"party political film\". The makers of the documentary, all lecturers at the Ngee Ann Polytechnic, later submitted written apologies and withdrew the documentary from being screened at the 2001  Singapore International Film Festival  in April, having been told they could be charged in court. [ 122 ]  Another short documentary called  Singapore Rebel  by  Martyn See , which documented  Singapore Democratic Party  leader Dr  Chee Soon Juan 's acts of civil disobedience, was banned from the 2005  Singapore International Film Festival  on the same grounds and See is being investigated for possible violations of the Films Act. [ 123 ] This law, however, is often disregarded when such political films are made supporting the ruling  People's Action Party  (PAP).  Channel NewsAsia 's five-part documentary series on Singapore's PAP ministers in 2005, for example, was not considered a party political film. [ 124 ] Exceptions are also made when political films are made concerning political parties of other nations. Films such as  Michael Moore 's 2004 documentary  Fahrenheit 911  are thus allowed to screen regardless of the law. [ 125 ] Since March 2009, the Films Act has been amended to allow party political films as long as they were deemed factual and objective by a consultative committee. Some months later, this committee lifted the ban on Singapore Rebel. [ 126 ] Independent journalism did not exist in the  Soviet Union  until  Mikhail Gorbachev  became its leader. Gorbachev adopted  glasnost  (openness), political reform aimed at reducing censorship; before glasnost all reporting was directed by the  Communist Party  or related organizations.  Pravda , the predominant newspaper in the Soviet Union, had a monopoly. Foreign newspapers were available only if they were published by  communist parties  sympathetic to the Soviet Union. Online access to all language versions of  Wikipedia  was blocked in  Turkey  on 29 April 2017 by  Erdoğan 's government. [ 127 ] Article 299  of the Turkish Penal Code deems it illegal to \"Insult the  President of Turkey \" .  A person who is sentenced for a violation of this article can be sentenced to a prison term between one and four years and if the violation was made in public the verdict can be elevated by a sixth. [ 128 ]  Prosecutions often target critics of the government, independent journalists, and political cartoonists. [ 129 ]  Between 2014 and 2019, 128,872 investigations were launched for this offense and prosecutors opened 27,717 criminal cases. [ 130 ] From December 1956 until 1974 the  Irish republican  political party  Sinn Féin  was banned from participating in elections by the Northern Ireland Government. [ 131 ]  From 1988 until 1994 the British government prevented the UK media from broadcasting the voices (but not words) of Sinn Féin and ten Irish republican and  Ulster loyalist  groups. [ 132 ] In the United States, most forms of censorship are self-imposed rather than enforced by the government. The government does not routinely censor material, although state and local governments often restrict what is provided in libraries and public schools. [ 133 ]  In addition, distribution, receipt, and transmission (but not  mere private possession ) of  obscene material  may be prohibited by law. Furthermore, under  FCC v. Pacifica Foundation , the FCC has the power to prohibit the transmission of indecent material over broadcast. Additionally, critics of  campaign finance reform in the United States  say this reform imposes widespread restrictions on political speech. [ 134 ] [ 135 ] In 1973, a military coup took power in Uruguay, and the State practiced censorship. For example, writer  Eduardo Galeano  was imprisoned and later was forced to flee. His book  Open Veins of Latin America  was banned by the right-wing military government, not only in Uruguay, but also in Chile and Argentina. [ 136 ]"
  },
  {
    "id": 18,
    "title": "Library classification",
    "content": "A  library classification  is a system used within a  library  to organize materials, including books, sound and video recordings, electronic materials, etc., both on shelves and in catalogs and indexes. Each item is typically assigned a call number, which identifies the location of the item within the system. Materials can be arrange by many different factors, typically in either a hierarchical tree structure based on the subject or using a  faceted classification  system, which allows the assignment of multiple classifications to an object, enabling the classifications to be ordered in many ways. [ 1 ] Library classification is an  important and crucial aspect in  library and information science . It is distinct from  scientific classification  in that it has as its goal to provide a useful ordering of documents rather than a theoretical organization of  knowledge . [ 2 ]  Although it has the practical purpose of creating a physical ordering of documents, it does generally attempt to adhere to accepted scientific knowledge. [ 3 ]  Library classification helps to accommodate all the newly published literature in an already created order of  arrangement in a filial sequence. [ 4 ] Library classification can be defined as the arrangement of books on shelves, or description of them, in the manner which is most useful to those who read with the ultimate aim of grouping similar things together. Library classification is meant to achieve these four purposes: ordering the fields of knowledge in a systematic way, bring related items together in the most helpful sequence, provide orderly access on the shelf, and provide a location for an item on the shelf. [ 5 ] Library classification is distinct from the application of  subject headings  in that classification organizes knowledge into a systematic order, while subject headings provide access to intellectual materials through vocabulary terms that may or may not be organized as a knowledge system. [ 6 ] \nThe characteristics that a bibliographic classification demands for the sake of reaching these purposes are: a useful sequence of subjects at all levels, a concise memorable notation, and a host of techniques and devices of number synthesis. [ 7 ] Library classifications were preceded by classifications used by bibliographers such as  Conrad Gessner . The earliest library classification schemes organized books in broad subject categories. The earliest known library classification scheme is the  Pinakes  by  Callimachus , a scholar at the  Library of Alexandria  during the third century BC.  During the Renaissance and Reformation era, \"Libraries were organized according to the whims or knowledge of individuals in charge.\" [ 8 ]    This changed the format in which various materials were classified.  Some collections were classified by language and others by how they were printed. After the  printing revolution  in the sixteenth century, the increase in available printed materials made such broad classification unworkable, and more granular classifications for library materials had to be developed in the nineteenth century. [ 9 ] In 1627  Gabriel Naudé  published a book called  Advice on Establishing a Library . At the time, he was working in the private library of President Henri de Mesmes II. Mesmes had around 8,000 printed books and many more Greek, Latin and French written manuscripts. Although it was a private library, scholars with references could access it. The purpose of  Advice on Establishing a Library  was to identify rules for private book collectors to organize their collections in a more orderly way to increase the collection's usefulness and beauty. Naudé developed a classification system based on seven different classes: theology, medicine, jurisprudence, history, philosophy, mathematics, and the humanities. These seven classes would later be increased to twelve. [ 10 ]   Advice on Establishing a Library  was about a private library, but within the same book, Naudé encouraged the idea of public libraries open to all people regardless of their ability to pay for access to the collection. One of the most famous libraries that Naudé helped improve was the  Bibliothèque Mazarine  in Paris. Naudé spent ten years there as a librarian. Because of Naudé's strong belief in free access to libraries to all people, the Bibliothèque Mazarine became the first public library in France around 1644. [ 11 ] Although libraries created order within their collections from as early as the fifth century BC, [ 9 ]  the Paris Bookseller's classification, developed in 1842 by  Jacques Charles Brunet , is generally seen as the first of the modern book classifications. Brunet provided five major classes: theology, jurisprudence, sciences and arts, belles-lettres, and history. [ 12 ]  Classification can now be seen as a provider of subject access to information in a networked environment. [ 13 ] There are many standard systems of library classification in use, and many more have been proposed over the years. However, in general, classification systems can be divided into three types depending on how they are used: In terms of functionality, classification systems are often described as: There are few completely enumerative systems or faceted systems; most systems are a blend but favouring one type or the other. The most common classification systems, LCC and DDC, are essentially enumerative, though with some hierarchical and faceted elements (more so for DDC), especially at the broadest and most general level. The first true faceted system was the  colon classification  of  S. R. Ranganathan . [ 14 ] Classification types denote the classification or categorization according to the form or characteristics or qualities of a classification scheme or schemes. Method and system has similar meaning. Method or methods or system means the classification schemes like Dewey Decimal Classification or Universal Decimal Classification. The types of classification is for identifying and understanding or education or research purposes while classification method means those classification schemes like DDC, UDC. The most common systems in  English -speaking countries are: Other systems include: Newer classification systems tend to use the principle of synthesis (combining codes from different lists to represent the different attributes of a work) heavily, which is comparatively lacking in LC or DDC. Library classification is associated with library (descriptive) cataloging under the rubric of  cataloging and classification , sometimes grouped together as  technical services . The library professional who engages in the process of cataloging and classifying library materials is called a  cataloger  or  catalog librarian . Library classification systems are one of the two tools used to facilitate  subject access .  The other consists of alphabetical indexing languages such as Thesauri and Subject Headings systems. The practice of library classification is a form of the more general task of  classification . The work consists of two steps. Firstly, the subject or topic of the material is ascertained. Next, a  call number  (essentially a book's address) based on the classification system in use at the particular library will be assigned to the work using the notation of the system. Unlike subject heading or thesauri where multiple terms can be assigned to the same work, in library classification systems, each work can only be placed in one class. This is due to shelving purposes: A book can have only one physical place. However, in classified catalogs one may have main entries as well as added entries. Most classification systems like the  Dewey Decimal Classification  (DDC) and  Library of Congress Classification  also add a  cutter number  to each work which adds a code for the main entry (primary access point) of the work (e.g. author). Classification systems in libraries generally play two roles. Firstly, they facilitate  subject access  by allowing the user to find out what works or documents the library has on a certain subject. [ 18 ]  Secondly, they provide a known location for the information source to be located (e.g. where it is shelved). Until the 19th century, most libraries had closed stacks, so the library classification only served to organize the subject  catalog . In the 20th century, libraries opened their stacks to the public and started to shelve library material itself according to some library classification to simplify subject browsing. Some classification systems are more suitable for aiding subject access, rather than for shelf location. For example,  Universal Decimal Classification , which uses a complicated notation of pluses and colons, is more difficult to use for the purpose of shelf arrangement but is more expressive compared to DDC in terms of showing relationships between subjects. Similarly  faceted classification  schemes are more difficult to use for shelf arrangement, unless the user has knowledge of the citation order. Depending on the size of the library collection, some libraries might use classification systems solely for one purpose or the other. In extreme cases, a public library with a small collection might just use a classification system for location of resources but might not use a complicated subject classification system. Instead all resources might just be put into a couple of wide classes (travel, crime, magazines etc.). This is known as a \"mark and park\" classification method, more formally called reader interest classification. [ 19 ] As a result of differences in notation, history, use of enumeration, hierarchy, and facets, classification systems can differ in the following ways:"
  },
  {
    "id": 19,
    "title": "Computer data storage",
    "content": "Computer data storage  or  digital data storage  is a technology consisting of  computer  components and  recording media  that are used to retain  digital data . It is a core function and fundamental component of computers. [ 1 ] : 15–16 The  central processing unit  (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a  storage hierarchy , [ 1 ] : 468–473   which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally, the fast [ a ]  technologies are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\". Even the first computer designs,  Charles Babbage 's  Analytical Engine  and  Percy Ludgate 's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the  Von Neumann architecture , where the CPU consists of two main parts: The  control unit  and the  arithmetic logic unit  (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and  logical operations  on data. Without a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This is acceptable for devices such as desk  calculators ,  digital signal processors , and other specialized devices.  Von Neumann  machines differ in having a memory in which they store their operating  instructions  and data. [ 1 ] : 20   Such computers are more versatile in that they do not need to have their hardware reconfigured for each new program, but can simply be  reprogrammed  with new in-memory instructions; they also tend to be simpler to design, in that a relatively simple processor may keep  state  between successive computations to build up complex procedural results. Most modern computers are von Neumann machines. A modern  digital computer  represents  data  using the  binary numeral system . Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of  bits , or binary digits, each of which has a value of 0 or 1. The most common unit of storage is the  byte , equal to 8 bits. A piece of information can be handled by any computer or device whose storage space is large enough to accommodate  the binary representation of the piece of information , or simply  data . For example, the  complete works of Shakespeare , about 1250 pages in print, can be stored in about five  megabytes  (40 million bits) with one byte per character. Data are  encoded  by assigning a bit pattern to each  character ,  digit , or  multimedia  object. Many standards exist for encoding (e.g.  character encodings  like  ASCII , image encodings like  JPEG , and video encodings like  MPEG-4 ). By adding bits to each encoded unit, redundancy allows the computer to detect errors in coded data and correct them based on mathematical algorithms. Errors generally occur in low probabilities due to  random  bit value flipping, or \"physical bit fatigue\", loss of the physical bit in the storage of its ability to maintain a distinguishable value (0 or 1), or due to errors in inter or intra-computer communication. A random  bit flip  (e.g. due to random  radiation ) is typically corrected upon detection. A bit or a group of malfunctioning physical bits (the specific defective bit is not always known; group definition depends on the specific storage device) is typically automatically fenced out, taken out of use by the device, and replaced with another functioning equivalent group in the device, where the corrected bit values are restored (if possible). The  cyclic redundancy check  (CRC) method is typically used in communications and storage for  error detection . A detected error is then retried. Data compression  methods allow in many cases (such as a database) to represent a string of bits by a shorter bit string (\"compress\") and reconstruct the original string (\"decompress\") when needed. This utilizes substantially less storage (tens of percent) for many types of data at the cost of more computation (compress and decompress when needed). Analysis of the trade-off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data compressed or not. For  security reasons , certain types of data (e.g.  credit card  information) may be kept  encrypted  in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots. Generally, the lower a storage is in the hierarchy, the lesser its  bandwidth  and the greater its access  latency  is from the CPU. This traditional division of storage to primary, secondary, tertiary, and off-line storage is also guided by cost per bit. In contemporary usage,  memory  is usually fast but temporary  semiconductor   read-write memory , typically  DRAM  (dynamic RAM) or other such devices.  Storage  consists of storage devices and their media not directly accessible by the  CPU  ( secondary  or  tertiary storage ), typically  hard disk drives ,  optical disc  drives, and other devices slower than RAM but  non-volatile  (retaining contents when powered down). [ 2 ] Historically,  memory  has, depending on technology, been called  central memory ,  core memory ,  core storage ,  drum ,  main memory ,  real storage , or  internal memory . Meanwhile, slower persistent storage devices have been referred to as  secondary storage ,  external memory , or  auxiliary/peripheral storage . Primary storage  (also known as  main memory ,  internal memory , or  prime memory ), often referred to simply as  memory , is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in a uniform manner. Historically,  early computers  used  delay lines ,  Williams tubes , or rotating  magnetic drums  as primary storage. By 1954, those unreliable methods were mostly replaced by  magnetic-core memory . Core memory remained dominant until the 1970s, when advances in  integrated circuit  technology allowed  semiconductor memory  to become economically competitive. This led to modern  random-access memory  (RAM). It is small-sized, light, but quite expensive at the same time. The particular types of RAM used for primary storage are  volatile , meaning that they lose the information when not powered. Besides storing opened programs, it serves as  disk cache  and  write buffer  to improve both reading and writing performance. Operating systems borrow RAM capacity for caching so long as it's not needed by running software. [ 3 ]  Spare memory can be utilized as  RAM drive  for temporary high-speed data storage. As shown in the diagram, traditionally there are two more sub-layers of the primary storage, besides main large-capacity RAM: Main memory is directly or indirectly connected to the central processing unit via a  memory bus . It is actually two buses (not on the diagram): an  address bus  and a  data bus . The CPU firstly sends a number through an address bus, a number called  memory address , that indicates the desired location of data. Then it reads or writes the data in the  memory cells  using the data bus. Additionally, a  memory management unit  (MMU) is a small device between CPU and RAM recalculating the actual memory address, for example to provide an abstraction of  virtual memory  or other tasks. As the RAM types used for primary storage are volatile (uninitialized at start up), a computer containing only such storage would not have a source to read instructions from, in order to start the computer. Hence,  non-volatile primary storage  containing a small startup program ( BIOS ) is used to  bootstrap  the computer, that is, to read a larger program from non-volatile  secondary  storage to RAM and start to execute it. A non-volatile technology used for this purpose is called ROM, for  read-only memory  (the terminology may be somewhat confusing as most ROM types are also capable of  random access ). Many types of \"ROM\" are not literally  read only , as updates to them are possible; however it is slow and memory must be erased in large portions before it can be re-written. Some  embedded systems  run programs directly from ROM (or similar), because such programs are rarely changed. Standard computers do not store non-rudimentary programs in ROM, and rather, use large capacities of secondary storage, which is non-volatile as well, and not as costly. Recently,  primary storage  and  secondary storage  in some uses refer to what was historically called, respectively,  secondary storage  and  tertiary storage . [ 4 ] The primary storage, including  ROM ,  EEPROM ,  NOR flash , and  RAM , [ 5 ]  are usually  byte-addressable . Secondary storage  (also known as  external memory  or  auxiliary storage ) differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfer the desired data to primary storage. Secondary storage is non-volatile (retaining data when its power is shut off). Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive. In modern computers,  hard disk drives  (HDDs) or  solid-state drives  (SSDs) are usually used as secondary storage. The  access time  per byte for HDDs or SSDs is typically measured in  milliseconds  (thousandths of a second), while the access time per byte for primary storage is measured in  nanoseconds  (billionths of a second). Thus, secondary storage is significantly slower than primary storage. Rotating  optical storage  devices, such as  CD  and  DVD  drives, have even longer access times. Other examples of secondary storage technologies include  USB flash drives ,  floppy disks ,  magnetic tape ,  paper tape ,  punched cards , and  RAM disks . Once the  disk read/write head  on HDDs reaches the proper placement and the data, subsequent data on the track are very fast to access. To reduce the seek time and rotational latency, data are transferred to and from disks in large contiguous blocks. Sequential or block access on disks is orders of magnitude faster than random access, and many sophisticated paradigms have been developed to design efficient algorithms based on sequential and block access. Another way to reduce the I/O bottleneck is to use multiple disks in parallel to increase the bandwidth between primary and secondary memory. [ 6 ] Secondary storage is often formatted according to a  file system  format, which provides the abstraction necessary to organize data into  files  and  directories , while also providing  metadata  describing the owner of a certain file, the access time, the access permissions, and other information. Most computer  operating systems  use the concept of  virtual memory , allowing the utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks ( pages ) to a swap file or page file on secondary storage, retrieving them later when needed. If a lot of pages are moved to slower secondary storage, the system performance is degraded. The secondary storage, including  HDD ,  ODD  and  SSD , are usually block-addressable. Tertiary storage  or  tertiary memory [ 7 ]  is a level below secondary storage. Typically, it involves a robotic mechanism which will  mount  (insert) and  dismount  removable mass storage media into a storage device according to the system's demands; such data are often copied to secondary storage before use. It is primarily used for archiving rarely accessed information since it is much slower than secondary storage (e.g. 5–60 seconds vs. 1–10 milliseconds). This is primarily useful for extraordinarily large data stores, accessed without human operators. Typical examples include  tape libraries  and  optical jukeboxes . When a computer needs to read information from the tertiary storage, it will first consult a catalog  database  to determine which tape or disc contains the information. Next, the computer will instruct a  robotic arm  to fetch the medium and place it in a drive. When the computer has finished reading the information, the robotic arm will return the medium to its place in the library. Tertiary storage is also known as  nearline storage  because it is \"near to online\". The formal distinction between online, nearline, and offline storage is: [ 8 ] For example, always-on spinning hard disk drives are online storage, while spinning drives that spin down automatically, such as in massive arrays of idle disks ( MAID ), are nearline storage. Removable media such as tape cartridges that can be automatically loaded, as in  tape libraries , are nearline storage, while tape cartridges that must be manually loaded are offline storage. Off-line storage  is computer data storage on a medium or a device that is not under the control of a  processing unit . [ 9 ]  The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. It must be inserted or connected by a human operator before a computer can access it again. Unlike tertiary storage, it cannot be accessed without human interaction. Off-line  storage is used to  transfer information  since the detached medium can easily be physically transported. Additionally, it is useful for cases of disaster, where, for example, a fire destroys the original data, a medium in a remote location will be unaffected, enabling  disaster recovery . Off-line storage increases general  information security  since it is physically inaccessible from a computer, and data confidentiality or integrity cannot be affected by computer-based attack techniques. Also, if the information stored for archival purposes is rarely accessed, off-line storage is less expensive than tertiary storage. In modern personal computers, most secondary and tertiary storage media are also used for off-line storage. Optical discs and flash memory devices are the most popular, and to a much lesser extent removable hard disk drives; older examples include floppy disks and Zip disks. In enterprise uses, magnetic tape cartridges are predominant; older examples include open-reel magnetic tape and punched cards. Storage technologies at all levels of the storage hierarchy can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are volatility, mutability, accessibility, and addressability. For any particular implementation of any storage technology, the characteristics worth measuring are capacity and performance. Non-volatile memory  retains the stored information even if not constantly supplied with electric power. It is suitable for long-term storage of information.  Volatile memory  requires constant power to maintain the stored information. The fastest memory technologies are volatile ones, although that is not a universal rule. Since the primary storage is required to be very fast, it predominantly uses volatile memory. Dynamic random-access memory  is a form of volatile memory that also requires the stored information to be periodically reread and rewritten, or  refreshed , otherwise it would vanish.  Static random-access memory  is a form of volatile memory similar to DRAM with the exception that it never needs to be refreshed as long as power is applied; it loses its content when the power supply is lost. An  uninterruptible power supply  (UPS) can be used to give a computer a brief window of time to move information from primary volatile storage into non-volatile storage before the batteries are exhausted. Some systems, for example  EMC Symmetrix , have integrated batteries that maintain volatile storage for several minutes. Utilities such as  hdparm  and  sar  can be used to measure IO performance in Linux. Full disk encryption ,  volume and virtual disk encryption, andor file/folder encryption  is readily available for most storage devices. [ 17 ] Hardware memory encryption is available in Intel Architecture, supporting Total Memory Encryption (TME) and page granular memory encryption with multiple keys (MKTME). [ 18 ] [ 19 ]  and in  SPARC  M7 generation since October 2015. [ 20 ] Distinct types of data storage have different points of failure and various methods of  predictive failure analysis . Vulnerabilities that can instantly lead to total loss are  head crashing  on mechanical hard drives and  failure of electronic components  on flash storage. Impending failure on  hard disk drives  is estimable using S.M.A.R.T. diagnostic data that includes the  hours of operation  and the count of spin-ups, though its reliability is disputed. [ 21 ] Flash storage may experience downspiking transfer rates as a result of accumulating errors, which the  flash memory controller  attempts to correct. The health of  optical media  can be determined by  measuring correctable minor errors , of which high counts signify deteriorating and/or low-quality media. Too many consecutive minor errors can lead to data corruption. Not all vendors and models of  optical drives  support error scanning. [ 22 ] As of 2011 [update] , the most commonly used data storage media are semiconductor, magnetic, and optical, while paper still sees some limited usage. Some other fundamental storage technologies, such as all-flash arrays (AFAs) are proposed for development. Semiconductor memory  uses  semiconductor -based  integrated circuit  (IC) chips to store information. Data are typically stored in  metal–oxide–semiconductor  (MOS)  memory cells . A semiconductor memory chip may contain millions of memory cells, consisting of tiny  MOS field-effect transistors  (MOSFETs) and/or  MOS capacitors . Both  volatile  and  non-volatile  forms of semiconductor memory exist, the former using standard MOSFETs and the latter using  floating-gate MOSFETs . In modern computers, primary storage almost exclusively consists of dynamic volatile semiconductor  random-access memory  (RAM), particularly  dynamic random-access memory  (DRAM). Since the turn of the century, a type of non-volatile  floating-gate  semiconductor memory known as  flash memory  has steadily gained share as off-line storage for home computers. Non-volatile semiconductor memory is also used for secondary storage in various advanced electronic devices and specialized computers that are designed for them. As early as 2006,  notebook  and  desktop computer  manufacturers started using flash-based  solid-state drives  (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD. [ 23 ] [ 24 ] [ 25 ] [ 26 ] [ 27 ] Magnetic storage  uses different patterns of  magnetization  on a  magnetically  coated surface to store information. Magnetic storage is  non-volatile . The information is accessed using one or more read/write heads which may contain one or more recording transducers. A read/write head only covers a part of the surface so that the head or medium or both must be moved relative to another in order to access data. In modern computers, magnetic storage will take these forms: In early computers, magnetic storage was also used as: Magnetic storage does not have a definite limit of rewriting cycles like flash storage and re-writeable optical media, as altering magnetic fields causes no physical wear. Rather, their life span is limited by mechanical parts. [ 28 ] [ 29 ] Optical storage , the typical  optical disc , stores information in deformities on the surface of a circular disc and reads this information by illuminating the surface with a  laser diode  and observing the reflection. Optical disc storage is  non-volatile . The deformities may be permanent (read only media), formed once (write once media) or reversible (recordable or read/write media). The following forms are in common use as of 2009 [update] : [ 30 ] Magneto-optical disc storage  is optical disc storage where the magnetic state on a  ferromagnetic  surface stores information. The information is read optically and written by combining magnetic and optical methods. Magneto-optical disc storage is  non-volatile ,  sequential access , slow write, fast read storage used for tertiary and off-line storage. 3D optical data storage  has also been proposed. Light induced magnetization melting in magnetic photoconductors has also been proposed for high-speed low-energy consumption magneto-optical storage. [ 31 ] Paper data storage , typically in the form of  paper tape  or  punched cards , has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole.  Barcodes  make it possible for objects that are sold or transported to have some computer-readable information securely attached. Relatively small amounts of digital data (compared to other digital data storage) may be backed up on paper as a  matrix barcode  for very long-term storage, as the longevity of paper typically exceeds even magnetic data storage. [ 32 ] [ 33 ] While a group of bits malfunction may be resolved by error detection and correction mechanisms (see above), storage device malfunction requires different solutions. The following solutions are commonly used and valid for most storage devices: Device mirroring and typical RAID are designed to handle a single device failure in the RAID group of devices. However, if a second failure occurs before the RAID group is completely repaired from the first failure, then data can be lost. The probability of a single failure is typically small. Thus the probability of two failures in the same RAID group in time proximity is much smaller (approximately the probability squared, i.e., multiplied by itself). If a database cannot tolerate even such a smaller probability of data loss, then the RAID group itself is replicated (mirrored). In many cases such mirroring is done geographically remotely, in a different storage array, to handle recovery from disasters (see disaster recovery above). A secondary or tertiary storage may connect to a computer utilizing  computer networks . This concept does not pertain to the primary storage, which is shared between multiple processors to a lesser degree. Large quantities of individual magnetic tapes, and optical or magneto-optical discs may be stored in robotic tertiary storage devices. In tape storage field they are known as  tape libraries , and in optical storage field  optical jukeboxes , or optical disk libraries per analogy. The smallest forms of either technology containing just one drive device are referred to as  autoloaders  or  autochangers . Robotic-access storage devices may have a number of slots, each holding individual media, and usually one or more picking robots that traverse the slots and load media to built-in drives. The arrangement of the slots and picking devices affects performance. Important characteristics of such storage are possible expansion options: adding slots, modules, drives, robots. Tape libraries may have from 10 to more than 100,000 slots, and provide  terabytes  or  petabytes  of near-line information. Optical jukeboxes are somewhat smaller solutions, up to 1,000 slots. Robotic storage is used for  backups , and for high-capacity archives in imaging, medical, and video industries.  Hierarchical storage management  is a most known archiving strategy of automatically  migrating  long-unused files from fast hard disk storage to libraries or jukeboxes. If the files are needed, they are  retrieved  back to disk. This article incorporates  public domain material  from  Federal Standard 1037C .  General Services Administration . Archived from  the original  on 22 January 2022."
  },
  {
    "id": 20,
    "title": "Cultural studies",
    "content": "Cultural studies  is an  academic field  that explores the dynamics of contemporary  culture  (including the politics of  popular culture ) and its social and historical foundations. [ 1 ]  Cultural studies researchers investigate how cultural practices relate to wider systems of  power  associated with, or operating through, social phenomena. These include  ideology ,  class structures ,  national formations ,  ethnicity ,  sexual orientation ,  gender , and generation. Employing  cultural analysis , cultural studies views cultures not as fixed, bounded, stable, and discrete entities, but rather as constantly interacting and changing sets of practices and processes. [ 2 ] [ 3 ] Cultural studies was initially developed by British Marxist academics in the late 1950s, 1960s, and 1970s, and has been subsequently taken up and transformed by scholars from many different disciplines around the world. Cultural studies is avowedly and even radically interdisciplinary and can sometimes be seen as anti-disciplinary. A key concern for cultural studies practitioners is the examination of the forces within and through which socially organized people conduct and participate in the construction of their everyday lives. [ 4 ] Cultural studies combines a variety of politically engaged critical approaches drawn including  semiotics ,  Marxism ,  feminist theory ,  ethnography ,  post-structuralism ,  postcolonialism ,  social theory ,  political theory ,  history ,  philosophy ,  literary theory ,  media theory ,  film/video studies ,  communication studies ,  political economy ,  translation studies ,  museum studies  and  art history /criticism to study cultural phenomena in various societies and historical periods. Cultural studies seeks to understand how meaning is generated, disseminated, contested, bound up with systems of power and control, and produced from the social, political and economic spheres within a particular social formation or conjuncture. The movement has generated important theories of  cultural hegemony  and  agency . Its practitioners attempt to explain and analyze the cultural forces related and processes of  globalization . During the rise of  neoliberalism  in Britain and the U.S., cultural studies both became a global phenomenon, and attracted the attention of many conservative opponents both within and beyond universities for a variety of reasons. A worldwide movement of students and practitioners with a raft of scholarly associations and programs, annual international conferences and publications carry on work in this field today. [ 5 ] [ 6 ]  Distinct approaches to cultural studies have emerged in different national and regional contexts. In his 1994 book,  Introducing Cultural Studies ,  orientalist  scholar  Ziauddin Sardar  lists the following five main characteristics of cultural studies: [ 7 ] Dennis Dworkin  writes that \"a critical moment\" in the beginning of cultural studies as a field was when  Richard Hoggart  used the term in 1964 in founding the  Centre for Contemporary Cultural Studies  (CCCS) at the  University of Birmingham . [ 8 ]  The centre would become home to the development of the intellectual orientation that has become known internationally as the \" Birmingham School \" of cultural studies, [ 8 ] [ 9 ]  thus becoming the world's first institutional home of cultural studies. [ 10 ] Hoggart appointed as his assistant  Stuart Hall , who would effectively be directing  CCCS  by 1968. [ 11 ]  Hall formally assumed the directorship of CCCS in 1971, when Hoggart left Birmingham to become Assistant Director-General of  UNESCO . [ 12 ]  Thereafter, the field of cultural studies became closely associated with Hall's work. [ 13 ] [ 14 ]  In 1979, Hall left Birmingham to accept a prestigious chair in  sociology  at the  Open University , and Richard Johnson took over the directorship of the centre. In the late 1990s, \"restructuring\" at the University of Birmingham led to the elimination of CCCS and the creation of a new Department of Cultural Studies and Sociology (CSS) in 1999. Then, in 2002, the university's senior administration abruptly announced the disestablishment of CSS, provoking a substantial international outcry. The immediate reason for disestablishment of the new department was an unexpectedly low result in the UK's  Research Assessment Exercise  of 2001, though a dean from the university attributed the decision to \"inexperienced 'macho management'.\" [ 15 ]  The RAE, a holdover initiative of the  Margaret Thatcher-led   British government  of 1986, determines research funding for university programs. [ 16 ] To trace the development of British Cultural Studies, see, for example, the work of  Richard Hoggart ,  E. P. Thompson ,  Raymond Williams , Stuart Hall,  Paul Willis ,  Angela McRobbie ,  Paul Gilroy , David Morley,  Charlotte Brunsdon ,  Richard Dyer , and others. [ 17 ]  There are also many published overviews of the historical development of cultural studies, including Graeme Turner's  British Cultural Studies: An Introduction , 3rd Ed. and John Hartley's  A Short History of Cultural Studies [ 18 ] [ 19 ] [ 20 ] Beginning in 1964, after the initial appearance of the founding works of British Cultural Studies in the late 1950s,  Stuart Hall 's pioneering work at  CCCS , along with that of his colleagues and  postgraduate  students, gave shape and substance to the field of cultural studies. This would include such people as  Paul Willis ,  Dick Hebdige , David Morley,  Charlotte Brunsdon , John Clarke,  Richard Dyer , Judith Williamson, Richard Johnson,  Iain Chambers , Dorothy Hobson,  Chris Weedon , Tony Jefferson, Michael Green and  Angela McRobbie . Many cultural studies scholars employed  Marxist methods of analysis , exploring the relationships between cultural forms (i.e., the  superstructure ) and that of the  political economy  (i.e., the  base ). By the 1970s, the work of  Louis Althusser  radically rethought the Marxist account of  base  and  superstructure  in ways that had a significant influence on the \"Birmingham School.\" Much of the work done at CCCS studied  youth-subcultural  expressions of antagonism toward \"respectable\"  middle-class   British culture  in the post-WWII period. Also during the 1970s, the politically formidable British  working classes  were in decline.  Britain's manufacturing industries  while continuing to grow in output and value, were decreasing in share of GDP and numbers employed, and  union  rolls were shrinking. Millions of working-class  Britons  backed the rise of  Margaret Thatcher , through the labour losses. For Stuart Hall and his colleagues, this shift in loyalty from the  Labour Party  to the  Conservative Party  had to be explained in terms of cultural politics, which they had been tracking even before Thatcher's first victory. Some of this work was presented in the cultural studies classic,  Policing the Crisis , [ 21 ]  and in other later texts such as Hall's  The Hard Road to Renewal: Thatcherism and the Crisis of the Left , [ 22 ]  and  New Times: The Changing Face of Politics in the 1990s . [ 23 ] In 2016, Duke University Press launched a new series of Stuart Hall's collected writings, many of which detail his major and decisive contributions toward the establishment of the field of cultural studies. [ 24 ]  In 2023, a new Stuart Hall Archive Project was launched at the University of Birmingham to commemorate Hall's contributions in pioneering the field of cultural studies at CCCS. [ 25 ] By the late 1970s, scholars associated with  The Birmingham School  had firmly placed questions of  gender  and  race  on the cultural studies agenda, where they have remained ever since. Also by the late 1970s, cultural studies had begun to attract a great deal of international attention. It spread globally throughout the 1980s and 1990s. As it did so, it both encountered new conditions of knowledge production, and engaged with other major international intellectual currents such as  poststructuralism ,  postmodernism , and  postcolonialism . [ 26 ]  The wide range of cultural studies journals now located throughout the world, as shown below, is one indication of the  globalization  of the field. For overviews of and commentaries on developments in cultural studies during the twenty-first century, see Lawrence Grossberg's  Cultural Studies in the Future Tense , Gilbert Rodman's  Why Cultural Studies?  and Graeme Turner's  What's Become of Cultural Studies? Hall's cultural studies explores culture as a system that affects individuals' identities through the meanings and practices that arise from the constant power dynamics that comprise culture. [ 27 ]  Hall viewed culture as a \"critical site of social action and intervention, where power relations are both established and potentially unsettled.\" [ 28 ]  He perceived culture as a power dynamic, in which the media unintentionally possesses more control over ideology than the public. [ 29 ]  Hall viewed the media as a source of preserving the status quo of a reflection that already exists in society. The media hegemony in question, he emphasized, \"is not a conscious plot or conspiracy, it’s not overtly coercive, and its effects are not total.\" [ 30 ]  Compared to other thinkers on this subject, he studied and analyzed symbols, ideologies, signs, and other representations within cultural studies. [ 31 ]  Most of his contributions occurred in the 1980s, where he looked at how media cultivates cultural power, how it is consumed, mediated and negotiated, etc. [ 32 ]  Hall has also been accredited with the expansion of cultural studies through “the primacy of culture’s role as an educational site where identities are being continually transformed, power is enacted, and learning assumes a political dynamic.” [ 33 ]  He viewed politics as being used mainly for power instead of the betterment of society. [ 34 ]  This led to the belief that political dynamics could change with a reform in the education system (if one changes the education system, then one can change the culture). [ 35 ]  Hall viewed culture as something that is institutionalized, which could only be studied through the interactional patterns that people within a culture exhibit and experience. [ 36 ]  Culture is something that makes up society, is a learned trait, and is influenced by various forms of media that help to establish it. [ 37 ]  Power is the underlying tone of Hall’s cultural studies. [ 38 ]  Hall believed that culture has some power, but the media's use of it is what sways and dictates culture itself. [ 39 ] In the US, prior to the emergence of British Cultural Studies, several versions of  cultural analysis  had emerged largely from pragmatic and  liberal-pluralist  philosophical traditions. [ 40 ]  However, in the late 1970s and 1980s, when British Cultural Studies began to spread internationally, and to engage with  feminism ,  poststructuralism ,  postmodernism , and race,  critical  cultural studies (i.e.,  Marxist , feminist, poststructuralist, etc.) expanded tremendously in American universities in fields such as  communication studies ,  education ,  sociology , and  literature . [ 41 ] [ 42 ] [ 43 ]   Cultural Studies , the flagship journal of the field, has been based in the US since its founding editor,  John Fiske , brought it there from  Australia  in 1987. A thriving cultural studies scene has existed in  Australia  since the late 1970s, when several key CS practitioners emigrated there from the UK, bringing British Cultural Studies with them, after  Margaret Thatcher  became Prime Minister of the UK in 1979. A school of cultural studies known as  cultural policy studies  is one of the distinctive Australian contributions to the field, though it is not the only one. Australia also gave birth to the world's first professional cultural studies association (now known as the Cultural Studies Association of Australasia) in 1990. [ 44 ] [ 45 ]  Cultural studies journals based in Australia include  International Journal of Cultural Studies ,  Continuum: Journal of Media & Cultural Studies , and  Cultural Studies Review . In  Canada , cultural studies has sometimes focused on issues of  technology and society , continuing the emphasis in the work of  Marshall McLuhan ,  Harold Innis , and others. Cultural studies journals based in Canada include  Topia: Canadian Journal of Cultural Studies . In Africa,  human rights  and  Third-World  issues are among the central topics treated. There is a thriving cultural and media studies scholarship in Southern Africa, with its locus in South Africa and Zimbabwe. [ 46 ]  Cultural Studies journals based in Africa include the  Journal of African Cultural Studies . In  Latin America , cultural studies have drawn on thinkers such as  José Martí ,  Ángel Rama , and other Latin-American figures, in addition to the Western theoretical sources associated with cultural studies in other parts of the world. Leading Latin American cultural studies scholars include  Néstor García Canclini ,  Jésus Martín-Barbero , and  Beatriz Sarlo . [ 47 ] [ 48 ]  Among the key issues addressed by Latin American cultural studies scholars are  decoloniality ,  urban cultures , and  postdevelopment theory . Latin American cultural studies journals include the  Journal of Latin American Cultural Studies . Even though cultural studies developed much more rapidly in the UK than in  continental Europe , there is significant cultural studies presence in countries such as  France ,  Spain , and  Portugal . The field is relatively undeveloped in  Germany , probably due to the continued influence of the  Frankfurt School , [ 49 ]  which is now often said to be in its third generation, which includes notable figures such as  Axel Honneth . Cultural studies journals based in continental Europe include the  European Journal of Cultural Studies , the  Journal of Spanish Cultural Studies ,  French Cultural Studies , and  Portuguese Cultural Studies . In Germany, the term  cultural studies  specifically refers to the field in the  Anglosphere , especially British Cultural Studies, [ 50 ]  to differentiate it from the German  Kulturwissenschaft  which developed along different lines and is characterized by its distance from political science. However,  Kulturwissenschaft  and cultural studies are often used interchangeably, particularly by lay people. Throughout Asia, cultural studies have boomed and thrived since at least the beginning of the 1990s. [ 51 ]  Cultural studies journals based in Asia include  Inter-Asia Cultural Studies . In India, the Centre for Study of Culture and Society, Bangalore and the Department of Cultural Studies at The  English and Foreign Languages  and the  University of Hyderabad  are two major institutional spaces for Cultural Studies. Marxism  has been an important influence upon cultural studies. Those associated with  CCCS  initially engaged deeply with the  structuralism  of  Louis Althusser , and later in the 1970s turned decisively toward  Antonio Gramsci . Cultural studies has also embraced the examination of race, gender, and other aspects of identity, as is illustrated, for example, by a number of key books published collectively under the name of  CCCS  in the late 1970s and early 1980s, including  Women Take Issue: Aspects of Women's Subordination  (1978), and  The Empire Strikes Back: Race and Racism in 70s Britain  (1982). To understand the changing political circumstances of  class ,  politics , and  culture  in the United Kingdom, scholars at  The Birmingham School  turned to the work of  Antonio Gramsci , an Italian thinker, writer, and  Communist Party  leader. Gramsci had been concerned with similar issues: why would Italian laborers and peasants vote for  fascists ? What strategic approach is necessary to mobilize popular support in more progressive directions? Gramsci modified  classical Marxism , and argued that culture must be understood as a key site of political and social struggle. In his view,  capitalists  used not only brute force ( police ,  prisons ,  repression ,  military ) to maintain  control , but also penetrated the everyday culture of  working people  in a variety of ways in their efforts to win popular \"consent.\" It is important to recognize that for Gramsci, historical leadership, or  hegemony , involves the formation of alliances between class factions, and struggles within the cultural realm of everyday common sense.  Hegemony  was always, for Gramsci, an interminable, unstable and contested process. [ 52 ] Scott Lash  writes: In the work of Hall, Hebdige and McRobbie, popular culture came to the fore... What Gramsci gave to this was the importance of consent and culture. If the fundamental Marxists saw the power in terms of class-versus-class, then Gramsci gave to us a question of  class alliance . The rise of cultural studies itself was based on the decline of the prominence of fundamental class-versus-class politics. [ 53 ] Edgar and Sedgwick write: The theory of  hegemony  was of central importance to the development of British cultural studies [particularly  The Birmingham School . It facilitated the analysis of the ways subordinate groups actively resist and respond to political and economic domination. The subordinate groups needed not to be seen merely as the passive dupes of the dominant class and its ideology. [ 54 ] The development of  hegemony   theory  in cultural studies was in some ways consonant with work in other fields exploring  agency , a theoretical concept that insists on the active, critical capacities of subordinated people (e.g. the  working classes , colonized peoples, women). [ 55 ]  As Stuart Hall famously argued in his 1981 essay, \"Notes on Deconstructing 'the Popular ' \": \"ordinary people are not cultural dopes.\" [ 56 ]  Insistence on accounting for the agency of subordinated people run counter to the work of traditional  structuralists . Some analysts [ who? ]  have however been critical of some work in cultural studies that they feel overstates the significance of or even romanticizes some forms of popular cultural agency. Cultural studies often concerns itself with the agency at the level of the practices of everyday life, and approaches such research from a standpoint of radical  contextualism . [ 57 ]  In other words, cultural studies rejects universal accounts of  cultural practices , meanings, and identities. Judith Butler , an American  feminist theorist  whose work is often associated with cultural studies, wrote that: the move from a  structuralist  account in which capital is understood to structure social relations in relatively homologous ways to a view of hegemony in which power relations are subject to repetition, convergence, and rearticulation brought the question of temporality into the thinking of structure. It has marked a shift from a form of  Althusserian  theory that takes structural totalities as theoretical objects to one in which the insights into the contingent possibility of structure inaugurate a renewed conception of hegemony as bound up with the contingent sites and strategies of the rearticulation of power. [ 58 ] In recent decades, as  capitalism  has spread throughout the world via contemporary forms of  globalization , cultural studies has generated important analyses of local sites and practices of negotiation with and resistance to Western  hegemony . [ 59 ] Cultural studies criticizes the traditional view of the passive consumer, particularly by underlining the different ways people  read , receive and interpret cultural texts, or appropriate other kinds of cultural products, or otherwise participate in the production and circulation of meanings. On this view, a consumer can  appropriate , actively rework, or challenge the meanings circulated through cultural texts. In some of its variants, cultural studies has shifted the analytical focus from traditional understandings of production to consumption – viewed as a form of production (of meanings, of identities, etc.) in its own right.  Stuart Hall ,  John Fiske , and others have been influential in these developments. A special 2008 issue of the field's flagship journal,  Cultural Studies , examined \" anti-consumerism \" from a variety of cultural studies angles. Jeremy Gilbert noted in the issue, cultural studies must grapple with the fact that \"we now live in an era when, throughout the capitalist world, the overriding aim of government economic policy is to maintain  consumer spending  levels. This is an era when 'consumer confidence' is treated as the key indicator and cause of economic effectiveness.\" [ 60 ] Cultural studies, drawing upon and developing  semiotics , uses the concept of  text  to designate not only written language, but also  television programs ,  films ,  photographs ,  fashion ,  hairstyles , and so forth; the texts of cultural studies comprise all the meaningful artifacts of culture. This conception of textuality derives especially from the work of the pioneering and influential semiotician,  Roland Barthes , but also owes debts to other sources, such as  Juri Lotman  and his colleagues from  Tartu–Moscow School . Similarly, the field widens the concept of  culture . Cultural studies approach the sites and spaces of everyday life, such as pubs, living rooms, gardens, and beaches, as \"texts.\" [ 61 ] Culture , in this context, includes not only  high culture , [ 62 ]  but also everyday meanings and practices, a central focus of cultural studies. Jeff Lewis  summarized much of the work on  textuality  and  textual analysis  in his cultural studies textbook and a  post-9/11   monograph  on media and terrorism. [ 63 ] [ 64 ]  According to Lewis,  textual studies  use complex and difficult  heuristic  methods and require both powerful interpretive skills and a subtle conception of politics and contexts. The task of the cultural analyst, for Lewis, is to engage with both knowledge systems and texts and observe and analyze the ways the two interact with one another. This engagement represents the critical dimensions of the analysis, its capacity to illuminate the  hierarchies  within and surrounding the given text and its  discourse . Cultural studies has evolved through its uptake across a variety of different disciplines— anthropology ,  media studies ,  communication studies ,  literary studies ,  education ,  geography ,  philosophy ,  sociology ,  politics , and others. While some [ who? ]  have accused certain areas of cultural studies of meandering into political  relativism  and a kind of empty version of \" postmodern \" analysis, others [ who? ]  hold that at its core, cultural studies provides a significant  conceptual  and  methodological  framework for  cultural ,  social , and economic critique. This critique is designed to \" deconstruct \" the meanings and assumptions that are inscribed in the institutions, texts, and practices that work with and through, and produce and re-present, culture. [ 65 ] [ page needed ]  Thus, while some scholars and disciplines have dismissed cultural studies for its methodological rejection of disciplinarity, its core strategies of  critique  and  analysis  have influenced areas of the  social sciences  and  humanities ; for example, cultural studies work on forms of  social differentiation ,  control  and  inequality ,  identity ,  community-building , media, and  knowledge production  has had a substantial impact. Moreover, the influence of cultural studies has become increasingly evident in areas as diverse as  translation studies , health studies,  international relations ,  development studies ,  computer studies ,  economics ,  archaeology , and  neurobiology . [ citation needed ] Cultural studies has also diversified its own interests and methodologies, incorporating a range of studies on  media policy ,  democracy ,  design ,  leisure ,  tourism ,  warfare , and development. While certain key concepts such as  ideology  or  discourse , class, hegemony, identity, and gender remain significant, cultural studies has long engaged with and integrated new concepts and approaches. The field thus continues to pursue political critique through its engagements with the forces of culture and politics. [ 66 ] [ page needed ] Many cultural studies practitioners work in departments of  English  or  comparative literature . Nevertheless, some traditional  literary scholars  such as  Yale  professor  Harold Bloom  have been outspoken critics of cultural studies. On the level of  methodology , these scholars dispute the theoretical underpinning of the movement's critical framework. Bloom stated his position during the 3 September 2000 episode of  C-SPAN 's  Booknotes , while discussing his book  How to Read and Why : [T]here are two enemies of reading now in the world, not just in the English-speaking world. One [is] the lunatic destruction of literary studies...and its replacement by what is called cultural studies in all of the universities and colleges in the English-speaking world, and everyone knows what that phenomenon is. I mean, the...now-weary phrase 'political correctness' remains a perfectly good descriptive phrase for what has gone on and is, alas, still going on almost everywhere and which dominates, I would say, rather more than three-fifths of the tenured faculties in the English-speaking world, who really do represent treason of the intellectuals, I think, a 'betrayal of the clerks'.\" [ 67 ] Marxist literary critic   Terry Eagleton  is not wholly opposed to cultural studies, but has criticised aspects of it and highlighted what he sees as its strengths and weaknesses in books such as  After Theory  (2003). For Eagleton, literary and cultural theory have the potential to say important things about the \"fundamental questions\" in life, but theorists have rarely realized this potential. English departments also host cultural  rhetorics  scholars. This academic field defines cultural rhetorics as \"the study and practice of making meaning and knowledge with the belief that all cultures are rhetorical and all rhetorics are cultural.\" [ 68 ]  Cultural rhetorics scholars are interested in investigating topics like  climate change , [ 69 ]   autism , [ 70 ]   Asian American   rhetoric , [ 71 ]  and more. Cultural studies have also had a substantial impact on  sociology . For example, when Stuart Hall left  CCCS  at Birmingham, it was to accept a prestigious  professorship  in Sociology at the  Open University  in Britain. The subfield of  cultural sociology , in particular, is disciplinary home to many cultural studies practitioners. Nevertheless, there are some differences between sociology as a  discipline  and the field of cultural studies as a whole. While sociology was founded upon various historic works purposefully distinguishing the subject from  philosophy  or  psychology , cultural studies have explicitly interrogated and criticized traditional understandings and practices of disciplinarity. Most CS practitioners think it is best that cultural studies neither emulate disciplines nor aspire to disciplinarity for cultural studies. Rather, they promote a kind of radical  interdisciplinarity  as the basis for cultural studies. One sociologist whose work has had a major influence on cultural studies is  Pierre Bourdieu , whose work makes innovative use of statistics and in-depth interviews. [ 72 ] [ 73 ]  However, although Bourdieu's work has been highly influential within cultural studies, and although Bourdieu regarded his work as a form of  science , cultural studies has never embraced the idea that it should aspire toward \"scientificity,\" and has marshalled a wide range of theoretical and methodological arguments against the  fetishization  of \"scientificity\" as a basis for cultural studies. Two sociologists who have been critical of cultural studies, Chris Rojek and  Bryan S. Turner , argue in their article, \"Decorative sociology: towards a critique of the cultural turn,\" that cultural studies, particularly the flavor championed by Stuart Hall, lacks a stable research agenda, and privileges the contemporary reading of texts, thus producing an ahistorical theoretical focus. [ 74 ]  Many, [ who? ]  however, would argue, following Hall, that cultural studies have always sought to avoid the establishment of a fixed research agenda; this follows from its critique of disciplinarity. Moreover, Hall and many others have long argued against the misunderstanding that  textual analysis  is the sole methodology of cultural studies, and have practiced numerous other approaches, as noted above. Rojek and Turner also level the accusation that there is \"a sense of moral superiority about the correctness of the political views articulated\" in cultural studies. [ 74 ] In 1996, physicist  Alan Sokal  expressed his opposition to cultural studies by  submitting a hoax article  to a cultural studies journal,  Social Text . The article, which was crafted as a parody of what Sokal referred to as the \"fashionable nonsense\" of  postmodernism , was accepted by the editors of the journal, which did not at the time practice  peer review . When the paper appeared in print, Sokal published a second article in a self-described \"academic gossip\" magazine,  Lingua Franca , revealing his hoax on  Social Text . Sokal stated that his motivation stemmed from his rejection of contemporary critiques of  scientific rationalism : [ 75 ] Politically, I'm angered because most (though not all) of this silliness is emanating from the self-proclaimed Left. We're witnessing here a profound historical  volte-face . For most of the past two centuries, the Left has been identified with science and against  obscurantism ; we have believed that  rational thought  and the fearless analysis of objective reality (both natural and social) are incisive tools for combating the mystifications promoted by the powerful – not to mention being desirable human ends in their own right. The recent turn of many \"progressive\" or \"leftist\" academic humanists and social scientists toward one or another form of epistemic relativism betrays this worthy heritage and undermines the already fragile prospects for progressive social critique. Theorizing about \"the social construction of reality\" won't help us find an effective treatment for AIDS or devise strategies for preventing global warming. Nor can we combat false ideas in history, sociology, economics and politics if we reject the notions of truth and falsity. In response to this critique,  Jacques Derrida  wrote: [ 76 ] In whose interest was it to go for a quick practical joke rather than taking part in the work which, sadly, it replaced? Hall and others have identified some core originating texts, or the original \" curricula ,\" of the field of cultural studies:"
  },
  {
    "id": 21,
    "title": "Data modeling",
    "content": "Data modeling  in  software engineering  is the process of creating a  data model  for an  information system  by applying certain formal techniques. It may be applied as part of broader  Model-driven engineering  (MDE) concept. Data modeling is a  process   used to define and analyze data  requirements  needed to support the  business processes  within the scope of corresponding information systems in organizations. Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system. There are three different types of data models produced while progressing from requirements to the actual database to be used for the information system. [ 2 ]   The data requirements are initially recorded as a  conceptual data model  which is essentially a set of technology independent specifications about the data and is used to discuss initial requirements with the business stakeholders. The  conceptual model  is then translated into a  logical data model , which documents structures of the data that can be implemented in databases. Implementation of one conceptual data model may require multiple logical data models. The last step in data modeling is transforming the logical data model to a  physical data model  that organizes the data into tables, and accounts for access, performance and storage details. Data modeling defines not just data elements, but also their structures and the relationships between them. [ 3 ] Data modeling techniques and methodologies are used to model data in a standard, consistent, predictable manner in order to manage it as a resource. The use of data modeling standards is strongly recommended for all projects requiring a standard means of defining and analyzing data within an organization, e.g., using data modeling: Data modeling may be performed during various types of projects and in multiple phases of projects. Data models are progressive; there is no such thing as the final data model for a business or application. Instead a data model should be considered a living document that will change in response to a changing business. The data models should ideally be stored in a repository so that they can be retrieved, expanded, and edited over time.  Whitten  et al. (2004) determined two types of data modeling: [ 4 ] Data modeling is also used as a technique for detailing business  requirements  for specific  databases . It is sometimes called  database modeling  because a  data model  is eventually implemented in a database. [ 4 ] Data models provide a framework for  data  to be used within  information systems  by providing specific definitions and formats. If a data model is used consistently across systems then compatibility of data can be achieved. If the same data structures are used to store and access data then different applications can share data seamlessly. The results of this are indicated in the diagram. However, systems and interfaces are often expensive to build, operate, and maintain. They may also constrain the business rather than support it. This may occur when the quality of the data models implemented in systems and interfaces is poor. [ 1 ] Some common problems found in data models are: In 1975  ANSI  described three kinds of data-model  instance : [ 5 ] According to ANSI, this approach allows the three perspectives to be relatively independent of each other. Storage technology can change without affecting either the logical or the conceptual schema. The table/column structure can change without (necessarily) affecting the conceptual schema.  In each case, of course, the structures must remain consistent across all schemas of the same data model. In the context of  business process integration  (see figure), data modeling complements  business process modeling , and ultimately results in database generation. [ 6 ] The process of designing a database involves producing the previously described three types of schemas – conceptual, logical, and physical. The database design documented in these schemas is converted through a  Data Definition Language , which can then be used to generate a database.  A fully attributed data model contains detailed attributes (descriptions) for every entity within it. The term \"database design\" can describe many different parts of the design of an overall  database system . Principally, and most correctly, it can be thought of as the logical design of the base data structures used to store the data. In the  relational model  these are the  tables  and  views . In an  object database  the entities and relationships map directly to object classes and named relationships. However, the term \"database design\" could also be used to apply to the overall process of designing, not just the base data structures, but also the forms and queries used as part of the overall database application within the  Database Management System  or DBMS. In the process, system  interfaces  account for 25% to 70% of the development and support costs of current systems. The primary reason for this cost is that these systems do not share a  common data model . If data models are developed on a system by system basis, then not only is the same analysis repeated in overlapping areas, but further analysis must be performed to create the interfaces between them. Most systems within an organization contain the same basic data, redeveloped for a specific purpose. Therefore, an efficiently designed basic data model can minimize rework with minimal modifications for the purposes of different systems within the organization [ 1 ] Data models represent information areas of interest. While there are many ways to create data models, according to  Len Silverston  (1997) [ 7 ]  only two modeling methodologies stand out, top-down and bottom-up: Sometimes models are created in a mixture of the two methods: by considering the data needs and structure of an application and by consistently referencing a subject-area model. In many environments the distinction between a logical data model and a physical data model is blurred. In addition, some  CASE  tools don't make a distinction between logical and  physical data models . [ 7 ] There are several notations for data modeling. The actual model is frequently called \"entity–relationship model\", because it depicts data in terms of the entities and relationships described in the  data . [ 4 ]  An entity–relationship model (ERM) is an abstract conceptual representation of structured data. Entity–relationship modeling is a relational schema  database modeling  method, used in  software engineering  to produce a type of  conceptual data model  (or  semantic data model ) of a system, often a  relational database , and its requirements in a  top-down  fashion. These models are being used in the first stage of  information system  design during the  requirements analysis  to describe information needs or the type of  information  that is to be stored in a  database . The  data modeling  technique can be used to describe any  ontology  (i.e. an overview and classifications of used terms and their relationships) for a certain  universe of discourse  i.e. area of interest. Several techniques have been developed for the design of data models. While these methodologies guide data modelers in their work, two different people using the same methodology will often come up with very different results. Most notable are: Generic data models are generalizations of conventional  data models . They define standardized general relation types, together with the kinds of things that may be related by such a relation type. \nThe definition of generic data model is similar to the definition of a natural language. For example, a generic data model may define relation types such as a 'classification relation', being a  binary relation  between an individual thing and a kind of thing (a class) and a 'part-whole relation', being a binary relation between two things, one with the role of part, the other with the role of whole, regardless the kind of things that are related. Given an extensible list of classes, this allows the classification of any individual thing and to specify part-whole relations for any individual object. By standardization of an extensible list of relation types, a generic data model enables the expression of an unlimited number of kinds of facts and will approach the capabilities of natural languages. Conventional data models, on the other hand, have a fixed and limited domain scope, because the instantiation (usage) of such a model only allows expressions of kinds of facts that are predefined in the model. The logical data structure of a DBMS, whether hierarchical, network, or relational, cannot totally satisfy the requirements for a conceptual definition of data because it is limited in scope and biased toward the implementation strategy employed by the DBMS. That is unless the semantic data model is implemented in the database on purpose, a choice which may slightly impact performance but generally vastly improves productivity. Therefore, the need to define data from a conceptual view has led to the development of  semantic data modeling  techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure the real world, in terms of resources, ideas, events, etc., is symbolically defined by its description within physical data stores. A semantic data model is an  abstraction  which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world. [ 8 ] The purpose of semantic data modeling is to create a structural model of a piece of the real world, called \"universe of discourse\". For this, three fundamental structural relations are considered: A semantic data model can be used to serve many purposes, such as: [ 8 ] The overall goal of semantic data models is to capture more meaning of data by integrating relational concepts with more powerful  abstraction  concepts known from the  artificial intelligence  field. The idea is to provide high level modeling primitives as integral part of a data model in order to facilitate the representation of real world situations. [ 10 ]"
  },
  {
    "id": 22,
    "title": "Informatics",
    "content": "Informatics  (a combination of the words \"information\" and \"automatic\") is the study of computational systems. [ 1 ] [ 2 ]  According to the  ACM  Europe Council and  Informatics Europe , informatics is synonymous with  computer science  and  computing  as a profession, [ 3 ]  in which the central notion is  transformation of information . [ 1 ] [ 4 ]  In some cases, the term \"informatics\" may also be used with different meanings, e.g. in the context of  social computing , [ 5 ]  or in context of  library science . [ 6 ] In some countries, depending on local interpretations and contexts, the term \"informatics\" is used synonymously to mean  information systems ,  information science ,  information theory ,  information engineering ,  information technology , information processing, or other theoretical or practical fields. In Germany, the term  informatics  closely corresponds to modern computer science. Accordingly, universities in  continental Europe  usually translate \"informatics\" as  computer science , or sometimes  information and computer science , although  technical universities  may translate it as  computer science & engineering . [ 7 ] [ 8 ] In the  United States , however, the term  informatics  is mostly used in context of  data science ,  library science [ 6 ]  or its applications in  healthcare  ( health informatics ), [ 9 ] [ 10 ]  where it first appeared in the US. The  University of Washington  uses this term to refer to  social computing . [ 5 ]  In some countries, this term is associated with  natural computation  and  neural computation . [ 1 ] [ 11 ] The  Government of Canada  uses the term to refer to operational units offering network and computer services to the various departments. [ 12 ] In 1956, the German informatician  Karl Steinbuch  and engineer  Helmut Gröttrup  coined the word  Informatik  when they developed the  Informatik-Anlage [ 13 ]  for the  Quelle  mail-order management, one of the earliest commercial applications of data processing. In April 1957, Steinbuch published a paper called  Informatik: Automatische Informationsverarbeitung  (\"Informatics: Automatic Information Processing\"). [ 14 ]  The morphology— informat -ion + - ics —uses \"the accepted form for names of sciences, as conics, mathematics, linguistics, optics, or matters of practice, as economics, politics, tactics\", [ 15 ]  and so, linguistically, the meaning extends easily to encompass both the science of information and the practice of information processing. The German word  Informatik  is usually translated to English as [ 16 ]   computer science  by universities or  computer science & engineering  by technical universities (German equivalents for institutes of technology). Depending on the context, informatics is also translated into  computing ,  scientific computing  or  information and computer technology . The  French  term  informatique  was coined in 1962 by  Philippe Dreyfus . [ 17 ]  In the same month was also proposed independently by Walter F. Bauer (1924–2015) and associates who co-founded software company  Informatics Inc .  The term for the new discipline quickly spread throughout Europe, but it did not catch on in the United States. Over the years, many different definitions of informatics have been developed, most of them claim that the essence of informatics is one of these concepts: information processing, algorithms, computation, information, algorithmic processes, computational processes or computational systems. [ 18 ] [ 1 ] The earliest uses of the term  informatics  in the  United States  was during the 1950s with the beginning of computer use in  healthcare . [ 19 ]   Early practitioners interested in the field soon learned that there were no formal education programs, and none emerged until the late 1960s. They introduced the term informatics only in the context of  archival science , which is only a small part of informatics. Professional development, therefore, played a significant role in the development of health informatics. [ 19 ]  According to Imhoff et al., 2001, healthcare informatics is not only the application of computer technology to problems in healthcare, but covers all aspects of generation, handling, communication, storage, retrieval, management, analysis, discovery, and synthesis of data information and knowledge in the entire scope of healthcare. Furthermore, they stated that the primary goal of health informatics can be distinguished as follows:  To provide solutions for problems related to data, information, and knowledge processing. To study general principles of processing data information and knowledge in medicine and healthcare . [ 20 ] [ 21 ]  The term  health informatics  quickly spread throughout the United States in various forms such as  nursing informatics ,  public health informatics  or  medical informatics . Analogous terms were later introduced for use of computers in various fields, such as  business informatics ,  forest informatics ,  legal informatics  etc. These fields still mainly use term informatics in context of library science. In the early 1980s, K.A Nicholas published \"Informatics: Ready for the Information Society\" proposing a definition of Informatics as \"the study and the practice of skills related to information, its collection, storage, retrieval, analysis and publication. In short; - Information Handling.\" It had been developed in the South Australian Education System at a grass roots level. <K.A Nicholas published \"Informatics: Ready for the Information Society\" 1983 - National Library of Australia> In the early 1990s, K.K. Kolin proposed an interpretation of informatics as a fundamental science that studies information processes in nature, society, and technical systems. [ 22 ] A broad interpretation of  informatics , as \"the study of the structure, algorithms, behaviour, and interactions of natural and artificial computational systems,\" was introduced by the  University of Edinburgh  in 1994. This has led to the merger of the institutes of computer science, artificial intelligence and cognitive science into a single  School of Informatics  in 2002. [ 23 ] More than a dozen nearby universities joined  Scottish Informatics and Computer Science Alliance . Some non-European universities have also adopted this definition (e.g.  Kyoto University School of Informatics ). In 2003, Yingxu Wang popularized term  cognitive informatics , described as follows: [ 24 ] Supplementary to matter and energy, information is the third essence for modeling the world. Cognitive informatics focuses on internal information processing mechanisms and the natural intelligence of the brain. Informatics as a fundamental science of information in natural and artificial systems was proposed again in Russia in 2006. [ 25 ] In 2007, the influential book  Decoding the Universe  was published. Former president of  Association for Computing Machinery , Peter Denning wrote in 2007: [ 26 ] The old definition of computer science - the study of phenomena surrounding computers - is now obsolete. Computing is the study of natural and artificial information processes. The 2008  Research Assessment Exercise , of the UK Funding Councils, includes a new,  Computer Science and Informatics,  unit of assessment (UoA), [ 27 ]  whose scope is described as follows: In 2008, the construction of the  Informatics Forum  was completed. In 2018, the  MIT Schwarzman College of Computing  was established. Its construction is planned to be completed in 2021. In the fields of  geoinformatics  or  irrigation informatics , the term -informatics usually mean  information science , in context related to  library science . This was the first meaning of informatics introduced in  Russia  in 1966 by A.I. Mikhailov, R.S. Gilyarevskii, and A.I. Chernyi, which referred to a scientific discipline that studies the structure and properties of scientific information. [ 22 ]  In this context, the term was also used by the  International Neuroinformatics Coordinating Facility . Some scientists use this term, however, to refer to the science of information processing, not  data management . [ 28 ] In the English-speaking world, the term  informatics  was first widely used in the compound  medical informatics , taken to include \"the cognitive, information processing, and communication tasks of medical practice, education, and research, including information science and the technology to support these tasks\". [ 29 ]  Many such compounds are now in use; they can be viewed as different areas of \" applied informatics \". In some countries such as Germany, Russia, France, and Italy, the term  informatics  in many contexts (but not always) can translate directly to  computer science . [ 30 ] Computer scientists study computational processes and systems. Computing Research Repository (CoRR) classification distinguishes the following main topics in computer science (alphabetic order): [ 31 ] [ 32 ] [ 33 ]"
  },
  {
    "id": 23,
    "title": "Information technology",
    "content": "Information technology  ( IT ) is a set of related fields that encompass computer systems,  software ,  programming languages ,  data  and information processing, and storage. [ 1 ]  IT forms part of  information and communications technology  (ICT). [ 2 ]  An  information technology system  ( IT system ) is generally an  information system , a  communications system , or, more specifically speaking, a  computer system  — including all  hardware ,  software , and  peripheral  equipment — operated by a limited group of IT users, and an  IT project  usually refers to the commissioning and implementation of an IT system. [ 3 ]  IT systems play a vital role in facilitating efficient data management, enhancing communication networks, and supporting organizational processes across various industries. Successful IT projects require meticulous planning, seamless integration, and ongoing maintenance to ensure optimal functionality and alignment with organizational objectives. [ 4 ] Although humans have been storing, retrieving, manipulating, and communicating information since the earliest writing systems were developed, [ 5 ]  the term  information technology  in its modern sense first appeared in a 1958 article published in the  Harvard Business Review ; authors  Harold J. Leavitt  and Thomas L. Whisler commented that \"the new technology does not yet have a single established name. We shall call it information technology (IT).\" [ 6 ]  Their definition consists of three categories: techniques for processing, the application of  statistical  and mathematical methods to  decision-making , and the simulation of higher-order thinking through computer programs. [ 6 ] The term is commonly used as a  synonym  for computers and  computer networks , but it also encompasses other information distribution technologies such as  television  and  telephones . Several products or services within an economy are associated with information technology, including  computer hardware ,  software , electronics, semiconductors,  internet ,  telecom equipment , and  e-commerce . [ 7 ] [ a ] Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC – 1450 AD),  mechanical  (1450 – 1840),  electromechanical  (1840 – 1940), and  electronic  (1940 to present). [ 5 ] Information technology is a branch of  computer science , defined as the study of procedures, structures, and the processing of various types of data. As this field continues to evolve globally, its priority and importance have grown, leading to the introduction of computer science-related courses in  K-12 education . Ideas of computer science were first mentioned before the 1950s under the  Massachusetts Institute of Technology  (MIT) and  Harvard University , where they had discussed and began thinking of computer circuits and numerical calculations.  As time went on, the field of information technology and computer science became more complex and was able to handle the processing of more data. Scholarly articles began to be published from different organizations. [ 9 ] Looking at early computing,  Alan Turing ,  J. Presper Eckert , and  John Mauchly  were considered some of the major pioneers of computer technology in the mid-1900s. Giving them such credit for their developments, most of their efforts were focused on designing the first digital computer. Along with that, topics such as  artificial intelligence  began to be brought up as Turing was beginning to question such technology of the time period. [ 10 ] Devices  have been used to aid computation for thousands of years, probably initially in the form of a  tally stick . [ 11 ]  The  Antikythera mechanism , dating from about the beginning of the first century BC, is generally considered the earliest known mechanical  analog computer , and the earliest known geared mechanism. [ 12 ]  Comparable geared devices did not emerge in  Europe  until the 16th century, and it was not until 1645 that the first  mechanical calculator  capable of performing the four basic arithmetical operations was developed. [ 13 ] Electronic computers , using either  relays  or  valves , began to appear in the early 1940s. The  electromechanical   Zuse Z3 , completed in 1941, was the world's first  programmable  computer, and by modern standards one of the first machines that could be considered a complete  computing  machine. During the  Second World War ,  Colossus  developed the first electronic  digital  computer to decrypt German messages. Although it was  programmable , it was not general-purpose, being designed to perform only a single task. It also lacked the ability to store its program in memory; programming was carried out using plugs and switches to alter the internal wiring. [ 14 ]  The first recognizably modern  electronic  digital  stored-program computer  was the  Manchester Baby , which ran its first program on 21 June 1948. [ 15 ] The development of  transistors  in the late 1940s at  Bell Laboratories  allowed a new generation of computers to be designed with greatly reduced power consumption. The first commercially available stored-program computer, the  Ferranti Mark I , contained 4050 valves and had a power consumption of 25 kilowatts. By comparison, the first transistorized computer developed at the  University of Manchester  and operational by November 1953, consumed only 150 watts in its final version. [ 16 ] Several other breakthroughs in  semiconductor  technology include the  integrated circuit  (IC) invented by  Jack Kilby  at  Texas Instruments  and  Robert Noyce  at  Fairchild Semiconductor  in 1959,  silicon dioxide surface passivation by  Carl Frosch  and Lincoln Derick in 1955, [ 17 ]  the first planar silicon dioxide transistors by Frosch and Derick in 1957, [ 18 ]  the  MOSFET  demonstration by a Bell Labs team. [ 19 ] [ 20 ] [ 21 ] [ 22 ]  the  planar process  by  Jean Hoerni  in 1959, [ 23 ] [ 24 ] [ 25 ] and the  microprocessor  invented by  Ted Hoff ,  Federico Faggin ,  Masatoshi Shima , and  Stanley Mazor  at  Intel  in 1971. These important inventions led to the development of the  personal computer  (PC) in the 1970s, and the emergence of  information and communications technology  (ICT). [ 26 ] By the year of 1984, according to the  National Westminster Bank Quarterly Review , the term  information technology  had been redefined as \"The development of cable television was made possible by the convergence of telecommunications and computing technology (…generally known in Britain as information technology).\"  We then begin to see the appearance of the term in 1990 contained within documents for the  International Organization for Standardization  (ISO). [ 27 ] Innovations in technology have already revolutionized the world by the twenty-first century as people were able to access different online services. This has changed the workforce drastically as thirty percent of U.S. workers were already in careers in this profession. 136.9 million people were personally connected to the  Internet , which was equivalent to 51 million households. [ 28 ]  Along with the Internet, new types of technology were also being introduced across the globe, which has improved efficiency and made things easier across the globe. Along with technology revolutionizing society, millions of processes could be done in seconds. Innovations in communication were also crucial as people began to rely on the computer to communicate through telephone lines and cable. The introduction of the email was considered revolutionary as \"companies in one part of the world could communicate by e-mail with suppliers and buyers in another part of the world...\" [ 29 ] Not only personally, computers and technology have also revolutionized the marketing industry, resulting in more buyers of their products. During the year of 2002, Americans exceeded $28 billion in goods just over the Internet alone while e-commerce a decade later resulted in $289 billion in sales. [ 29 ]  And as computers are rapidly becoming more sophisticated by the day, they are becoming more used as people are becoming more reliant on them during the twenty-first century. Early electronic computers such as  Colossus  made use of  punched tape , a long strip of paper on which data was represented by a series of holes, a technology now obsolete. [ 30 ]  Electronic data storage, which is used in modern computers, dates from World War II, when a form of  delay-line memory  was developed to remove the clutter from  radar  signals, the first practical application of which was the mercury delay line. [ 31 ]  The first  random-access  digital storage device was the  Williams tube , which was based on a standard  cathode ray tube . [ 32 ]  However, the information stored in it and delay-line memory was volatile in the fact that it had to be continuously refreshed, and thus was lost once power was removed. The earliest form of non-volatile computer storage was the  magnetic drum , invented in 1932 [ 33 ]  and used in the  Ferranti Mark 1 , the world's first commercially available general-purpose electronic computer. [ 34 ] IBM  introduced the first  hard disk drive  in 1956, as a component of their  305 RAMAC  computer system. [ 35 ] : 6   Most digital data today is still stored magnetically on hard disks, or optically on media such as  CD-ROMs . [ 36 ] : 4–5   Until 2002 most information was stored on  analog devices , but that year digital storage capacity exceeded analog for the first time. As of 2007 [update] , almost 94% of the data stored worldwide was held digitally: [ 37 ]  52% on hard disks, 28% on optical devices, and 11% on digital magnetic tape. It has been estimated that the worldwide capacity to store information on electronic devices grew from less than 3  exabytes  in 1986 to 295 exabytes in 2007, [ 38 ]  doubling roughly every 3 years. [ 39 ] Database Management Systems (DMS)  emerged in the 1960s to address the problem of storing and retrieving large amounts of data accurately and quickly. An early such system was  IBM 's  Information Management System  (IMS), [ 40 ]  which is still widely deployed more than 50 years later. [ 41 ]  IMS stores data  hierarchically , [ 40 ]  but in the 1970s  Ted Codd  proposed an alternative relational storage model based on  set theory  and  predicate logic  and the familiar concepts of tables, rows, and columns. In 1981, the first commercially available  relational database management system  (RDBMS) was released by  Oracle . [ 42 ] All DMS consist of components, they allow the data they store to be accessed simultaneously by many users while maintaining its integrity. [ 43 ]  All databases are common in one point that the structure of the data they contain is defined and stored separately from the data itself, in a  database schema . [ 40 ] In recent years, the  extensible markup language  (XML) has become a popular format for data representation. Although XML data can be stored in normal  file systems , it is commonly held in  relational databases  to take advantage of their \"robust implementation verified by years of both theoretical and practical effort.\" [ 44 ]  As an evolution of the  Standard Generalized Markup Language  (SGML), XML's text-based structure offers the advantage of being both  machine-  and  human-readable . [ 45 ] Data transmission  has three aspects: transmission, propagation, and reception. [ 46 ]  It can be broadly categorized as  broadcasting , in which information is transmitted unidirectionally downstream, or  telecommunications , with bidirectional upstream and downstream channels. [ 38 ] XML has been increasingly employed as a means of data interchange since the early 2000s, [ 47 ]  particularly for machine-oriented interactions such as those involved in web-oriented  protocols  such as  SOAP , [ 45 ]  describing \"data-in-transit rather than... data-at-rest\". [ 47 ] Hilbert and Lopez identify the exponential pace of technological change (a kind of  Moore's law ): machines' application-specific capacity to compute information per capita roughly doubled every 14 months between 1986 and 2007; the per capita capacity of the world's general-purpose computers doubled every 18 months during the same two decades; the global  telecommunication  capacity per capita doubled every 34 months; the world's storage capacity per capita required roughly 40 months to double (every 3 years); and per capita broadcast information has doubled every 12.3 years. [ 38 ] Massive amounts of data are stored worldwide every day, but unless it can be analyzed and presented effectively it essentially resides in what have been called data tombs: \"data archives that are seldom visited\". [ 48 ]  To address that issue, the field of  data mining  — \"the process of discovering interesting patterns and knowledge from large amounts of data\" [ 49 ]  — emerged in the late 1980s. [ 50 ] The technology and services it provides for sending and receiving electronic messages (called \"letters\" or \"electronic letters\") over a distributed (including global) computer network. In terms of the composition of elements and the principle of operation, electronic mail practically repeats the system of regular (paper) mail, borrowing both terms (mail, letter, envelope, attachment, box, delivery, and others) and characteristic features — ease of use, message transmission delays, sufficient reliability and at the same time no guarantee of delivery. The advantages of e-mail are: easily perceived and remembered by a person addresses of the form user_name@domain_name (for example, somebody@example.com); the ability to transfer both plain text and formatted, as well as arbitrary files; independence of servers (in the general case, they address each other directly); sufficiently high reliability of message delivery; ease of use by humans and programs. Disadvantages of e-mail: the presence of such a phenomenon as spam (massive advertising and viral mailings); the theoretical impossibility of guaranteed delivery of a particular letter; possible delays in message delivery (up to several days); limits on the size of one message and on the total size of messages in the mailbox (personal for users). A software and hardware complex with a web interface that provides the ability to search for information on the Internet. A search engine usually means a site that hosts the interface (front-end) of the system. The software part of a search engine is a search engine (search engine) — a set of programs that provides the functionality of a search engine and is usually a trade secret of the search engine developer company. Most search engines look for information on  World Wide Web  sites, but there are also systems that can look for files on FTP servers, items in online stores, and information on Usenet newsgroups. Improving search is one of the priorities of the modern Internet (see the Deep Web article about the main problems in the work of search engines). Companies in the information technology field are often discussed as a group as the \"tech sector\" or the \"tech industry.\" [ 51 ] [ 52 ] [ 53 ]   These titles can be misleading at times and should not be mistaken for \"tech companies;\" which are generally large scale, for-profit corporations that sell consumer technology and software.  It is also worth noting that from a business perspective, Information technology departments are a \" cost center \" the majority of the time.  A cost center is a department or staff which incurs expenses, or \"costs\", within a company rather than generating profits or revenue streams. Modern businesses rely heavily on technology for their day-to-day operations, so the expenses delegated to cover technology that facilitates business in a more efficient manner are usually seen as \"just the cost of doing business.\"  IT departments are allocated funds by senior leadership and must attempt to achieve the desired deliverables while staying within that budget. Government and the private sector might have different funding mechanisms, but the principles are more-or-less the same. This is an often overlooked reason for the rapid interest in automation and  Artificial Intelligence , but the constant pressure to do more with less is opening the door for automation to take control of at least some minor operations in large companies. Many companies now have IT departments for managing the  computers , networks, and other technical areas of their businesses. Companies have also sought to integrate IT with business outcomes and decision-making through a BizOps or business operations department. [ 54 ] In a business context, the  Information Technology Association of America  has defined information technology as \"the study, design, development, application, implementation, support, or management of computer-based information systems\". [ 55 ] [ page needed ]  The responsibilities of those working in the field include network administration, software development and installation, and the planning and management of an organization's technology life cycle, by which hardware and software are maintained, upgraded, and replaced. Information services is a term somewhat loosely applied to a variety of IT-related services offered by commercial companies, [ 56 ] [ 57 ] [ 58 ]  as well as  data brokers . The field of information ethics was established by mathematician  Norbert Wiener  in the 1940s. [ 60 ] : 9   Some of the ethical issues associated with the use of information technology include: [ 61 ] : 20–21 Research suggests that IT projects in business and public administration can easily become significant in scale. Work conducted by  McKinsey  in collaboration with the  University of Oxford  suggested that half of all large-scale IT projects (those with initial  cost estimates  of $15 million or more) often failed to maintain costs within their initial budgets or to complete on time. [ 62 ]"
  },
  {
    "id": 24,
    "title": "Intellectual freedom",
    "content": "Intellectual freedom  encompasses the freedom to hold, receive and disseminate ideas without restriction. [ 1 ]  Viewed as an integral component of a democratic society, intellectual freedom protects an individual's right to access, explore, consider, and express ideas and information as the basis for a self-governing, well-informed citizenry. Intellectual freedom comprises the bedrock for freedoms of expression,  speech , and the  press  and relates to  freedoms of information  and the  right to privacy . The  United Nations  upholds intellectual freedom as a basic human right through Article 19 of the  Universal Declaration of Human Rights  which asserts: Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers. [ 2 ] The institution of libraries in particular values intellectual freedom as part of their mission to provide and protect  access to information  and ideas. The  American Library Association  (ALA) defines intellectual freedom as \"the right of every individual to both seek and receive information from all points of view without restriction. It provides for free access to all expressions of ideas through which any and all sides of a question, cause or movement can be explored.\" [ 1 ] The modern concept of intellectual freedom developed out of an opposition to book censorship. [ 3 ]  It is promoted by several professions and movements. These entities include, among others,  librarianship ,  education , and the  free software movement . Intellectual freedom encompasses many areas including issues of  academic freedom ,  Internet filtering , and  censorship . [ 4 ]  Because proponents of intellectual freedom value an individual's right to choose informational concepts and media to formulate thought and opinion without repercussion, restrictions to access and barriers to privacy of information constitute intellectual freedom issues. \nIssues surrounding restrictions to access include: Issues concerning barriers to privacy of information include: While proponents of intellectual freedom work to prohibit acts of censorship, calls for censorship are valued as free speech. \"In expressing their opinions and concerns, would-be censors are exercising the same rights librarians seek to protect when they confront censorship. In making their criticisms known, people who object to certain ideas are exercising the same rights as those who created and disseminated the material to which they object.\" [ 1 ]  The  first amendment  right to voice opinions and persuade others—both for the exclusion and inclusion of content and concepts—should be protected. The contemporary definition, limits, and inclusions of intellectual freedom primarily developed through a number of  common law  judgments by the  United States Supreme Court  regarding the  First Amendment  and policy statements of groups dedicated to the advocacy and defense of  civil liberties . In his oft-quoted dissent on the free speech case of two defendants convicted of inciting anti-war sentiment and action, Supreme Court justice  Oliver Wendell Holmes Jr.  aligns the freedoms of speech and expression with the freedom of thought as follows: A case in which the Supreme Court sustains the conviction of a woman for anti-government speech akin to terrorism. In his opinion on the matter, Justice Brandeis delineates the role of freedom of thought to inform free speech, attributing the value of intellectual freedom as a civil liberty to the founders of the United States, asserting: A case in which the US Supreme Court deliberated whether a citizen's Fourth or Fifth Amendment rights were violated when evidence to convict him of bootlegging was obtained through wiretapping. Justice Brandeis provides precedence for the inclusion of intellectual freedom as a constitutional right in his dissenting opinion, claiming the US Constitution's authors \"recognized the significance of man's spiritual nature, his feelings, and his intellect\" and \"sought to protect Americans in their beliefs, their thoughts, their emotions and their sensations.\" Brandeis would ultimately argue for the right to privacy, another important dimension of intellectual freedom, as an extension of American civil rights. [ 7 ] In the Supreme Court's upheld decision to deny citizenship to Rosika Schwimmer, a Hungarian immigrant, because she refused to pledge to take up arms to defend the United States out of her pacifist views and beliefs, Justice Oliver Wendell Holmes Jr. personally disagrees with the defendant's views but professionally upholds Schwimmer's position when he writes, The  American Library Association  adopts the Library Bill of Rights affirming \"that all libraries are forums for information and ideas.\" Originally a three-point declaration to guide services in American free public libraries including statements on \"growing intolerance, suppression of free speech, and censorship,\" today the Library Bill of Rights includes six basic policies to guide library services that affirm intellectual freedom. Following World War II, the  United Nations  adopts The Universal Declaration of Human Rights as a \"foundation of human rights law\" consisting of 30 articles on international freedoms among the nations of the UN General Assembly. [ 9 ]  Articles 18 and 19 specifically affirm rights to freedoms of thought, opinion, and expression, as well as the right to \"seek, receive, and impart information and ideas through any media and regardless of frontiers.\" [ 10 ] In his 1953 acceptance speech for the Lauterbach Award for support of civil liberties, Supreme Court justice  William O. Douglas  affirms that \"safety of our civilization lies in making freedom of thought and freedom of speech vital, vivid features of life\" and condemns \"[r]estriction of free thought and free speech,\" labeling it \"the most dangerous of all subversions,\" and an \"un-American act.\" [ 11 ] The American Library Association adopts The Freedom to Read, a key library policy endorsing an individual's civil rights to free expression and intellectual freedom through the exchange of ideas through reading and writing. The ALA's  The Freedom to Read  includes seven affirmations and responsibilities to protect an individual's right to read as a basic tenet of democracy.  In 1979, the ALA expands upon The Freedom to Read, adopting The Freedom to View, a policy which extends the understanding of intellectual freedom to include the visual acquisition of information through visual media such as art, video, movies, pictures, the internet, and more. A case in which the US Supreme Court established the  Imminent Lawless Action  standard.  The Supreme Court overturned KKK leader, Clarence Brandenburg's conviction of one to ten years in prison and a fine of $1000 sentenced by the  Court of Common Pleas  of  Hamilton County . The Court ruled that hate speech is protected under  First Amendment  rights as long as it does not incite violence. [ 12 ]  This ruling established the modern doctrine of  clear and present danger  which determines what limits may be placed on First Amendment freedoms.  Only speech that directly incites lawless action may be restricted. The profession of librarianship views intellectual freedom as a core responsibility.  The International Federation of Library Associations and Institutions'  (IFLA)  Statement on Libraries and Intellectual Freedom  \"calls upon libraries and library staff to adhere to the principles of intellectual freedom, uninhibited access to information and freedom of expression and to recognize the privacy of library user.\"  IFLA urges its members to actively promote the acceptance and realization of intellectual freedom principles.  IFLA states: \"The right to know is a requirement for freedom of thought and conscience; freedom of thought and freedom of expression are necessary conditions for freedom of access to information.\" [ 13 ] Individual national library associations expand upon these principles when defining intellectual freedom for their constituents. For example, the  American Library Association  defines intellectual freedom as: \"[T]he right of every individual to both seek and receive information from all points of view without restriction. It provides for free access to all expressions of ideas through which any and all sides of a question, cause or movement may be explored. ....  Intellectual freedom encompasses the freedom to hold, receive and disseminate ideas.\" [ 1 ] The  Canadian Library Association's   Position Statement on Intellectual Freedom  states that all persons possess \"the fundamental right ... to have access to all expressions of knowledge, creativity and intellectual activity, and to express their thoughts publicly.\" [ 14 ]  This right was enshrined into law in 2004 in  British Columbia , which grants protection against litigation for libraries for their holdings. [ 15 ] Many other national library associations have similarly adopted  statements on intellectual freedom . The ALA's Office for Intellectual Freedom organizes the relationship between librarianship and intellectual freedom into five distinct categories: Libraries protect, defend, and advocate for intellectual freedom through a variety of organizations and resources. The Intellectual Freedom Committee (IFC) is a council committee of the American Library Association (ALA), composed of 11 ALA members who are appointed by ALA's Council to serve 2-year terms. [ 17 ]  The Intellectual Freedom Committee functions as an advisory and educational arm of the ALA's commitment to intellectual freedom. The IFC recommends policies concerning intellectual freedom and censorship, drafts guidelines for library professionals to advocate and defend intellectual freedom including The Universal Right to Free Expression and Importance of Education to Intellectual Freedom, and drafts policy statements adopted by the ALA including several interpretation statements on the Library Bill of Rights such as: The IFC drafts and submits statements to the ALA as part of the committee's charge to \"recommend such steps as may be necessary to safeguard the rights of library users, libraries, and librarians, in accordance with the first amendment to the united states constitution and the Library Bill of Rights as adopted by the ALA Council [and] work closely with the Office for Intellectual Freedom and with other units and officers of the association in matters touching intellectual freedom and censorship.\" [ 19 ] Formed in 1940 and originally titled 'Committee on Intellectual Freedom to Safeguard the Rights of Library Users to Freedom of Inquiry,' the committee has also been known as 'Committee on Intellectual Freedom' before the currently titled 'Intellectual Freedom Committee.' [ 20 ]  Following the ALA's formation of the IFC to promote intellectual freedom on a national level, many regional and state library associations have established additional intellectual freedom committees on the state level. The American Library Association's Office for Intellectual Freedom (OIF) serves as an administrative arm of ALA committees such as the Intellectual Freedom Committee and the Committee on Professional Ethics. [ 21 ]  Principally charged with implementing ALA policies concerning intellectual freedom, the OIF focuses efforts on intellectual freedom education and coordination of intellectual freedom activities, events, and organizations and views the \"responsibility of the office to recommend, develop, implement, and maintain a total intellectual freedom program for ALA. [ 22 ] \nOIF functions include: The ALA's Intellectual Freedom Round Table (IFRT) functions as a forum for ALA members to participate in intellectual freedom initiatives and efforts. The IFRT serves as a communication channel and promotional group for ALA members seeking increasing participation and knowledge in intellectual freedom concepts and issues. While the IFRT mirrors other intellectual freedom organizations through monitoring, support, and educational efforts, the IFRT provides more varied intellectual freedom discussion forums for librarians in two ways: In addition to encouraging and fostering a community of librarians learning, promoting, and defending intellectual freedom principles, the IFRT administers three intellectual freedom awards (see below) and produces an Intellectual Freedom Report to members of the American Library Association four times per year. [ 29 ] The  Freedom to Read Foundation  was incorporated in 1969 by members of the American Library Association. Although founded by ALA members, the FTRF is a separate organization from ALA with separate membership focused upon the legal defense of intellectual freedom for libraries, librarians, library staff, and library trustees. [ 30 ]  While the FTRF participates in intellectual freedom education efforts, the FTRF primarily aims to \"support and defend librarians whose positions are jeopardized because of their resistance to abridgments of the First Amendment; and to set legal precedent for the freedom to read on behalf of all the people.\" [ 31 ]  In the foundation's commitment to \"the principle that the solution to offensive speech is more speech, and the suppression of speech on the grounds that it gives offense to some infringes on the rights of all to a free, open and robust marketplace of ideas,\" [ 32 ]  the FTRF awards and distributes grants to aid intellectual freedom litigation, directly participates as a party to intellectual freedom litigation, and submits amicus curiae briefs in freedom of speech and freedom of the press cases. [ 33 ]  FTRF assistance to library staff whose jobs have been jeopardized due to their defense of intellectual freedom \"attempts to obviate the choice between upholding intellectual freedom principles and\" what lauded librarian and library-science scholar  Lester Asheim  called \"three square meals a day.\" [ 34 ] [ 35 ]  \nThe organization's charter describes four purposes for the Foundation, including: The LeRoy C. Merritt Humanitarian Fund provides financial assistance to librarians who are: Originally established by the Freedom to Read Foundation in 1970, [ 37 ]  the Merritt Fund now functions independently, governed by three trustees elected by donors to the fund. [ 38 ]  The fund's namesake LeRoy C. Merritt participated in the defense and advocacy of intellectual freedom throughout his life in a variety of ways including authoring numerous intellectual freedom and anti-censorship books and articles, editing the ALA's Newsletter on Intellectual Freedom from 1962 to 1970, as the first recipient of the Robert B. Downs Intellectual Freedom Award, and, donating the entirety of the Downs prize to the Freedom to Read Foundation, as the FTRF's first benefactor. [ 39 ] The  American Library Association 's Office for Intellectual Freedom publishes the  Intellectual Freedom Manual , now in its ninth edition. Considered an authoritative resource on intellectual freedom for library professionals, it is also of use to members of the public who wish to stay informed of the most recent policies and developments in the field. [ 40 ]  As well as providing an historical overview of the topic, it is divided into parts which cover key issues such as the  Library Bill of Rights , protecting the freedom to read, intellectual freedom and the law, and preserving, protecting and working for intellectual freedom. [ 3 ]  Expanding on the new addition to the manual is the section on Privacy; an Interpretation of the Library Bill of Rights  [ 41 ] Many of the entities listed above collaborate with one another and other organizations including: Since 1969, the  Graduate School of Library and Information Science  (GSLIS) at the University of Illinois annually awards the Robert B. Downs Intellectual Freedom Award. GSLIS faculty named this award for  Robert B. Downs  on his 25th anniversary as director of the School in honor of his role as a champion for intellectual freedom. [ 42 ]  Downs, also a former president and vice-president of the ALA, focused his library career working against, and voicing opposition to, literary censorship and authored many books and publications on topics of censorship and intellectual freedom. [ 43 ]  Awarded to acknowledge individuals or groups who have furthered the cause of intellectual freedom in libraries, the Robert B. Downs Intellectual Freedom Award is \"[g]ranted to those who have resisted censorship or efforts to abridge the freedom of individuals to read or view materials of their choice, the award may be in recognition of a particular action or long-term interest in, and dedication to, the cause of intellectual freedom.\" [ 44 ] Since 1986, the American Library Association Intellectual Freedom Round Table biennially sponsors the Eli M. Oboler Memorial Award. Consisting of a $500 prize and certificate, the award acknowledges \"the best published work in the area of intellectual freedom.\" [ 45 ]  The IFRT posthumously named this award for  Eli M. Oboler , a former Idaho State University librarian known as a “champion of intellectual freedom who demanded the dismantling of all barriers to freedom of expression.” [ 46 ]  Oboler, also a former member and officer in numerous intellectual freedom organizations including the Intellectual Freedom Round Table, the ALA Intellectual Freedom Committee, the Freedom to Read Foundation, and the Idaho Library Association's Intellectual Freedom Committee, authored over 200 publications, many on censorship and intellectual freedom, including: Awarded to acknowledge authorship in the area of intellectual freedom, the IFRT considers \"single articles (including review pieces), a series of thematically connected articles, books, or manuals published on the local, state or national level in English or English translation\" for receipt of the Eli M. Oboler Award. [ 46 ] Since 1976, the ALA's Intellectual Freedom Round Table annually sponsors the John Phillip Immroth Memorial Award. Consisting of a $500 prize and a citation, the award \"honors the courage, dedication, and contribution of a living individual, group, or organization who has set the finest kind of example for the defense and furtherance of the principles of intellectual freedom.\" [ 48 ]  Upon his death in 1979, the award was renamed for John Phillip Immroth, the founder and first Chair of the Intellectual Freedom Round Table. [ 49 ]  The Immroth award differs from other intellectual freedom awards in that it recognizes \"extraordinary personal courage in the defense of intellectual freedom.\" [ 50 ] Since 1984, the ALA's Intellectual Freedom Round Table annually sponsors a regional intellectual freedom award, currently named the Gerald Hodges Intellectual Freedom Chapter Relations Award. Consisting of a $1000 prize and citation, the award \"recognizes an intellectual freedom focused organization that has developed a strong multi-year, ongoing program or a single, one-year project that exemplifies support for intellectual freedom, patron confidentiality, and anti-censorship efforts.\" [ 51 ]  The IFRT posthumously named this award for Gerald Hodges, a longtime ALA officer who devoted his library career to his passion for both intellectual freedom and chapter relations until his death in 2006. [ 52 ]  In 2010 the Gerald Hodges Intellectual Freedom Award replaced the IFRT State and Regional Intellectual Freedom Achievement Award which had been annually awarded \"to the most innovative and effective intellectual freedom project covering a state or region.\" [ 52 ] Since 1982, the  American Association of School Librarians  (AASL), a division of the ALA, annually awards the Intellectual Freedom Award. Consisting of a $2000 prize to the recipient and a $1000 prize to the school library program of the recipient's choice, the award honors a school librarian \"for upholding the principles of intellectual freedom as set forth by the American Association of School Librarians and the American Library Association.\" [ 53 ] Since 2007, the  Public Library Association  (PLA), a division of the ALA, annually awards the Gordon M. Conable Award. Consisting of a $1500 prize and commemorative plaque, the award \"honors a public library staff member, a library trustee, or a public library that has demonstrated a commitment to intellectual freedom and the Library Bill of Rights.\" [ 54 ] Intellectual freedom is often suppressed under authoritarian rule [ 55 ]  and such governments often claim to have nominal intellectual freedom, although the degree of freedom is a matter of dispute. The former  USSR , for example, claimed to provide intellectual freedom, but some analysts in the West have stated that the degree of intellectual freedom was nominal at best. [ 55 ] [ 56 ] During times of crises there is often debate within democratic countries as to the balance between national security, a successful conclusion to the crises and the maintenance of democratic civil liberties. This debate often takes the form of to what extent a democratic government can curtail civil liberties in the interest of successfully ending the crises. Such a debate existed in Canada during the Second World War. Since the  First World War  the  War Measures Act  had existed as legislation in Canada to allow the government to operate with greater powers during times of national crises, such as in wartime. During the  Second World War  the federal Liberal government of  Prime Minister William Lyon Mackenzie King  enacted the measure by  Order-in-Council . The War Measures Act and with it the  Defence of Canada Regulations  were passed by the federal government in early September 1939. With their implementation civil liberties, especially the intellectual freedom of political dissenters, were curtailed. [ 57 ]    As well, in Quebec the  Union Nationale  government of  Premier Maurice Duplessis  enacted “An Act Respecting Communist Propaganda”, which came to be known as the  Padlock Act . It gave Premier Duplesis, as Attorney General of Quebec, the power to close (hence padlock) any premises used for the purposes of “propagating Communism or Bolshevism.” The Act was criticized by  Eugene Forsey , for example, as being far too broad in definition and that it gave the Premier the power to suppress any opinions that he wished to. Forsey cited examples of such abuse in the  Canadian Forum . [ 58 ] All of these measures were criticized by writers in the Canadian Forum such as Eugene Forsey [ 58 ]   and  Frank R. Scott  and by the  League for Social Reconstruction  in general; a group to which both Forsey and Scott belonged. Indeed, during the Second World War the Canadian Forum printed an anonymous monthly column outlining civil liberties abuses by Canadian authorities. [ 57 ] In the aftermath of the  September 11th attacks  issues concerning the suspension or reduction of civil liberties in the name of national security have arisen. Legislation such as the  Homeland Security Act  (HSA) of 2002 and the  USA PATRIOT Act  (often shortened to the Patriot Act) of 2001 encroach upon intellectual freedom rights to privacy and freedom of information to enhance domestic security from potential terrorist threats and acts. The Patriot Act in particular has come under fire from numerous intellectual freedom organizations. The  Electronic Privacy Information Center  (EPIC) has criticized the Patriot Act as unconstitutional, especially when \"the private communications of law-abiding American citizens might be intercepted incidentally,\" [ 59 ]  Additionally, the  Electronic Frontier Foundation  (EFF) maintains that the lower standard applied to wiretaps \"gives the FBI a 'blank check' to violate the communications privacy of countless innocent Americans\". [ 60 ]  The  American Library Association  (ALA) has partnered with American libraries in opposition to a provision in Section 215 which allows the FBI to apply for an order to produce materials that assist in an investigation undertaken to protect against international terrorism or clandestine intelligence activities. The \"tangible things\" that can be targeted include \"books, records, papers, documents, and other items\". [ 61 ]"
  },
  {
    "id": 25,
    "title": "Intellectual property",
    "content": "This is an accepted version of this page Intellectual property  ( IP ) is a category of  property  that includes intangible creations of the human intellect. [ 1 ] [ 2 ]  There are many types of intellectual property, and some countries recognize more than others. [ 3 ] [ 4 ] [ 5 ]  The best-known types are  patents ,  copyrights ,  trademarks , and  trade secrets . The modern concept of intellectual property developed in  England  in the 17th and 18th centuries. The term \"intellectual property\" began to be used in the 19th century, though it was not until the late 20th century that intellectual property became commonplace in most of the world's  legal systems . [ 6 ] Supporters of intellectual property laws often describe their main purpose as encouraging the creation of a wide variety of intellectual goods. [ 7 ]  To achieve this, the law gives people and businesses property rights to certain information and intellectual goods they create, usually for a limited period of time. Supporters argue that because IP laws allow people to protect their original ideas and prevent unauthorized copying, creators derive greater individual economic benefit from the information and intellectual goods they create, and thus have more economic incentives to create them in the first place. [ 7 ]  Advocates of IP believe that these economic incentives and legal protections stimulate  innovation  and contribute to technological progress of certain kinds. [ 8 ] The  intangible  nature of intellectual property presents difficulties when compared with traditional property like land or goods. Unlike traditional property, intellectual property is \"indivisible\", since an unlimited number of people can in theory \"consume\" an intellectual good without its being depleted. [ 9 ]  Additionally, investments in intellectual goods suffer from appropriation problems: Landowners can surround their land with a robust fence and hire armed guards to protect it, but producers of information or literature can usually do little to stop their first buyer from replicating it and selling it at a lower price. Balancing rights so that they are strong enough to encourage the creation of intellectual goods but not so strong that they prevent the goods' wide use is the primary focus of modern intellectual property law. [ 10 ] The  Venetian Patent Statute  of 19 March 1474, established by the  Republic of Venice , is usually considered to be the earliest codified patent system in the world. [ 11 ] [ 12 ]  It states that patents might be granted for \"any  new  and ingenious device, not previously made\", provided it was useful. By and large, these principles still remain the basic principles of current patent laws. The  Statute of Monopolies  (1624) and the British  Statute of Anne  (1710) are seen as the origins of the current  patent law  and  copyright  respectively, [ 13 ]  firmly establishing the concept of intellectual property. \"Literary property\" was the term predominantly used in the British legal debates of the 1760s and 1770s over the extent to which authors and publishers of works also had rights deriving from the common law of property ( Millar v Taylor  (1769),  Hinton v Donaldson  (1773),  Donaldson v Becket  (1774)). The first known use of the term  intellectual property  dates to this time, when a piece published in the  Monthly Review  in 1769 used the phrase. [ 14 ]  The first clear example of modern usage goes back as early as 1808, when it was used as a heading title in a collection of essays. [ 15 ] The German equivalent was used with the founding of the  North German Confederation  whose  constitution  granted legislative power over the protection of intellectual property ( Schutz des geistigen Eigentums ) to the confederation. [ 16 ]  When the administrative secretariats established by the  Paris Convention  (1883) and the  Berne Convention  (1886) merged in 1893, they located in Berne, and also adopted the term intellectual property in their new combined title, the  United International Bureaux for the Protection of Intellectual Property . The organization subsequently relocated to Geneva in 1960 and was succeeded in 1967 with the establishment of the  World Intellectual Property Organization  (WIPO) by  treaty  as an agency of the  United Nations . According to legal scholar  Mark Lemley , it was only at this point that the term really began to be used in the United States (which had not been a party to the Berne Convention), [ 6 ]  and it did not enter popular usage there until passage of the  Bayh–Dole Act  in 1980. [ 17 ] The history of patents does not begin with inventions, but rather with royal grants by  Queen Elizabeth I  (1558–1603) for monopoly privileges. Approximately 200 years after the end of Elizabeth's reign, however, a patent represents a legal  right  obtained by an inventor providing for exclusive control over the production and sale of his mechanical or scientific invention. demonstrating the evolution of patents from royal prerogative to common-law doctrine. [ 18 ] The term can be found used in an October 1845 Massachusetts Circuit Court ruling in the patent case  Davoll et al. v. Brown , in which Justice Charles L. Woodbury wrote that \"only in this way can we protect intellectual property, the labors of the mind, productions and interests are as much a man's own ... as the wheat he cultivates, or the flocks he rears.\" [ 19 ]  The statement that \"discoveries are ... property\" goes back earlier. Section 1 of the French law of 1791 stated, \"All new discoveries are the property of the author; to assure the inventor the property and temporary enjoyment of his discovery, there shall be delivered to him a patent for five, ten or fifteen years.\" [ 20 ]  In Europe,  French  author A. Nion mentioned  propriété intellectuelle  in his  Droits civils des auteurs, artistes et inventeurs , published in 1846. Until recently, the purpose of intellectual property law was to give as little protection as possible in order to encourage  innovation . Historically, therefore, legal protection was granted only when necessary to encourage invention, and it was limited in time and scope. [ 21 ]  This is mainly as a result of knowledge being traditionally viewed as a public good, in order to allow its extensive dissemination and improvement. [ 22 ] The concept's origin can potentially be traced back further.  Jewish law  includes several considerations whose effects are similar to those of modern intellectual property laws, though the notion of intellectual creations as property does not seem to exist—notably the principle of Hasagat Ge'vul (unfair encroachment) was used to justify limited-term publisher (but not author) copyright in the 16th century. [ 23 ]  In 500 BCE, the government of the Greek state of  Sybaris  offered one year's patent \"to all who should discover any new refinement in luxury\". [ 24 ] According to Jean-Frédéric Morin, \"the global intellectual property regime is currently in the midst of a paradigm shift\". [ 25 ]  Indeed, up until the early 2000s, the global IP regime used to be dominated by high standards of protection characteristic of IP laws from Europe or the United States, with a vision that uniform application of these standards over every country and to several fields with little consideration over social, cultural or environmental values or of the national level of economic development. Morin argues that \"the emerging discourse of the global IP regime advocates for greater policy flexibility and greater access to knowledge, especially for developing countries.\" Indeed, with the Development Agenda adopted by WIPO in 2007, a set of 45 recommendations to adjust WIPO's activities to the specific needs of developing countries and aim to reduce distortions especially on issues such as patients' access to medicines, Internet users' access to information, farmers' access to seeds, programmers' access to source codes or students' access to scientific articles. [ 26 ]  However, this paradigm shift has not yet manifested itself in concrete legal reforms at the international level. [ 27 ] Similarly, it is based on these background that the Trade-Related Aspects of Intellectual Property Rights (TRIPS) agreement requires members of the WTO to set minimum standards of legal protection, but its objective to have a \"one-fits-all\" protection law on Intellectual Property has been viewed with controversies regarding differences in the development level of countries. [ 28 ]  Despite the controversy, the agreement has extensively incorporated intellectual property rights into the global trading system for the first time in 1995, and has prevailed as the most comprehensive agreement reached by the world. [ 29 ] Intellectual property rights include  patents ,  copyright ,  industrial design rights ,  trademarks ,  plant variety rights ,  trade dress ,  geographical indications , [ 30 ]  and in some jurisdictions  trade secrets . There are also more specialized or derived varieties of  sui generis  exclusive rights, such as circuit design rights (called  mask work  rights in the US),  supplementary protection certificates  for pharmaceutical products (after expiry of a patent protecting them), and  database rights  (in  European law ). The term \"industrial property\" is sometimes used to refer to a large subset of intellectual property rights including patents, trademarks, industrial designs, utility models, service marks, trade names, and geographical indications. [ 31 ] A  patent  is a form of right granted by the government to an inventor or their successor-in-title, giving the owner the right to exclude others from making, using, selling, offering to sell, and importing an  invention  for a limited period of time, in exchange for the public disclosure of the invention. An invention is a solution to a specific technological problem, which may be a product or a process, and generally has to fulfill three main requirements: it has to be  new ,  not obvious  and there needs to be an  industrial applicability . [ 32 ] : 17   To enrich the body of knowledge and to stimulate innovation, it is an obligation for patent owners to disclose valuable information about their inventions to the public. [ 33 ] A  copyright  gives the creator of an original work  exclusive rights  to it, usually for a limited time. Copyright may apply to a wide range of creative, intellectual, or artistic forms, or \"works\". [ 34 ] [ 35 ]  Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed. [ 36 ] An  industrial design right  (sometimes called \"design right\" or  design patent ) protects the visual design of objects that are not purely utilitarian. An industrial design consists of the creation of a shape, configuration or composition of pattern or color, or combination of pattern and color in three-dimensional form containing aesthetic value. An industrial design can be a two- or three-dimensional pattern used to produce a product, industrial commodity or handicraft. Generally speaking, it is what makes a product look appealing, and as such, it increases the commercial value of goods. [ 33 ] Plant breeders' rights  or plant variety rights are the rights to commercially use a  new variety of a plant . The variety must, amongst others, be novel and distinct and for registration the evaluation of propagating material of the variety is considered. A  trademark  is a recognizable  sign ,  design  or  expression  that distinguishes a particular trader's  products  or  services  from similar products or services of other traders. [ 37 ] [ 38 ] [ 39 ] Trade dress  is a legal term of art that generally refers to characteristics of the visual and aesthetic appearance of a product or its packaging (or even the design of a building) that signify the source of the product to consumers. [ 40 ] A  trade secret  is a  formula , practice, process,  design , instrument,  pattern , or compilation of  information  which is not generally known or reasonably ascertainable, by which a  business  can obtain an economic advantage over competitors and customers. There is no formal government protection granted; each business must take measures to guard its own trade secrets (e.g., Formula of its soft drinks is a trade secret for  Coca-Cola .) The main purpose of intellectual property law is to encourage the creation of a wide variety of intellectual goods for consumers. [ 7 ]  To achieve this, the law gives people and businesses property rights to the information and intellectual goods they create, usually for a limited period of time. Because they can then profit from them, this gives economic incentive for their creation. [ 7 ]  The intangible nature of intellectual property presents difficulties when compared with traditional property like land or goods. Unlike traditional property, intellectual property is indivisible—an unlimited number of people can \"consume\" an intellectual good without it being depleted. Additionally, investments in intellectual goods suffer from problems of appropriation—while a landowner can surround their land with a robust fence and hire armed guards to protect it, a producer of information or an intellectual good can usually do very little to stop their first buyer from replicating it and selling it at a lower price. Balancing rights so that they are strong enough to encourage the creation of information and intellectual goods but not so strong that they prevent their wide use is the primary focus of modern intellectual property law. [ 10 ] By exchanging limited exclusive rights for disclosure of inventions and creative works, society and the patentee/copyright owner mutually benefit, and an incentive is created for inventors and authors to create and disclose their work. Some commentators have noted that the objective of intellectual property legislators and those who support its implementation appears to be \"absolute protection\". \"If some intellectual property is desirable because it encourages innovation, they reason, more is better. The thinking is that creators will not have sufficient incentive to invent unless they are legally entitled to capture the full social value of their inventions\". [ 21 ]  This absolute protection or full value view treats intellectual property as another type of \"real\" property, typically adopting its law and rhetoric. Other recent developments in intellectual property law, such as the  America Invents Act , stress international harmonization. Recently there has also been much debate over the desirability of using intellectual property rights to protect cultural heritage, including intangible ones, as well as over risks of  commodification  derived from this possibility. [ 41 ]  The issue still remains open in legal scholarship. These exclusive rights allow intellectual property owners to benefit from the property they have created, providing a financial incentive for the creation of an investment in intellectual property, and, in case of patents, pay associated  research and development  costs. [ 42 ]  In the United States Article I Section 8 Clause 8 of the Constitution, commonly called the Patent and Copyright Clause, reads; \"The Congress shall have power 'To promote the progress of science and useful arts, by securing for limited times to authors and inventors the exclusive right to their respective writings and discoveries. ' \" [ 43 ]  \"Some commentators, such as  David Levine  and  Michele Boldrin , dispute this justification. [ 44 ] In 2013, the  United States Patent and Trademark Office  approximated that the worth of intellectual property to the  U.S. economy  is more than US$5 trillion and creates employment for an estimated 18 million American people. The value of intellectual property is considered similarly high in other developed nations, such as those in the European Union. [ 45 ]  In the UK, IP has become a recognised asset class for use in  pension-led funding  and other types of business finance. However, in 2013, the  UK Intellectual Property Office  stated: \"There are millions of intangible business assets whose value is either not being leveraged at all, or only being leveraged inadvertently\". [ 46 ] An October 2023 study released by  Americans for the Arts  (AFTA) found that \"nonprofit arts and culture organizations and their audiences generated $151.7 billion in economic activity—$73.3 billion in spending by the organizations, which leveraged an additional $78.4 billion in event-related spending by their audiences.\" This spending supported 2.6 million jobs and generated $29.1 billion in local, state and federal tax revenue.\" 224,000 audience members and over 16,000 organizations in all 50 states and Puerto Rico were surveyed over an 18-month period to collect the data. [ 47 ] The WIPO treaty and several related international agreements underline that the protection of intellectual property rights is essential to maintaining economic growth. The  WIPO Intellectual Property Handbook  gives two reasons for intellectual property laws: One is to give statutory expression to the moral and economic rights of creators in their creations and the rights of the public in access to those creations. The second is to promote, as a deliberate act of Government policy, creativity and the dissemination and application of its results and to encourage fair trading which would contribute to economic and social development. [ 48 ] The  Anti-Counterfeiting Trade Agreement  (ACTA) states that \"effective enforcement of intellectual property rights is critical to sustaining economic growth across all industries and globally\". [ 49 ] Economists estimate that two-thirds of the value of large businesses in the United States can be traced to intangible assets. [ 50 ]  \"IP-intensive industries\" are estimated to generate 72% more  value added  (price minus material cost) per employee than \"non-IP-intensive industries\". [ 51 ] [ dubious  –  discuss ] A joint research project of the  WIPO  and the  United Nations University  measuring the impact of IP systems on six Asian countries found \"a positive correlation between the strengthening of the IP system and subsequent economic growth.\" [ 52 ] According to Article 27 of the  Universal Declaration of Human Rights , \"everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author\". [ 53 ]  Although the relationship between intellectual property and  human rights  is complex, [ 54 ]  there are moral arguments for intellectual property. The arguments that justify intellectual property fall into three major categories. Personality theorists believe intellectual property is an extension of an individual. Utilitarians believe that intellectual property stimulates social progress and pushes people to further innovation. Lockeans argue that intellectual property is justified based on deservedness and hard work. [ 55 ] Various moral justifications for private property can be used to argue in favor of the morality of intellectual property, such as: Lysander Spooner  (1855) argues \"that a man has a natural and absolute right—and if a natural and absolute, then necessarily a perpetual, right—of property, in the ideas, of which he is the discoverer or creator; that his right of property, in ideas, is intrinsically the same as, and stands on identically the same grounds with, his right of property in material things; that no distinction, of principle, exists between the two cases\". [ 62 ] Writer  Ayn Rand  argued in her book  Capitalism: The Unknown Ideal  that the protection of intellectual property is essentially a moral issue. The belief is that the human mind itself is the source of wealth and survival and that all property at its base is intellectual property. To violate intellectual property is therefore no different morally than violating other property rights which compromises the very processes of survival and therefore constitutes an immoral act. [ 63 ] Violation of intellectual property rights, called \"infringement\" with respect to patents, copyright, and trademarks, and \"misappropriation\" with respect to trade secrets, may be a breach of civil law or criminal law, depending on the type of intellectual property involved, jurisdiction, and the nature of the action. As of 2011, trade in counterfeit copyrighted and trademarked works was a $600 billion industry worldwide and accounted for 5‍–‍7% of global trade. [ 64 ]  During the  Russian invasion of Ukraine , IP has been a consideration in punishment of the aggressor through trade sanctions, [ 65 ]  has been proposed as a method to prevent future wars of aggression involving  nuclear weapons , [ 66 ]  and has caused concern about stifling innovation by keeping patent information secret. [ 67 ] Patent infringement typically is caused by using or selling a patented invention without permission from the patent holder, i.e. from the patent owner. The scope of the patented invention or the extent of protection [ 68 ]  is defined in the  claims  of the granted patent. There is  safe harbor  in many jurisdictions to use a patented invention for research. This safe harbor does not exist in the US unless the research is done for purely philosophical purposes, or to gather data to prepare an application for regulatory approval of a drug. [ 69 ]  In general, patent infringement cases are handled under civil law (e.g., in the United States) but several jurisdictions incorporate infringement in criminal law also (for example, Argentina, China, France, Japan, Russia, South Korea). [ 70 ] Copyright infringement is reproducing, distributing, displaying or performing a  work , or to make  derivative works , without permission from the copyright holder, which is typically a publisher or other business representing or assigned by the work's creator. It is often called \"piracy\". [ 71 ]  In the United States, while copyright is created the instant a work is fixed, generally the copyright holder can only get money damages if the owner registers the copyright. [ 72 ]  Enforcement of copyright is generally the responsibility of the copyright holder. [ 73 ]  The  ACTA trade agreement , signed in May 2011 by the United States, Japan, Switzerland, and the EU, and which has not entered into force, requires that its parties add criminal penalties, including incarceration and fines, for copyright and trademark infringement, and obligated the parties to actively police for infringement. [ 64 ] [ 74 ]  There are  limitations and exceptions to copyright , allowing limited use of copyrighted works, which does not constitute infringement. Examples of such doctrines are the  fair use  and  fair dealing  doctrine. Trademark infringement occurs when one party uses a trademark that is identical or  confusingly similar  to a trademark owned by another party, in relation to products or services which are identical or similar to the products or services of the other party. In many countries, a trademark receives protection without registration, but registering a trademark provides legal advantages for enforcement. Infringement can be addressed by civil litigation and, in several jurisdictions, under criminal law. [ 64 ] [ 74 ] Trade secret misappropriation is different from violations of other intellectual property laws, since by definition trade secrets are secret, while patents and registered copyrights and trademarks are publicly available. In the United States, trade secrets are protected under state law, and states have nearly universally adopted the  Uniform Trade Secrets Act . The United States also has federal law in the form of the  Economic Espionage Act of 1996  ( 18 U.S.C.   §§ 1831 – 1839 ), which makes the theft or misappropriation of a trade secret a federal crime. This law contains two provisions criminalizing two sorts of activity. The first,  18 U.S.C.   § 1831(a) , criminalizes the theft of trade secrets to benefit foreign powers. The second,  18 U.S.C.   § 1832 , criminalizes their theft for commercial or economic purposes. (The statutory penalties are different for the two offenses.) In  Commonwealth   common law  jurisdictions, confidentiality and trade secrets are regarded as an  equitable  right rather than a  property  right but penalties for theft are roughly the same as in the United States. [ citation needed ] The international governance of IP involves multiple overlapping institutions and forums. [ 75 ] : 25   There is no overall rule-making body. [ 75 ] : 25 One of the most important aspects of global IP governance is the  Agreement on Trade Related Aspects of Intellectual Property Rights  (TRIPS). [ 75 ] : 7   The TRIPS Agreement sets minimum international standards for IP which every member of the  World Trade Organization  (WTO) must comply with. [ 75 ] : 7   A member's non-compliance with the TRIPS Agreement may be grounds for suit under the  WTO's Dispute Settlement Mechanism . [ 75 ] : 7 Bilateral and multi-lateral agreements often establish IP requirements above the requirements of the TRIPS Agreement. [ 75 ] : 7 Criticism of the term  intellectual property  ranges from discussing its vagueness and abstract overreach to direct contention to the semantic validity of using words like  property  and  rights  in fashions that contradict practice and law. Many detractors think this term specially serves the doctrinal agenda of parties opposing reform in the public interest or otherwise abusing related legislations, and that it disallows intelligent discussion about specific and often unrelated aspects of copyright, patents, trademarks, etc. [ 76 ] Free Software Foundation  founder  Richard Stallman  argues that, although the term  intellectual property  is in wide use, it should be rejected altogether, because it \"systematically distorts and confuses these issues, and its use was and is promoted by those who gain from this confusion\". He claims that the term \"operates as a catch-all to lump together disparate laws [which] originated separately, evolved differently, cover different activities, have different rules, and raise different public policy issues\" and that it creates a \"bias\" by confusing these monopolies with ownership of limited physical things, likening them to \"property rights\". [ 77 ]  Stallman advocates referring to copyrights, patents and trademarks in the singular and warns against abstracting disparate laws into a collective term. He argues that, \"to avoid spreading unnecessary bias and confusion, it is best to adopt a firm policy not to speak or even think in terms of 'intellectual property'.\" [ 78 ] Similarly, economists  Boldrin  and  Levine  prefer to use the term \"intellectual monopoly\" as a more appropriate and clear definition of the concept, which, they argue, is very dissimilar from property rights. [ 79 ]  They further argued that \"stronger patents do little or nothing to encourage innovation\", mainly explained by its tendency to create market monopolies, thereby restricting further innovations and technology transfer. [ 80 ] On the assumption that intellectual property rights are actual rights, Stallman says that this claim does not live to the historical intentions behind these laws, which in the case of copyright served as a censorship system, and later on, a regulatory model for the printing press that may have benefited authors incidentally, but never interfered with the freedom of average readers. [ 81 ]  Still referring to copyright, he cites legal literature such as the United States Constitution and  case law  to demonstrate that the law is meant to be an optional and experimental bargain to temporarily trade property rights and free speech for public, not private, benefits in the form of increased artistic production and knowledge. He mentions that \"if copyright were a natural right nothing could justify terminating this right after a certain period of time\". [ 82 ] Law professor, writer and political activist  Lawrence Lessig , along with many other  copyleft  and free software activists, has criticized the implied analogy with physical property (like land or an automobile). They argue such an analogy fails because physical property is generally rivalrous while intellectual works are non-rivalrous (that is, if one makes a copy of a work, the enjoyment of the copy does not prevent enjoyment of the original). [ 83 ] [ 84 ]  Other arguments along these lines claim that unlike the situation with tangible property, there is  no natural scarcity  of a particular idea or information: once it exists at all, it can be re-used and duplicated indefinitely without such re-use diminishing the original.  Stephan Kinsella  has objected to  intellectual property  on the grounds that the word \"property\" implies scarcity, which is not applicable to ideas. [ 85 ] Entrepreneur and politician  Rick Falkvinge  and  hacker  Alexandre Oliva have independently compared George Orwell's fictional dialect  Newspeak  to the terminology used by intellectual property supporters as a linguistic weapon to shape public opinion regarding copyright debate and  digital rights management  (DRM). [ 86 ] [ 87 ] In  civil law  jurisdictions, intellectual property has often been referred to as intellectual rights, traditionally a somewhat broader concept that has included  moral rights  and other personal protections that cannot be bought or sold. Use of the term  intellectual rights  has declined since the early 1980s, as use of the term  intellectual property  has increased. Alternative terms  monopolies on information  and  intellectual monopoly  have emerged among those who argue against the  property  or  intellect  or  rights  assumptions, notably  Richard Stallman . The  backronyms   intellectual protectionism  and  intellectual poverty , [ 88 ]  whose initials are also  IP , have also found supporters, especially among those who have used the backronym  digital restrictions management . [ 89 ] [ 90 ] The argument that an intellectual property right should (in the interests of better balancing of relevant private and public interests) be termed an  intellectual monopoly privilege  (IMP) has been advanced by several academics including Birgitte Andersen [ 91 ]  and  Thomas Faunce . [ 92 ] Some critics of intellectual property, such as those in the  free-culture movement , point at intellectual monopolies as harming health (in the case of  pharmaceutical patents ), preventing progress, and benefiting concentrated interests to the detriment of the masses, [ 93 ] [ 94 ] [ 95 ] [ 96 ]  and argue that ever-expansive monopolies in the form of  copyright extensions ,  software patents , and  business method patents  harm the public interest. More recently, scientists and engineers are expressing concern that  patent thickets  are undermining technological development even in high-tech fields like  nanotechnology . [ 97 ] [ 98 ] Petra Moser  has asserted that historical analysis suggests that intellectual property laws may harm innovation: Overall, the weight of the existing historical evidence suggests that patent policies, which grant strong intellectual property rights to early generations of inventors, may discourage innovation. On the contrary, policies that encourage the diffusion of ideas and modify patent laws to facilitate entry and encourage competition may be an effective mechanism to encourage innovation. [ 99 ] In support of that argument,  Jörg Baten , Nicola Bianchi and Petra Moser [ 100 ]  find historical evidence that especially compulsory licensing—which allows governments to license patents without the consent of patent-owners—encouraged invention in Germany in the early 20th century by increasing the threat of competition in fields with low pre-existing levels of competition. Peter Drahos  notes, \"Property rights confer authority over resources. When authority is granted to the few over resources on which the many depend, the few gain power over the goals of the many. This has consequences for both political and economic freedom within a society.\" [ 101 ] : 13 The  World Intellectual Property Organization  (WIPO) recognizes that conflicts may exist between respecting  and implementing current intellectual property systems and other human rights. [ 102 ]  In 2001 the UN  Committee on Economic, Social and Cultural Rights  issued a document called \"Human rights and intellectual property\" that argued that intellectual property tends to be governed by economic goals when it should be viewed primarily as a social product; in order to serve human well-being, intellectual property systems must respect and conform to human rights laws. According to the Committee, when systems fail to do so, they risk infringing upon the human right to food and health, and to cultural participation and scientific benefits. [ 103 ] [ 104 ]  In 2004, the General Assembly of WIPO adopted  The Geneva Declaration on the Future of the World Intellectual Property Organization  which argues that WIPO should \"focus more on the needs of developing countries, and to view IP as one of many tools for development—not as an end in itself\". [ 105 ] Ethical problems are most pertinent when socially valuable goods like life-saving medicines are given IP protection. While the application of IP rights can allow companies to charge higher than the marginal cost of production in order to recoup the costs of research and development, the price may exclude from the market anyone who cannot afford the cost of the product, in this case a life-saving drug. [ 106 ]  \"An IPR driven regime is therefore not a regime that is conductive to the investment of R&D of products that are socially valuable to predominately poor populations\". [ 106 ] : 1108–9 Libertarians  have  differing views on intellectual property . [ 107 ]   Stephan Kinsella , an  anarcho-capitalist  on the  right-wing of libertarianism , [ 108 ]  argues against intellectual property because allowing property rights in ideas and information creates  artificial scarcity  and infringes on the right to own tangible property. Kinsella uses the following scenario to argue this point: [I]magine the time when men lived in caves. One bright guy—let's call him Galt-Magnon—decides to build a log cabin on an open field, near his crops. To be sure, this is a good idea, and others notice it. They naturally imitate Galt-Magnon, and they start building their own cabins. But the first man to invent a house, according to IP advocates, would have a right to prevent others from building houses on their own land, with their own logs, or to charge them a fee if they do build houses. It is plain that the innovator in these examples becomes a partial owner of the tangible property (e.g., land and logs) of others, due not to first occupation and use of that property (for it is already owned), but due to his coming up with an idea. Clearly, this rule flies in the face of the first-user homesteading rule, arbitrarily and groundlessly overriding the very homesteading rule that is at the foundation of all property rights. [ 109 ] Thomas Jefferson  once said in a letter to Isaac McPherson on 13 August 1813: If nature has made any one thing less susceptible than all others of exclusive property, it is the action of the thinking power called an idea, which an individual may exclusively possess as long as he keeps it to himself; but the moment it is divulged, it forces itself into the possession of every one, and the receiver cannot dispossess himself of it. Its peculiar character, too, is that no one possesses the less, because every other possesses the whole of it. He who receives an idea from me, receives instruction himself without lessening mine; as he who lights his  taper  at mine, receives light without darkening me. [ 110 ] In 2005, the  Royal Society of Arts  launched the  Adelphi Charter , aimed at creating an international policy statement to frame how governments should make balanced intellectual property law. [ 111 ] Another aspect of current U.S. Intellectual Property legislation is its focus on individual and joint works; thus, copyright protection can only be obtained in 'original' works of authorship. Critics like Philip Bennet argue that this does not provide adequate protection against  cultural appropriation  of indigenous knowledge, for which a  collective IP regime  is needed. [ 112 ] Intellectual property law has been criticized as not recognizing new forms of art such as the  remix culture , whose participants often commit what technically constitutes violations of such laws, creation works such as  anime music videos  and others, or are otherwise subject to unnecessary burdens and limitations which prevent them from fully expressing themselves. [ 113 ] : 70  [ 114 ] [ 115 ] [ 116 ] Other criticism of intellectual property law concerns the expansion of intellectual property, both in duration and in scope. As scientific knowledge has expanded and allowed new industries to arise in fields such as biotechnology and nanotechnology, originators of technology have sought IP protection for the new technologies. Patents have been granted for living organisms, [ 117 ]  and in the United States,  certain living organisms  have been patentable for over a century. [ 118 ] The increase in terms of protection is particularly seen in relation to copyright, which has recently been the subject of serial extensions  in the United States  and  in Europe . [ 83 ] [ 119 ] [ 120 ] [ 121 ] [ 122 ]  With no need for registration or copyright notices, this is thought to have led to an increase in  orphan works  (copyrighted works for which the copyright owner cannot be contacted), a problem that has been noticed and addressed by governmental bodies around the world. [ 123 ] Also with respect to copyright, the American film industry helped to change the social construct of intellectual property via its trade organization, the  Motion Picture Association  (MPA). In amicus briefs in important cases, in lobbying before Congress, and in its statements to the public, the MPAA has advocated strong protection of intellectual property rights. In framing its presentations, the association has claimed that people are entitled to the property that is produced by their labor. Additionally Congress's awareness of the position of the United States as the world's largest producer of films has made it convenient to expand the conception of intellectual property. [ 124 ]  These doctrinal reforms have further strengthened the industry, lending the MPAA even more power and authority. [ 125 ] The growth of the  Internet , and particularly distributed search engines like  Kazaa  and  Gnutella , have represented a challenge for copyright policy. The  Recording Industry Association of America , in particular, has been on the front lines of the fight against  copyright infringement , which the industry calls \"piracy\". The industry has had victories against some services, including a highly publicized case against the file-sharing company  Napster , and some people have been prosecuted for sharing files in violation of copyright. The electronic age has seen an increase in the attempt to use software-based DRM tools to restrict the copying and use of digitally based works. Laws such as the  Digital Millennium Copyright Act  have been enacted that use criminal law to prevent any circumvention of software used to enforce DRM systems. Equivalent provisions, to prevent circumvention of copyright protection have existed in EU for some time, and are being expanded in, for example, Article 6 and 7 the  Copyright Directive . Other examples are Article 7 of the Software Directive of 1991 (91/250/EEC), and the  Conditional Access Directive  of 1998 (98/84/EEC). This can hinder legal uses, affecting  public domain  works,  limitations and exceptions to copyright , or uses allowed by the copyright holder. Some  copyleft  licenses, like the  GNU GPL 3 , are designed to counter this. [ 126 ]  Laws may permit circumvention under specific conditions, such as when it is necessary to achieve interoperability with the circumventor's program, or for  accessibility  reasons; however, distribution of circumvention tools or instructions may be illegal. In the context of trademarks, this expansion has been driven by international efforts to harmonise the definition of \"trademark\", as exemplified by the  Agreement on Trade-Related Aspects of Intellectual Property Rights  ratified in 1994, which formalized regulations for IP rights that had been handled by common law, or not at all, in member states. Pursuant to TRIPS, any  sign  which is \"capable of distinguishing\" the products or services of one business from the products or services of another business is capable of constituting a trademark. [ 127 ] Make no mistake: the headline [tax] rate is not what triggers tax evasion and aggressive tax planning.  That comes from schemes that facilitate profit shifting. Intellectual property has become a core tool in corporate tax planning and  tax avoidance . [ 129 ] [ 130 ] [ 131 ]  IP is a key component of the leading multinational tax avoidance  base erosion and profit shifting  (BEPS) tools, [ 132 ] [ 133 ]  which the OECD estimates costs $100‍–‍240 billion in lost annual tax revenues. [ 134 ] In 2017–2018, both the U.S. and the EU Commission simultaneously decided to depart from the  OECD BEPS Project  timetable, which was set up in 2013 to combat IP BEPS tax tools like the above, [ 134 ]  and launch their own anti-IP BEPS tax regimes: The departure of the U.S. and EU Commission from the OECD BEPS Project process, is attributed to frustrations with the rise in IP as a key BEPS tax tool, creating intangible assets, which are then turned into royalty payment BEPS schemes (double Irish), and/or  capital allowance  BEPS schemes (capital allowances for intangibles).  In contrast, the OECD has spent years developing and advocating intellectual property as a legal and a GAAP accounting concept. [ 141 ] Women have historically been underrepresented in the creation and ownership of intellectual property covered by intellectual property rights. According to the World Intellectual Property Organization, women composed only 16.5% of patent holders even as recently as 2020. [ 142 ]  This disparity is the result of several factors including systemic bias, sexism and discrimination within the intellectual property space, underrepresentation within  STEM , and barriers to access of necessary finance and knowledge in order to obtain intellectual property rights, among other reasons. [ 143 ] The global increase in intellectual property protection is sometimes referred to as a global IP ratchet in which a spiral of bilateral and multilateral agreements result in growing obligations where new agreements never recede from existing standards and very often further heighten them. [ 75 ] : 7 The global IP ratchet has limited the freedom of  developing countries  to set their own IP standards. [ 75 ] : 7   Developing countries' lack of bargaining power relative to the developed countries driving the global IP ratchet means that developing countries' ability to regulate intellectual property to advance domestic interests is eroding. [ 75 ] : 6–7"
  },
  {
    "id": 26,
    "title": "Library and information science",
    "content": "Library and Information Science  ( LIS ) [ 1 ] [ 2 ]  are two interconnected disciplines that deal with information management. This includes organization, access, collection, and regulation of information, both in physical and digital forms. [ 3 ] [ 4 ] Library science and  information science  are two original disciplines; however, they are within the same field of study. [ 5 ] [ 6 ]  Library science is applied information science. [ 7 ]  Library science is both an application and a subfield of information science. Due to the strong connection, sometimes the two terms are used synonymously. Library science  (previously termed  library studies  and  library economy ) [ note 1 ]  is an  interdisciplinary  or multidisciplinary field that applies the practices, perspectives, and tools of  management ,  information technology ,  education , and other areas to  libraries ; the collection, organization,  preservation , and  dissemination  of information resources; and the  political economy  of information.  Martin Schrettinger , a Bavarian  librarian , coined the discipline within his work (1808–1828)  Versuch eines vollständigen Lehrbuchs der Bibliothek-Wissenschaft oder Anleitung zur vollkommenen Geschäftsführung eines Bibliothekars . [ 8 ]  Rather than classifying information based on nature-oriented elements, as was previously done in his Bavarian library, Schrettinger organized books in alphabetical order. [ 9 ]  The first American school for library science was founded by  Melvil Dewey  at  Columbia University  in 1887. [ 10 ] Historically, library science has also included  archival science . [ 11 ]  This includes: how information resources are organized to serve the needs of selected user groups; how people interact with classification systems and technology; how information is acquired, evaluated and applied by people in and outside libraries as well as cross-culturally; how people are trained and educated for careers in libraries; the  ethics  that guide library service and organization; the legal status of  libraries  and information resources; and the applied science of computer technology used in documentation and  records management . LIS should not be confused with  information theory , the mathematical study of the concept of information.  Library philosophy  has been contrasted with  library science  as the study of the aims and justifications of librarianship as opposed to the development and refinement of techniques. [ 12 ] Academic courses in library science include  collection management , information systems and technology, research methods, user studies,  information literacy ,  cataloging  and  classification ,  preservation ,  reference ,  statistics  and  management . Library science is constantly evolving, incorporating new topics like  database management ,  information architecture  and  information management , among others. With the mounting acceptance of Wikipedia as a valued and reliable reference source, many libraries, museums, and archives have introduced the role of  Wikipedian in residence . As a result, some universities are including coursework relating to Wikipedia and Knowledge Management in their MLIS programs. Becoming a library staff member does not always need a degree, and in some contexts the difference between being a library staff member and a librarian is the level of education. [ 13 ] [ 14 ]  Most professional library jobs require a professional degree in library science or equivalent. In the United States and  Canada  the certification usually comes from a master's degree granted by an  ALA -accredited institution. [ 15 ]  In Australia, a number of institutions offer degrees accepted by the  ALIA (Australian Library and Information Association) . [ 16 ]  Global standards of accreditation or certification in librarianship have yet to be developed. [ 17 ] The Master of Library and Information Science (MLIS) is the master's degree that is required for most professional librarian positions in the United States and Canada. The MLIS was created after the older Master of Library Science (MLS) was reformed to reflect the information science and technology needs of the field. According to the American Library Association (ALA), \"ALA-accredited degrees have [had] various names such as Master of Arts, Master of Librarianship, Master of Library and Information Studies, or Master of Science. The degree name is determined by the program. The [ALA] Committee for Accreditation evaluates programs based on their adherence to the Standards for Accreditation of Master's Programs in Library and Information Studies, not based on the name of the degree.\" [ 18 ] The study of librarianship for  public libraries  covers issues such as cataloging;  collection development  for a diverse community;  information literacy ;  readers' advisory ; community standards; public services-focused librarianship via community-centered programming; serving a diverse community of adults, children, and teens;  intellectual freedom ;  censorship ; and legal and budgeting issues. The public library as a commons or public sphere based on the work of  Jürgen Habermas  has become a central metaphor in the 21st century. [ 19 ] In the United States there are four different types of public libraries:  association libraries , municipal public libraries, school district libraries, and special district public libraries. Each receives funding through different sources, each is established by a different set of voters, and not all are subject to municipal civil service governance. [ 20 ] The study of  school librarianship  covers library services for children in Nursery, primary through secondary school. In some regions, the local government may have stricter standards for the education and certification of  school librarians  (who are sometimes considered a special case of teacher), than for other librarians, and the educational program will include those local criteria. School librarianship may also include issues of  intellectual freedom ,  pedagogy ,  information literacy , and how to build a cooperative  curriculum  with the teaching staff. The study of  academic librarianship  covers library services for colleges and universities. Issues of special importance to the field may include  copyright ; technology;  digital libraries  and digital repositories;  academic freedom ;  open access  to scholarly works; and specialized knowledge of subject areas important to the institution and the relevant  reference works . Librarians often divide focus individually as liaisons on particular schools within a college or university. Academic librarians may be  subject specific librarians . Some academic librarians are considered  faculty , and hold similar academic ranks to those of professors, while others are not. In either case, the minimal qualification is a Master of Arts in Library Studies or a Master of Arts in Library Science. Some academic libraries may only require a master's degree in a specific academic field or a related field, such as educational technology. The study of archives includes the training of  archivists , librarians specially trained to maintain and build  archives  of  records  intended for  historical preservation . Special issues include physical preservation, conservation, and restoration of materials and  mass deacidification ; specialist catalogs; solo work; access; and appraisal. Many archivists are also trained historians specializing in the period covered by the archive. There have been attempts to revive the concept of  documentation  and to speak of  Library, information and documentation  studies (or science). [ 21 ] The archival mission includes three major goals: To identify papers and records with enduring value, preserve the identified papers, and make the papers available to others. [ 22 ]  While libraries receive items individually, archival items will usually become part of the archive's collection as a cohesive group. [ 22 ]  Major difference in collections is that library collections typically comprise published items (books, magazines, etc.), while archival collections are usually unpublished works (letters, diaries, etc.). Library collections are created by many individuals, as each author and illustrator create their own publication; in contrast, an archive usually collects the records of one person, family, institution, or organization, so the archival items will have fewer sources of authors. [ 22 ] Behavior in an archive differs from behavior in other libraries. In most libraries, items are openly available to the public. Archival items almost never circulate, and someone interested in viewing documents must request them of the archivist and may only be able view them in a closed reading room. [ 22 ] Special libraries  are libraries established to meet the highly specialized requirements of professional or business groups. A library is special depending on whether it covers a specialized collection, a special subject, or a particular group of users, or even the type of parent organization, such as  medical libraries  or  law libraries . The issues at these libraries are specific to their industries but may include solo work, corporate financing, specialized collection development, and extensive self-promotion to potential patrons. Special librarians have their own professional organization, the  Special Libraries Association  (SLA). Some special libraries, such as the  CIA Library , may contain classified works. It is a resource to employees of the  Central Intelligence Agency , containing over 125,000 written materials, subscribes to around 1,700 periodicals, and had collections in three areas: Historical Intelligence, Circulating, and Reference. [ 23 ]  In February 1997, three librarians working at the institution spoke to  Information Outlook , a publication of the SLA, revealing that the library had been created in 1947, the importance of the library in disseminating information to employees, even with a small staff, and how the library organizes its materials. [ 24 ] Preservation librarians most often work in academic libraries. Their focus is on the management of preservation activities that seek to maintain access to content within books, manuscripts, archival materials, and other library resources. Examples of activities managed by preservation librarians include binding, conservation, digital and analog reformatting,  digital preservation , and environmental monitoring. Libraries have existed for many centuries but library science is a more recent phenomenon, as early libraries were managed primarily by academics. [ 25 ] The earliest text on \"library operations\",  Advice on Establishing a Library  was published in 1627 by French librarian and scholar  Gabriel Naudé . Naudé wrote on many subjects including politics, religion, history, and the supernatural. He put into practice all the ideas put forth in  Advice  when given the opportunity to build and maintain the library of  Cardinal Jules Mazarin . [ 26 ] In 1726  Gottfried Wilhelm Leibniz  wrote  Idea of Arranging a Narrower Library. [ 27 ] Martin Schrettinger  wrote the second textbook (the first in Germany) on the subject from 1808 to 1829. Some of the main tools used by LIS to provide access to the resources originated in 19th century to make information accessible by recording, identifying, and providing bibliographic control of printed knowledge. [ 28 ]  The origin for some of these tools were even earlier. In the 17th century, during the  'golden age of libraries' , publishers and sellers seeking to take advantage of the burgeoning book trade developed descriptive catalogs of their wares for distribution – a practice was adopted and further extrapolated by many libraries of the time to cover areas like philosophy, sciences, linguistics, and medicine [ 29 ] Thomas Jefferson , whose library at  Monticello  consisted of thousands of books, devised a classification system inspired by the  Baconian method , which grouped books more or less by subject rather than alphabetically, as it was previously done. [ 30 ]  The Jefferson collection provided the start of what became the  Library of Congress . The first American school of librarianship opened at  Columbia University  under the leadership of  Melvil Dewey , noted for his  1876 decimal classification , on January 5, 1887, as the School of Library Economy. The term  library economy  was common in the U.S. until 1942, with the term,  library science,  predominant through much of the 20th century. [ 31 ] In the English-speaking world the term \"library science\" seems to have been used for the first time in India [ 32 ]  in the 1916 book  Punjab Library Primer , written by Asa Don Dickinson and published by the  University of Punjab , Lahore, Pakistan. [ 33 ]  This university was the first in Asia to begin teaching \"library science\". The  Punjab Library Primer  was the first textbook on library science published in English anywhere in the world. The first textbook in the United States was the  Manual of Library Economy  by  James Duff Brown , published in 1903. Later, the term was used in the title of  S. R. Ranganathan 's  The Five Laws of Library Science , published in 1931, which contains Ranganathan's  titular theory . Ranganathan is also credited with the development of the first major analytical-synthetic classification system, the  colon classification . [ 34 ] In the United States, Lee Pierce Butler published his 1933 book  An Introduction to Library Science  (University of Chicago Press), where he advocated for research using  quantitative methods  and ideas in the  social sciences  with the aim of using librarianship to address society's information needs. He was one of the first faculty at the  University of Chicago Graduate Library School , which changed the structure and focus of education for librarianship in the twentieth century. This research agenda went against the more procedure-based approach of the \"library economy\", which was mostly confined to practical problems in the administration of libraries. In 1923,  Charles C. Williamson , who was appointed by the Carnegie Corporation, published an assessment of library science education entitled \"The Williamson Report\", which designated that universities should provide library science training. [ 35 ]  This report had a significant impact on library science training and education. Library research and practical work, in the area of information science, have remained largely distinct both in training and in research interests. William Stetson Merrill 's  A Code for Classifiers , released in several editions from 1914 to 1939, [ 36 ]  is an example of a more pragmatic approach, where arguments stemming from in-depth knowledge about each field of study are employed to recommend a system of classification. While Ranganathan's approach was philosophical, it was also tied more to the day-to-day business of running a library. A reworking of Ranganathan's laws was published in 1995 which removes the constant references to books.  Michael Gorman 's  Our Enduring Values: Librarianship in the 21st Century  features the eight principles necessary by library professionals and incorporates knowledge and information in all their forms, allowing for digital information to be considered. By the late 1960s, mainly due to the meteoric rise of human computing power and the new academic disciplines formed therefrom, academic institutions began to add the term \"information science\" to their names. The first school to do this was at the  University of Pittsburgh  in 1964. [ 37 ]  More schools followed during the 1970s and 1980s. By the 1990s almost all library schools in the US had added information science to their names. Although there are exceptions, similar developments have taken place in other parts of the world. In  India , the Dept of Library Science, University of Madras  (southern state of  TamiilNadu , India) became the  Dept. of Library and Information Science in 1976.  In  Denmark , for example, the 'Royal School of Librarianship' changed its English name to  The Royal School of Library and Information Science  in 1997. The  digital age  has transformed how information is accessed and  retrieved . \"The library is now a part of a complex and dynamic educational, recreational, and informational infrastructure.\" [ 35 ]   Mobile devices  and applications with  wireless networking , high-speed computers and networks, and the  computing cloud  have deeply impacted and developed information science and information services. The evolution of the library sciences maintains its mission of access equity and community space, as well as the new means for information retrieval called information literacy skills. All catalogs,  databases , and a growing number of books are available on the  Internet . In addition, the expanding free access to  open access  journals and sources such as  Wikipedia  has fundamentally impacted how information is accessed. Information literacy  is the ability to \"determine the extent of information needed, access the needed information effectively and efficiently, evaluate information and its sources critically, incorporate selected information into one's knowledge base, use information effectively to accomplish a specific purpose, and understand the economic, legal, and social issues surrounding the use of information, and access and use information ethically and legally.\" [ 38 ] In the early 2000s, dLIST, Digital Library for Information Sciences and Technology was established. It was the first  open access  archive for the multidisciplinary 'library and information sciences' building a global scholarly communication consortium and the LIS Commons in order to increase the visibility of research literature, bridge the divide between practice, teaching, and research communities, and improve visibility, uncitedness, and integrate scholarly work in the critical information infrastructures of archives, libraries, and museums. [ 39 ] [ 40 ] [ 41 ] [ 42 ] Social justice , an important ethical value in librarianship and in the 21st century has become an important research area, if not subdiscipline of LIS. [ 43 ] See also Some core journals in LIS are: Important bibliographical databases in LIS are, among others,  Social Sciences Citation Index  and  Library and Information Science Abstracts [ 45 ] This is a list of some of the major conferences in the field. Information science grew out of  documentation science [ 48 ]  and therefore has a tradition for considering scientific and scholarly communication,  bibliographic databases , subject knowledge and terminology etc. An advertisement for a full Professor in information science at the Royal School of Library and Information Science, spring 2011, provides one view of which sub-disciplines are well-established: [ 49 ]  \"The research and teaching/supervision must be within some (and at least one) of these well-established information science areas A curriculum study by Kajberg & Lørring in 2005 [ 50 ]  reported a \"degree of overlap of the ten curricular themes with subject areas in the current curricula of responding LIS schools\". There is often an overlap between these subfields of LIS and other fields of study. Most information retrieval research, for example, belongs to computer science. Knowledge management is considered a subfield of management or organizational studies. [ 51 ] Pre-Internet classification systems and  cataloging  systems were mainly concerned with two objectives: The development of the Internet and the information explosion that followed found many communities needing mechanisms for the description, authentication and management of their information. [ 52 ]  These communities developed taxonomies and  controlled vocabularies  to describe their knowledge, as well as unique information architectures to communicate these classifications and libraries found themselves as liaison or translator between these metadata systems. [ 52 ]  The concerns of cataloging in the Internet era have gone beyond simple bibliographic descriptions and the need for descriptive information about the ownership and copyright of a digital product – a publishing concern – and description for the different formats and accessibility features of a resource – a sociological concern – show the continued development and cross discipline necessity of resource description. [ 52 ] In the 21st century, the usage of  open data ,  open source  and  open protocols  like  OAI-PMH  has allowed thousands of libraries and institutions to collaborate on the production of global metadata services previously offered only by increasingly expensive commercial proprietary products. Tools like  BASE  and  Unpaywall  automate the search of an  academic paper  across thousands of repositories by libraries and research institutions. [ 53 ] Library science is very closely related to issues of knowledge organization; however, the latter is a broader term that covers how knowledge is represented and stored (computer science/linguistics), how it might be automatically processed (artificial intelligence), and how it is organized outside the library in global systems such as the internet. In addition, library science typically refers to a specific community engaged in managing holdings as they are found in university and government libraries, while knowledge organization, in general, refers to this and also to other communities (such as publishers) and other systems (such as the Internet). The library system is thus one socio-technical structure for knowledge organization. [ citation needed ] The terms 'information organization' and 'knowledge organization' are often used synonymously. [ 28 ] : 106   The fundamentals of their study - particularly theory relating to indexing and classification - and many of the main tools used by the disciplines in modern times to provide access to digital resources such as abstracting, metadata, resource description, systematic and alphabetic subject description, and terminology, originated in the 19th century and were developed, in part, to assist in making humanity's intellectual output accessible by recording, identifying, and providing bibliographic control of printed knowledge. [ 28 ] : 105 Information has been published that analyses the relations between the philosophy of information (PI), library and information science (LIS), and social epistemology (SE). [ 54 ] Practicing library professionals and members of the  American Library Association  recognize and abide by the ALA Code of Ethics. According to the American Library Association, \"In a political system grounded in an informed citizenry, we are members of a profession explicitly committed to intellectual freedom and freedom of access to information. We have a special obligation to ensure the free flow of information and ideas to present and future generations.\" [ 55 ]  The ALA Code of Ethics was adopted in the winter of 1939, and updated on June 29, 2021. [ 55 ]"
  },
  {
    "id": 27,
    "title": "Memory",
    "content": "Memory  is the faculty of the  mind  by which  data  or  information  is  encoded , stored, and retrieved when needed. It is the retention of information over time for the purpose of influencing future  action . [ 1 ]  If  past events  could not be remembered, it would be impossible for language, relationships, or  personal identity  to develop. [ 2 ]  Memory loss is usually described as  forgetfulness  or  amnesia . [ 3 ] [ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] Memory is often understood as an  informational processing  system with explicit and implicit functioning that is made up of a  sensory processor ,  short-term  (or  working ) memory, and  long-term memory . [ 9 ]  This can be related to the  neuron .\nThe sensory processor allows information from the outside world to be sensed in the form of chemical and physical stimuli and attended to various levels of focus and intent. Working memory serves as an encoding and retrieval processor. Information in the form of stimuli is encoded in accordance with explicit or implicit functions by the working memory processor. The working memory also retrieves information from previously stored material. Finally, the function of long-term memory is to store through various categorical models or systems. [ 9 ] Declarative, or  explicit memory , is the conscious storage and recollection of data. [ 10 ]  Under declarative memory resides  semantic  and  episodic memory . Semantic memory refers to memory that is encoded with specific meaning. [ 2 ]  Meanwhile, episodic memory refers to information that is encoded along a spatial and temporal plane. [ 11 ] [ 12 ] [ 13 ]  Declarative memory is usually the primary process thought of when referencing memory. [ 2 ]   Non-declarative, or implicit, memory  is the unconscious storage and recollection of information. [ 14 ]  An example of a non-declarative process would be the unconscious learning or retrieval of information by way of  procedural memory , or a priming phenomenon. [ 2 ] [ 14 ] [ 15 ]   Priming  is the process of  subliminally  arousing specific responses from memory and shows that not all memory is consciously activated, [ 15 ]  whereas procedural memory is the slow and gradual learning of skills that often occurs without conscious attention to learning. [ 2 ] [ 14 ] Memory is not a perfect processor and is affected by many factors. The ways by which information is encoded, stored, and retrieved can all be corrupted. Pain, for example, has been identified as a physical condition that impairs memory, and has been noted in animal models as well as chronic pain patients. [ 16 ] [ 17 ] [ 18 ] [ 19 ]  The amount of attention given new stimuli can diminish the amount of information that becomes encoded for storage. [ 2 ]  Also, the storage process can become corrupted by physical damage to areas of the brain that are associated with memory storage, such as the hippocampus. [ 20 ] [ 21 ]  Finally, the retrieval of information from long-term memory can be disrupted because of decay within long-term memory. [ 2 ]  Normal functioning, decay over time, and brain damage all affect the accuracy and capacity of the memory. [ 22 ] [ 23 ] Sensory memory holds information, derived from the senses, less than one second after an item is perceived. The ability to look at an item and remember what it looked like with just a split second of observation, or memorization, is an example of sensory memory. It is out of cognitive control and is an automatic response. With very short presentations, participants often report that they seem to \"see\" more than they can actually report. The first precise experiments exploring this form of sensory memory were conducted by  George Sperling  (1963) [ 24 ]  using the \"partial report paradigm.\" Subjects were presented with a grid of 12 letters, arranged into three rows of four. After a brief presentation, subjects were then played either a high, medium or low tone, cuing them which of the rows to report. Based on these partial report experiments, Sperling was able to show that the capacity of sensory memory was approximately 12 items, but that it degraded very quickly (within a few hundred milliseconds). Because this form of memory degrades so quickly, participants would see the display but be unable to report all of the items (12 in the \"whole report\" procedure) before they decayed. This type of memory cannot be prolonged via rehearsal. Three types of sensory memories exist.  Iconic memory  is a fast decaying store of visual information, a type of sensory memory that briefly stores an image that has been perceived for a small duration.  Echoic memory  is a fast decaying store of auditory information, also a sensory memory that briefly stores sounds that have been perceived for short durations. [ 25 ] [ 26 ]   Haptic memory  is a type of sensory memory that represents a database for touch stimuli. Short-term memory, not to be confused with working memory, allows recall for a period of several seconds to a minute without rehearsal. Its capacity, however, is very limited. In 1956,  George A. Miller  (1920–2012), when working at  Bell Laboratories , conducted experiments showing that the store of short-term memory was 7±2 items. (Hence, the title of his famous paper,  \"The Magical Number 7±2.\" ) Modern perspectives estimate the capacity of short-term memory to be lower, typically on the order of 4–5 items, [ 27 ]  or argue for a more flexible limit based on information instead of items. [ 28 ]  Memory capacity can be increased through a process called  chunking . [ 29 ]  For example, in recalling a ten-digit  telephone number , a person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456), and, last, a four-digit chunk (7890). This method of remembering telephone numbers is far more effective than attempting to remember a string of 10 digits; this is because we are able to chunk the information into meaningful groups of numbers. This is reflected in some countries' tendencies to display telephone numbers as several chunks of two to four numbers. Short-term memory is believed to rely mostly on an acoustic code for storing information, and to a lesser extent on a visual code. Conrad (1964) [ 30 ]  found that test subjects had more difficulty recalling collections of letters that were acoustically similar, e.g., E, P, D. Confusion with recalling acoustically similar letters rather than visually similar letters implies that the letters were encoded acoustically. Conrad's (1964) study, however, deals with the encoding of written text. Thus, while the memory of written language may rely on acoustic components, generalizations to all forms of memory cannot be made. The storage in sensory memory and short-term memory generally has a strictly limited capacity and duration. This means that information is not retained indefinitely. By contrast, while the total capacity of long-term memory has yet to be established, it can store much larger quantities of information. Furthermore, it can store this information for a much longer duration, potentially for a whole life span. For example, given a random seven-digit number, one may remember it for only a few seconds before forgetting, suggesting it was stored in short-term memory. On the other hand, one can remember telephone numbers for many years through repetition; this information is said to be stored in long-term memory. While short-term memory encodes information acoustically, long-term memory encodes it semantically: Baddeley (1966) [ 31 ]  discovered that, after 20 minutes, test subjects had the most difficulty recalling a collection of words that had similar meanings (e.g. big, large, great, huge) long-term. Another part of long-term memory is episodic memory, \"which attempts to capture information such as 'what', 'when' and 'where ' \". [ 32 ]  With episodic memory, individuals are able to recall specific events such as birthday parties and weddings. Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the  frontal lobe  (especially  dorsolateral prefrontal cortex ) and the  parietal lobe . Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The  hippocampus  is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. It was thought that without the hippocampus new memories were unable to be stored into long-term memory and that there would be a very short  attention span , as first gleaned from patient  Henry Molaison [ 33 ] [ 34 ]  after what was thought to be the full removal of both his hippocampi. More recent examination of his brain, post-mortem, shows that the hippocampus was more intact than first thought, throwing theories drawn from the initial data into question. The hippocampus may be involved in changing neural connections for a period of three months or more after the initial learning. Research has suggested that long-term memory storage in humans may be maintained by  DNA methylation , [ 35 ]  and the  'prion' gene . [ 36 ] [ 37 ] Further research investigated the molecular basis for  long-term memory . By 2015 it had become clear that long-term memory requires gene transcription activation and  de novo protein synthesis . [ 38 ]  Long-term memory formation depends on both the activation of memory promoting genes and the inhibition of memory suppressor genes, and  DNA methylation / DNA demethylation  was found to be a major mechanism for achieving this dual regulation. [ 39 ] Rats with a new, strong long-term memory due to  contextual fear conditioning  have reduced expression of about 1,000 genes and increased expression of about 500 genes in the hippocampus 24 hours after training, thus exhibiting modified expression of 9.17% of the rat hippocampal genome. Reduced gene expressions were associated with methylations of those genes. [ 40 ] Considerable further research into long-term memory has illuminated the molecular mechanisms by which methylations are established or removed, as reviewed in 2022. [ 41 ]  These mechanisms include, for instance, signal-responsive  TOP2B -induced double-strand breaks in  immediate early genes . Also the  messenger RNAs  of many genes that had been subjected to methylation-controlled increases or decreases are transported by neural granules ( messenger RNP ) to the  dendritic spines . At these locations the messenger RNAs can be  translated  into the proteins that control signaling at  neuronal   synapses . [ 41 ] The transition of a memory from short term to long term is called  memory consolidation . Little is known about the physiological processes involved. Two propositions of how the brain achieves this task are  backpropagation  or  backprop  and  positive feedback  from the endocrine system. Backprop has been proposed as a mechanism the brain uses to achieve memory consolidation and has been used, for example by Geoffrey E. Hinton, Nobel Prize for Physics in 2024, to build AI software. It implies a feedback to neurons consolidating a given memory to erase that information when the brain learns that that information is misleading or wrong.  However, empirical evidence of its existence is not available. [ 42 ] On the contrary, positive feedback for consolidating a certain short term memory registered in neurons, and considered by the neuro-endocrine systems to be useful, will make that short term memory to consolidate into a permanent one. This has been shown to be true experimentally first in insects, [ 43 ] [ 44 ] [ 45 ] [ 46 ] [ 47 ]  which use arginine and nitric oxide levels in their brains and endorphin receptors for this task. The involvemnt of arginine and nitric oxide in memory consolidation  has been confirmed in birds, mammals and other creatures, including humans. [ 48 ] Glial cells have also an important role in memory formation, although how they do their work remains to be unveiled. [ 49 ] [ 50 ] Other mechanisms for memory consolidation can not be discarded. The multi-store model (also known as  Atkinson–Shiffrin memory model ) was first described in 1968 by  Atkinson  and  Shiffrin . The multi-store model has been criticised for being too simplistic. For instance, long-term memory is believed to be actually made up of multiple subcomponents, such as episodic and  procedural memory . It also proposes that rehearsal is the only mechanism by which information eventually reaches long-term storage, but evidence shows us capable of remembering things without rehearsal. The model also shows all the memory stores as being a single unit whereas research into this shows differently. For example, short-term memory can be broken up into different units such as visual information and acoustic information. In a study by Zlonoga and Gerber (1986), patient 'KF' demonstrated certain deviations from the Atkinson–Shiffrin model. Patient KF was  brain damaged , displaying difficulties regarding short-term memory. Recognition of sounds such as spoken numbers, letters, words, and easily identifiable noises (such as doorbells and cats meowing) were all impacted. Visual short-term memory was unaffected, suggesting a dichotomy between visual and audial memory. [ 51 ] In 1974 Baddeley and Hitch proposed a \"working memory model\" that replaced the general concept of short-term memory with active maintenance of information in short-term storage. In this model, working memory consists of three basic stores: the central executive, the phonological loop, and the visuo-spatial sketchpad. In 2000 this model was expanded with the multimodal episodic buffer ( Baddeley's model of working memory ). [ 52 ] The central executive essentially acts as an attention sensory store. It channels information to the three component processes: the phonological loop, the visuospatial sketchpad, and the episodic buffer. The phonological loop stores auditory information by silently rehearsing sounds or words in a continuous loop: the articulatory process (for example the repetition of a telephone number over and over again). A short list of data is easier to remember. The phonological loop is occasionally disrupted.  Irrelevant speech  or background noise can impede the phonological loop.  Articulatory suppression  can also confuse encoding and words that sound similar can be switched or misremembered through the phonological similarity effect. the phonological loop also has a limit to how much it can hold at once which means that it is easier to remember a lot of short words rather than a lot of long words, according to the word length effect. The  visuospatial sketchpad  stores visual and spatial information. It is engaged when performing spatial tasks (such as judging distances) or visual ones (such as counting the windows on a house or imagining images). Those with  aphantasia  will not be able to engage the visuospatial sketchpad. The episodic buffer is dedicated to linking information across domains to form integrated units of visual, spatial, and verbal information and chronological ordering (e.g., the memory of a story or a movie scene). The episodic buffer is also assumed to have links to long-term memory and semantic meaning. The working memory model explains many practical observations, such as why it is easier to do two different tasks, one verbal and one visual, than two similar tasks, and the aforementioned word-length effect. Working memory is also the premise for what allows us to do everyday activities involving thought. It is the section of memory where we carry out thought processes and use them to learn and reason about topics. [ 52 ] Researchers distinguish between  recognition  and  recall  memory. Recognition memory tasks require individuals to indicate whether they have encountered a stimulus (such as a picture or a word) before. Recall memory tasks require participants to retrieve previously learned information. For example, individuals might be asked to produce a series of actions they have seen before or to say a list of words they have heard before. Topographical memory  involves the ability to orient oneself in space, to recognize and follow an itinerary, or to recognize familiar places. [ 53 ]  Getting lost when traveling alone is an example of the failure of topographic memory. [ 54 ] Flashbulb memories  are clear  episodic memories  of unique and highly emotional events. [ 55 ]  People remembering where they were or what they were doing when they first heard the news of  President Kennedy 's  assassination , [ 56 ]  the  Sydney Siege  or of  9/11  are examples of flashbulb memories. Anderson  (1976) [ 57 ]  divides long-term memory into  declarative (explicit)  and  procedural (implicit)  memories. Declarative memory  requires conscious  recall , in that some  conscious  process must call back the information. It is sometimes called  explicit memory , since it consists of information that is explicitly stored and retrieved. Declarative memory can be further sub-divided into  semantic memory , concerning principles and facts taken independent of context; and  episodic memory , concerning information specific to a particular context, such as a time and place. Semantic memory allows the encoding of abstract  knowledge  about the world, such as \"Paris is the capital of France\". Episodic memory, on the other hand, is used for more personal memories, such as the sensations, emotions, and personal  associations  of a particular place or time. Episodic memories often reflect the \"firsts\" in life such as a first kiss, first day of school or first time winning a championship. These are key events in one's life that can be remembered clearly. Research suggests that declarative memory is supported by several functions of the medial temporal lobe system which includes the hippocampus. [ 58 ]   Autobiographical memory  – memory for particular events within one's own life – is generally viewed as either equivalent to, or a subset of, episodic memory.  Visual memory  is part of memory preserving some characteristics of our senses pertaining to visual experience. One is able to place in memory information that resembles objects, places, animals or people in sort of a  mental image . Visual memory can result in  priming  and it is assumed some kind of perceptual representational system underlies this phenomenon. [ 58 ] In contrast,  procedural memory  (or  implicit memory ) is not based on the conscious recall of information, but on  implicit learning . It can best be summarized as remembering how to do something. Procedural memory is primarily used in learning  motor skills  and can be considered a subset of implicit memory. It is revealed when one does better in a given task due only to repetition – no new explicit memories have been formed, but one is  unconsciously  accessing aspects of those previous experiences. Procedural memory involved in  motor learning  depends on the  cerebellum  and  basal ganglia . [ 59 ] A characteristic of procedural memory is that the things remembered are automatically translated into actions, and thus sometimes difficult to describe. Some examples of procedural memory include the ability to ride a bike or tie shoelaces. [ 60 ] Another major way to distinguish different memory functions is whether the content to be remembered is in the past,  retrospective memory , or in the future,  prospective memory . John Meacham introduced this distinction in a paper presented at the 1975  American Psychological Association  annual meeting and subsequently included by  Ulric Neisser  in his 1982 edited volume,  Memory Observed: Remembering in Natural Contexts . [ 61 ] [ 62 ]  Thus, retrospective memory as a category includes semantic, episodic and autobiographical memory. In contrast, prospective memory is memory for future intentions, or  remembering to remember  (Winograd, 1988). Prospective memory can be further broken down into event- and time-based prospective remembering. Time-based prospective memories are triggered by a time-cue, such as going to the doctor (action) at 4pm (cue). Event-based prospective memories are intentions triggered by cues, such as remembering to post a letter (action) after seeing a mailbox (cue). Cues do not need to be related to the action (as the mailbox/letter example), and lists, sticky-notes, knotted handkerchiefs, or string around the finger all exemplify cues that people use as strategies to enhance prospective memory. Infants do not have the language ability to report on their memories and so verbal reports cannot be used to assess very young children's memory. Throughout the years, however, researchers have adapted and developed a number of measures for assessing both infants' recognition memory and their recall memory.  Habituation  and  operant conditioning  techniques have been used to assess infants' recognition memory and the deferred and elicited imitation techniques have been used to assess infants' recall memory. Techniques used to assess infants' recognition memory include the following: Techniques used to assess infants' recall memory include the following: Researchers use a variety of tasks to assess older children and adults' memory. Some examples are: Brain areas involved in the  neuroanatomy of memory  such as the  hippocampus , the  amygdala , the  striatum , or the  mammillary bodies  are thought to be involved in specific types of memory. For example, the hippocampus is believed to be involved in spatial learning and  declarative learning , while the amygdala is thought to be involved in  emotional memory . [ 77 ] Damage to certain areas in patients and animal models and subsequent memory deficits is a primary source of information. However, rather than implicating a specific area, it could be that damage to adjacent areas, or to a pathway  traveling  through the area is actually responsible for the observed deficit. Further, it is not sufficient to describe memory, and its counterpart,  learning , as solely dependent on specific brain regions. Learning and memory are usually attributed to changes in neuronal  synapses , thought to be mediated by  long-term potentiation  and  long-term depression . In general, the more emotionally charged an event or experience is, the better it is remembered; this phenomenon is known as the  memory enhancement effect . Patients with amygdala damage, however, do not show a memory enhancement effect. [ 78 ] [ 79 ] Hebb  distinguished between short-term and long-term memory. He postulated that any memory that stayed in short-term storage for a long enough time would be consolidated into a long-term memory. Later research showed this to be false. Research has shown that direct injections of  cortisol  or  epinephrine  help the storage of recent experiences. This is also true for stimulation of the amygdala. This proves that excitement enhances memory by the stimulation of hormones that affect the amygdala. Excessive or prolonged stress (with prolonged cortisol) may hurt memory storage. Patients with amygdalar damage are no more likely to remember emotionally charged words than nonemotionally charged ones. The hippocampus is important for explicit memory. The hippocampus is also important for memory consolidation. The hippocampus receives input from different parts of the cortex and sends its output out to different parts of the brain also. The input comes from secondary and tertiary sensory areas that have processed the information a lot already. Hippocampal damage may also cause  memory loss  and problems with memory storage. [ 80 ]  This memory loss includes  retrograde amnesia  which is the loss of memory for events that occurred shortly before the time of brain damage. [ 76 ] Cognitive neuroscientists consider memory as the retention, reactivation, and reconstruction of the experience-independent internal representation. The term of internal  representation  implies that such a definition of memory contains two components: the expression of memory at the behavioral or conscious level, and the underpinning physical neural changes (Dudai 2007). The latter component is also called  engram  or memory traces (Semon 1904). Some neuroscientists and psychologists mistakenly equate the concept of engram and memory, broadly conceiving all persisting after-effects of experiences as memory; others argue against this notion that memory does not exist until it is revealed in behavior or thought (Moscovitch 2007). One question that is crucial in  cognitive neuroscience  is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007).  Convergence-divergence zones  might be the neural networks where memories are stored and retrieved. Considering that there are several kinds of memory, depending on types of represented knowledge, underlying mechanisms, processes functions and modes of acquisition, it is likely that different brain areas support different memory systems and that they are in mutual relationships in neuronal networks: \"components of memory representation are distributed widely across different parts of the brain as mediated by multiple neocortical circuits\". [ 81 ] Study of the genetics of human memory is in its infancy though many genes have been investigated for their association to memory in humans and non-human animals. A notable initial success was the association of  APOE  with memory dysfunction in  Alzheimer's disease . The search for genes associated with normally varying memory continues. One of the first candidates for normal variation in memory is the protein  KIBRA , [ 82 ] [ medical citation needed ]  which appears to be associated with the rate at which material is forgotten over a delay period. There has been some evidence that memories are stored in the nucleus of neurons. [ 83 ] [ 84 ] Several  genes , proteins and enzymes have been extensively researched for their association with memory. Long-term memory, unlike short-term memory, is dependent upon the synthesis of new proteins. [ 85 ]  This occurs within the cellular body, and concerns the particular transmitters, receptors, and new synapse pathways that reinforce the communicative strength between neurons. The production of new proteins devoted to synapse reinforcement is triggered after the release of certain signaling substances (such as calcium within hippocampal neurons) in the cell. In the case of hippocampal cells, this release is dependent upon the expulsion of magnesium (a binding molecule) that is expelled after significant and repetitive synaptic signaling. The temporary expulsion of magnesium frees NMDA receptors to release calcium in the cell, a signal that leads to gene transcription and the construction of reinforcing proteins. [ 86 ]  For more information, see  long-term potentiation  (LTP). One of the newly synthesized proteins in LTP is also critical for maintaining long-term memory. This protein is an autonomously active form of the enzyme  protein kinase C  (PKC), known as  PKMζ . PKMζ maintains the activity-dependent enhancement of synaptic strength and inhibiting PKMζ erases established long-term memories, without affecting short-term memory or, once the inhibitor is eliminated, the ability to encode and store new long-term memories is restored. Also,  BDNF  is important for the persistence of long-term memories. [ 87 ] The long-term stabilization of synaptic changes is also determined by a parallel increase of pre- and postsynaptic structures such as  axonal bouton ,  dendritic spine  and  postsynaptic density . [ 88 ] \nOn the molecular level, an increase of the postsynaptic scaffolding proteins  PSD-95  and  HOMER1c  has been shown to correlate with the stabilization of synaptic enlargement. [ 88 ]  The cAMP response element-binding protein ( CREB ) is a  transcription factor  which is believed to be important in consolidating short-term to long-term memories, and which is believed to be downregulated in Alzheimer's disease. [ 89 ] Rats exposed to an intense  learning  event may retain a life-long memory of the event, even after a single training session. The long-term memory of such an event appears to be initially stored in the  hippocampus , but this storage is transient. Much of the long-term storage of the memory seems to take place in the  anterior cingulate cortex . [ 90 ]  When such an exposure was experimentally applied, more than 5,000 differently methylated DNA regions appeared in the hippocampus neuronal  genome  of the rats at one and at 24 hours after training. [ 91 ]  These alterations in methylation pattern occurred at many genes that were  downregulated , often due to the formation of new  5-methylcytosine  sites in CpG rich regions of the genome. Furthermore, many other genes were upregulated, likely often due to hypomethylation. Hypomethylation often results from the removal of methyl groups from previously existing 5-methylcytosines in DNA. Demethylation is carried out by several proteins acting in concert, including the  TET enzymes  as well as enzymes of the DNA  base excision repair  pathway (see  Epigenetics in learning and memory ). The pattern of induced and repressed genes in brain neurons subsequent to an intense learning event likely provides the molecular basis for a long-term memory of the event. Studies of the molecular basis for memory formation indicate that  epigenetic  mechanisms operating in neurons in the  brain  play a central role in determining this capability. Key epigenetic mechanisms involved in memory include the  methylation  and  demethylation  of neuronal DNA, as well as modifications of  histone  proteins including  methylations ,  acetylations and deacetylations . Stimulation of brain activity in memory formation is often accompanied by the generation of  damage in neuronal DNA  that is followed by repair associated with persistent epigenetic alterations. In particular the DNA repair processes of  non-homologous end joining  and  base excision repair  are employed in memory formation. [ 92 ] During a new learning experience, a set of genes is rapidly expressed in the brain. This induced  gene expression  is considered to be essential for processing the information being learned. Such genes are referred to as  immediate early genes  (IEGs).  DNA topoisomerase 2-beta (TOP2B)  activity is essential for the expression of IEGs in a type of learning experience in mice termed associative fear memory. [ 93 ]  Such a learning experience appears to rapidly trigger TOP2B to induce double-strand breaks in the  promoter DNA  of IEG genes that function in  neuroplasticity .  Repair  of these induced breaks is associated with DNA demethylation of IEG gene promoters allowing immediate expression of these IEG genes. [ 93 ] The double-strand breaks that are induced during a learning experience are not immediately repaired. About 600 regulatory sequences in promoters and about 800 regulatory sequences in  enhancers  appear to depend on double strand breaks initiated by topoisomerase 2-beta (TOP2B) for activation. [ 94 ] [ 95 ]  The induction of particular double-strand breaks are specific with respect to their inducing signal. When neurons are activated  in vitro , just 22 of TOP2B-induced double-strand breaks occur in their genomes. [ 96 ] Such TOP2B-induced double-strand breaks are accompanied by at least four enzymes of the  non-homologous end joining (NHEJ) DNA repair pathway  (DNA-PKcs, KU70, KU80, and DNA LIGASE IV) (see Figure). These enzymes repair the double-strand breaks within about 15 minutes to two hours. [ 96 ] [ 97 ]  The double-strand breaks in the promoter are thus associated with TOP2B and at least these four repair enzymes. These proteins are present simultaneously on a single promoter  nucleosome  (there are about 147 nucleotides in the DNA sequence wrapped around a single nucleosome) located near the transcription start site of their target gene. [ 97 ] The double-strand break introduced by TOP2B apparently frees the part of the promoter at an RNA polymerase-bound  transcription start site  to physically move to its associated enhancer (see  regulatory sequence ). This allows the enhancer, with its bound  transcription factors  and  mediator proteins , to directly interact with the RNA polymerase paused at the transcription start site to start  transcription . [ 96 ] [ 98 ] Contextual  fear conditioning  in the mouse causes the mouse to have a long-term memory and fear of the location in which it occurred. Contextual fear conditioning causes hundreds of DSBs in mouse brain medial prefrontal cortex (mPFC) and hippocampus neurons (see Figure: Brain regions involved in memory formation). These DSBs predominately activate genes involved in synaptic processes, that are important for learning and memory. [ 99 ] Up until the mid-1980s it was assumed that infants could not encode, retain, and retrieve information. [ 100 ]  A growing body of research now indicates that infants as young as 6-months can recall information after a 24-hour delay. [ 101 ]  Furthermore, research has revealed that as infants grow older they can store information for longer periods of time; 6-month-olds can recall information after a 24-hour period, 9-month-olds after up to five weeks, and 20-month-olds after as long as twelve months. [ 102 ]  In addition, studies have shown that with age, infants can store information faster. Whereas 14-month-olds can recall a three-step sequence after being exposed to it once, 6-month-olds need approximately six exposures in order to be able to remember it. [ 67 ] [ 101 ] Although 6-month-olds can recall information over the short-term, they have difficulty recalling the temporal order of information. It is only by 9 months of age that infants can recall the actions of a two-step sequence in the correct temporal order – that is, recalling step 1 and then step 2. [ 103 ] [ 104 ]  In other words, when asked to imitate a two-step action sequence (such as putting a toy car in the base and pushing in the plunger to make the toy roll to the other end), 9-month-olds tend to imitate the actions of the sequence in the correct order (step 1 and then step 2). Younger infants (6-month-olds) can only recall one step of a two-step sequence. [ 101 ]  Researchers have suggested that these age differences are probably due to the fact that the  dentate gyrus  of the hippocampus and the frontal components of the neural network are not fully developed at the age of 6-months. [ 68 ] [ 105 ] [ 106 ] In fact, the term 'infantile amnesia' refers to the phenomenon of accelerated forgetting during infancy. Importantly, infantile amnesia is not unique to humans, and preclinical research (using rodent models) provides insight into the precise neurobiology of this phenomenon. A review of the literature from behavioral neuroscientist  Jee Hyun Kim  suggests that accelerated forgetting during early life is at least partly due to rapid growth of the brain during this period. [ 107 ] One of the key concerns of older adults is the experience of  memory loss , especially as it is one of the hallmark symptoms of  Alzheimer's disease . However, memory loss is qualitatively different in normal  aging  from the kind of memory loss associated with a diagnosis of Alzheimer's (Budson & Price, 2005). Research has revealed that individuals' performance on memory tasks that rely on frontal regions declines with age. Older adults tend to exhibit deficits on tasks that involve knowing the temporal order in which they learned information, [ 108 ]  source memory tasks that require them to remember the specific circumstances or context in which they learned information, [ 109 ]  and prospective memory tasks that involve remembering to perform an act at a future time. Older adults can manage their problems with prospective memory by using appointment books, for example. Gene  transcription  profiles were determined for the human  frontal cortex  of individuals from age 26 to 106 years. Numerous genes were identified with reduced expression after age 40, and especially after age 70. [ 110 ]  Genes that play central roles in memory and  learning  were among those showing the most significant reduction with age. There was also a marked increase in  DNA damage , likely  oxidative damage , in the  promoters  of those genes with reduced expression. It was suggested that DNA damage may reduce the expression of selectively vulnerable genes involved in memory and learning. [ 110 ] Much of the current knowledge of memory has come from studying  memory disorders , particularly loss of memory, known as  amnesia . Amnesia can result from extensive damage to: (a) the regions of the medial temporal lobe, such as the hippocampus, dentate gyrus, subiculum, amygdala, the parahippocampal, entorhinal, and perirhinal cortices [ 111 ]  or the (b) midline diencephalic region, specifically the dorsomedial nucleus of the thalamus and the mammillary bodies of the hypothalamus. [ 112 ]  There are many sorts of amnesia, and by studying their different forms, it has become possible to observe apparent defects in individual sub-systems of the brain's memory systems, and thus hypothesize their function in the normally working brain. Other  neurological  disorders such as  Alzheimer's disease  and  Parkinson's disease [ 113 ] [ 114 ]  can also affect memory and cognition. [ 115 ] \n Hyperthymesia , or hyperthymesic syndrome, is a disorder that affects an individual's autobiographical memory, essentially meaning that they cannot forget small details that otherwise would not be stored. [ 116 ] [ 117 ] [ 118 ]   Korsakoff's syndrome , also known as Korsakoff's psychosis, amnesic-confabulatory syndrome, is an organic brain disease that adversely affects memory by widespread loss or shrinkage of neurons within the prefrontal cortex. [ 76 ] While not a disorder, a common  temporary  failure of word retrieval from memory is the  tip-of-the-tongue  phenomenon. Those with  anomic aphasia  (also called nominal aphasia or Anomia), however, do experience the tip-of-the-tongue phenomenon on an ongoing basis due to damage to the frontal and parietal  lobes of the brain . Memory dysfunction can also occur after viral infections. [ 119 ]  Many patients recovering from  COVID-19 experience memory lapses . Other viruses can also elicit memory dysfunction, including  SARS-CoV-1 ,  MERS-CoV ,  Ebola virus  and even  influenza virus . [ 119 ] [ 120 ] Interference can hamper memorization and retrieval. There is  retroactive interference , when learning new information makes it harder to recall old information [ 121 ]  and  proactive interference , where prior learning disrupts recall of new information. Although interference can lead to forgetting, it is important to keep in mind that there are situations when old information can facilitate learning of new information. Knowing Latin, for instance, can help an individual learn a related language such as French – this phenomenon is known as positive transfer. [ 122 ] Stress has a significant effect on memory formation and learning. In response to stressful situations, the brain releases hormones and neurotransmitters (ex. glucocorticoids and catecholamines) which affect memory encoding processes in the hippocampus. Behavioural research on animals shows that chronic stress produces adrenal hormones which impact the hippocampal structure in the brains of rats. [ 123 ]  An experimental study by German cognitive psychologists L. Schwabe and O. Wolf demonstrates how learning under stress also decreases memory recall in humans. [ 124 ]  In this study, 48 healthy female and male university students participated in either a stress test or a control group. Those randomly assigned to the stress test group had a hand immersed in ice cold water (the reputable SECPT or 'Socially Evaluated Cold Pressor Test') for up to three minutes, while being monitored and videotaped. Both the stress and control groups were then presented with 32 words to memorize. Twenty-four hours later, both groups were tested to see how many words they could remember (free recall) as well as how many they could recognize from a larger list of words (recognition performance). The results showed a clear impairment of memory performance in the stress test group, who recalled 30% fewer words than the control group. The researchers suggest that stress experienced during learning distracts people by diverting their attention during the memory encoding process. However, memory performance can be enhanced when material is linked to the learning context, even when learning occurs under stress. A separate study by cognitive psychologists Schwabe and Wolf shows that when retention testing is done in a context similar to or congruent with the original learning task (i.e., in the same room), memory impairment and the detrimental effects of stress on learning can be attenuated. [ 125 ]  Seventy-two healthy female and male university students, randomly assigned to the  SECPT stress test  or to a control group, were asked to remember the locations of 15 pairs of picture cards – a computerized version of the card game \"Concentration\" or \"Memory\". The room in which the experiment took place was infused with the scent of vanilla, as odour is a strong cue for memory. Retention testing took place the following day, either in the same room with the vanilla scent again present, or in a different room without the fragrance. The memory performance of subjects who experienced stress during the object-location task decreased significantly when they were tested in an unfamiliar room without the vanilla scent (an incongruent context); however, the memory performance of stressed subjects showed no impairment when they were tested in the original room with the vanilla scent (a congruent context). All participants in the experiment, both stressed and unstressed, performed faster when the learning and retrieval contexts were similar. [ 126 ] This research on the effects of stress on memory may have practical implications for education, for  eyewitness testimony  and for psychotherapy: students may perform better when tested in their regular classroom rather than an exam room, eyewitnesses may recall details better at the scene of an event than in a courtroom, and persons with  post-traumatic stress  may improve when helped to situate their  memories of a traumatic event  in an appropriate context. Stressful life experiences may be a cause of memory loss as a person ages.  Glucocorticoids  that are released during stress cause damage to neurons that are located in the  hippocampal  region of the brain. Therefore, the more stressful situations that someone encounters, the more susceptible they are to memory loss later on. The  CA1 neurons  found in the hippocampus are destroyed due to glucocorticoids decreasing the release of glucose and the reuptake of  glutamate . This high level of extracellular glutamate allows calcium to enter  NMDA receptors  which in return kills neurons. Stressful life experiences can also cause repression of memories where a person moves an unbearable memory to the unconscious mind. [ 76 ]  This directly relates to traumatic events in one's past such as kidnappings, being prisoners of war or sexual abuse as a child. The more long term the exposure to stress is, the more impact it may have. However, short term exposure to stress also causes impairment in memory by interfering with the function of the hippocampus. Research shows that subjects placed in a stressful situation for a short amount of time still have blood glucocorticoid levels that have increased drastically when measured after the exposure is completed. When subjects are asked to complete a learning task after short term exposure they often have difficulties. Prenatal stress also hinders the ability to learn and memorize by disrupting the development of the hippocampus and can lead to unestablished long term potentiation in the offspring of severely stressed parents. Although the stress is applied prenatally, the offspring show increased levels of glucocorticoids when they are subjected to stress later on in life. [ 127 ]  One explanation for why children from lower socioeconomic backgrounds tend to display poorer memory performance than their higher-income peers is the effects of stress accumulated over the course of the lifetime. [ 128 ]  The effects of low income on the developing hippocampus is also thought be mediated by chronic stress responses which may explain why children from lower and higher-income backgrounds differ in terms of memory performance. [ 128 ] Making memories occurs through a three-step process, which can be enhanced by  sleep . The three steps are as follows: Sleep affects memory consolidation. During sleep, the neural connections in the brain are strengthened. This enhances the brain's abilities to stabilize and retain memories. There have been several studies which show that sleep improves the retention of memory, as memories are enhanced through active consolidation. System consolidation takes place during slow-wave sleep (SWS). [ 129 ] [ 130 ]  This process implicates that memories are reactivated during sleep, but that the process does not enhance every memory. It also implicates that qualitative changes are made to the memories when they are transferred to long-term store during sleep. During sleep, the hippocampus replays the events of the day for the neocortex. The neocortex then reviews and processes memories, which moves them into long-term memory. When one does not get enough sleep it makes it more difficult to learn as these neural connections are not as strong, resulting in a lower retention rate of memories. Sleep deprivation makes it harder to focus, resulting in inefficient learning. [ 129 ]  Furthermore, some studies have shown that sleep deprivation can lead to  false memories  as the memories are not properly transferred to long-term memory.\nOne of the primary functions of sleep is thought to be the improvement of the consolidation of information, as several studies have demonstrated that memory depends on getting sufficient sleep between training and test. [ 131 ]  Additionally, data obtained from neuroimaging studies have shown activation patterns in the sleeping brain that mirror those recorded during the learning of tasks from the previous day, [ 131 ]  suggesting that new memories may be solidified through such rehearsal. [ 132 ] Although people often think that memory operates like recording equipment, this is not the case. The molecular mechanisms underlying the induction and maintenance of memory are very dynamic and comprise distinct phases covering a time window from seconds to even a lifetime. [ 133 ]  In fact, research has revealed that our memories are constructed: \"current hypotheses suggest that constructive processes allow individuals to simulate and imagine future episodes, [ 134 ]  happenings, and scenarios. Since the future is not an exact repetition of the past, simulation of future episodes requires a complex system that can draw on the past in a manner that flexibly extracts and recombines elements of previous experiences – a constructive rather than a reproductive system.\" [ 81 ]  People can construct their memories when they encode them and/or when they recall them. To illustrate, consider a classic study conducted by Elizabeth Loftus and John Palmer (1974) [ 135 ]  in which people were instructed to watch a film of a traffic accident and then asked about what they saw. The researchers found that the people who were asked, \"How fast were the cars going when they  smashed  into each other?\" gave higher estimates than those who were asked, \"How fast were the cars going when they  hit  each other?\" Furthermore, when asked a week later whether they had seen broken glass in the film, those who had been asked the question with  smashed  were twice more likely to report that they had seen broken glass than those who had been asked the question with  hit  (there was no broken glass depicted in the film). Thus, the wording of the questions distorted viewers' memories of the event. Importantly, the wording of the question led people to construct different memories of the event – those who were asked the question with  smashed  recalled a more serious car accident than they had actually seen. The findings of this experiment were replicated around the world, and researchers consistently demonstrated that when people were provided with misleading information they tended to misremember, a phenomenon known as the  misinformation effect . [ 136 ] Research has revealed that asking individuals to repeatedly imagine actions that they have never performed or events that they have never experienced could result in false memories. For instance, Goff and Roediger [ 137 ]  (1998) asked participants to imagine that they performed an act (e.g., break a toothpick) and then later asked them whether they had done such a thing. Findings revealed that those participants who repeatedly imagined performing such an act were more likely to think that they had actually performed that act during the first session of the experiment. Similarly, Garry and her colleagues (1996) [ 138 ]  asked college students to report how certain they were that they experienced a number of events as children (e.g., broke a window with their hand) and then two weeks later asked them to imagine four of those events. The researchers found that one-fourth of the students asked to imagine the four events reported that they had actually experienced such events as children. That is, when asked to imagine the events they were more confident that they experienced the events. Research reported in 2013 revealed that it is possible to artificially stimulate prior memories and artificially implant false memories in mice. Using  optogenetics , a team of RIKEN-MIT scientists caused the mice to incorrectly associate a benign environment with a prior unpleasant experience from different surroundings. Some scientists believe that the study may have implications in studying false memory formation in humans, and in treating PTSD and  schizophrenia . [ 139 ] [ 140 ] [ medical citation needed ] Memory reconsolidation is when previously consolidated memories are recalled or retrieved from long-term memory to your active consciousness. During this process, memories can be further strengthened and added to but there is also risk of manipulation involved. We like to think of our memories as something stable and constant when they are stored in long-term memory but this is not the case. There are a large number of studies that found that consolidation of memories is not a singular event but are put through the process again, known as reconsolidation. [ 141 ]  This is when a memory is recalled or retrieved and placed back into your working memory. The memory is now open to manipulation from outside sources and the misinformation effect which could be due to misattributing the source of the inconsistent information, with or without an intact original memory trace (Lindsay and Johnson, 1989). [ 142 ]  One thing that can be sure is that memory is malleable. This new research into the concept of reconsolidation has opened the door to methods to help those with unpleasant memories or those that struggle with memories. An example of this is if you had a truly frightening experience and recall that memory in a less arousing environment, the memory will be weaken the next time it is retrieved. [ 141 ]  \"Some studies suggest that over-trained or strongly reinforced memories do not undergo reconsolidation if reactivated the first few days after training, but do become sensitive to reconsolidation interference with time.\" [ 141 ]  This, however does not mean that all memory is susceptible to reconsolidation. There is evidence to suggest that memory that has undergone strong training and whether or not is it intentional is less likely to undergo reconsolidation. [ 143 ]  There was further testing done with rats and mazes that showed that reactivated memories were more susceptible to manipulation, in both good and bad ways, than newly formed memories. [ 144 ]  It is still not known whether or not these are new memories formed and it is an inability to retrieve the proper one for the situation or if it is a reconsolidated memory. Because the study of reconsolidation is still a newer concept, there is still debate on whether it should be considered scientifically sound. A UCLA research study published in the June 2008 issue of the  American Journal of Geriatric Psychiatry  found that people can improve  cognitive function  and brain efficiency through simple lifestyle changes such as incorporating memory exercises,  healthy eating ,  physical fitness  and  stress reduction  into their daily lives. This study examined 17 subjects, (average age 53) with normal memory performance.\nEight subjects were asked to follow a \"brain healthy\" diet, relaxation, physical, and mental exercise (brain teasers and verbal memory training techniques). After 14 days, they showed greater word fluency (not memory) compared to their baseline performance. No long-term follow-up was conducted; it is therefore unclear if this intervention has lasting effects on memory. [ 145 ] There are a loosely associated group of mnemonic principles and techniques that can be used to vastly improve memory known as the  art of memory . The  International Longevity Center  released in 2001 a report [ 146 ]  which includes in pages 14–16 recommendations for keeping the mind in good functionality until advanced age. Some of the recommendations are: Memorization  is a method of learning that allows an individual to recall information verbatim.  Rote learning  is the method most often used. Methods of memorizing things have been the subject of much discussion over the years with some writers, such as  Cosmos Rossellius  using visual  alphabets . The  spacing effect  shows that an individual is more likely to remember a list of items when rehearsal is spaced over an extended period of time. In contrast to this is  cramming : an intensive memorization in a short period of time. The spacing effect is exploited to improve memory in  spaced repetition  flashcard training. Also relevant is the  Zeigarnik effect , which states that people remember uncompleted or interrupted tasks better than completed ones. The so-called  Method of loci  uses spatial memory to memorize non-spatial information. [ 147 ] Plants lack a specialized organ devoted to memory retention, so plant memory has been a controversial topic in recent years. New advances in the field have identified the presence of  neurotransmitters  in plants, adding to the hypothesis that plants are capable of remembering. [ 148 ]   Action potentials , a physiological response characteristic of  neurons , have been shown to have an influence on plants as well, including in wound responses and  photosynthesis . [ 148 ]  In addition to these homologous features of memory systems in both plants and animals, plants have also been observed to encode, store and retrieve basic short-term memories. One of the most well-studied plants to show rudimentary memory is the  Venus flytrap . Native to the  subtropical   wetlands  of the  eastern United States , Venus flytraps have evolved the ability to obtain meat for sustenance, likely due to the lack of nitrogen in the soil. [ 149 ]  This is done by two trap-forming leaf tips that snap shut once triggered by a potential prey. On each lobe, three trigger hairs await stimulation. In order to maximize the benefit-to-cost ratio, the plant enables a rudimentary form of memory in which two trigger hairs must be stimulated within thirty  seconds in order to result in trap closure. [ 149 ]  This system ensures that the trap only closes when potential prey is within grasp. The time lapse between trigger hair stimulations suggests that the plant can remember an initial stimulus long enough for a second stimulus to initiate trap closure. This memory is not encoded in a brain, as plants lack this specialized organ. Rather, information is stored in the form of  cytoplasmic  calcium levels. The first trigger causes a subthreshold cytoplasmic calcium influx. [ 149 ]  This initial trigger is not enough to activate trap closure, so a subsequent stimulus allows for a secondary influx of calcium. The latter calcium rise superimposes on the initial one, creating an action potential that passes threshold, resulting in trap closure. [ 149 ]  Researchers, to prove that an electrical threshold must be met to stimulate trap closure, excited a single trigger hair with a constant mechanical stimulus using Ag/AgCl electrodes. [ 150 ]  The trap closed after only a few seconds. This experiment demonstrated that the electrical threshold, not necessarily the number of trigger hair stimulations, was the contributing factor in Venus flytrap memory. It has been shown that trap closure can be blocked using uncouplers and inhibitors of  voltage-gated channels . [ 150 ]  After trap closure, these electrical signals stimulate  glandular  production of  jasmonic acid  and  hydrolases , allowing for digestion of prey. [ 151 ] Many other plants exhibit the capacity to remember, including  Mimosa pudica . [ 152 ]  An experimental apparatus was designed to drop potted mimosa plants repeatedly from the same distance and at the same speed. It was observed that the plants' defensive response of curling up their leaves decreased over the sixty times the experiment was repeated. To confirm that this was a mechanism of memory rather than  exhaustion , some of the plants were shaken post experiment and displayed normal defensive responses of leaf curling. This experiment demonstrated long-term memory in the plants, as it was repeated a month later, and the plants were observed to remain unfazed by the dropping. [ 152 ]"
  },
  {
    "id": 28,
    "title": "Preservation (library and archive)",
    "content": "In  conservation ,  library  and  archival science ,  preservation  is a set of preventive conservation activities aimed at prolonging the life of a record, book, or object while making as few changes as possible. Preservation activities vary widely and may include monitoring the condition of items, maintaining the temperature and humidity in collection storage areas, writing a plan in case of emergencies, digitizing items, writing relevant  metadata , and increasing accessibility. Preservation, in this definition, is practiced in a  library  or an  archive  by a  conservator ,  librarian ,  archivist , or other professional when they perceive a collection or record is in need of maintenance. Preservation should be distinguished from interventive  conservation and restoration , which refers to the treatment and repair of individual items to slow the process of decay, or restore them to a usable state. [ 1 ]  \" Preventive conservation \" is used interchangeably with \"preservation\". [ 2 ] A relatively new concept,  digitization , has been hailed as a way to preserve historical items for future use. \"Digitizing refers to the process of converting analog materials into digital form.\" [ 3 ] For manuscripts, digitization is achieved through scanning an item and saving it to a digital format. For example, the Google Book Search program has partnered with over forty libraries around the world to digitize books. The goal of this library partnership project is to \"make it easier for people to find relevant books – specifically, books they wouldn't find any other way such as those that are out of print – while carefully respecting authors' and publishers' copyrights.\" [ 4 ] Although digitization seems to be a promising area for future preservation, there are also problems. The main problems are that digital space costs money, media and file formats may become obsolete, and backwards compatibility is not guaranteed. [ 5 ]  Higher-quality images take a longer time to scan, but are often more valuable for future use. Fragile items are often more difficult or more expensive to scan, which creates a selection problem for preservationists where they must decide if digital access in the future is worth potentially damaging the item during the scanning process. Other problems include scan quality, redundancy of digitized books among different libraries, and copyright law. [ 6 ] However, many of these problems are being solved through educational initiatives. Educational programs are tailoring themselves to fit preservation needs and help new students understand preservation practices. Programs teaching graduate students about digital librarianship are especially important. [ 7 ] Groups such as the Digital Preservation Network strive to ensure that \"the complete scholarly record is preserved for future generations\". [ 8 ]  The Library of Congress maintains a Sustainability of Digital Formats web site that educates institutions on various aspects of preservation: most notably, on approximately 200 digital format types and which are most likely to last into the future. [ 9 ] Digital Preservation  is another name for digitization, and is the term more commonly used in archival courses. The main goal of digital preservation is to guarantee that people will have access to the digitally preserved materials long into the future. [ 10 ] When practicing preservation, one has several factors to consider in order to properly preserve a record: 1) the storage environment of the record, 2) the criteria to determine when preservation is necessary, 3) what the standard preservation practices are for that particular institution, 4) research and testing, and 5) if any vendor services will be needed for further preservation and potentially conservation. Environmental controls are necessary to facilitate the preservation of organic materials and are especially important to monitor in rare and  special collections . Key environmental factors to watch include  temperature ,  relative humidity , pests, pollutants, and  light  exposure. In general, the lower the temperature is, the better it is for the collection. However, since books and other materials are often housed in areas with people, a compromise must be struck to accommodate human comfort. A reasonable temperature to accomplish both goals is 65–68˚F (18–20 °C) however, if possible, film and photography collections should be kept in a segregated area at 55 ˚F (13 °C). [ 11 ] Books and other materials take up and give off moisture making them sensitive to relative humidity. Very high humidity encourages mold growth and insect infestations. Low humidity causes materials to lose their flexibility. Fluctuations in relative humidity are more damaging than a constant humidity in the middle or low range. Generally, the relative humidity should be between 30–50% with as little variation as possible, however recommendations on specific levels to maintain vary depending on the type of material, i.e. paper-based, film, etc. [ 12 ]  A specialized dew point calculator for book preservation is available. [ 13 ] Pests, such as insects and vermin, eat and destroy paper and the adhesive that secures book bindings. Food and drink in libraries, archives, and museums can increase the attraction of pests. [ 14 ]  An  Integrated Pest Management  system is one way to control pests in libraries. Particulate and gaseous pollutants, such as soot,  ozone ,  sulfur dioxide , oxides of nitrogen, can cause dust, soiling, and irreversible molecular damage to materials. Pollutants are exceedingly small and not easily detectable or removable. A special filtration system in the building's  HVAC  is a helpful defense. Exposure to light also has a significant effect on materials. It is not only the light visible to humans that can cause damage, but also  ultraviolet  light and  infrared  radiation. Measured in  lux  or the amount of lumens/m 2 , the generally accepted level of illumination with sensitive materials is limited to 50 lux per day. Materials receiving more lux than recommended can be placed in dark storage periodically to prolong the original appearance of the object. [ 15 ] Recent concerns about the  impact of climate change  on the management of cultural heritage objects as well as the historic environment [ 16 ]  has prompted research efforts to investigate alternative climate control methods and strategies [ 17 ]  that include the implementation of alternative climate control systems to replace or supplement traditional high-energy consuming HVAC systems as well as the introduction of passive preservation techniques. [ 18 ]  Rather than maintaining a flat line, consistent 24/7 condition for a collection's environment, fluctuation can occur within acceptable limits to create a preservation environment while also thinking of energy efficiency and taking advantage of the outside environment. [ 19 ] Bound materials are sensitive to rapid temperature or humidity cycling due to differential expansion of the binding and pages, which may cause the binding to crack and/or the pages to warp. Changes in temperature and humidity should be done slowly so as to minimize the difference in expansion rates. However, an accelerated aging study on the effects of fluctuating temperature and humidity on paper color and strength showed no evidence that cycling of one temperature to another or one RH to another caused a different mechanism of decay. [ 20 ] The preferred method for storing  manuscripts , archival records, and other paper documents is to place them in  acid-free paper  folders which are then placed in acid-free of low-lignin boxes for further protection. [ 21 ]  Similarly, books that are fragile, valuable, oddly shaped, or in need of protection can be stored in archival boxes and enclosures. Additionally, housing books can protect them from many of the contributing factors to book damage: pests, light, temperature changes, and water. [ 22 ] Contamination can occur at the time of manufacture, especially with electronic materials. [ 23 ]  It must be stopped before it spreads, but it is usually irreversible. Making a proper decision is an important factor before starting preservation practices.  Decision making  for preservation should be made considering significance and value of materials. Significance is considered to have two major components: importance and quality. [ 25 ]  \"Importance\" relates to the collection's role as a record, and \"quality\" covers comprehensiveness, depth, uniqueness, authenticity and reputation of the collection. Moreover, analyzing the significance of materials can be used to uncover more about their meaning. [ 26 ]  Assessment of significance can also aid in documenting the provenance and context to argue the case for grant funding for the object and collection. [ 27 ] Forms of significance can be historically, culturally, socially, or spiritually significant. In the preservation context, libraries and archives make decisions in different ways. In libraries, decision-making likely targets existing holding materials, whereas in archives, decisions for preservation are often made when they acquire materials. Therefore, different criteria might be needed on different occasions. In general, for archive criteria, the points include: For archival criteria, the following are evidence of significance: Since the 1970s, the  Northeast Document Conservation Center  has stated that the study of understanding the needs of the archive/library is inherently important to their survival. To prolong the life of a collection, it is important that a systematic preservation plan is in place. The first step in planning a preservation program is to assess the institution's existing preservation needs. This process entails identifying the general and specific needs of the collection, establishing priorities, and gathering the resources to execute the plan. [ 29 ] Because budget and time limitations require priorities to be set, standards have been established by the profession to determine what should be preserved in a collection. Considerations include existing condition, rarity, and evidentiary and market values. With non-paper formats, the availability of equipment to access the information will be a factor (for example, playback equipment for audio-visual materials, or microform readers). An institution should determine how many, if any, other institutions hold the material, and consider coordinating efforts with those that do. [ 30 ] Institutions should establish an environment that prioritizes preservation and create an understanding among administration and staff. Additionally, the institution's commitment to preservation should be communicated to funders and stakeholders so that funds can be allocated towards preservation efforts. The first steps an institution should implement, according to the NEDCC, are to establish a policy that defines and charts the course of action and create a framework for carrying out goals and priorities. There are three methods for carrying out a preservation survey: general preservation assessment, collection condition surveys, and an item-by-item survey. [ 31 ]  General condition surveys can be part of a library  inventory . Selection for treatment determines the survival of materials and should be done by a specialist, whether in relation to an established collection development policy or on an item by item basis. [ 32 ]  Once an object or collection has been chosen for preservation, the treatment must be determined that is most appropriate to the material and its collecting institution. If the information is most important, reformatting or creation of a surrogate is a likely option. If the artifact itself is of value, it will receive conservation treatment, ideally of a reversible nature. [ 30 ] With old media deteriorating or showing their vulnerabilities and new media becoming available, research remains active in the field of conservation and preservation. Everything from how to preserve paper media to creating and maintaining electronic resources and gauging their  digital permanence  is being explored by students and professionals in archives/libraries. The two main issues that most institutions tend to face are the rapid disintegration of acidic paper and water damage (due to flooding, plumbing problems, etc.). Therefore, these areas of preservation, as well as new digital technologies, receive much of the research attention. The  American Library Association  has many scholarly journals that publish articles on preservation topics, such as  College and Research Libraries, Information Technology and Libraries,  and  Library Resources and Technical Services . Scholarly periodicals in this field from other publishers include  International Preservation News, Journal of the American Institute for Conservation , and  Collection Management  among many others. Learning the proper methods of preservation is important and most archivists are educated on the subject at academic institutions that specifically cover archives and preservation. In the United States most repositories require archivists to have a degree from an ALA-accredited library school. [ 33 ]  Similar institutions exist in countries outside the US. Since 2010, the Andrew W. Mellon Foundation has enhanced funding for library and archives conservation education in three major conservation programs. [ 34 ]  These programs are all part of the Association of North American Graduate Programs in the Conservation of Cultural Property (ANAGPIC). [ 35 ] Another educational resource available to preservationists is the Northeast Document Conservation Center or NEDCC. [ 36 ] The Preservation, Planning and Publications Committee of the Preservation and Reformatting Section (PARS) in the Association for Library Collections & Technical Services has created a Preservation Education Directory of ALA Accredited schools in the U.S. and Canada offering courses in preservation. The directory is updated approximately every three years. The 10th Edition was made available on the ALCTS web site in March 2015. [ 37 ] Additional preservation education is available to librarians through various professional organizations, such as: Limited, tax-driven funding can often interfere with the ability for public libraries to engage in extensive preservation activities. Materials, particularly books, are often much easier to replace than to repair when damaged or worn. Public libraries usually try to tailor their services to meet the needs and desires of their local communities, which could cause an emphasis on acquiring new materials over preserving old ones. Librarians working in public facilities frequently have to make complicated decisions about how to best serve their patrons. Commonly, public library systems work with each other and sometimes with more academic libraries through interlibrary loan programs. By sharing resources, they are able to expand upon what might be available to their own patrons and share the burdens of preservation across a greater array of systems. Archival facilities focus specifically on rare and fragile materials. With staff trained in appropriate techniques, archives are often available to many public and private library facilities as an alternative to destroying older materials. Items that are unique, such as photographs, or items that are out of print, can be preserved in archival facilities more easily than in many library settings. [ 52 ] Because so many museum holdings are unique, including print materials, art, and other objects, preservationists are often most active in this setting; however, since most holdings are usually much more fragile, or possibly corrupted,  conservation  may be more necessary than preservation. This is especially common in  art museums . Museums typically hold to the same practices led by archival institutions. Preservation as a formal profession in libraries and archives dates from the twentieth century, but its philosophy and practice has roots in many earlier traditions. [ 53 ] In many ancient societies, appeals to heavenly protectors were used to preserve books, scrolls and manuscripts from insects, fire and decay. Human record-keeping arguably dates back to the  cave painting  boom of the  Upper Paleolithic , some 32,000–40,000 years ago. More direct antecedents are the  writing systems  that developed in the 4th millennium BC. Written record keeping and information sharing practices, along with  oral tradition , sustain and transmit information from one group to another. This level of preservation has been supplemented over the last century with the professional practice of preservation and conservation in the cultural heritage community. The  Paul Banks and Carolyn Harris Preservation Award  for outstanding preservation specialists in library and archival science, is given annually by the Association for Library Collections & Technical Services, [ 60 ]  a subdivision of the  American Library Association . It is awarded in recognition of professional preservation specialists who have made significant contributions to the field. Reformatting, or in any other way copying an item's contents, raises obvious  copyright  issues. In many cases, a library is allowed to make a limited number of copies of an item for preservation purposes. In the United States, certain exceptions have been made for libraries and archives. [ 61 ] Ethics will play an important role in many aspects of the conservator's activities. When choosing which objects are in need of treatment, the conservator should do what is best for the object in question and not yield to pressure or opinion from outside sources. Conservators should refer to the AIC Code of Ethics and Guidelines for Practice, [ 62 ]  which states that the conservation professional must \"strive to attain the highest possible standards in all aspects of conservation.\" One instance in which these decisions may get tricky is when the conservator is dealing with cultural objects. The AIC Code of Ethics and Guidelines for Practice [ 62 ]  has addressed such concerns, stating \"All actions of the conservation professional must be governed by an informed respect for cultural property, its unique character and significance and the people or person who created it.\" This can be applied in both the care and long-term storage of objects in archives and institutions. It is important that preservation specialists be respectful of cultural property and the societies that created it, and it is also important for them to be aware of international and national laws pertaining to stolen items. In recent years there has been a rise in nations seeking out artifacts that have been stolen and are now in museums. In many cases museums are working with the nations to find a compromise to balance the need for reliable supervision as well as access for both the public and researchers. [ 63 ] Conservators are not just bound by ethics to treat cultural and religious objects with respect, but also in some cases by law. For example, in the United States, conservators must comply with the  Native American Graves Protection and Repatriation Act  (NAGPRA). The First Archivists Circle, a group of Native American archivists, has also created Protocols for Native American Archival Materials. The non-binding guidelines are suggestions for libraries and archives with Native American archival materials. The care of cultural and sacred objects often affects the physical storage or the object. For example, sacred objects of the native peoples of the Western United States are supposed to be stored with sage to ensure their spiritual well-being. The idea of storing an object with plant material is inherently problematic to an archival collection because of the possibility of insect infestation. When conservators have faced this problem, they have addressed it by using freeze-dried sage, thereby meeting both conservation and cultural needs. Some individuals in the archival community have explored the possible moral responsibility to preserve all cultural phenomena, in regards to the concept of monumental preservation. [ 64 ]  Other advocates argue that such an undertaking is something that the indigenous or native communities that produce such cultural objects are better suited to perform. Currently, however, many indigenous communities are not financially able to support their own archives and museums. Still, indigenous archives are on the rise in the United States. [ 65 ] There is a longstanding tension between preservation of and access to library materials, particularly in the area of  special collections . Handling materials promotes their progression to an unusable state, especially if they are handled carelessly. On the other hand, materials must be used in order to gain any benefit from them. In a collection with valuable materials, this conflict is often resolved by a number of measures which can include heightened security, requiring the use of gloves for photographs, restricting the materials researchers may bring with them into a reading room, and restricting use of materials to patrons who are not able to satisfy their research needs with less valuable copies of an item. These restrictions can be considered hindrances to researchers who feel that these measures are in place solely to keep materials out of the hands of the public. [ citation needed ] There is also controversy surrounding preservation methods. A major controversy at the end of the twentieth century centered on the practice of discarding items that had been microfilmed. This was the subject of novelist  Nicholson Baker 's book  Double Fold , which chronicled his efforts to save many old runs of American newspapers (formerly owned by the British Library) from being sold to dealers or pulped. A similar concern persists over the retention of original documents reformatted by any means, analog or digital. Concerns include scholarly needs and legal requirements for authentic or original records as well as questions about the longevity, quality, and completeness of reformatted materials. [ 66 ] [ 67 ]  Retention of originals as a source or fail-safe copy is now a fairly common practice. Another controversy revolving around different preservation methods is that of digitization of original material to maintain the intellectual content of the material while ignoring the physical nature of the book. [ 68 ]  Further, the  Modern Language Association 's Committee on the Future of the Print Record structured its \"Statement on the Significance of Primary Records\" on the inherent theoretical ideology that there is a need to preserve as many copies of a printed edition as is possible as texts and their textual settings are, quite simply, not separable, just as the artifactual characteristics of texts are as relevant and varied as the texts themselves (in the report mentioned herewith,  G. Thomas Tanselle  suggests that presently existing book stacks need not be abandoned with emerging technologies; rather they serve as vitally important original (primary) sources for future study). [ 69 ] Many digitized items, such as back issues of periodicals, are provided by publishers and databases on a subscription basis. If these companies were to cease providing access to their digital information, facilities that elected to discard paper copies of these periodicals could face significant difficulties in providing access to these items. Discussion as to the best ways to utilize digital technologies is therefore ongoing, and the practice continues to evolve. Of course, the issues surrounding digital objects and their care in libraries and archives continues to expand as more and more of contemporary culture is created, stored, and used digitally. These  born-digital  materials raise their own new kinds of preservation challenges and in some cases they may even require use new kinds of tools and techniques. [ 70 ] In her book  Sacred Stacks: The Higher Purpose of Libraries and Librarianship , Nancy Kalikow Maxwell discusses how libraries are capable of performing some of the same functions as religion. [ 71 ]  Many librarians feel that their work is done for some higher purpose. [ 71 ]  The same can be said for preservation librarians. One instance of the library's role as sacred is to provide a sense of  immortality : with the ever-changing world outside, the library will remain stable and dependable. [ 71 ]  Preservation is a great help in this regard. Through digitization and reformatting, preservation librarians are able to retain material while at the same time adapting to new methods. In this way, libraries can adapt to the changes in user needs without changing the quality of the material itself. Through preservation efforts, patrons can rest assured that although materials are constantly deteriorating over time, the library itself will remain a stable, reliable environment for their information needs. Another sacred ability of the library is to provide information and a connection to the past. [ 71 ]  By working to slow down the processes of deterioration and decay of library materials, preservation practices help keep this link to the past alive."
  },
  {
    "id": 29,
    "title": "Privacy",
    "content": "Privacy  ( UK :  / ˈ p r ɪ v ə s i / ,  US :  / ˈ p r aɪ -/ ) [ 1 ] [ 2 ]  is the ability of an individual or group to seclude themselves or information about themselves, and thereby express themselves selectively. The domain of privacy partially overlaps with  security , which can include the concepts of appropriate use and protection of information. Privacy may also take the form of  bodily integrity . Throughout history, there have been various conceptions of privacy. Most cultures acknowledge the right of individuals to keep aspects of their personal lives out of the public domain. The right to be free from unauthorized invasions of privacy by governments, corporations, or individuals is enshrined in the privacy laws of many countries and, in some instances, their constitutions. With the rise of technology, the debate regarding privacy has expanded from a bodily sense to include a digital sense. In most countries, the right to  digital privacy  is considered an extension of the original  right to privacy , and many countries have passed acts that further protect digital privacy from public and private entities. There are multiple techniques to invade privacy, which may be employed by corporations or governments for profit or political reasons. Conversely, in order to protect privacy, people may employ  encryption  or  anonymity  measures. The word privacy is derived from the Latin word and concept of ‘ privatus ’, which referred to things set apart from what is public; personal and belonging to oneself, and not to the state. [ 3 ]  Literally, ‘ privatus ’ is the past participle of the Latin verb ‘ privere ’ meaning ‘to be deprived of’. [ 4 ] The concept of privacy has been explored and discussed by numerous philosophers throughout history. Privacy has historical roots in ancient Greek philosophical discussions. The most well-known of these was  Aristotle 's distinction between two spheres of life: the public sphere of the  polis , associated with political life, and the private sphere of the  oikos , associated with domestic life. [ 5 ]  Privacy is valued along with other basic necessities of life in the Jewish  deutero-canonical   Book of Sirach . [ 6 ] Islam's holy text, the Qur'an, states the following regarding privacy: ‘Do not spy on one another’ (49:12); ‘Do not enter any houses except your own homes unless you are sure of their occupants' consent’ (24:27). [ 7 ] English philosopher  John Locke ’s (1632-1704) writings on natural rights and the social contract laid the groundwork for modern conceptions of individual rights, including the right to privacy. In his  Second Treatise of Civil Government (1689), Locke argued that a man is entitled to his own self through one’s natural rights of life, liberty, and property. [ 8 ]  He believed that the government was responsible for protecting these rights so individuals were guaranteed private spaces to practice personal activities. [ 9 ] In the political sphere, philosophers hold differing views on the right of private judgment. German philosopher  Georg Wilhelm Friedrich Hegel  (1770-1831) makes the distinction between  moralität , which refers to an individual’s private judgment, and  sittlichkeit , pertaining to one’s rights and obligations as defined by an existing corporate order. On the contrary,  Jeremy Bentham  (1748-1832), an English philosopher, interpreted law as an invasion of privacy. His theory of  utilitarianism  argued that legal actions should be judged by the extent of their contribution to human wellbeing, or necessary utility. [ 10 ] Hegel’s notions were modified by prominent 19th century English philosopher  John Stuart Mill . Mill’s essay  On Liberty  (1859) argued for the importance of protecting individual liberty against the tyranny of the majority and the interference of the state. His views emphasized the right of privacy as essential for personal development and self-expression. [ 11 ] Discussions surrounding surveillance coincided with philosophical ideas on privacy. Jeremy Bentham developed the phenomenon known as the Panoptic effect through his 1791 architectural design of a prison called  Panopticon . The phenomenon explored the possibility of surveillance as a general awareness of being watched that could never be proven at any particular moment. [ 12 ]  French philosopher  Michel Foucault  (1926-1984) concluded that the possibility of surveillance in the instance of the Panopticon meant a prisoner had no choice but to conform to the prison's rules. [ 12 ] As technology has advanced, the way in which privacy is protected and violated has changed with it. In the case of some technologies, such as the  printing press  or the  Internet , the increased ability to share information can lead to new ways in which privacy can be breached. It is generally agreed that the first publication advocating privacy in the United States was the 1890 article by  Samuel Warren  and  Louis Brandeis , \"The Right to Privacy\", [ 13 ]  and that it was written mainly in response to the increase in newspapers and photographs made possible by printing technologies. [ 14 ] In 1948,  1984 ,  written by  George Orwell , was published. A classic dystopian novel,  1984  describes the life of Winston Smith in 1984, located in Oceania, a totalitarian state. The all-controlling Party, the party in power led by Big Brother, is able to control power through mass  surveillance  and limited freedom of speech and thought. George Orwell provides commentary on the negative effects of  totalitarianism , particularly on privacy and  censorship . [ 15 ]  Parallels have been drawn between  1984  and modern censorship and privacy, a notable example being that large social media companies, rather than the government, are able to monitor a user's data and decide what is allowed to be said online through their censorship policies, ultimately for monetary purposes. [ 16 ] In the 1960s, people began to consider how changes in technology were bringing changes in the concept of privacy. [ 17 ]   Vance Packard 's  The Naked Society  was a popular book on privacy from that era and led US discourse on privacy at that time. [ 17 ]  In addition,  Alan Westin 's  Privacy and Freedom  shifted the debate regarding privacy from a physical sense, how the government controls a person's body (i.e.  Roe v. Wade ) and other activities such as wiretapping and photography. As important records became digitized, Westin argued that personal data was becoming too accessible and that a person should have complete jurisdiction over their data, laying the foundation for the modern discussion of privacy. [ 18 ] New technologies can also create new ways to gather private information. In 2001, the legal case  Kyllo v. United States  (533 U.S. 27) determined that the use of  thermal imaging  devices that can reveal previously unknown information without a warrant constitutes a violation of privacy. In 2019, after developing a corporate rivalry in competing voice-recognition software,  Apple  and  Amazon  required employees to listen to  intimate  moments and faithfully transcribe the contents. [ 19 ] Police and citizens often conflict on what degree the police can intrude a citizen's digital privacy. For instance, in 2012, the  Supreme Court  ruled unanimously in  United States v. Jones  (565 U.S. 400), in the case of Antoine Jones who was arrested of drug possession using a  GPS  tracker on his car that was placed without a warrant, that warrantless tracking infringes the Fourth Amendment. The Supreme Court also justified that there is some \"reasonable expectation of privacy\" in transportation since the reasonable expectation of privacy had already been established under  Griswold v. Connecticut  (1965). The Supreme Court also further clarified that the Fourth Amendment did not only pertain to physical instances of intrusion but also digital instances, and thus  United States v. Jones  became a landmark case. [ 20 ] In 2014, the Supreme Court ruled unanimously in  Riley v. California  (573 U.S. 373), where David Leon Riley was arrested after he was pulled over for driving on expired license tags when the police searched his phone and discovered that he was tied to a shooting, that searching a citizen's phone without a warrant was an unreasonable search, a violation of the Fourth Amendment. The Supreme Court concluded that the cell phones contained personal information different from trivial items, and went beyond to state that information stored on the cloud was not necessarily a form of evidence.  Riley v. California  evidently became a landmark case, protecting the digital protection of citizen's privacy when confronted with the police. [ 21 ] A recent notable occurrence of the conflict between law enforcement and a citizen in terms of digital privacy has been in the 2018 case,  Carpenter v. United States  (585 U.S. ____). In this case, the FBI used cell phone records without a warrant to arrest Timothy Ivory Carpenter on multiple charges, and the Supreme Court ruled that the warrantless search of cell phone records violated the Fourth Amendment, citing that the Fourth Amendment protects \"reasonable expectations of privacy\" and that information sent to third parties still falls under data that can be included under \"reasonable expectations of privacy\". [ 22 ] Beyond law enforcement, many interactions between the government and citizens have been revealed either lawfully or unlawfully, specifically through whistleblowers. One notable example is  Edward Snowden , who released multiple operations related to the mass surveillance operations of the  National Security Agency  (NSA), where it was discovered that the NSA continues to breach the security of millions of people, mainly through mass surveillance programs whether it was collecting great amounts of data through third party private companies, hacking into other embassies or frameworks of international countries, and various breaches of data, which prompted a culture shock and stirred international debate related to digital privacy. [ 23 ] The Internet and technologies built on it enable new forms of social interactions at increasingly faster speeds and larger scales. Because the computer networks which underlie the Internet introduce such a wide range of novel security concerns, the discussion of  privacy  on the Internet is often conflated with  security . [ 24 ]  Indeed, many entities such as corporations involved in the  surveillance economy  inculcate a security-focused conceptualization of privacy which reduces their obligations to uphold privacy into a matter of  regulatory compliance , [ 25 ]  while at the same time  lobbying  to minimize those regulatory requirements. [ 26 ] The Internet's effect on privacy includes all of the ways that computational technology and the entities that control it can subvert the privacy expectations of their  users . [ 27 ] [ 28 ]  In particular, the  right to be forgotten  is motivated by both the  computational ability  to store and search through massive amounts of data as well as the  subverted expectations  of users who share information online without expecting it to be stored and retained indefinitely. Phenomena such as  revenge porn  and  deepfakes  are not merely individual because they require both the ability to obtain images without someone's consent as well as the social and economic infrastructure to disseminate that content widely. [ 29 ]  Therefore, privacy advocacy groups such as the  Cyber Civil Rights Initiative  and the  Electronic Frontier Foundation  argue that addressing the new privacy harms introduced by the Internet requires both technological improvements to  encryption  and  anonymity  as well as societal efforts such as  legal regulations  to restrict corporate and government power. [ 30 ] [ 31 ] While the  Internet  began as a government and academic effort up through the 1980s, private corporations began to enclose the hardware and software of the Internet in the 1990s, and now most Internet infrastructure is owned and managed by for-profit corporations. [ 32 ]  As a result, the ability of governments to protect their citizens' privacy is largely restricted to  industrial policy , instituting controls on corporations that handle communications or  personal data . [ 33 ] [ 34 ]  Privacy regulations are often further constrained to only protect specific demographics such as children, [ 35 ]  or specific industries such as credit card bureaus. [ 36 ] Several online social network sites (OSNs) are among the top 10 most visited websites globally. Facebook for example, as of August 2015, was the largest social-networking site, with nearly 2.7 billion [ 37 ]  members, who upload over 4.75 billion pieces of content daily. While  Twitter  is significantly smaller with 316 million registered users, the US  Library of Congress  recently announced that it will be acquiring and permanently storing the entire archive of public Twitter posts since 2006. [ 27 ] A review and evaluation of scholarly work regarding the current state of the value of individuals' privacy of online social networking show the following results: \"first, adults seem to be more concerned about potential privacy threats than younger users; second, policy makers should be alarmed by a large part of users who underestimate risks of their information privacy on OSNs; third, in the case of using OSNs and its services, traditional one-dimensional privacy approaches fall short\". [ 38 ]  This is exacerbated by  deanonymization  research indicating that personal traits such as sexual orientation, race, religious and political views, personality, or intelligence can be inferred based on a wide variety of  digital footprints , such as samples of text, browsing logs, or Facebook Likes. [ 39 ] Intrusions of social media privacy are known to affect employment in the United States.  Microsoft  reports that 75 percent of U.S. recruiters and human-resource professionals now do online research about candidates, often using information provided by search engines, social-networking sites, photo/video-sharing sites, personal web sites and blogs, and  Twitter . They also report that 70 percent of U.S. recruiters have rejected candidates based on internet information. This has created a need by many candidates to control various online  privacy settings  in addition to controlling their online reputations, the conjunction of which has led to legal suits against both social media sites and US employers. [ 27 ] Selfies  are popular today. A search for photos with the hashtag #selfie retrieves over 23 million results on Instagram and 51 million with the hashtag #me. [ 40 ]  However, due to modern corporate and governmental surveillance, this may pose a risk to privacy. [ 41 ]  In a research study which takes a sample size of 3763, researchers found that for users posting selfies on social media, women generally have greater concerns over privacy than men, and that users' privacy concerns inversely predict their selfie behavior and activity. [ 42 ] An invasion of someone's privacy may be widely and quickly disseminated over the Internet. When social media sites and other online communities fail to invest in  content moderation , an invasion of privacy can expose people to a much greater volume and degree of harassment than would otherwise be possible.  Revenge porn  may lead to  misogynist  or  homophobic  harassment, such as in the  suicide of Amanda Todd  and the  suicide of Tyler Clementi . When someone's physical location or other sensitive information is leaked over the Internet via  doxxing , harassment may escalate to direct physical harm such as  stalking  or  swatting . Despite the way breaches of privacy can magnify online harassment, online harassment is often used as a justification to curtail  freedom of speech , by removing the expectation of privacy via  anonymity , or by enabling law enforcement to invade privacy without a  search warrant . In the wake of Amanda Todd's death, the Canadian parliament proposed a motion purporting to stop bullying, but Todd's mother herself gave testimony to parliament rejecting the bill due to its provisions for warrantless breaches of privacy, stating \"I don't want to see our children victimized again by losing privacy rights.\" [ 43 ] [ 44 ] [ 45 ] Even where these laws have been passed despite privacy concerns, they have not demonstrated a reduction in online harassment. When the  Korea Communications Commission  introduced a registration system for online commenters in 2007, they reported that malicious comments only decreased by 0.9%, and in 2011 it was repealed. [ 46 ]  A subsequent analysis found that the set of users who posted the most comments actually increased the number of \"aggressive expressions\" when forced to use their real name. [ 47 ] In the US, while federal law only prohibits online harassment based on protected characteristics such as gender and race, [ 48 ]  individual states have expanded the definition of harassment to further curtail speech: Florida's definition of online harassment includes \"any use of data or computer software\" that \"Has the effect of substantially disrupting the orderly operation of a school.\" [ 49 ] Increasingly, mobile devices facilitate  location tracking .  This creates user privacy problems.  A user's location and preferences constitute  personal information , and their improper use violates that user's privacy.  A recent MIT study by de Montjoye et al. showed that four spatio-temporal points constituting approximate places and times are enough to uniquely identify 95% of 1.5M people in a mobility database. The study further shows that these constraints hold even when the resolution of the dataset is low. Therefore, even coarse or blurred datasets confer little privacy protection. [ 50 ] Several methods to protect user privacy in location-based services have been proposed, including the use of anonymizing servers and blurring of information. Methods to quantify privacy have also been proposed, to calculate the equilibrium between the benefit of obtaining accurate location information and the risks of breaching an individual's privacy. [ 51 ] There have been scandals regarding location privacy. One instance was the scandal concerning  AccuWeather , where it was revealed that AccuWeather was selling locational data. This consisted of a user's locational data, even if they opted out within Accuweather, which tracked users' location. Accuweather sold this data to Reveal Mobile, a company that monetizes data related to a user's location. [ 52 ]   Other international cases are similar to the Accuweather case. In 2017, a leaky API inside the McDelivery App exposed private data, which consisted of home addresses, of 2.2 million users. [ 53 ] In the wake of these types of scandals, many large American technology companies such as Google, Apple, and Facebook have been subjected to hearings and pressure under the U.S. legislative system. In 2011, US Senator  Al Franken  wrote an open letter to  Steve Jobs , noting the ability of  iPhones  and  iPads  to record and store users' locations in unencrypted files. [ 54 ] [ 55 ]  Apple claimed this was an unintentional  software bug , but Justin Brookman of the  Center for Democracy and Technology  directly challenged that portrayal, stating \"I'm glad that they are fixing what they call bugs, but I take exception with their strong denial that they track users.\" [ 56 ]  In 2021, the U.S. state of Arizona found in a court case that Google misled its users and stored the location of users regardless of their location settings. [ 57 ] The Internet has become a significant medium for advertising, with digital marketing making up approximately half of the global ad spending in 2019. [ 58 ]  While websites are still able to sell advertising space without tracking, including via  contextual advertising , digital ad brokers such as  Facebook  and  Google  have instead encouraged the practice of  behavioral advertising , providing code snippets used by website owners to track their users via  HTTP cookies . This tracking data is also sold to other third parties as part of the  mass surveillance industry . Since the introduction of mobile phones, data brokers have also been planted within apps, resulting in a $350 billion digital industry especially focused on mobile devices. [ 59 ] Digital privacy has become the main source of concern for many mobile users, especially with the rise of privacy scandals such as the  Facebook–Cambridge Analytica data scandal . [ 59 ]   Apple  has received  some reactions for features that prohibit advertisers from tracking a user's data without their consent. [ 60 ]  Google attempted to introduce an alternative to cookies named  FLoC  which it claimed reduced the privacy harms, but it later retracted the proposal due to antitrust probes and analyses that contradicted their claims of privacy. [ 61 ] [ 62 ] [ 63 ] The ability to do online inquiries about individuals has expanded dramatically over the last decade. Importantly, directly observed behavior, such as browsing logs, search queries, or contents of a public Facebook profile, can be automatically processed to infer secondary information about an individual, such as sexual orientation, political and religious views, race, substance use, intelligence, and personality. [ 64 ] In Australia, the  Telecommunications (Interception and Access) Amendment (Data Retention) Act 2015  made a distinction between collecting the contents of messages sent between users and the metadata surrounding those messages. Most countries give citizens rights to privacy in their constitutions. [ 17 ]  Representative examples of this include the  Constitution of Brazil , which says \"the privacy, private life, honor and image of people are inviolable\"; the  Constitution of South Africa  says that \"everyone has a right to privacy\"; and the  Constitution of the Republic of Korea  says \"the privacy of no citizen shall be infringed.\" [ 17 ]  The  Italian Constitution  also defines the right to privacy. [ 65 ]  Among most countries whose constitutions do not explicitly describe privacy rights, court decisions have interpreted their constitutions to intend to give privacy rights. [ 17 ] Many countries have broad privacy laws outside their constitutions, including Australia's  Privacy Act 1988 , Argentina's Law for the Protection of Personal Data of 2000, Canada's 2000  Personal Information Protection and Electronic Documents Act , and Japan's 2003 Personal Information Protection Law. [ 17 ] Beyond national privacy laws, there are international privacy agreements. [ 66 ]  The United Nations  Universal Declaration of Human Rights  says \"No one shall be subjected to arbitrary interference with [their] privacy, family, home or correspondence, nor to attacks upon [their] honor and reputation.\" [ 17 ]  The  Organisation for Economic Co-operation and Development  published its Privacy Guidelines in 1980. The European Union's 1995 Data Protection Directive guides privacy protection in Europe. [ 17 ]  The 2004 Privacy Framework by the  Asia-Pacific Economic Cooperation  is a privacy protection agreement for the members of that organization. [ 17 ] Approaches to privacy can, broadly, be divided into two categories: free market or  consumer protection . [ 67 ] One example of the free market approach is to be found in the voluntary OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data. [ 68 ]  The principles reflected in the guidelines, free of legislative interference, are analyzed in an article putting them into perspective with concepts of the  GDPR  put into law later in the European Union. [ 69 ] In a consumer protection approach, in contrast, it is claimed that individuals may not have the time or knowledge to make informed choices, or may not have reasonable alternatives available. In support of this view, Jensen and Potts showed that most  privacy policies  are above the reading level of the average person. [ 70 ] The  Privacy Act 1988  is administered by the Office of the Australian Information Commissioner. The initial introduction of privacy law in 1998 extended to the public sector, specifically to Federal government departments, under the Information Privacy Principles. State government agencies can also be subject to state based privacy legislation. This built upon the already existing privacy requirements that applied to telecommunications providers (under Part 13 of the  Telecommunications Act 1997 ), and confidentiality requirements that already applied to banking, legal and patient / doctor relationships. [ 71 ] In 2008 the Australian Law Reform Commission (ALRC) conducted a review of Australian privacy law and produced a report titled \"For Your Information\". [ 72 ]  Recommendations were taken up and implemented by the Australian Government via the Privacy Amendment (Enhancing Privacy Protection) Bill 2012. [ 73 ] In 2015, the  Telecommunications (Interception and Access) Amendment (Data Retention) Act 2015  was passed, to some  controversy over its human rights implications  and the role of media. Canada is a federal state whose provinces and territories abide by the  common law  save the province of Quebec whose legal tradition is the  civil law . Privacy in Canada was first addressed through the  Privacy Act , [ 74 ]  a 1985 piece of legislation applicable to personal information held by government institutions. The provinces and territories would later follow suit with their own legislation. Generally, the purposes of said legislation are to provide individuals rights to access personal information; to have inaccurate personal information corrected; and to prevent unauthorized collection, use, and disclosure of personal information. [ 75 ]  In terms of regulating personal information in the private sector, the federal  Personal Information Protection and Electronic Documents Act   [ 76 ]  (\"PIPEDA\") is enforceable in all jurisdictions unless a substantially similar provision has been enacted on the provincial level. [ 77 ]  However, inter-provincial or international information transfers still engage PIPEDA. [ 77 ]  PIPEDA has gone through two law overhaul efforts in 2021 and 2023 with the involvement of the Office of the Privacy Commissioner and Canadian academics. [ 78 ]  In the absence of a statutory private right of action absent an OPC investigation, the common law torts of intrusion upon seclusion and public disclosure of private facts, as well as the Civil Code of Quebec may be brought for an infringement or violation of privacy. [ 79 ] [ 80 ]  Privacy is also protected under ss. 7 and 8 of the  Canadian Charter of Rights and Freedoms [ 81 ]  which is typically applied in the criminal law context. [ 82 ]  In Quebec, individuals' privacy is safeguarded by articles 3 and 35 to 41 of the  Civil Code of Quebec [ 83 ]  as well as by s. 5 of the  Charter of human rights and freedoms . [ 84 ] In 2016, the European Union passed the General Data Protection Regulation (GDPR), which was intended to reduce the misuse of personal data and enhance individual privacy, by requiring companies to receive consent before acquiring personal information from users. [ 85 ] Although there are comprehensive regulations for data protection in the European Union, one study finds that despite the laws, there is a lack of enforcement in that no institution feels responsible to control the parties involved and enforce their laws. [ 86 ]  The European Union also champions the  Right to be Forgotten  concept in support of its adoption by other countries. [ 87 ] Since the introduction of the  Aadhaar  project in 2009, which resulted in all 1.2 billion Indians being associated with a 12-digit biometric-secured number. Aadhaar has uplifted the poor in India [ how? ] [ promotion? ]  by providing them with a form of  identity and preventing the fraud and waste of resources, as normally the government would not be able to allocate its resources to its intended assignees due to the ID issues. [ citation needed ]  With the rise of Aadhaar, India has debated whether Aadhaar violates an individual's privacy and whether any organization should have access to an individual's digital profile, as the Aadhaar card became associated with other economic sectors, allowing for the tracking of individuals by both public and private bodies. [ 88 ]  Aadhaar databases have suffered from security attacks as well and the project was also met with mistrust regarding the safety of the social protection infrastructures. [ 89 ]   In 2017, where the Aadhar was challenged, the Indian Supreme Court declared privacy as a human right, but postponed the decision regarding the constitutionality of Aadhaar for another bench. [ 90 ]  In September 2018, the Indian Supreme Court determined that the Aadhaar project did not violate the legal right to privacy. [ 91 ] In the United Kingdom, it is not possible to bring an action for invasion of privacy. An action may be brought under another  tort  (usually breach of confidence) and privacy must then be considered under EC law. In the UK, it is sometimes a defence that disclosure of private information was in the public interest. [ 92 ]  There is, however, the  Information Commissioner's Office  (ICO), an independent public body set up to promote access to official information and protect personal information. They do this by promoting good practice, ruling on eligible complaints, giving information to individuals and organisations, and taking action when the law is broken. The relevant UK laws include:  Data Protection Act 1998 ;  Freedom of Information Act 2000 ;  Environmental Information Regulations 2004 ;  Privacy and Electronic Communications Regulations 2003 . The ICO has also provided a \"Personal Information Toolkit\" online which explains in more detail the various ways of protecting privacy online. [ 93 ] In the United States, more systematic treatises of privacy did not appear until the 1890s, with the development of  privacy law in America . [ 94 ]  Although the  US Constitution  does not explicitly include the  right to privacy , individual as well as  locational privacy  may be implicitly granted by the Constitution under the  4th Amendment . [ 95 ]  The  Supreme Court of the United States  has found that other guarantees have  penumbras  that implicitly grant a right to privacy against government intrusion, for example in  Griswold v. Connecticut  and  Roe v. Wade .   Dobbs v. Jackson Women's Health Organization  later overruled  Roe v. Wade , with Supreme Court Justice  Clarence Thomas  characterizing  Griswold 's penumbral argument as having a \"facial absurdity\", [ 96 ]  casting doubt on the validity of a constitutional right to privacy in the United States and of previous decisions relying on it. [ 97 ]  In the United States, the right of  freedom of speech  granted in the  First Amendment  has limited the effects of lawsuits for breach of privacy. Privacy is regulated in the US by the  Privacy Act of 1974 , and various state laws. The Privacy Act of 1974 only applies to federal agencies in the executive branch of the federal government. [ 98 ]  Certain privacy rights have been established in the United States via legislation such as the  Children's Online Privacy Protection Act  (COPPA), [ 99 ]  the  Gramm–Leach–Bliley Act  (GLB), and the  Health Insurance Portability and Accountability Act  (HIPAA).\n [ 100 ] Unlike the EU and most EU-member states, the US does not recognize the right to privacy of non-US citizens. The UN's Special Rapporteur on the right to privacy, Joseph A. Cannataci, criticized this distinction. [ 101 ] The theory of  contextual integrity , [ 102 ]  developed by  Helen Nissenbaum , defines privacy as an appropriate information flow, where appropriateness, in turn, is defined as conformance with legitimate, informational norms specific to social contexts. In 1890, the United States  jurists  Samuel D. Warren and Louis Brandeis wrote \"The Right to Privacy\", an article in which they argued for the \"right to be let alone\", using that phrase as a definition of privacy. [ 103 ]  This concept relies on the theory of  natural rights  and focuses on protecting individuals. The citation was a response to recent technological developments, such as photography, and sensationalist journalism, also known as  yellow journalism . [ 104 ] There is extensive commentary over the meaning of being \"let alone\", and among other ways, it has been interpreted to mean the right of a person to choose  seclusion  from the attention of others if they wish to do so, and the right to be immune from scrutiny or being observed in private settings, such as one's own home. [ 103 ]  Although this early vague legal concept did not describe privacy in a way that made it easy to design broad legal protections of privacy, it strengthened the notion of privacy rights for individuals and began a legacy of discussion on those rights in the US. [ 103 ] Limited access refers to a person's ability to participate in society without having other individuals and organizations collect information about them. [ 105 ] Various theorists have imagined privacy as a system for limiting access to one's personal information. [ 105 ]   Edwin Lawrence Godkin  wrote in the late 19th century that \"nothing is better worthy of legal protection than private life, or, in other words, the right of every man to keep his affairs to himself, and to decide for himself to what extent they shall be the subject of public observation and discussion.\" [ 105 ] [ 106 ]  Adopting an approach similar to the one presented by Ruth Gavison [ 107 ]  Nine years earlier, [ 108 ]   Sissela Bok  said that privacy is \"the condition of being protected from unwanted access by others—either physical access, personal information, or attention.\" [ 105 ] [ 109 ] Control over one's personal information is the concept that \"privacy is the claim of individuals, groups, or institutions to determine for themselves when, how, and to what extent information about them is communicated to others.\" Generally, a person who has  consensually formed an interpersonal relationship  with another person is not considered \"protected\" by privacy rights with respect to the person they are in the relationship with. [ 110 ] [ 111 ]   Charles Fried  said that \"Privacy is not simply an absence of information about us in the minds of others; rather it is the control we have over information about ourselves. Nevertheless, in the era of  big data , control over information is under pressure. [ 112 ] [ 113 ] [ This quote needs a citation ] [ check quotation syntax ] Alan Westin defined four states—or experiences—of privacy: solitude, intimacy, anonymity, and reserve.  Solitude  is a physical separation from others; [ 114 ]  Intimacy is a \"close, relaxed; and frank relationship between two or more individuals\" that results from the seclusion of a pair or small group of individuals. [ 114 ]  Anonymity is the \"desire of individuals for times of 'public privacy.'\" [ 114 ]  Lastly, reserve is the \"creation of a psychological barrier against unwanted intrusion\"; this creation of a psychological barrier requires others to respect an individual's need or desire to restrict communication of information concerning themself. [ 114 ] In addition to the psychological barrier of reserve, Kirsty Hughes identified three more kinds of privacy barriers: physical, behavioral, and normative. Physical barriers, such as walls and doors, prevent others from accessing and experiencing the individual. [ 115 ]  (In this sense, \"accessing\" an individual includes accessing personal information about them.) [ 115 ]  Behavioral barriers communicate to others—verbally, through language, or non-verbally, through personal space, body language, or clothing—that an individual does not want the other person to access or experience them. [ 115 ]  Lastly, normative barriers, such as laws and social norms, restrain others from attempting to access or experience an individual. [ 115 ] Psychologist  Carl A. Johnson has identified the psychological concept of “personal control” as closely tied to privacy. His concept was developed as a process containing four stages and two behavioural outcome relationships, with one’s outcomes depending on situational as well as personal factors. [ 116 ]  Privacy is described as “behaviors falling at specific locations on these two dimensions”. [ 117 ] Johnson examined the following four stages to categorize where people exercise personal control: outcome choice control is the selection between various outcomes. Behaviour selection control is the selection between behavioural strategies to apply to attain selected outcomes. Outcome effectance describes the fulfillment of selected behaviour to achieve chosen outcomes. Outcome realization control is the personal interpretation of one’s achieved outcome. The relationship between two factors– primary and secondary control, is defined as the two-dimensional phenomenon where one reaches personal control: primary control describes behaviour directly causing outcomes, while secondary control is behaviour indirectly causing outcomes. [ 118 ]  Johnson explores the concept that privacy is a behaviour that has secondary control over outcomes. Lorenzo Magnani  expands on this concept by highlighting how privacy is essential in maintaining personal control over one's identity and consciousness. [ 119 ]  He argues that consciousness is partly formed by external representations of ourselves, such as narratives and data, which are stored outside the body. However, much of our consciousness consists of internal representations that remain private and are rarely externalized. This internal privacy, which Magnani refers to as a form of \"information property\" or \"moral capital,\" is crucial for preserving free choice and personal agency. According to Magnani, [ 120 ]  when too much of our identity and data is externalized and subjected to scrutiny, it can lead to a loss of personal control, dignity, and responsibility. The protection of privacy, therefore, safeguards our ability to develop and pursue personal projects in our own way, free from intrusive external forces. Acknowledging other conceptions of privacy while arguing that the fundamental concern of privacy is behavior selection control, Johnson converses with other interpretations including those of Maxine Wolfe and Robert S. Laufer, and Irwin Altman. He clarifies the continuous relationship between privacy and personal control, where outlined behaviours not only depend on privacy, but the conception of one’s privacy also depends on his defined behavioural outcome relationships. [ 121 ] Privacy is sometimes defined as an option to have secrecy. Richard Posner said that privacy is the right of people to \"conceal information about themselves that others might use to their disadvantage\". [ 122 ] [ 123 ] In various legal contexts, when privacy is described as secrecy, a conclusion is reached: if privacy is secrecy, then rights to privacy do not apply for any information which is already publicly disclosed. [ 124 ]  When privacy-as-secrecy is discussed, it is usually imagined to be a selective kind of secrecy in which individuals keep some information secret and private while they choose to make other information public and not private. [ 124 ] Privacy may be understood as a necessary precondition for the development and preservation of personhood. Jeffrey Reiman defined privacy in terms of a recognition of one's ownership of their physical and mental reality and a moral right to  self-determination . [ 125 ]  Through the \"social ritual\" of privacy, or the social practice of respecting an individual's privacy barriers, the social group communicates to developing children that they have exclusive moral rights to their bodies—in other words, moral ownership of their body. [ 125 ]  This entails control over both active (physical) and cognitive appropriation, the former being control over one's movements and actions and the latter being control over who can experience one's physical existence and when. [ 125 ] Alternatively, Stanley Benn defined privacy in terms of a recognition of oneself as a subject with agency—as an individual with the capacity to choose. [ 126 ]  Privacy is required to exercise choice. [ 126 ]  Overt observation makes the individual aware of himself or herself as an object with a \"determinate character\" and \"limited probabilities.\" [ 126 ]  Covert observation, on the other hand, changes the conditions in which the individual is exercising choice without his or her knowledge and consent. [ 126 ] In addition, privacy may be viewed as a state that enables autonomy, a concept closely connected to that of personhood. According to Joseph Kufer, an autonomous self-concept entails a conception of oneself as a \"purposeful, self-determining, responsible agent\" and an awareness of one's capacity to control the boundary between self and other—that is, to control who can access and experience him or her and to what extent. [ 127 ]  Furthermore, others must acknowledge and respect the self's boundaries—in other words, they must respect the individual's privacy. [ 127 ] The studies of psychologists such as Jean Piaget and Victor Tausk show that, as children learn that they can control who can access and experience them and to what extent, they develop an autonomous self-concept. [ 127 ]  In addition, studies of adults in particular institutions, such as Erving Goffman's study of \"total institutions\" such as prisons and mental institutions, [ 128 ]  suggest that systemic and routinized deprivations or violations of privacy deteriorate one's sense of autonomy over time. [ 127 ] Privacy may be understood as a prerequisite for the development of a sense of self-identity. Privacy barriers, in particular, are instrumental in this process. According to Irwin Altman, such barriers \"define and limit the boundaries of the self\" and thus \"serve to help define [the self].\" [ 129 ]  This control primarily entails the ability to regulate contact with others. [ 129 ]  Control over the \"permeability\" of the self's boundaries enables one to control what constitutes the self and thus to define what is the self. [ 129 ] In addition, privacy may be seen as a state that fosters personal growth, a process integral to the development of self-identity. Hyman Gross suggested that, without privacy—solitude, anonymity, and temporary releases from social roles—individuals would be unable to freely express themselves and to engage in self-discovery and  self-criticism . [ 127 ]  Such self-discovery and self-criticism contributes to one's understanding of oneself and shapes one's sense of identity. [ 127 ] In a way analogous to how the personhood theory imagines privacy as some essential part of being an individual, the intimacy theory imagines privacy to be an essential part of the way that humans have strengthened or  intimate relationships  with other humans. [ 130 ]  Because part of  human relationships  includes individuals volunteering to self-disclose most if not all personal information, this is one area in which privacy does not apply. [ 130 ] James Rachels  advanced this notion by writing that privacy matters because \"there is a close connection between our ability to control who has access to us and to information about us, and our ability to create and maintain different sorts of social relationships with different people.\" [ 130 ] [ 131 ]  Protecting intimacy is at the core of the concept of sexual privacy, which law professor  Danielle Citron  argues should be protected as a unique form of privacy. [ 132 ] Physical privacy could be defined as preventing \"intrusions into one's physical space or solitude.\" [ 133 ]  An example of the legal basis for the right to physical privacy is the U.S.  Fourth Amendment , which guarantees \"the right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures\". [ 134 ] Physical privacy may be a matter of cultural sensitivity, personal dignity, and/or shyness. There may also be concerns about safety, if, for example one is wary of becoming the victim of crime or  stalking . [ 135 ]  There are different things that can be prevented to protect one's physical privacy, including people watching (even through recorded images) one's  intimate behaviours  or  intimate parts  and unauthorized access to one's personal possessions or places. Examples of possible efforts used to avoid the former, especially for  modesty  reasons, are  clothes ,  walls ,  fences , privacy screens,  cathedral glass ,  window coverings , etc. Government agencies, corporations, groups/societies and other organizations may desire to keep their activities or secrets from being revealed to other organizations or individuals, adopting various  security  practices and controls in order to keep private information confidential. Organizations may seek legal protection for their secrets. For example, a government administration may be able to invoke  executive privilege [ 136 ]  or declare certain information to be  classified , or a corporation might attempt to protect valuable proprietary information as  trade secrets . [ 134 ] Privacy self-synchronization is a hypothesized mode by which the stakeholders of an enterprise privacy program spontaneously contribute collaboratively to the program's maximum success. The stakeholders may be customers, employees, managers, executives, suppliers, partners or investors. When self-synchronization is reached, the model states that the personal interests of individuals toward their privacy is in balance with the business interests of enterprises who collect and use the personal information of those individuals. [ 137 ] David Flaherty  believes networked computer databases pose threats to privacy. He develops 'data protection' as an aspect of privacy, which involves \"the collection, use, and dissemination of personal information\". This concept forms the foundation for fair information practices used by governments globally. Flaherty forwards an idea of privacy as information control, \"[i]ndividuals want to be left alone and to exercise some control over how information about them is used\". [ 138 ] Richard Posner  and Lawrence Lessig focus on the economic aspects of personal information control. Posner criticizes privacy for concealing information, which reduces market efficiency. For Posner, employment is selling oneself in the labour market, which he believes is like selling a product. Any 'defect' in the 'product' that is not reported is fraud. [ 139 ]  For Lessig, privacy breaches online can be regulated through code and law. Lessig claims \"the protection of privacy would be stronger if people conceived of the right as a property right\", [ 140 ]  and that \"individuals should be able to control information about themselves\". [ 141 ] There have been attempts to establish privacy as one of the fundamental  human rights , whose social value is an essential component in the functioning of democratic societies. [ 142 ] Priscilla Regan believes that individual concepts of privacy have failed philosophically and in policy. She supports a social value of privacy with three dimensions: shared perceptions, public values, and  collective  components. Shared ideas about privacy allows freedom of conscience and diversity in thought. Public values guarantee democratic participation, including freedoms of speech and association, and limits government power. Collective elements describe privacy as collective good that cannot be divided. Regan's goal is to strengthen privacy claims in policy making: \"if we did recognize the collective or public-good value of privacy, as well as the common and public value of privacy, those advocating privacy protections would have a stronger basis upon which to argue for its protection\". [ 143 ] Leslie Regan Shade argues that the human right to privacy is necessary for meaningful democratic participation, and ensures human dignity and autonomy. Privacy depends on norms for how information is distributed, and if this is appropriate. Violations of privacy depend on context. The human right to privacy has precedent in the  United Nations Declaration of Human Rights : \"Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.\" [ 144 ]  Shade believes that privacy must be approached from a people-centered perspective, and not through the marketplace. [ 145 ] Dr. Eliza Watt, Westminster Law School, University of Westminster in London, UK, proposes application of the International Human Right Law (IHRL) concept of “virtual control” as an approach to deal with extraterritorial mass surveillance by state intelligence agencies. Dr. Watt envisions the “virtual control” test, understood as a remote control over the individual's right to privacy of communications, where privacy is recognized under the ICCPR, Article 17. This, she contends, may help to close the normative gap that is being exploited by nation states. [ 146 ] The  privacy paradox  is a phenomenon in which online users state that they are concerned about their privacy but behave as if they were not. [ 147 ]  While this term was coined as early as 1998, [ 148 ]  it was not used in its current popular sense until the year 2000. [ 149 ] [ 147 ] Susan B. Barnes similarly used the term  privacy paradox  to refer to the ambiguous boundary between private and public space on social media. [ 150 ]  When compared to adults, young people tend to disclose more information on  social media . However, this does not mean that they are not concerned about their privacy. Susan B. Barnes gave a case in her article: in a television interview about Facebook, a student addressed her concerns about disclosing personal information online. However, when the reporter asked to see her Facebook page, she put her home address, phone numbers, and pictures of her young son on the page. The privacy paradox has been studied and scripted in different research settings. Several studies have shown this inconsistency between privacy attitudes and behavior among online users. [ 151 ]  However, by now an increasing number of studies have also shown that there are significant and at times large correlations between privacy concerns and information sharing behavior, [ 152 ]  which speaks against the privacy paradox. A meta-analysis of 166 studies published on the topic reported an overall small but significant relation between privacy concerns and informations sharing or use of privacy protection measures. [ 153 ]  So although there are several individual instances or anecdotes where behavior appear paradoxical, on average privacy concerns and privacy behaviors seem to be related, and several findings question the general existence of the privacy paradox. [ 154 ] However, the relationship between concerns and behavior is likely only small, and there are several arguments that can explain why that is the case. According to the  attitude-behavior gap , attitudes and behaviors are  in general  and in most cases not closely related. [ 155 ]  A main explanation for the partial mismatch in the context of privacy specifically is that users lack awareness of the risks and the degree of protection. [ 156 ]  Users may underestimate the harm of disclosing information online. [ 157 ]  On the other hand, some researchers argue that the mismatch comes from lack of technology literacy and from the design of sites. [ 158 ]   For example, users may not know how to change their  default settings  even though they care about their privacy. Psychologists Sonja Utz and Nicole C. Krämer particularly pointed out that the privacy paradox can occur when users must trade-off between their privacy concerns and impression management. [ 159 ] A study conducted by Susanne Barth and Menno D.T. de Jo demonstrates that decision making takes place on an irrational level, especially when it comes to mobile computing. Mobile applications in particular are often built up in such a way that spurs decision making that is fast and automatic without assessing risk factors. Protection measures against these unconscious mechanisms are often difficult to access while downloading and installing apps. Even with mechanisms in place to protect user privacy, users may not have the knowledge or experience to enable these mechanisms. [ 160 ] Users of mobile applications generally have very little knowledge of how their personal data are used. When they decide which application to download, they typically are not able to effectively interpret the information provided by application vendors regarding the collection and use of personal data. [ 161 ]  Other research finds that this lack of interpretability means users are much more likely to be swayed by cost, functionality, design, ratings, reviews and number of downloads than requested permissions for usage of their personal data. [ 162 ] The willingness to incur a privacy risk is suspected to be driven by a complex array of factors including risk attitudes, personal value for private information, and general attitudes to privacy (which are typically measured using surveys). [ 163 ]  One experiment aiming to determine the monetary value of several types of personal information indicated relatively low evaluations of personal information. [ 161 ]  Despite claims that ascertaining the value of data requires a \"stock-market for personal information\", [ 164 ]   surveillance capitalism  and the  mass surveillance industry  regularly place price tags on this form of data as it is shared between corporations and governments. Users are not always given the tools to live up to their professed privacy concerns, and they are sometimes willing to trade private information for convenience, functionality, or financial gain, even when the gains are very small. [ 165 ]  One study suggests that people think their browser history is worth the equivalent of a cheap meal. [ 166 ]  Another finds that attitudes to privacy risk do not appear to depend on whether it is already under threat or not. [ 163 ]  The methodology of  user empowerment  describes how to provide users with sufficient context to make privacy-informed decisions. It is suggested by  Andréa Belliger  and David J. Krieger that the privacy paradox should not be considered a paradox, but more of a  privacy dilemma , for services that cannot exist without the user sharing private data. [ 166 ]  However, the general public is typically not given the choice whether to share private data or not, [ 19 ] [ 57 ]  making it difficult to verify any claim that a service truly cannot exist without sharing private data. The privacy calculus model posits that two factors determine privacy behavior, namely privacy concerns (or perceived risks) and expected benefits. [ 167 ] [ 168 ]  By now, the privacy calculus has been supported by several studies. [ 169 ] [ 170 ] As with other  conceptions of privacy , there are various ways to discuss what kinds of processes or actions remove, challenge, lessen, or attack privacy. In 1960 legal scholar  William Prosser  created the following list of activities which can be remedied with privacy protection: [ 171 ] [ 172 ] From 2004 to 2008, building from this and other historical precedents, Daniel J. Solove presented another classification of actions which are harmful to privacy, including collection of information which is already somewhat public, processing of information, sharing information, and invading personal space to get private information. [ 173 ] In the context of harming privacy, information collection means gathering whatever information can be obtained by doing something to obtain it. [ 173 ]  Examples include surveillance and  interrogation . [ 173 ]  Another example is how consumers and marketers also collect information in the business context through facial recognition which has recently caused a concern for things such as privacy. There is currently research being done related to this topic. [ 174 ] Companies like Google and Meta collect vast amounts of personal data from their users through various services and platforms. This data includes browsing habits, search history, location information, and even personal communications. These companies then analyze and aggregate this data to create detailed user profiles, which are sold to advertisers and other third parties. This practice is often done without explicit user consent, leading to an invasion of privacy as individuals have little control over how their information is used. The sale of personal data can result in targeted advertising, manipulation, and even potential security risks, as sensitive information can be exploited by malicious actors. This commercial exploitation of personal data undermines user trust and raises significant ethical and legal concerns regarding data protection and privacy rights.  [ 175 ] It can happen that privacy is not harmed when information is available, but that the harm can come when that information is collected as a set, then processed together in such a way that the collective reporting of pieces of information encroaches on privacy. [ 176 ]  Actions in this category which can lessen privacy include the following: [ 176 ] Count not him among your friends who will retail your privacies to the world. Information dissemination is an attack on privacy when information which was shared in confidence is shared or threatened to be shared in a way that harms the subject of the information. [ 176 ] There are various examples of this. [ 176 ]  Breach of confidentiality is when one entity promises to keep a person's information private, then breaks that promise. [ 176 ]  Disclosure is making information about a person more accessible in a way that harms the subject of the information, regardless of how the information was collected or the intent of making it available. [ 176 ]  Exposure is a special type of disclosure in which the information disclosed is emotional to the subject or taboo to share, such as revealing their private life experiences, their nudity, or perhaps private body functions. [ 176 ]  Increased accessibility means advertising the availability of information without actually distributing it, as in the case of  doxing . [ 176 ]  Blackmail is making a threat to share information, perhaps as part of an effort to coerce someone. [ 176 ]  Appropriation is an attack on the  personhood  of someone, and can include using the value of someone's reputation or likeness to advance interests which are not those of the person being appropriated. [ 176 ]  Distortion is the creation of misleading information or lies about a person. [ 176 ] Invasion of privacy, a subset of  expectation of privacy , is a different concept from the collecting, aggregating, and disseminating information because those three are a misuse of available data, whereas invasion is an attack on the right of individuals to keep personal secrets. [ 176 ]  An invasion is an attack in which information, whether intended to be public or not, is captured in a way that insults the personal dignity and right to private space of the person whose data is taken. [ 176 ] An  intrusion  is any unwanted entry into a person's private personal space and solitude for any reason, regardless of whether data is taken during that breach of space. [ 176 ]   Decisional interference  is when an entity somehow injects itself into the personal decision-making process of another person, perhaps to influence that person's private decisions but in any case doing so in a way that disrupts the private personal thoughts that a person has. [ 176 ] Similarly to  actions which reduce privacy , there are multiple angles of privacy and multiple techniques to improve them to varying extents. When actions are done at an  organizational level , they may be referred to as  cybersecurity . Individuals can encrypt e-mails via enabling either two encryption protocols,  S/MIME , which is built into companies like Apple or  Outlook  and thus most common, or  PGP . [ 177 ]  The  Signal  messaging app, which encrypts messages so that only the recipient can read the message, is notable for being available on many mobile devices and implementing a form of  perfect forward secrecy . [ 178 ]  Signal has received praise from whistleblower  Edward Snowden . [ 179 ]  Encryption and other privacy-based security measures are also used in some cryptocurrencies such as  Monero  and  ZCash . [ 180 ] [ 181 ] Anonymizing proxies  or anonymizing networks like  I2P  and  Tor  can be used to prevent Internet service providers (ISP) from knowing which sites one visits and with whom one communicates, by hiding IP addresses and location, but does not necessarily protect a user from third party data mining. Anonymizing proxies are built into a user's device, in comparison to a  Virtual Private Network  (VPN), where users must download software. [ 182 ]  Using a VPN hides all data and connections that are exchanged between servers and a user's computer, resulting in the online data of the user being unshared and secure, providing a barrier between the user and their ISP, and is especially important to use when a user is connected to public Wi-Fi. However, users should understand that all their data does flow through the VPN's servers rather than the ISP. Users should decide for themselves if they wish to use either an anonymizing proxy or a VPN. In a more non-technical sense, using incognito mode or private browsing mode will prevent a user's computer from saving history, Internet files, and cookies, but the ISP will still have access to the users' search history. Using  anonymous search engines  will not share a user's history, clicks, and will obstruct ad blockers. [ 183 ] Concrete solutions on how to solve paradoxical behavior still do not exist. Many efforts are focused on processes of decision making, like restricting data access permissions during application installation, but this would not completely bridge the gap between user intention and behavior. Susanne Barth and Menno D.T. de Jong believe that for users to make more conscious decisions on privacy matters, the design needs to be more user-oriented. [ 160 ] In a social sense, simply limiting the amount of personal information that users posts on social media could increase their security, which in turn makes it harder for criminals to perform identity theft. [ 183 ]   Moreover, creating a set of complex passwords and using two-factor authentication can allow users to be less susceptible to their accounts being compromised when various data leaks occur. Furthermore, users should protect their digital privacy by using anti-virus software, which can block harmful viruses like a pop-up scanning for personal information on a users' computer. [ 184 ] Although there are laws that promote the protection of users, in some countries, like the U.S., there is no federal digital privacy law and privacy settings are essentially limited by the state of current enacted privacy laws. To further their privacy, users can start conversing with representatives, letting representatives know that privacy is a main concern, which in turn increases the likelihood of further privacy laws being enacted. [ 185 ] David Attenborough , a  biologist  and  natural historian , affirmed that  gorillas  \"value their privacy\" while discussing a brief escape by a gorilla in  London Zoo . [ 186 ] Lack of privacy in public spaces, caused by overcrowding, increases health issues in animals, including  heart disease  and  high blood pressure . Also, the stress from overcrowding is connected to an increase in infant mortality rates and maternal stress. The lack of privacy that comes with overcrowding is connected to other issues in animals, which causes their relationships with others to diminish. How they present themselves to others of their species is a necessity in their life, and overcrowding causes the relationships to become disordered. [ 187 ] For example, David Attenborough claims that the gorilla's right to privacy is being violated when they are looked at through glass enclosures. They are aware that they are being looked at, therefore they do not have control over how much the onlookers can see of them. Gorillas and other animals may be in the enclosures due to safety reasons, however Attenborough states that this is not an excuse for them to be constantly watched by unnecessary eyes. Also, animals will start hiding in unobserved spaces. [ 187 ]  Animals in zoos have been found to exhibit harmful or different behaviours due to the presence of visitors watching them: [ 188 ]"
  },
  {
    "id": 30,
    "title": "Quantum information science",
    "content": "Quantum Information Science  is a field that combines the principles of  quantum mechanics  with  information theory  to study the processing, analysis, and transmission of information. It covers both theoretical and experimental aspects of quantum physics, including the limits of what can be achieved with  quantum information . The term  quantum information theory  is sometimes used, but it does not include experimental research and can be confused with a subfield of quantum information science that deals with the processing of quantum information. Quantum teleportation ,  entanglement  and the manufacturing of  quantum computers  depend on a comprehensive understanding of quantum physics and engineering.  Google  and  IBM  have invested significantly in quantum computer hardware research, leading to significant progress in manufacturing quantum computers since the 2010s. Currently, it is possible to create a quantum computer with over 100  qubits , but the error rate is high due to the lack of suitable materials for quantum computer manufacturing. [ 1 ]   Majorana fermions  may be a crucial missing material. [ 2 ] Quantum cryptography  devices are now available for commercial use. The  one time pad , a cipher used by spies during the  Cold War , uses a sequence of random keys for encryption. These keys can be securely exchanged using quantum entangled particle pairs, as the principles of the  no-cloning theorem  and  wave function collapse  ensure the secure exchange of the random keys. The development of devices that can transmit quantum entangled particles is a significant scientific and engineering goal. [ citation needed ] Qiskit ,  Cirq  and  Q Sharp  are popular quantum programming languages. Additional  programming languages  for quantum computers are needed, as well as a larger community of competent quantum programmers. To this end, additional learning resources are needed, since there are many fundamental differences in quantum programming which limits the number of skills that can be carried over from traditional programming. [ citation needed ] Quantum algorithm  and  quantum complexity theory  are two of the subjects in  algorithms  and  computational complexity theory . In 1994, mathematician  Peter Shor  introduced a quantum algorithm for  prime factorization  that, with a quantum computer containing 4,000  logical qubits , could potentially break widely used ciphers like  RSA  and  ECC , posing a major security threat. This led to increased investment in  quantum computing  research and the development of  post-quantum cryptography  to prepare for the fault-tolerant quantum computing (FTQC) era. [ 3 ]"
  },
  {
    "id": 31,
    "title": "Computing",
    "content": "Computing  is any goal-oriented activity requiring, benefiting from, or creating  computing machinery . [ 1 ]  It includes the study and experimentation of  algorithmic  processes, and the development of both  hardware  and software. Computing has scientific, engineering, mathematical, technological, and social aspects. Major computing disciplines include  computer engineering ,  computer science ,  cybersecurity ,  data science ,  information systems ,  information technology , and  software engineering . [ 2 ] The term  computing  is also  synonymous  with  counting  and  calculating . In earlier times, it was used in reference to the action performed by  mechanical computing machines , and before that, to  human computers . [ 3 ] The history of computing is longer than the  history of computing hardware  and includes the history of methods intended for pen and paper (or for chalk and slate) with or without the aid of tables. Computing is intimately tied to the representation of numbers, though mathematical concepts necessary for computing existed before  numeral systems . The earliest known tool for use in computation is the  abacus , and it is thought to have been invented in  Babylon  circa between 2700 and 2300 BC. Abaci, of a more modern design, are still used as calculation tools today. The first recorded proposal for using digital electronics in computing was the 1931 paper \"The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena\" by  C. E. Wynn-Williams . [ 4 ]   Claude Shannon 's 1938 paper \" A Symbolic Analysis of Relay and Switching Circuits \" then introduced the idea of using electronics for  Boolean algebraic  operations. The concept of a  field-effect transistor  was proposed by  Julius Edgar Lilienfeld  in 1925.  John Bardeen  and  Walter Brattain , while working under  William Shockley  at  Bell Labs , built the first working  transistor , the  point-contact transistor , in 1947. [ 5 ] [ 6 ]  In 1953, the  University of Manchester  built the first  transistorized computer , the  Manchester Baby . [ 7 ]  However, early  junction transistors  were relatively bulky devices that were difficult to mass-produce, which limited them to a number of specialised applications. [ 8 ] In 1957, Frosch and Derick were able to manufacture the first silicon dioxide field effect transistors at Bell Labs, the first transistors in which drain and source were adjacent at the surface. [ 9 ]  Subsequently, a team demonstrated a working  MOSFET  at Bell Labs 1960. [ 10 ] [ 11 ]  The MOSFET made it possible to build  high-density integrated circuits , [ 12 ] [ 13 ]  leading to what is known as the  computer revolution [ 14 ]  or  microcomputer revolution . [ 15 ] A computer is a machine that manipulates  data  according to a set of instructions called a  computer program . [ 16 ]  The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable  source code  form, enables a programmer to study and develop a sequence of steps known as an  algorithm . [ 17 ]  Because the instructions can be carried out in different types of computers, a single set of source instructions converts to  machine instructions  according to the  CPU  type. [ 18 ] The execution  process  carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the  semantics  of the instructions. Computer hardware includes the physical parts of a computer, including the  central processing unit ,  memory , and  input/output . [ 19 ]   Computational logic  and  computer architecture  are key topics in the field of computer hardware. [ 20 ] [ 21 ] Computer software, or just  software , is a collection of computer programs and related data, which provides instructions to a computer. Software refers to one or more computer programs and data held in the storage of the computer. It is a set of  programs, procedures, algorithms,  as well as its  documentation  concerned with the operation of a data processing system. [ citation needed ]  Program software performs the  function  of the  program  it implements, either by directly providing  instructions  to the computer hardware or by serving as input to another piece of software. The  term  was coined to contrast with the old term  hardware  (meaning physical devices). In contrast to hardware, software is intangible. [ 22 ] Software is also sometimes used in a more narrow sense, meaning application software only. System software, or systems software, is computer software designed to operate and control computer hardware, and to provide a platform for running application software. System software includes  operating systems ,  utility software ,  device drivers ,  window systems , and  firmware . Frequently used development tools such as  compilers ,  linkers , and  debuggers  are classified as system software. [ 23 ]   System software  and  middleware  manage and integrate a computer's capabilities, but typically do not directly apply them in the performance of tasks that benefit the user, unlike application software. Application software, also known as an  application  or an  app , is computer software designed to help the user perform specific tasks. Examples include  enterprise software ,  accounting software ,  office suites ,  graphics software , and  media players . Many application programs deal principally with  documents . [ 24 ]  Apps may be  bundled  with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install additional applications. The system software manages the hardware and serves the application, which in turn serves the user. Application software applies the power of a particular  computing platform  or system software to a particular purpose. Some apps, such as  Microsoft Office , are developed in multiple versions for several different platforms; others have narrower requirements and are generally referred to by the platform they run on. For example, a  geography  application for  Windows  or an  Android  application for  education  or  Linux gaming . Applications that run only on one platform and increase the desirability of that platform due to the popularity of the application, known as  killer applications . [ 25 ] A computer network, often simply referred to as a network, is a collection of hardware components and computers  interconnected  by communication channels that allow the sharing of resources and information. [ 26 ]  When at least one process in one device is able to send or receive data to or from at least one process residing in a remote device, the two devices are said to be in a network. Networks may be classified according to a wide variety of characteristics such as the medium used to transport the data,  communications protocol  used, scale,  topology , and organizational scope. Communications protocols  define the rules and data formats for exchanging information in a computer network, and provide the basis for  network programming . One well-known communications protocol is  Ethernet , a hardware and  link layer  standard that is ubiquitous in  local area networks . Another common protocol is the  Internet Protocol Suite , which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, host-to-host data transfer, and application-specific data transmission formats. [ 27 ] Computer networking is sometimes considered a sub-discipline of  electrical engineering , telecommunications,  computer science , information technology, or  computer engineering , since it relies upon the theoretical and practical application of these disciplines. [ 28 ] The Internet is a global system of interconnected  computer networks  that use the standard  Internet Protocol Suite  (TCP/IP) to serve billions of users. This includes millions of private, public, academic, business, and government networks, ranging in scope from local to global. These networks are linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked  hypertext  documents of the  World Wide Web  and the  infrastructure  to support email. [ 29 ] Computer programming is the process of writing, testing, debugging, and maintaining the source code and documentation of computer programs. This source code is written in a  programming language , which is an  artificial language  that is often more restrictive than  natural languages , but easily translated by the computer. Programming is used to invoke some desired behavior (customization) from the machine. [ 30 ] Writing high-quality source code requires knowledge of both the computer science domain and the domain in which the application will be used. The highest-quality software is thus often developed by a team of domain experts, each a specialist in some area of development. [ 31 ]  However, the term  programmer  may apply to a range of program quality, from  hacker  to  open source contributor  to professional. It is also possible for a single programmer to do most or all of the computer programming needed to generate the  proof of concept  to launch a new  killer application . [ 32 ] A programmer, computer programmer, or coder is a person who writes computer software. The term  computer programmer  can refer to a specialist in one area of  computer programming  or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. [ citation needed ]  A programmer's primary computer language ( C ,  C++ ,  Java ,  Lisp ,  Python , etc.) is often prefixed to the above titles, and those who work in a web environment often prefix their titles with  Web . The term  programmer  can be used to refer to a  software developer , software engineer,  computer scientist , or  software analyst . However, members of these  professions  typically possess other software engineering skills, beyond programming. [ 33 ] The computer industry is made up of businesses involved in developing computer software, designing computer hardware and  computer networking  infrastructures, manufacturing computer components, and providing information technology services, including  system administration  and maintenance. [ citation needed ] The software industry includes businesses engaged in  development ,  maintenance , and  publication  of software. The industry also includes software  services , such as  training ,  documentation , and consulting. [ citation needed ] Computer engineering is a  discipline  that integrates several fields of  electrical engineering  and  computer science  required to develop computer hardware and software. [ 34 ]  Computer engineers usually have training in  electronic engineering  (or  electrical engineering ),  software design , and hardware-software integration, rather than just software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual  microprocessors , personal computers, and  supercomputers , to  circuit design . This field of engineering includes not only the design of hardware within its own domain, but also the interactions between hardware and the context in which it operates. [ 35 ] Software engineering is the application of a systematic, disciplined, and quantifiable approach to the design, development, operation, and maintenance of software, and the study of these approaches. That is, the application of engineering to software. [ 36 ] [ 37 ] [ 38 ]  It is the act of using insights to conceive, model and scale a solution to a problem. The first reference to the term is the 1968  NATO Software Engineering Conference , and was intended to provoke thought regarding the perceived  software crisis  at the time. [ 39 ] [ 40 ] [ 41 ]   Software development , a widely used and more generic term, does not necessarily subsume the engineering paradigm. The generally accepted concepts of Software Engineering as an engineering discipline have been specified in the Guide to the  Software Engineering Body of Knowledge  (SWEBOK). The SWEBOK has become an internationally accepted standard in ISO/IEC TR 19759:2015. [ 42 ] Computer science or computing science (abbreviated CS or Comp Sci) is the  scientific  and practical approach to  computation  and its applications. A  computer scientist  specializes in the theory of computation and the design of computational systems. [ 43 ] Its subfields can be divided into practical techniques for its implementation and application in  computer systems , and purely theoretical areas. Some, such as  computational complexity theory , which studies fundamental properties of  computational problems , are highly abstract, while others, such as  computer graphics , emphasize real-world applications. Others focus on the challenges in implementing computations. For example,  programming language theory  studies approaches to the description of computations, while the study of  computer programming  investigates the use of  programming languages  and  complex systems . The field of  human–computer interaction  focuses on the challenges in making computers and computations useful, usable, and universally accessible to humans.  [ 44 ] The field of cybersecurity pertains to the protection of computer systems and networks. This includes  information and data privacy , preventing  disruption  of IT services and prevention of theft of and damage to hardware, software, and data. [ 45 ] Data science is a field that uses scientific and computing tools to extract information and insights from data, driven by the increasing volume and availability of data. [ 46 ]   Data mining ,  big data , statistics,  machine learning  and  deep learning  are all interwoven with data science. [ 47 ] Information systems (IS) is the study of complementary networks of hardware and software (see information technology) that people and organizations use to collect, filter, process, create, and distribute  data . [ 48 ] [ 49 ] [ 50 ]  The  ACM 's  Computing Careers  describes IS as: \"A majority of IS [degree] programs are located in business schools; however, they may have different names such as management information systems, computer information systems, or business information systems. All IS degrees combine business and computing topics, but the emphasis between technical and organizational issues varies among programs. For example, programs differ substantially in the amount of programming required.\" [ 51 ] The study of IS bridges business and  computer science , using the theoretical foundations of information and  computation  to study various business models and related  algorithmic  processes within a computer science discipline. [ 52 ] [ 53 ] [ 54 ]   The field of Computer Information Systems (CIS) studies computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society [ 55 ] [ 56 ]  while IS emphasizes functionality over design. [ 57 ] Information technology (IT) is the application of computers and  telecommunications equipment  to store, retrieve, transmit, and manipulate data, [ 58 ]  often in the context of a business or other enterprise. [ 59 ]  The term is commonly used as a synonym for computers and computer networks, but also encompasses other information distribution technologies such as television and telephones. Several  industries  are associated with information technology, including computer hardware, software,  electronics ,  semiconductors , internet,  telecom equipment ,  e-commerce , and  computer services . [ 60 ] [ 61 ] DNA-based computing  and  quantum computing  are areas of active research for both computing hardware and software, such as the development of  quantum algorithms . Potential infrastructure for future technologies includes  DNA origami  on photolithography [ 62 ]  and  quantum antennae  for transferring information between ion traps. [ 63 ]  By 2011, researchers had  entangled  14  qubits . [ 64 ] [ 65 ]  Fast  digital circuits , including those based on  Josephson junctions  and  rapid single flux quantum  technology, are becoming more nearly realizable with the discovery of  nanoscale superconductors . [ 66 ] Fiber-optic and photonic (optical) devices, which already have been used to transport data over long distances, are starting to be used by data centers, along with CPU and semiconductor memory components. This allows the separation of RAM from CPU by optical interconnects. [ 67 ]  IBM has created an  integrated circuit  with both electronic and optical information processing in one chip. This is denoted CMOS-integrated nanophotonics (CINP). [ 68 ]  One benefit of optical interconnects is that motherboards, which formerly required a certain kind of system on a chip (SoC), can now move formerly dedicated memory and network controllers off the motherboards, spreading the controllers out onto the rack. This allows standardization of backplane interconnects and motherboards for multiple types of SoCs, which allows more timely upgrades of CPUs. [ 69 ] Another field of research is  spintronics . Spintronics can provide computing power and storage, without heat buildup. [ 70 ]  Some research is being done on hybrid chips, which combine  photonics  and spintronics. [ 71 ] [ 72 ]  There is also research ongoing on combining  plasmonics , photonics, and electronics. [ 73 ] Cloud computing is a model that allows for the use of computing resources, such as servers or applications, without the need for interaction between the owner of these resources and the end user. It is typically offered as a service, making it an example of  Software as a Service ,  Platforms as a Service , and  Infrastructure as a Service , depending on the functionality offered. Key characteristics include on-demand access, broad network access, and the capability of rapid scaling. [ 74 ]  It allows individual users or small business to benefit from  economies of scale . One area of interest in this field is its potential to support energy efficiency. Allowing thousands of instances of computation to occur on one single machine instead of thousands of individual machines could help save energy. It could also ease the transition to renewable energy source, since it would suffice to power one server farm with renewable energy, rather than millions of homes and offices. [ 75 ] However, this centralized computing model poses several challenges, especially in security and privacy. Current legislation does not sufficiently protect users from companies mishandling their data on company servers. This suggests potential for further legislative regulations on cloud computing and tech companies. [ 76 ] Quantum computing  is an area of research that brings together the disciplines of computer science, information theory, and quantum physics. While the idea of information as part of physics is relatively new, there appears to be a strong tie between information theory and quantum mechanics. [ 77 ]  Whereas traditional computing operates on a binary system of ones and zeros, quantum computing uses  qubits . Qubits are capable of being in a superposition, i.e. in both states of one and zero, simultaneously. Thus, the value of the qubit is not between 1 and 0, but changes depending on when it is measured. This trait of qubits is known as  quantum entanglement , and is the core idea of quantum computing that allows quantum computers to do large scale computations. [ 78 ]  Quantum computing is often used for scientific research in cases where traditional computers do not have the computing power to do the necessary calculations, such in  molecular modeling . Large molecules and their reactions are far too complex for traditional computers to calculate, but the computational power of quantum computers could provide a tool to perform such calculations. [ 79 ]"
  },
  {
    "id": 32,
    "title": "Information system",
    "content": "An  information system  ( IS ) is a formal,  sociotechnical , organizational system designed to collect, process,  store , and  distribute   information . [ 1 ]  From a sociotechnical perspective, information systems comprise four components: task, people, structure (or roles), and technology. [ 2 ]  Information systems can be defined as an integration of components for collection, storage and  processing  of  data , comprising  digital products  that process data to facilitate  decision making [ 3 ]  and the data being used to provide information and contribute to knowledge. A  computer information system  is a system, which consists of people and computers that process or interpret information. [ 4 ] [ 5 ] [ 6 ] [ 7 ]  The term is also sometimes used to simply refer to a computer system with software installed. \" Information systems \" is also an academic field of study about systems with a specific reference to information and the complementary networks of  computer hardware  and software that people and organizations use to collect, filter, process, create and also  distribute   data . [ 8 ]  An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks. [ 9 ] In many organizations, the department or unit responsible for information systems and  data processing  is known as \" information services \". [ 10 ] [ 11 ] [ 12 ] [ 13 ] Any specific information system aims to support operations, management and  decision-making . [ 14 ] [ 15 ]  An information system is the  information and communication technology  (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes. [ 16 ] Some authors make a clear distinction between information systems,  computer systems , and  business processes . Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end-use of  information technology . Information systems are also different from business processes. Information systems help to control the performance of business processes. [ 17 ] Alter [ 18 ] [ 19 ]  argues that viewing an information system as a special type of  work system  has its advantages. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system in which activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information. [ 20 ] As such, information systems inter-relate with data systems on the one hand and activity systems on the other. [ 21 ]  An information system is a form of  communication  system in which data represent and are processed as a form of social memory. An information system can also be considered a semi- formal language  which supports human decision making and action. Information systems are the primary focus of study for organizational informatics. [ 22 ] Silver et al. (1995) provided two views on IS that includes software, hardware, data, people, and procedures. [ 23 ] The  Association for Computing Machinery  defines \"Information systems specialists [as] focus[ing] on integrating  information technology  solutions and business processes to meet the information needs of businesses and other enterprises.\" [ 24 ] There are various types of information systems, : including  transaction processing systems ,  decision support systems ,  knowledge management systems ,  learning management systems ,  database management systems , and office information systems. Critical to most information systems are information technologies, which are typically designed to enable humans to perform tasks for which the human brain is not well suited, such as: handling large amounts of information, performing complex calculations, and controlling many simultaneous processes. Information technologies are a very important and malleable resource available to executives. [ 25 ]  Many companies have created a position of  chief information officer  (CIO) that sits on the executive board with the  chief executive officer  (CEO),  chief financial officer  (CFO),  chief operating officer  (COO), and  chief technical officer  (CTO). The CTO may also serve as CIO, and vice versa. The  chief information security officer  (CISO) focuses on  information security  management. The  six components  that must come together in order to produce an information system are: [ 26 ] Data is the bridge between hardware and people. This means that the data we collect is only data until we involve people. At that point, data becomes information. The \"classic\" view of Information systems found in textbooks [ 28 ]  in the 1980s was a pyramid of systems that reflected the hierarchy of the organization, usually  transaction processing systems  at the bottom of the pyramid, followed by  management information systems ,  decision support systems , and ending with  executive information systems  at the top. Although the pyramid model remains useful since it was first formulated, a number of new technologies have been developed and new categories of information systems have emerged, some of which no longer fit easily into the original pyramid model. Some examples of such systems are: A computer(-based) information system is essentially an IS using computer technology to carry out some or all of its planned tasks. The basic components of computer-based information systems are: The first four components (hardware, software, database, and network) make up what is known as the information technology platform.\nInformation technology workers could then use these components to create information systems that watch over safety measures, risk and the management of data. These actions are known as information technology services. [ 29 ] Certain information systems support parts of organizations, others support entire organizations, and still others, support groups of organizations. Each department or functional area within an organization has its own collection of application programs or information systems. These functional area information systems (FAIS) are supporting pillars for more general IS namely,  business intelligence  systems and  dashboards . [ citation needed ]  As the name suggests, each FAIS supports a particular function within the organization, e.g.: accounting IS, finance IS, production-operation management (POM) IS, marketing IS, and human resources IS. In finance and accounting, managers use IT systems to forecast revenues and business activity, to determine the best sources and uses of funds, and to perform audits to ensure that the organization is fundamentally sound and that all financial reports and documents are accurate. Other types of organizational information systems are FAIS,  transaction processing systems ,  enterprise resource planning ,  office automation  system,  management information system ,  decision support system ,  expert system , executive dashboard,  supply chain management system , and  electronic commerce  system. Dashboards are a special form of IS that support all managers of the organization. They provide rapid access to timely information and direct access to structured information in the form of reports. Expert systems attempt to duplicate the work of human experts by applying reasoning capabilities, knowledge, and expertise within a specific domain. Information technology departments in larger organizations tend to strongly influence the development, use, and application of information technology in the business.\nA series of methodologies and processes can be used to develop and use an information system. Many developers use a systems engineering approach such as the  system development life cycle  (SDLC), to systematically develop an information system in stages. The stages of the system development lifecycle are planning, system analysis, and requirements, system design, development, integration and testing, implementation and operations, and maintenance.\nRecent research aims at enabling [ 30 ]  and measuring [ 31 ]  the ongoing, collective development of such systems within an organization by the entirety of human actors themselves.\nAn information system can be developed in house (within the organization) or outsourced. This can be accomplished by outsourcing certain components or the entire system. [ 32 ]  A specific case is the geographical distribution of the development team ( offshoring ,  global information system ). A computer-based information system, following a definition of  Langefors , [ 33 ]  is a technologically implemented medium for recording, storing, and disseminating linguistic expressions, as well as for drawing conclusions from such expressions. Geographic information systems , land information systems, and disaster information systems are examples of emerging information systems, but they can be broadly considered as spatial information systems.\nSystem development is done in stages which include: [ 34 ] The field of study called  information systems  encompasses a variety of topics including systems analysis and design, computer networking, information security, database management, and decision support systems.  Information management  deals with the practical and theoretical problems of collecting and analyzing information in a business function area including business productivity tools, applications programming and implementation, electronic commerce, digital media production, data mining, and decision support.  Communications and networking  deals with telecommunication technologies. \nInformation systems bridges  business  and  computer science  using the theoretical foundations of  information  and  computation  to study various business models and related  algorithmic  processes  [ 35 ]  on building the IT systems  [ 36 ] [ 37 ]  within a computer science discipline. [ 38 ] [ 39 ] [ 40 ] [ 41 ] [ 42 ] [ 43 ] [ 44 ] [ 45 ] [ 46 ] [ 47 ] [ 48 ] [ 49 ] [ 50 ]   Computer information systems  (CIS) is a field studying computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society, [ 51 ] [ 52 ] [ 53 ]  whereas IS emphasizes functionality over design. [ 54 ] Several IS scholars have debated the nature and foundations of information systems which have its roots in other reference disciplines such as  computer science ,  engineering ,  mathematics ,  management science ,  cybernetics , and others. [ 55 ] [ 56 ] [ 57 ] [ 58 ]  Information systems also can be defined as a collection of hardware, software, data, people, and procedures that work together to produce quality information. Similar to computer science, other disciplines can be seen as both related and foundation disciplines of IS. The domain of study of IS involves the study of theories and practices related to the social and technological phenomena, which determine the development, use, and effects of information systems in organizations and society. [ 59 ]  But, while there may be considerable overlap of the disciplines at the boundaries, the disciplines are still differentiated by the focus, purpose, and orientation of their activities. [ 60 ] In a broad scope,  information systems  is a scientific field of study that addresses the range of strategic, managerial, and operational activities involved in the gathering, processing, storing, distributing, and use of information and its associated technologies in society and organizations. [ 60 ]  The term  information systems  is also used to describe an organizational function that applies IS knowledge in the industry, government agencies, and not-for-profit organizations. [ 60 ] Information systems  often refers to the interaction between algorithmic processes and technology. This interaction can occur within or across organizational boundaries. An information system is a technology an organization uses and also the way in which the organizations interact with the technology and the way in which the technology works with the organization's business processes. Information systems are distinct from  information technology  (IT) in that an information system has an information technology component that interacts with the processes' components. One problem with that approach is that it prevents the IS field from being interested in non-organizational use of ICT, such as in social networking, computer gaming, mobile personal usage, etc. A different way of differentiating the IS field from its neighbours is to ask, \"Which aspects of reality are most meaningful in the IS field and other fields?\" [ 61 ]  This approach, based on philosophy, helps to define not just the focus, purpose, and orientation, but also the dignity, destiny and, responsibility of the field among other fields. [ 62 ] Business informatics  is a related discipline that is well-established in several countries, especially in Europe. While  Information systems  has been said to have an \"explanation-oriented\" focus,  business informatics  has a more \"solution-oriented\" focus and includes  information technology  elements and construction and implementation-oriented elements. Information systems workers enter a number of different careers: There is a wide variety of career paths in the information systems discipline. \"Workers with specialized technical knowledge and strong communications skills will have the best prospects. Workers with management skills and an understanding of business practices and principles will have excellent opportunities, as companies are increasingly looking to technology to drive their revenue.\" [ 63 ] Information technology is important to the operation of contemporary businesses, it offers many employment opportunities. The information systems field includes the people in organizations who design and build information systems, the people who use those systems, and the people responsible for managing those systems.\nThe demand for traditional IT staff such as programmers, business analysts, systems analysts, and designer is significant. Many well-paid jobs exist in areas of Information technology. At the top of the list is the chief information officer (CIO). The CIO is the executive who is in charge of the IS function. In most organizations, the CIO works with the chief executive officer (CEO), the chief financial officer (CFO), and other senior executives. Therefore, he or she actively participates in the organization's strategic planning process. Information systems research is generally  interdisciplinary  concerned with the study of the effects of information systems on the behaviour of individuals, groups, and organizations. [ 68 ] [ 69 ]  Hevner et al. (2004) [ 70 ]  categorized research in IS into two scientific paradigms including  behavioural science  which is to develop and verify theories that explain or predict human or organizational behavior and  design science  which extends the boundaries of human and organizational capabilities by creating new and innovative artifacts. Salvatore March and Gerald Smith [ 71 ]  proposed a framework for researching different aspects of information technology including outputs of the research (research outputs) and activities to carry out this research (research activities). They identified research outputs as follows: Also research activities including: Although Information Systems as a discipline has been evolving for over 30 years now, [ 72 ]  the core focus or identity of IS research is still subject to debate among scholars. [ 73 ] [ 74 ] [ 75 ]  There are two main views around this debate: a narrow view focusing on the IT artifact as the core subject matter of IS research, and a broad view that focuses on the interplay between social and technical aspects of IT that is embedded into a dynamic evolving context. [ 76 ]  A third view [ 77 ]  calls on IS scholars to pay balanced attention to both the IT artifact and its context. Since the study of information systems is an applied field, industry practitioners expect information systems research to generate findings that are immediately applicable in practice. This is not always the case however, as information systems researchers often explore behavioral issues in much more depth than practitioners would expect them to do. This may render information systems research results difficult to understand, and has led to criticism. [ 78 ] In the last ten years, the business trend is represented by the considerable increase of Information Systems Function (ISF) role, especially with regard to the enterprise strategies and operations supporting. It became a key factor to increase  productivity  and to support  value creation . [ 79 ]  To study an information system itself, rather than its effects, information systems models are used, such as  EATPUT . The international body of Information Systems researchers, the  Association for Information Systems  (AIS), and its Senior Scholars Forum Subcommittee on Journals (202), proposed a list of 11 journals that the AIS deems as 'excellent'. [ 80 ]  According to the AIS, this list of journals recognizes topical, methodological, and geographical diversity. The review processes are stringent, editorial board members are widely-respected and recognized, and there is international readership and contribution. The list is (or should be) used, along with others, as a point of reference for promotion and tenure and, more generally, to evaluate scholarly excellence. A number of annual  information systems conferences  are run in various parts of the world, the majority of which are peer reviewed. The AIS directly runs the  International Conference on Information Systems  (ICIS) and the  Americas Conference on Information Systems  (AMCIS), while AIS affiliated conferences [ 81 ]  include the  Pacific Asia Conference on Information Systems  (PACIS),  European Conference on Information Systems  (ECIS), the Mediterranean Conference on Information Systems (MCIS), the International Conference on Information Resources Management (Conf-IRM) and the Wuhan International Conference on E-Business (WHICEB). AIS chapter conferences [ 82 ]  include  Australasian Conference on Information Systems  (ACIS),  Scandinavian Conference on Information Systems  (SCIS),  Information Systems International Conference  (ISICO), Conference of the Italian Chapter of AIS (itAIS), Annual Mid-Western AIS Conference (MWAIS) and Annual Conference of the Southern AIS (SAIS). EDSIG, [ 83 ]  which is the special interest group on education of the AITP, [ 84 ]  organizes the Conference on Information Systems and Computing Education [ 85 ]  and the Conference on Information Systems Applied Research [ 86 ]  which are both held annually in November."
  },
  {
    "id": 33,
    "title": "Information needs",
    "content": "The term  information need  is often understood as an individual or group's desire to locate and obtain  information  to satisfy a conscious or unconscious  need . Rarely mentioned in general literature about  needs , it is a common term in  information science . According to Hjørland (1997) it is closely related to the concept of  relevance : If something is relevant for a person in relation to a given task, we might say that the person needs the information for that task. Information needs are related to, but distinct from  information requirements . They are studied for: The concept of information needs was coined by an American information journalist  Robert S. Taylor  in his 1962 article  \"The Process of Asking Questions\"  published in  American Documentation  (renamed  Journal of the American Society for Information Science and Technology ). In this paper, Taylor attempted to describe how an inquirer obtains an answer from an  information system , by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system. According to Taylor, information need has four levels: There are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback). Herbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles: William J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research. In the framework, he places the information users at the centre of ten systems, which are: \"In 2012, the  University of Southern California  was funded by the Federal Communications Commission to examine a wide range of social sciences from multiple disciplines to propose a set of critical information needs,\" according to Friedland. [ 1 ]  He continued, \"USC reached out to a team of scholars collectively identified as the Communications Policy Research Network (CPRN).  ... CPRN found that communities need access to eight categories of critical information ...:"
  },
  {
    "id": 34,
    "title": "Full-text search",
    "content": "In  text retrieval ,  full-text search  refers to techniques for searching a single  computer -stored  document  or a collection in a  full-text database . Full-text search is distinguished from searches based on  metadata  or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references). In a full-text search, a  search engine  examines all of the words in every stored document as it tries to match search criteria (for example, text specified by a user). Full-text-searching techniques appeared in the 1960s, for example  IBM STAIRS  from 1969, and became common in online  bibliographic databases  in the 1990s. [ verification needed ]  Many websites and application programs (such as  word processing  software) provide full-text-search capabilities. Some web search engines, such as the former  AltaVista , employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems. [ 1 ] When dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each  query , a strategy called \" serial scanning \". This is what some tools, such as  grep , do when searching. However, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an  index , but more correctly named a  concordance ). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents. [ 2 ] The indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore  stop words  (such as \"the\" and \"and\") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific  stemming  on the words being indexed. For example, the words \"drives\", \"drove\", and \"driven\" will be recorded in the index under the single concept word \"drive\". Recall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned to all relevant results. Precision is the ratio of the number of relevant results returned to the total number of results returned. The diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only 1 relevant result of 3 possible relevant results was returned, so the recall is a very low ratio of 1/3, or 33%. The precision for the example is a very low 1/4, or 25%, since only 1 of the 4 results returned was relevant. [ 3 ] Due to the ambiguities of  natural language , full-text-search systems typically includes options like  filtering  to increase precision and  stemming  to increase recall.  Controlled-vocabulary  searching also helps alleviate low-precision issues by  tagging  documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision. [ 4 ] Full-text searching is likely to retrieve many documents that are not  relevant  to the  intended  search question. Such documents are called  false positives  (see  Type I error ). The retrieval of irrelevant documents is often caused by the inherent ambiguity of  natural language . In the sample diagram to the right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background). Clustering techniques based on  Bayesian  algorithms can help reduce false positives. For a search term of \"bank\", clustering can be used to categorize the document/data universe into \"financial institution\", \"place to sit\", \"place to store\" etc. Depending on the occurrences of words relevant to the categories, search terms or a search result can be placed in one or more of the categories. This technique is being extensively deployed in the  e-discovery  domain. [ clarification needed ] The deficiencies of full text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision. The  PageRank  algorithm developed by  Google  gives more prominence to documents to which other  Web pages  have linked. [ 6 ]  See  Search engine  for additional examples. The following is a partial list of available software products whose predominant purpose is to perform full-text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full-text search may be accomplished."
  },
  {
    "id": 35,
    "title": "Science",
    "content": "Science  is a  systematic  discipline that builds and organises  knowledge  in the form of testable  hypotheses  and  predictions  about the universe. [ 1 ] [ 2 ]  Modern science is typically divided into two or three major branches: [ 3 ]  the  natural sciences  (e.g.,  physics ,  chemistry , and  biology ), which study the  physical world ; and the  behavioural sciences  (e.g.,  economics ,  psychology , and  sociology ), which study  individuals  and societies. [ 4 ] [ 5 ]  The  formal sciences  (e.g.,  logic , mathematics, and  theoretical computer science ), which study  formal systems  governed by  axioms  and rules, [ 6 ] [ 7 ]  are sometimes described as being sciences as well; however, they are often regarded as a separate field because they rely on  deductive reasoning  instead of the  scientific method  or  empirical evidence  as their main methodology. [ 8 ] [ 9 ]   Applied sciences  are disciplines that use scientific knowledge for practical purposes, such as  engineering  and  medicine . [ 10 ] [ 11 ] [ 12 ] The  history of science  spans the majority of the historical record, with the earliest written records of identifiable predecessors to modern science dating to  Bronze Age   Egypt  and  Mesopotamia  from around 3000 to 1200  BCE . Their contributions to mathematics,  astronomy , and medicine entered and shaped the Greek  natural philosophy  of  classical antiquity , whereby formal attempts were made to provide explanations of events in the  physical world  based on natural causes, while further advancements, including the introduction of the  Hindu–Arabic numeral system , were made during the  Golden Age of India . [ 13 ] : 12  [ 14 ] [ 15 ] [ 16 ]  Scientific research deteriorated in these regions after the  fall of the Western Roman Empire  during the  Early Middle Ages  (400 to 1000 CE), but in the  Medieval renaissances  ( Carolingian Renaissance ,  Ottonian Renaissance  and the  Renaissance of the 12th century ) scholarship flourished again. Some Greek manuscripts lost in Western Europe were preserved and expanded upon in the Middle East during the  Islamic Golden Age , [ 17 ]  along with the later efforts of  Byzantine Greek scholars  who brought Greek manuscripts from the dying  Byzantine Empire  to Western Europe at the start of the  Renaissance . The recovery and assimilation of  Greek works  and  Islamic inquiries  into Western Europe from the 10th to 13th century revived \" natural philosophy \", [ 18 ] [ 19 ] [ 20 ]  which was later transformed by the  Scientific Revolution  that began in the 16th century [ 21 ]  as new ideas and discoveries departed from previous Greek conceptions and traditions. [ 22 ] [ 23 ]  The  scientific method  soon played a greater role in knowledge creation and it was not until the  19th century  that many of the  institutional  and  professional  features of science began to take shape, [ 24 ] [ 25 ]  along with the changing of \"natural philosophy\" to \"natural science\". [ 26 ] New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems. [ 27 ] [ 28 ]  Contemporary scientific research is highly collaborative and is usually done by teams in academic and  research institutions , [ 29 ]  government agencies, [ 30 ]  and companies. [ 31 ]  The practical impact of their work has led to the emergence of  science policies  that seek to influence the scientific enterprise by prioritising the  ethical and moral development  of commercial products, armaments, health care, public infrastructure, and  environmental protection . The word  science  has been used in  Middle English  since the 14th century in the sense of \"the state of knowing\". The word was borrowed from the  Anglo-Norman language  as the suffix  -cience , which was borrowed from the  Latin  word  scientia , meaning \"knowledge, awareness, understanding\". It is a  noun derivative  of the Latin  sciens  meaning \"knowing\", and undisputedly derived from the Latin  sciō , the  present participle   scīre , meaning \"to know\". [ 32 ] There are many hypotheses for  science ' s ultimate word origin. According to  Michiel de Vaan , Dutch linguist and  Indo-Europeanist ,  sciō  may have its origin in the  Proto-Italic language  as  * skije-  or  * skijo-  meaning \"to know\", which may originate from  Proto-Indo-European language  as  *skh 1 -ie ,  *skh 1 -io , meaning \"to incise\". The  Lexikon der indogermanischen Verben  proposed  sciō  is a  back-formation  of  nescīre , meaning \"to not know, be unfamiliar with\", which may derive from Proto-Indo-European  *sekH-  in Latin  secāre , or  *skh 2 - , from  *sḱʰeh2(i)-  meaning \"to cut\". [ 33 ] In the past, science was a synonym for \"knowledge\" or \"study\", in keeping with its Latin origin. A person who conducted scientific research was called a \"natural philosopher\" or \"man of science\". [ 34 ]  In 1834,  William Whewell  introduced the term  scientist  in a review of  Mary Somerville 's book  On the Connexion of the Physical Sciences , [ 35 ]  crediting it to \"some ingenious gentleman\" (possibly himself). [ 36 ] Science has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years, [ 37 ] [ 38 ]  taking different forms around the world, and few details are known about the very earliest developments.  Women  likely played a central role in prehistoric science, [ 39 ]  as did  religious rituals . [ 40 ]  Some scholars use the term \" protoscience \" to label activities in the past that resemble modern science in some but not all features; [ 41 ] [ 42 ] [ 43 ]  however, this label has also been criticised as denigrating, [ 44 ]  or too suggestive of  presentism , thinking about those activities only in relation to modern categories. [ 45 ] Direct evidence for scientific processes becomes clearer with the advent of  writing systems  in early civilisations like  Ancient Egypt  and  Mesopotamia , creating the earliest written records in the  history of science  in around 3000 to 1200  BCE . [ 13 ] : 12–15  [ 14 ]  Although the words and concepts of \"science\" and \"nature\" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine. [ 46 ] [ 13 ] : 12   From the 3rd millennium BCE, the ancient Egyptians developed a  decimal numbering system , [ 47 ]  solved practical problems using  geometry , [ 48 ]  and developed a  calendar . [ 49 ]  Their healing therapies involved drug treatments and the supernatural, such as prayers,  incantations , and rituals. [ 13 ] : 9 The ancient  Mesopotamians  used knowledge about the properties of various natural chemicals for manufacturing  pottery ,  faience , glass, soap, metals,  lime plaster , and waterproofing. [ 50 ]  They studied  animal physiology ,  anatomy ,  behaviour , and  astrology  for  divinatory  purposes. [ 51 ]  The Mesopotamians had an  intense interest in medicine  and the earliest  medical prescriptions  appeared in  Sumerian  during the  Third Dynasty of Ur . [ 50 ] [ 52 ]  They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity. [ 50 ] In  classical antiquity , there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time. [ 53 ]  Before the invention or discovery of the  concept  of  phusis  or nature by the  pre-Socratic philosophers , the same words tend to be used to describe the natural \"way\" in which a plant grows, [ 54 ]  and the \"way\" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish \"nature\" and \"convention\". [ 55 ] The early  Greek philosophers  of the Milesian school, which was founded by  Thales of Miletus  and later continued by his successors  Anaximander  and  Anaximenes , were the first to attempt to explain  natural phenomena  without relying on the  supernatural . [ 56 ]  The  Pythagoreans  developed a complex number philosophy [ 57 ] : 467–68   and contributed significantly to the development of mathematical science. [ 57 ] : 465   The  theory of atoms  was developed by the Greek philosopher  Leucippus  and his student  Democritus . [ 58 ] [ 59 ]  Later,  Epicurus  would develop a full natural cosmology based on atomism, and would adopt a \"canon\" (ruler, standard) which established physical criteria or standards of scientific truth. [ 60 ]  The Greek doctor  Hippocrates  established the tradition of systematic medical science [ 61 ] [ 62 ]  and is known as \" The Father of Medicine \". [ 63 ] A turning point in the history of early philosophical science was  Socrates ' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The  Socratic method  as documented by  Plato 's dialogues is a  dialectic  method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly-held truths that shape beliefs and scrutinises them for consistency. [ 64 ]  Socrates criticised the older type of study of physics as too purely speculative and lacking in  self-criticism . [ 65 ] Aristotle  in the 4th century BCE created a systematic programme of  teleological  philosophy. [ 66 ]  In the 3rd century BCE, Greek astronomer  Aristarchus of Samos  was the first to propose a  heliocentric model  of the universe, with the Sun at the centre and all the planets orbiting it. [ 67 ]  Aristarchus's model was widely rejected because it was believed to violate the laws of physics, [ 67 ]  while Ptolemy's  Almagest , which contains a geocentric description of the  Solar System , was accepted through the early Renaissance instead. [ 68 ] [ 69 ]  The inventor and mathematician  Archimedes of Syracuse  made major contributions to the beginnings of  calculus . [ 70 ]   Pliny the Elder  was a Roman writer and polymath, who wrote the seminal encyclopaedia  Natural History . [ 71 ] [ 72 ] [ 73 ] Positional notation  for representing numbers likely emerged between the 3rd and 5th centuries CE along Indian trade routes. This numeral system made efficient  arithmetic  operations more accessible and would eventually become standard for mathematics worldwide. [ 74 ] Due to the  collapse of the Western Roman Empire , the 5th century saw an intellectual decline and knowledge of  Greek conceptions of the world  deteriorated in Western Europe. [ 13 ] : 194   During the period, Latin encyclopaedists such as  Isidore of Seville  preserved the majority of general ancient knowledge. [ 75 ]  In contrast, because the  Byzantine Empire  resisted attacks from invaders, they were able to preserve and improve prior learning. [ 13 ] : 159    John Philoponus , a Byzantine scholar in the 500s, started to question Aristotle's teaching of physics, introducing the  theory of impetus . [ 13 ] : 307, 311, 363, 402   His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later. [ 13 ] : 307–308  [ 76 ] During  late antiquity  and the  early Middle Ages , natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's  four causes : material, formal, moving, and final cause. [ 77 ]  Many Greek classical texts were preserved by the  Byzantine empire  and  Arabic  translations were done by groups such as the  Nestorians  and the  Monophysites . Under the  Caliphate , these Arabic translations were later improved and developed by Arabic scientists. [ 78 ]  By the 6th and 7th centuries, the neighbouring  Sassanid Empire  established the medical  Academy of Gondeshapur , which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world. [ 79 ] The  House of Wisdom  was established in  Abbasid -era  Baghdad ,  Iraq , [ 80 ]  where the Islamic study of  Aristotelianism  flourished [ 81 ]  until the  Mongol invasions  in the 13th century.  Ibn al-Haytham , better known as Alhazen, used controlled experiments in his optical study. [ a ] [ 83 ] [ 84 ]   Avicenna 's compilation of the  Canon of Medicine , a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used until the 18th century. [ 85 ] By the eleventh century most of Europe had become Christian, [ 13 ] : 204   and in 1088, the  University of Bologna  emerged as the first university in Europe. [ 86 ]  As such, demand for Latin translation of ancient and scientific texts grew, [ 13 ] : 204   a major contributor to the  Renaissance of the 12th century . Renaissance  scholasticism  in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature. [ 87 ]  In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by  Mondino de Luzzi . [ 88 ] New developments in optics played a role in the inception of the  Renaissance , both by challenging long-held  metaphysical  ideas on perception, as well as by contributing to the improvement and development of technology such as the  camera obscura  and the  telescope . At the start of the Renaissance,  Roger Bacon ,  Vitello , and  John Peckham  each built up a scholastic  ontology  upon a causal chain beginning with sensation, perception, and finally  apperception  of the individual and universal  forms  of Aristotle. [ 82 ] : Book I   A model of vision later known as  perspectivism  was  exploited and studied  by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final. [ 89 ] In the sixteenth century  Nicolaus Copernicus  formulated a  heliocentric model  of the Solar System, stating that the planets revolve around the Sun, instead of the  geocentric model  where the planets and the Sun revolve around the Earth. This was based on a theorem that the  orbital periods  of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model. [ 90 ] Johannes Kepler  and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light. [ 89 ] [ 91 ]  Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of  Kepler's laws of planetary motion . Kepler did not reject Aristotelian metaphysics and described his work as a search for the  Harmony of the Spheres . [ 92 ]   Galileo  had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model. [ 93 ] The  printing press  was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature. [ 94 ]   Francis Bacon  and  René Descartes  published philosophical arguments in favour of a new type of non-Aristotelian science. Bacon emphasised the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the  laws of nature  and the improvement of all human life. [ 95 ]  Descartes emphasised individual thought and argued that mathematics rather than geometry should be used to study nature. [ 96 ] At the start of the  Age of Enlightenment ,  Isaac Newton  formed the foundation of  classical mechanics  by his  Philosophiæ Naturalis Principia Mathematica , greatly influencing future physicists. [ 97 ]   Gottfried Wilhelm Leibniz  incorporated terms from  Aristotelian physics , now used in a new non- teleological  way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes. [ 98 ] During this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the  materialistic  sense of having more food, clothing, and other things. In  Bacon's words , \"the real and legitimate goal of sciences  is the endowment of human life with new inventions and riches \", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond \"the fume of subtle, sublime or pleasing [speculation]\". [ 99 ] Science during the Enlightenment was dominated by  scientific societies  and  academies , [ 100 ]  which had largely replaced universities as centres of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the  popularisation  of science among an increasingly literate population. [ 101 ]  Enlightenment philosophers turned to a few of their scientific predecessors –  Galileo ,  Kepler ,  Boyle , and Newton principally – as the guides to every physical and social field of the day. [ 102 ] [ 103 ] The 18th century saw significant advancements in the practice of medicine [ 104 ]  and physics; [ 105 ]  the development of biological  taxonomy  by  Carl Linnaeus ; [ 106 ]  a new understanding of  magnetism  and electricity; [ 107 ]  and the maturation of  chemistry  as a discipline. [ 108 ]  Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed  A Treatise of Human Nature , which was expressed historically in works by authors including  James Burnett ,  Adam Ferguson ,  John Millar  and  William Robertson , all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of  modernity . [ 109 ]  Modern sociology largely originated from this movement. [ 110 ]  In 1776,  Adam Smith  published  The Wealth of Nations , which is often considered the first work on modern economics. [ 111 ] During the nineteenth century many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as \"biologist\", \"physicist\", and \"scientist\"; an increased professionalisation of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialisation of numerous countries; the thriving of popular science writings; and the emergence of science journals. [ 112 ]  During the late 19th century, psychology emerged as a separate discipline from philosophy when  Wilhelm Wundt  founded the first laboratory for psychological research in 1879. [ 113 ] During the mid-19th century  Charles Darwin  and  Alfred Russel Wallace  independently proposed the theory of evolution by  natural selection  in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book  On the Origin of Species , published in 1859. [ 114 ]  Separately,  Gregor Mendel  presented his paper, \" Experiments on Plant Hybridisation \" in 1865, [ 115 ]  which outlined the principles of biological inheritance, serving as the basis for modern genetics. [ 116 ] Early in the 19th century  John Dalton  suggested the modern  atomic theory , based on Democritus's original idea of indivisible particles called  atoms . [ 117 ]  The laws of  conservation of energy ,  conservation of momentum  and  conservation of mass  suggested a highly stable universe where there could be little loss of resources. However, with the advent of the  steam engine  and the  Industrial Revolution  there was an increased understanding that not all forms of energy have the same  energy qualities , the ease of conversion to useful  work  or to another form of energy. [ 118 ]  This realisation led to the development of the laws of  thermodynamics , in which the free energy of the universe is seen as constantly declining: the  entropy  of a closed universe increases over time. [ b ] The  electromagnetic theory  was established in the 19th century by the works of  Hans Christian Ørsted ,  André-Marie Ampère ,  Michael Faraday ,  James Clerk Maxwell ,  Oliver Heaviside , and  Heinrich Hertz . The new theory raised questions that could not easily be answered using Newton's framework. The discovery of  X-rays  inspired the discovery of  radioactivity  by  Henri Becquerel  and  Marie Curie  in 1896, [ 121 ]  Marie Curie then became the first person to win two  Nobel Prizes . [ 122 ]  In the next year came the discovery of the first subatomic particle, the  electron . [ 123 ] In the first half of the century the development of  antibiotics  and  artificial fertilisers  improved human living standards globally. [ 124 ] [ 125 ]  Harmful  environmental issues  such as  ozone depletion ,  ocean acidification ,  eutrophication , and  climate change  came to the public's attention and caused the onset of  environmental studies . [ 126 ] During this period scientific experimentation became increasingly  larger in scale and funding . [ 127 ]  The extensive technological innovation stimulated by  World War I ,  World War II , and the  Cold War  led to competitions between  global powers , such as the  Space Race  and  nuclear arms race . [ 128 ] [ 129 ]  Substantial international collaborations were also made, despite armed conflicts. [ 130 ] In the late 20th century active recruitment of women and elimination of  sex discrimination  greatly increased the number of women scientists, but large gender disparities remained in some fields. [ 131 ]  The discovery of the  cosmic microwave background  in 1964 [ 132 ]  led to a rejection of the  steady-state model of the universe  in favour of the  Big Bang  theory of  Georges Lemaître . [ 133 ] The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th-century when the  modern synthesis  reconciled Darwinian evolution with  classical genetics . [ 134 ]   Albert Einstein 's  theory of relativity  and the development of  quantum mechanics  complement classical mechanics to describe physics in extreme  length , time and  gravity . [ 135 ] [ 136 ]  Widespread use of  integrated circuits  in the last quarter of the 20th century combined with  communications satellites  led to a revolution in information technology and the rise of the global internet and  mobile computing , including  smartphones . The need for mass systematisation of long, intertwined causal chains and large amounts of data led to the rise of the fields of  systems theory  and computer-assisted  scientific modelling . [ 137 ] The  Human Genome Project  was completed in 2003 by identifying and mapping all of the genes of the  human genome . [ 138 ]  The first  induced pluripotent human stem cells  were made in 2006, allowing adult cells to be transformed into  stem cells  and turn into any cell type found in the body. [ 139 ]  With the affirmation of the  Higgs boson  discovery in 2013, the last particle predicted by the  Standard Model  of particle physics was found. [ 140 ]  In 2015,  gravitational waves , predicted by  general relativity  a century before, were  first observed . [ 141 ] [ 142 ]  In 2019, the international collaboration  Event Horizon Telescope  presented the first direct image of a  black hole 's  accretion disc . [ 143 ] Modern science is commonly divided into three major  branches :  natural science ,  social science , and  formal science . [ 3 ]  Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own  nomenclature  and expertise. [ 144 ]  Both natural and social sciences are  empirical sciences , [ 145 ]  as their knowledge is based on  empirical observations  and is capable of being tested for its validity by other researchers working under the same conditions. [ 146 ] Natural science  is the study of the physical world. It can be divided into two main branches:  life science  and  physical science . These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics,  chemistry ,  astronomy , and  earth science . Modern natural science is the successor to the  natural philosophy  that began in  Ancient Greece .  Galileo ,  Descartes ,  Bacon , and  Newton  debated the benefits of using approaches that were more  mathematical  and more experimental in a methodical way. Still, philosophical perspectives,  conjectures , and  presuppositions , often overlooked, remain necessary in natural science. [ 147 ]  Systematic data collection, including  discovery science , succeeded  natural history , which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings. [ 148 ]  Today, \"natural history\" suggests observational descriptions aimed at popular audiences. [ 149 ] Social science  is the study of human behaviour and the functioning of societies. [ 4 ] [ 5 ]  It has many disciplines that include, but are not limited to  anthropology , economics, history,  human geography ,  political science , psychology, and sociology. [ 4 ]  In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing  research programmes  such as the  functionalists ,  conflict theorists , and  interactionists  in sociology. [ 4 ]  Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the  historical method ,  case studies , and  cross-cultural studies . Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes. [ 4 ] Formal science  is an area of study that generates knowledge using  formal systems . [ 150 ] [ 6 ] [ 7 ]  A formal system is an  abstract structure  used for inferring  theorems  from  axioms  according to a set of rules. [ 151 ]  It includes mathematics, [ 152 ] [ 153 ]   systems theory , and  theoretical computer science . The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts. [ 8 ] [ 154 ] [ 146 ]  The formal sciences are therefore  a priori  disciplines and because of this, there is disagreement on whether they constitute a science. [ 155 ] [ 156 ]  Nevertheless, the formal sciences play an important role in the empirical sciences.  Calculus , for example, was initially invented to understand  motion  in physics. [ 157 ]  Natural and social sciences that rely heavily on mathematical applications include  mathematical physics , [ 158 ]   chemistry , [ 159 ]   biology , [ 160 ]   finance , [ 161 ]  and  economics . [ 162 ] Applied science  is the use of the  scientific method  and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine. [ 163 ] [ 12 ]  Engineering is the use of scientific principles to invent, design and build machines, structures and technologies. [ 164 ]  Science may contribute to the development of new technologies. [ 165 ]  Medicine is the practice of caring for patients by maintaining and restoring health through the  prevention ,  diagnosis , and  treatment  of injury or disease. [ 166 ] [ 167 ]  The applied sciences are often contrasted with the  basic sciences , which are focused on advancing scientific theories and laws that explain and predict events in the natural world. [ 168 ] [ 169 ] Computational science  applies  computing power to simulate  real-world situations, enabling a better understanding of scientific problems than formal mathematics alone can achieve. The use of  machine learning  and  artificial intelligence  is becoming a central feature of computational contributions to science, for example in  agent-based computational economics ,  random forests ,  topic modeling  and various forms of prediction. However, machines alone rarely advance knowledge as they require human guidance and capacity to reason; and they can introduce bias against certain social groups or sometimes underperform against humans. [ 170 ] [ 171 ] Interdisciplinary science  involves the combination of two or more disciplines into one, [ 172 ]  such as  bioinformatics , a combination of biology and computer science [ 173 ]  or  cognitive sciences . The concept has existed since the ancient Greek period and it became popular again in the 20th century. [ 174 ] Scientific research can be labelled as either basic or applied research.  Basic research  is the search for knowledge and  applied research  is the search for solutions to practical problems using this knowledge. Most understanding comes from basic research, though sometimes applied research targets specific practical problems. This leads to technological advances that were not previously imaginable. [ 175 ] Scientific research involves using the  scientific method , which seeks to  objectively  explain the events of  nature  in a  reproducible  way. [ 176 ]  Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: there is an  objective reality  shared by all rational observers; this objective reality is governed by  natural laws ; these laws were discovered by means of systematic  observation  and experimentation. [ 2 ]  Mathematics is essential in the formation of  hypotheses ,  theories , and laws, because it is used extensively in quantitative modelling, observing, and collecting  measurements . [ 177 ]  Statistics is used to summarise and analyse data, which allows scientists to assess the reliability of experimental results. [ 178 ] In the scientific method an explanatory  thought experiment  or hypothesis is put forward as an explanation using  parsimony principles  and is expected to seek  consilience  – fitting with other accepted facts related to an observation or scientific question. [ 179 ]  This tentative explanation is used to make  falsifiable  predictions, which are typically posted before being tested by experimentation. Disproof of a prediction is evidence of progress. [ 176 ] : 4–5  [ 180 ]  Experimentation is especially important in science to help establish  causal relationships  to avoid the  correlation fallacy , though in some sciences such as astronomy or geology, a predicted observation might be more appropriate. [ 181 ] When a hypothesis proves unsatisfactory it is modified or discarded. [ 182 ]  If the hypothesis survives testing, it may become adopted into the framework of a  scientific theory , a  validly   reasoned , self-consistent model or framework for describing the behaviour of certain natural events. A theory typically describes the behaviour of much broader sets of observations than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus, a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. Scientists may generate a  model , an attempt to describe or depict an observation in terms of a logical, physical or mathematical representation, and to generate new hypotheses that can be tested by experimentation. [ 183 ] While performing experiments to test hypotheses, scientists may have a preference for one outcome over another. [ 184 ] [ 185 ]  Eliminating the bias can be achieved through transparency, careful  experimental design , and a thorough  peer review  process of the experimental results and conclusions. [ 186 ] [ 187 ]  After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be. [ 188 ]  Taken in its entirety, the scientific method allows for highly creative problem solving while minimising the effects of subjective and  confirmation bias . [ 189 ]   Intersubjective verifiability , the ability to reach a consensus and reproduce results, is fundamental to the creation of all scientific knowledge. [ 190 ] Scientific research is published in a range of literature. [ 191 ]   Scientific journals  communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals,  Journal des sçavans  followed by  Philosophical Transactions , began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500. [ 192 ] Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a  scientific paper . Science has become so pervasive in modern societies that it is considered necessary to communicate the achievements, news, and ambitions of scientists to a wider population. [ 193 ] The  replication crisis  is an ongoing  methodological  crisis that affects parts of the  social  and  life sciences . In subsequent investigations, the results of many scientific studies have been proven to be  unrepeatable . [ 194 ]  The crisis has long-standing roots; the phrase was coined in the early 2010s [ 195 ]  as part of a growing awareness of the problem. The replication crisis represents an important body of research in  metascience , which aims to improve the quality of all scientific research while reducing waste. [ 196 ] An area of study or speculation that masquerades as science in an attempt to claim legitimacy that it would not otherwise be able to achieve is sometimes referred to as  pseudoscience ,  fringe science , or  junk science . [ 197 ] [ 198 ]  Physicist  Richard Feynman  coined the term \" cargo cult science \" for cases in which researchers believe, and at a glance, look like they are doing science but lack the honesty to allow their results to be rigorously evaluated. [ 199 ]  Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as \"the most important tool\" for separating valid claims from invalid ones. [ 200 ] There can also be an element of political or ideological bias on all sides of scientific debates. Sometimes, research may be characterised as \"bad science\", research that may be well-intended but is incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term \" scientific misconduct \" refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person. [ 201 ] There are different schools of thought in the  philosophy of science . The most popular position is  empiricism , which holds that knowledge is created by a process involving observation; scientific theories generalise observations. [ 202 ]  Empiricism generally encompasses  inductivism , a position that explains how general theories can be made from the finite amount of empirical evidence available. Many versions of empiricism exist, with the predominant ones being  Bayesianism  and the  hypothetico-deductive method . [ 203 ] [ 202 ] Empiricism has stood in contrast to  rationalism , the position originally associated with  Descartes , which holds that knowledge is created by the human intellect, not by observation. [ 204 ]   Critical rationalism  is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher  Karl Popper . Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories, and that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation. [ 205 ] \nPopper proposed replacing verifiability with  falsifiability  as the landmark of scientific theories, replacing induction with  falsification  as the empirical method. [ 205 ]  Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism,  trial and error , [ 206 ]  covering all products of the human mind, including science, mathematics, philosophy, and art. [ 207 ] Another approach,  instrumentalism , emphasises the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes, with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be things that should be ignored. [ 208 ]  Close to instrumentalism is  constructive empiricism , according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true. [ 209 ] Thomas Kuhn  argued that the process of observation and evaluation takes place within a paradigm, a  logically consistent  \"portrait\" of the world that is consistent with observations made from its framing. He characterised  normal science  as the process of observation and \"puzzle solving\", which takes place within a paradigm, whereas  revolutionary science  occurs when one paradigm overtakes another in a  paradigm shift . [ 210 ]  Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more \"portraits\" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of  relativism . [ 211 ] Finally, another approach often cited in debates of  scientific scepticism  against controversial movements like \" creation science \" is  methodological naturalism . Naturalists maintain that a difference should be made between natural and supernatural, and science should be restricted to natural explanations. [ 212 ]  Methodological naturalism maintains that science requires strict adherence to  empirical  study and independent verification. [ 213 ] The  scientific community  is a network of interacting scientists who conduct scientific research. The community consists of smaller groups working in scientific fields. By having  peer review , through discussion and debate within journals and conferences, scientists maintain the quality of research methodology and objectivity when interpreting results. [ 214 ] Scientists are individuals who conduct scientific research to advance knowledge in an area of interest. [ 215 ] [ 216 ]  In modern times, many professional scientists are trained in an academic setting and, upon completion, attain an  academic degree , with the highest degree being a doctorate such as a Doctor of Philosophy or PhD. [ 217 ]  Many scientists pursue careers in various  sectors of the economy  such as  academia ,  industry ,  government , and nonprofit organisations. [ 218 ] [ 219 ] [ 220 ] Scientists exhibit a strong curiosity about reality and a desire to apply scientific knowledge for the benefit of health, nations, the environment, or industries. Other motivations include recognition by their peers and prestige. In modern times, many scientists have  advanced degrees  in an area of science and pursue careers in various sectors of the economy, such as  academia ,  industry ,  government , and nonprofit environments. [ 221 ] [ 222 ] [ 223 ] Science has historically been a male-dominated field, with notable exceptions.  Women in science  faced considerable discrimination in science, much as they did in other areas of male-dominated societies. For example, women were frequently passed over for job opportunities and denied credit for their work. [ 224 ]  The achievements of women in science have been attributed to the defiance of their traditional role as labourers within the  domestic sphere . [ 225 ] Learned societies  for the communication and promotion of scientific thought and experimentation have existed since the Renaissance. [ 226 ]  Many scientists belong to a learned society that promotes their respective scientific discipline,  profession , or group of related disciplines. [ 227 ]  Membership may either be open to all, require possession of scientific credentials, or conferred by election. [ 228 ]  Most scientific societies are nonprofit organisations, [ 229 ]  and many are  professional associations . Their activities typically include holding regular  conferences  for the presentation and discussion of new research results and publishing or sponsoring  academic journals  in their discipline. Some societies act as  professional bodies , regulating the activities of their members in the public interest, or the collective interest of the membership. The professionalisation of science, begun in the 19th century, was partly enabled by the creation of national distinguished  academies of sciences  such as the Italian  Accademia dei Lincei  in 1603, [ 230 ]  the British  Royal Society  in 1660, [ 231 ]  the  French Academy of Sciences  in 1666, [ 232 ]  the American  National Academy of Sciences  in 1863, [ 233 ]  the German  Kaiser Wilhelm Society  in 1911, [ 234 ]  and the  Chinese Academy of Sciences  in 1949. [ 235 ]  International scientific organisations, such as the  International Science Council , are devoted to  international cooperation  for science advancement. [ 236 ] Science awards  are usually given to individuals or organisations that have made significant contributions to a discipline. They are often given by prestigious institutions; thus, it is considered a great honour for a scientist receiving them. Since the early Renaissance, scientists have often been awarded medals, money, and titles. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, and  chemistry . [ 237 ] Scientific research is often funded  through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most  developed countries  is between 1.5% and 3% of GDP. [ 238 ]  In the  OECD , around two-thirds of  research and development  in scientific and technical fields is carried out by industry, and 20% and 10%, respectively, by universities and government. The government funding proportion in certain fields is higher, and it dominates research in social science and the  humanities . In less developed nations, the government provides the bulk of the funds for their basic scientific research. [ 239 ] Many governments have dedicated agencies to support scientific research, such as the  National Science Foundation  in the United States, [ 240 ]  the  National Scientific and Technical Research Council  in Argentina, [ 241 ]   Commonwealth Scientific and Industrial Research Organisation  in Australia, [ 242 ]   National Centre for Scientific Research  in France, [ 243 ]  the  Max Planck Society  in Germany, [ 244 ]  and  National Research Council  in Spain. [ 245 ]  In commercial research and development, all but the most research-orientated corporations focus more heavily on near-term commercialisation possibilities than research driven by curiosity. [ 246 ] Science policy  is concerned with policies that affect the conduct of the scientific enterprise, including  research funding , often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care, and environmental monitoring. Science policy sometimes refers to the act of applying scientific knowledge and consensus to the development of public policies. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public. [ 247 ]  Public policy can directly affect the funding of  capital equipment  and intellectual infrastructure for industrial research by providing tax incentives to those organisations that fund research. [ 193 ] Science education  for the general public is embedded in the school curriculum, and is supplemented by  online pedagogical content  (for example, YouTube and Khan Academy), museums, and science magazines and blogs. Major organisations of scientists such as the American Association for the Advancement of Science (AAAS) consider the sciences to be a part of the liberal arts traditions of learning, along with philosophy and history. [ 248 ]  Scientific literacy is chiefly concerned with an understanding of the  scientific method , units and methods of  measurement ,  empiricism , a basic understanding of statistics ( correlations ,  qualitative  versus  quantitative  observations,  aggregate statistics ), and a basic understanding of core scientific fields such as physics,  chemistry ,  biology , ecology, geology, and  computation . As a student advances into higher stages of  formal education , the curriculum becomes more in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well. [ 249 ] The mass media face pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a  scientific debate  may require considerable expertise regarding the matter. [ 250 ]  Few journalists have real scientific knowledge, and even  beat reporters  who are knowledgeable about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover. [ 251 ] [ 252 ] Science magazines  such as  New Scientist ,  Science & Vie , and  Scientific American  cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research. [ 253 ]  The science fiction genre, primarily  speculative fiction , can transmit the ideas and methods of science to the general public. [ 254 ]  Recent efforts to intensify or develop links between science and non-scientific disciplines, such as literature or poetry, include the  Creative Writing Science  resource developed through the  Royal Literary Fund . [ 255 ] While the scientific method is broadly accepted in the scientific community, some fractions of society reject certain scientific positions or are sceptical about science. Examples are the common notion that  COVID-19  is not a major health threat to the US (held by 39% of Americans in August 2021) [ 256 ]  or the belief that  climate change  is not a major threat to the US (also held by 40% of Americans, in late 2019 and early 2020). [ 257 ]  Psychologists have pointed to four factors driving rejection of scientific results: [ 258 ] Anti-science attitudes often seem to be caused by fear of rejection in social groups. For instance, climate change is perceived as a threat by only 22% of Americans on the right side of the political spectrum, but by 85% on the left. [ 260 ]  That is, if someone on the left would not consider climate change as a threat, this person may face contempt and be rejected in that social group. In fact, people may rather deny a scientifically accepted fact than lose or jeopardise their social status. [ 261 ] Attitudes towards science are often determined by political opinions and goals. Government, business and  advocacy groups  have been known to use legal and economic pressure to influence scientific researchers. Many factors can act as facets of the  politicisation of science  such as  anti-intellectualism , perceived threats to religious beliefs, and fear for business interests. [ 263 ]  Politicisation of science is usually accomplished when scientific information is presented in a way that emphasises the uncertainty associated with the  scientific evidence . [ 264 ]  Tactics such as shifting conversation, failing to acknowledge facts, and capitalising on doubt of  scientific consensus  have been used to gain more attention for views that have been undermined by scientific evidence. [ 265 ]  Examples of issues that have involved the politicisation of science include the  global warming controversy ,  health effects of pesticides , and  health effects of tobacco . [ 265 ] [ 266 ]"
  },
  {
    "id": 36,
    "title": "Metadata",
    "content": "Metadata  (or  metainformation ) is \" data  that provides information about other data\", [ 1 ]  but not the content of the data itself, such as the text of a message or the image itself. [ 2 ]  There are many distinct types of metadata, including: Metadata is not strictly bound to one of these categories, as it can describe a piece of data in many other ways. Metadata has various purposes. It can help users  find relevant information  and  discover resources . It can also help organize electronic resources, provide digital identification, and archive and preserve resources. Metadata allows users to access resources by \"allowing resources to be found by relevant criteria, identifying resources, bringing similar resources together, distinguishing dissimilar resources, and giving location information\". [ 8 ]  Metadata of  telecommunication  activities including  Internet  traffic is very widely collected by various national governmental organizations. This data is used for the purposes of  traffic analysis  and can be used for  mass surveillance . [ 9 ] Metadata was traditionally used in the  card catalogs  of  libraries  until the 1980s when libraries converted their catalog data to digital  databases . [ 10 ]  In the 2000s, as data and information were increasingly stored digitally, this digital data was described using  metadata standards . [ 11 ] The first description of \"meta data\" for computer systems is purportedly noted by MIT's Center for International Studies experts David Griffel and Stuart McIntosh in 1967: \"In summary then, we have statements in an object language about subject descriptions of data and token codes for the data. We also have statements in a meta language describing the data relationships and transformations, and ought/is relations between norm and data.\" [ 12 ] Unique metadata standards exist for different disciplines (e.g.,  museum  collections,  digital audio files ,  websites , etc.). Describing the  contents  and  context  of data or  data files  increases its usefulness. For example, a  web page  may include metadata specifying what software language the page is written in (e.g., HTML), what tools were used to create it, what subjects the page is about, and where to find more information about the subject. This metadata can automatically improve the reader's experience and make it easier for users to find the web page online. [ 13 ]  A  CD  may include metadata providing information about the musicians, singers, and songwriters whose work appears on the disc. In many countries, government organizations routinely store metadata about emails, telephone calls, web pages, video traffic, IP connections, and cell phone locations. [ 14 ] Metadata means \"data about data\". Metadata is defined as the data providing information about one or more aspects of the data; it is used to summarize basic information about data that can make tracking and working with specific data easier. [ 15 ]  Some examples include: For example, a  digital image  may include metadata that describes the size of the image, its color depth, resolution, when it was created, the shutter speed, and other data. [ 16 ]  A text document's metadata may contain information about how long the document is, who the author is, when the document was written, and a short summary of the document. Metadata within web pages can also contain descriptions of page content, as well as key words linked to the content. [ 17 ]  These links are often called \"Metatags\", which were used as the primary factor in determining order for a web search until the late 1990s. [ 17 ]  The reliance on metatags in web searches was decreased in the late 1990s because of \"keyword stuffing\", [ 17 ]  whereby metatags were being largely misused to trick search engines into thinking some websites had more relevance in the search than they really did. [ 17 ] Metadata can be stored and managed in a  database , often called a  metadata registry  or  metadata repository . [ 18 ]  However, without context and a point of reference, it might be impossible to identify metadata just by looking at it. [ 19 ]  For example: by itself, a database containing several numbers, all 13 digits long could be the results of calculations or a list of numbers to plug into an  equation  –   without any other context, the numbers themselves can be perceived as the data. But if given the context that this database is a log of a book collection, those 13-digit numbers may now be identified as  ISBNs   –   information that refers to the book, but is not itself the information within the book. The term \"metadata\" was coined in 1968 by Philip Bagley, in his book \"Extension of Programming Language Concepts\" where it is clear that he uses the term in the ISO 11179 \"traditional\" sense, which is \"structural metadata\" i.e. \"data about the containers of data\"; rather than the alternative sense \"content about individual instances of data content\" or metacontent, the type of data usually found in library catalogs. [ 20 ] [ 21 ]  Since then the fields of information management, information science, information technology, librarianship, and  GIS  have widely adopted the term. In these fields, the word  metadata  is defined as \"data about data\". [ 22 ]  While this is the generally accepted definition, various disciplines have adopted their own more specific explanations and uses of the term. Slate  reported in 2013 that the United States government's interpretation of \"metadata\" could be broad, and might include message content such as the subject lines of emails. [ 23 ] While the metadata application is manifold, covering a large variety of fields, there are specialized and well-accepted models to specify types of metadata.  Bretherton  & Singley (1994) distinguish between two distinct classes: structural/control metadata and guide metadata. [ 24 ]   Structural metadata  describes the structure of database objects such as tables, columns, keys and indexes.  Guide metadata  helps humans find specific items and is usually expressed as a set of keywords in a natural language. According to  Ralph Kimball , metadata can be divided into three categories:  technical metadata  (or internal metadata),  business metadata  (or external metadata), and  process metadata . NISO  distinguishes three types of metadata: descriptive, structural, and administrative. [ 22 ]   Descriptive metadata  is typically used for discovery and identification, as information to search and locate an object, such as title, authors, subjects, keywords, and publisher.  Structural metadata  describes how the components of an object are organized. An example of structural metadata would be how pages are ordered to form chapters of a book. Finally,  administrative metadata  gives information to help manage the source. Administrative metadata refers to the technical information, such as file type, or when and how the file was created. Two sub-types of administrative metadata are rights management metadata and preservation metadata.  Rights management metadata  explains  intellectual property rights , while  preservation metadata  contains information to preserve and save a resource. [ 8 ] Statistical data repositories have their own requirements for metadata in order to describe not only the source and quality of the data [ 6 ]  but also what statistical processes were used to create the data, which is of particular importance to the statistical community in order to both validate and improve the process of statistical data production. [ 7 ] An additional type of metadata beginning to be more developed is  accessibility metadata . Accessibility metadata is not a new concept to libraries; however, advances in universal design have raised its profile. [ 25 ] : 213–214   Projects like Cloud4All and GPII identified the lack of common terminologies and models to describe the needs and preferences of users and information that fits those needs as a major gap in providing universal access solutions. [ 25 ] : 210–211   Those types of information are accessibility metadata. [ 25 ] : 214    Schema.org  has incorporated several accessibility properties based on IMS Global Access for All Information Model Data Element Specification. [ 25 ] : 214   The Wiki page  WebSchemas/Accessibility  lists several properties and their values.  While the efforts to describe and standardize the varied accessibility needs of information seekers are beginning to become more robust, their adoption into established metadata schemas has not been as developed. For example, while Dublin Core (DC)'s \"audience\" and MARC 21's \"reading level\" could be used to identify resources suitable for users with dyslexia and DC's \"format\" could be used to identify resources available in braille, audio, or large print formats, there is more work to be done. [ 25 ] : 214 Metadata (metacontent) or, more correctly, the vocabularies used to assemble metadata (metacontent) statements, is typically structured according to a standardized concept using a well-defined metadata scheme, including  metadata standards  and  metadata models . Tools such as  controlled vocabularies ,  taxonomies ,  thesauri ,  data dictionaries , and  metadata registries  can be used to apply further standardization to the metadata. Structural metadata commonality is also of paramount importance in  data model  development and in  database design . Metadata (metacontent) syntax refers to the rules created to structure the fields or elements of metadata (metacontent). [ 26 ]  A single metadata scheme may be expressed in a number of different markup or programming languages, each of which requires a different syntax. For example, Dublin Core may be expressed in plain text,  HTML ,  XML , and  RDF . [ 27 ] A common example of (guide) metacontent is the bibliographic classification, the subject, the  Dewey Decimal class number . There is always an implied statement in any \"classification\" of some object. To classify an object as, for example, Dewey class number 514 (Topology) (i.e. books having the number 514 on their spine) the implied statement is: \"<book><subject heading><514>\". This is a subject-predicate-object triple, or more importantly, a class-attribute-value triple. The first 2 elements of the triple (class, attribute) are pieces of some structural metadata having a defined semantic. The third element is a value, preferably from some controlled vocabulary, some reference (master) data. The combination of the metadata and master data elements results in a statement which is a metacontent statement i.e. \"metacontent = metadata + master data\". All of these elements can be thought of as \"vocabulary\". Both metadata and master data are vocabularies that can be assembled into metacontent statements. There are many sources of these vocabularies, both meta and master data: UML, EDIFACT, XSD, Dewey/UDC/LoC, SKOS, ISO-25964, Pantone, Linnaean Binomial Nomenclature, etc. Using controlled vocabularies for the components of metacontent statements, whether for indexing or finding, is endorsed by  ISO 25964 : \"If both the indexer and the searcher are guided to choose the same term for the same concept, then relevant documents will be retrieved.\" [ 28 ]  This is particularly relevant when considering search engines of the internet, such as Google. The process indexes pages and then matches text strings using its complex algorithm; there is no intelligence or \"inferencing\" occurring, just the illusion thereof. Metadata schemata can be hierarchical in nature where relationships exist between metadata elements and elements are nested so that parent-child relationships exist between the elements.\nAn example of a hierarchical metadata schema is the  IEEE LOM  schema, in which metadata elements may belong to a parent metadata element.\nMetadata schemata can also be one-dimensional, or linear, where each element is completely discrete from other elements and classified according to one dimension only.\nAn example of a linear metadata schema is the  Dublin Core  schema, which is one-dimensional.\nMetadata schemata are often 2 dimensional, or planar, where each element is completely discrete from other elements but classified according to 2 orthogonal dimensions. [ 29 ] The degree to which the data or metadata is structured is referred to as  \"granularity\" . \"Granularity\" refers to how much detail is provided. Metadata with a high granularity allows for deeper, more detailed, and more structured information and enables a greater level of technical manipulation. A lower level of granularity means that metadata can be created for considerably lower costs but will not provide as detailed information. The major impact of granularity is not only on creation and capture, but moreover on maintenance costs. As soon as the metadata structures become outdated, so too is the access to the referred data. Hence granularity must take into account the effort to create the metadata as well as the effort to maintain it. In all cases where the metadata schemata exceed the planar depiction, some type of hypermapping is required to enable display and view of metadata according to chosen aspect and to serve special views. Hypermapping frequently applies to layering of geographical and geological information overlays. [ 30 ] International standards apply to metadata. Much work is being accomplished in the national and international standards communities, especially  ANSI  (American National Standards Institute) and  ISO  (International Organization for Standardization) to reach a consensus on standardizing metadata and registries. The core metadata registry standard is  ISO / IEC  11179 Metadata Registries (MDR), the framework for the standard is described in ISO/IEC 11179-1:2004. [ 31 ]  A new edition of Part 1 is in its final stage for publication in 2015 or early 2016. It has been revised to align with the current edition of Part 3, ISO/IEC 11179-3:2013 [ 32 ]  which extends the MDR to support the registration of Concept Systems.\n(see  ISO/IEC 11179 ). This standard specifies a schema for recording both the meaning and technical structure of the data for unambiguous usage by humans and computers. ISO/IEC 11179 standard refers to metadata as information objects about data, or \"data about data\". In ISO/IEC 11179 Part-3, the information objects are data about Data Elements, Value Domains, and other reusable semantic and representational information objects that describe the meaning and technical details of a data item. This standard also prescribes the details for a metadata registry, and for registering and administering the information objects within a Metadata Registry. ISO/IEC 11179 Part 3 also has provisions for describing compound structures that are derivations of other data elements, for example through calculations, collections of one or more data elements, or other forms of derived data. While this standard describes itself originally as a \"data element\" registry, its purpose is to support describing and registering metadata content independently of any particular application, lending the descriptions to being discovered and reused by humans or computers in developing new applications, databases, or for analysis of data collected in accordance with the registered metadata content. This standard has become the general basis for other kinds of metadata registries, reusing and extending the registration and administration portion of the standard. The Geospatial community has a tradition of specialized  geospatial metadata  standards, particularly building on traditions of map- and image-libraries and catalogs. Formal metadata is usually essential for geospatial data, as common text-processing approaches are not applicable. The  Dublin Core  metadata terms are a set of vocabulary terms that can be used to describe resources for the purposes of discovery. The original set of 15 classic [ 33 ]  metadata terms, known as the Dublin Core Metadata Element Set [ 34 ]  are endorsed in the following standards documents: The W3C Data Catalog Vocabulary (DCAT) [ 38 ]  is an RDF vocabulary that supplements Dublin Core with classes for Dataset, Data Service, Catalog, and Catalog Record. DCAT also uses elements from FOAF, PROV-O, and OWL-Time. DCAT provides an RDF model to support the typical structure of a catalog that contains records, each describing a dataset or service. Although not a standard,  Microformat  (also mentioned in the section  metadata on the internet  below) is a web-based approach to semantic markup which seeks to re-use existing HTML/XHTML tags to convey metadata. Microformat follows XHTML and HTML standards but is not a standard in itself. One advocate of microformats,  Tantek Çelik , characterized a problem with alternative approaches: Here's a new language we want you to learn, and now you need to output these additional files on your server. It's a hassle. (Microformats) lower the barrier to entry. [ 39 ] Most common types of  computer files  can embed metadata, including documents, (e.g.  Microsoft Office  files,  OpenDocument  files,  PDF ) images, (e.g.  JPEG ,  PNG ) Video files, (e.g.  AVI ,  MP4 ) and audio files. (e.g.  WAV ,  MP3 ) Metadata may be added to files by users, but some metadata is often automatically added to files by authoring applications or by devices used to produce the files, without user intervention. While metadata in files are useful for finding them, they can be a  privacy  hazard when the files are shared. Using  metadata removal tools  to clean files before sharing them can mitigate this risk. Metadata may be written into a  digital photo  file that will identify who owns it, copyright and contact information, what brand or model of camera created the file, along with exposure information (shutter speed, f-stop, etc.) and descriptive information, such as keywords about the photo, making the file or image searchable on a computer and/or the Internet. Some metadata is created by the camera such as, color space, color channels, exposure time, and aperture (EXIF), while some is input by the photographer and/or software after downloading to a computer. [ 40 ]  Most digital cameras write metadata about the model number, shutter speed, etc., and some enable you to edit it; [ 41 ]  this functionality has been available on most Nikon DSLRs since the  Nikon D3 , on most new Canon cameras since the  Canon EOS 7D , and on most Pentax DSLRs since the Pentax K-3. Metadata can be used to make organizing in post-production easier with the use of key-wording. Filters can be used to analyze a specific set of photographs and create selections on criteria like rating or capture time. On devices with geolocation capabilities like  GPS  (smartphones in particular), the location the photo was taken from may also be included. Photographic Metadata Standards are governed by organizations that develop the following standards. They include, but are not limited to: Metadata is particularly useful in video, where information about its contents (such as transcripts of conversations and text descriptions of its scenes) is not directly understandable by a computer, but where an efficient search of the content is desirable. This is particularly useful in video applications such as  Automatic Number Plate Recognition  and Vehicle Recognition Identification software, wherein license plate data is saved and used to create reports and alerts. [ 43 ]  There are 2 sources in which video metadata is derived: (1) operational gathered metadata, that is information about the content produced, such as the type of equipment, software, date, and location; (2) human-authored metadata, to improve search engine visibility, discoverability, audience engagement, and providing advertising opportunities to video publishers. [ 44 ]  Avid's MetaSync and Adobe's Bridge are examples of professional video editing software with access to metadata. [ 45 ] Information on the times, origins and destinations of phone calls, electronic messages, instant messages, and other modes of telecommunication, as opposed to message content, is another form of metadata. Bulk collection of this  call detail record  metadata by intelligence agencies has proven controversial after disclosures by  Edward Snowden  of the fact that certain Intelligence agencies such as the  NSA  had been (and perhaps still are) keeping online metadata on millions of internet users for up to a year, regardless of whether or not they [ever] were persons of interest to the agency. Geospatial metadata relates to Geographic Information Systems (GIS) files, maps, images, and other data that is location-based. Metadata is used in GIS to document the characteristics and attributes of geographic data, such as database files and data that is developed within a GIS. It includes details like who developed the data, when it was collected, how it was processed, and what formats it's available in, and then delivers the context for the data to be used effectively. [ 46 ] Metadata can be created either by automated information processing or by manual work. Elementary metadata captured by computers can include information about when an object was created, who created it, when it was last updated, file size, and file extension. In this context an  object  refers to any of the following: A  metadata engine  collects, stores and analyzes information about data and metadata in use within a domain. [ 47 ] Data virtualization emerged in the 2000s as the new software technology to complete the virtualization \"stack\" in the enterprise. Metadata is used in data virtualization servers which are enterprise infrastructure components, alongside database and application servers. Metadata in these servers is saved as persistent repository and describe  business objects  in various enterprise systems and applications. Structural metadata commonality is also important to support data virtualization. Standardization and harmonization work has brought advantages to industry efforts to build metadata systems in the statistical community. [ 48 ] [ 49 ]  Several metadata guidelines and standards such as the European Statistics Code of Practice [ 50 ]  and ISO 17369:2013 ( Statistical Data and Metadata Exchange  or SDMX) [ 48 ]  provide key principles for how businesses, government bodies, and other entities should manage statistical data and metadata. Entities such as  Eurostat , [ 51 ]   European System of Central Banks , [ 51 ]  and the  U.S. Environmental Protection Agency [ 52 ]  have implemented these and other such standards and guidelines with the goal of improving \"efficiency when managing statistical business processes\". [ 51 ] Metadata has been used in various ways as a means of cataloging items in libraries in both digital and analog formats. Such data helps classify, aggregate, identify, and locate a particular book, DVD, magazine, or any object a library might hold in its collection. [ 53 ]  Until the 1980s, many library catalogs used 3x5 inch cards in file drawers to display a book's title, author, subject matter, and an abbreviated  alpha-numeric  string ( call number ) which indicated the physical location of the book within the library's shelves. The  Dewey Decimal System  employed by libraries for the classification of library materials by subject is an early example of metadata usage. The early paper catalog had information regarding whichever item was described on said card: title, author, subject, and a number as to where to find said item. [ 54 ]  Beginning in the 1980s and 1990s, many libraries replaced these paper file cards with computer databases. These computer databases make it much easier and faster for users to do keyword searches. Another form of older metadata collection is the use by the US Census Bureau of what is known as the \"Long Form\". The Long Form asks questions that are used to create demographic data to find patterns of distribution. [ 55 ]   Libraries  employ metadata in  library catalogues , most commonly as part of an  Integrated Library Management System . Metadata is obtained by  cataloging  resources such as books, periodicals, DVDs, web pages or digital images. This data is stored in the integrated library management system,  ILMS , using the  MARC  metadata standard. The purpose is to direct patrons to the physical or electronic location of items or areas they seek as well as to provide a description of the item/s in question. More recent and specialized instances of library metadata include the establishment of  digital libraries  including  e-print  repositories and digital image libraries. While often based on library principles, the focus on non-librarian use, especially in providing metadata, means they do not follow traditional or common cataloging approaches. Given the custom nature of included materials, metadata fields are often specially created e.g. taxonomic classification fields, location fields, keywords, or copyright statement. Standard file information such as file size and format are usually automatically included. [ 56 ]  Library operation has for decades been a key topic in efforts toward  international standardization . Standards for metadata in digital libraries include  Dublin Core ,  METS ,  MODS ,  DDI ,  DOI ,  URN ,  PREMIS  schema,  EML , and  OAI-PMH . Leading libraries in the world give hints on their metadata standards strategies. [ 57 ] [ 58 ]  The use and creation of metadata in library and information science also include scientific publications: Metadata for scientific publications is often created by journal publishers and citation databases such as  PubMed  and  Web of Science . The data contained within manuscripts or accompanying them as supplementary material is less often subject to metadata creation, [ 59 ] [ 60 ]  though they may be submitted to e.g. biomedical databases after publication. The original authors and database curators then become responsible for metadata creation, with the assistance of automated processes. Comprehensive metadata for all experimental data is the foundation of the  FAIR Guiding Principles , or the standards for ensuring research data are  findable ,  accessible ,  interoperable , and  reusable . [ 61 ] Such metadata can then be utilized, complemented, and made accessible in useful ways.  OpenAlex  is a free online index of over 200 million scientific documents that integrates and provides metadata such as sources,  citations ,  author information ,  scientific fields , and research topics. Its  API  and open source website can be used for metascience,  scientometrics , and novel tools that query this  semantic  web of  papers . [ 62 ] [ 63 ] [ 64 ]  Another project under development,  Scholia , uses the metadata of scientific publications for various visualizations and aggregation features such as providing a simple user interface summarizing literature about a specific feature of the SARS-CoV-2 virus using  Wikidata 's \"main subject\" property. [ 65 ] In research labor, transparent metadata about authors' contributions to works have been proposed – e.g. the role played in the production of the paper, the level of contribution and the responsibilities. [ 66 ] [ 67 ] Moreover, various metadata about scientific outputs can be created or complemented – for instance, scite.ai attempts to track and link citations of papers as 'Supporting', 'Mentioning' or 'Contrasting' the study. [ 68 ]  Other examples include developments of  alternative metrics [ 69 ]  – which, beyond providing help for assessment and findability, also aggregate many of the public discussions about a scientific paper on social media such as  Reddit ,  citations on Wikipedia , and  reports about the study  in the news media [ 70 ]  – and a call for showing  whether or not the original findings are confirmed or could get reproduced . [ 71 ] [ 72 ] Metadata in a museum context is the information that trained cultural documentation specialists, such as  archivists ,  librarians , museum  registrars  and  curators , create to index, structure, describe, identify, or otherwise specify works of art, architecture, cultural objects and their images. [ 73 ] [ 74 ] [ 75 ]  Descriptive metadata is most commonly used in museum contexts for object identification and resource recovery purposes. [ 74 ] Metadata is developed and applied within collecting institutions and museums in order to: Many museums and cultural heritage centers recognize that given the diversity of artworks and cultural objects, no single model or standard suffices to describe and catalog cultural works. [ 73 ] [ 74 ] [ 75 ]  For example, a sculpted Indigenous artifact could be classified as an artwork, an archaeological artifact, or an Indigenous heritage item. The early stages of standardization in archiving, description and cataloging within the museum community began in the late 1990s with the development of standards such as  Categories for the Description of Works of Art  (CDWA), Spectrum,  CIDOC Conceptual Reference Model  (CRM), Cataloging Cultural Objects (CCO) and the CDWA Lite XML schema. [ 74 ]  These standards use  HTML  and  XML  markup languages for machine processing, publication and implementation. [ 74 ]  The  Anglo-American Cataloguing Rules  (AACR), originally developed for characterizing books, have also been applied to cultural objects, works of art and architecture. [ 75 ]  Standards, such as the CCO, are integrated within a Museum's  Collections Management System  (CMS), a database through which museums are able to manage their collections, acquisitions, loans and conservation. [ 75 ]  Scholars and professionals in the field note that the \"quickly evolving landscape of standards and technologies\" creates challenges for cultural documentarians, specifically non-technically trained professionals. [ 76 ] [ page needed ]  Most collecting institutions and museums use a  relational database  to categorize cultural works and their images. [ 75 ]  Relational databases and metadata work to document and describe the complex relationships amongst cultural objects and multi-faceted works of art, as well as between objects and places, people, and artistic movements. [ 74 ] [ 75 ]  Relational database structures are also beneficial within collecting institutions and museums because they allow for archivists to make a clear distinction between cultural objects and their images; an unclear distinction could lead to confusing and inaccurate searches. [ 75 ] An object's materiality, function, and purpose, as well as the size (e.g., measurements, such as height, width, weight), storage requirements (e.g., climate-controlled environment), and focus of the museum and collection, influence the descriptive depth of the data attributed to the object by cultural documentarians. [ 75 ]  The established institutional cataloging practices, goals, and expertise of cultural documentarians and database structure also influence the information ascribed to cultural objects and the ways in which cultural objects are categorized. [ 73 ] [ 75 ]  Additionally, museums often employ standardized commercial collection management software that prescribes and limits the ways in which archivists can describe artworks and cultural objects. [ 76 ]  As well, collecting institutions and museums use  Controlled Vocabularies  to describe cultural objects and artworks in their collections. [ 74 ] [ 75 ]  Getty Vocabularies and the Library of Congress Controlled Vocabularies are reputable within the museum community and are recommended by CCO standards. [ 75 ]  Museums are encouraged to use controlled vocabularies that are contextual and relevant to their collections and enhance the functionality of their digital information systems. [ 74 ] [ 75 ]  Controlled Vocabularies are beneficial within databases because they provide a high level of consistency, improving resource retrieval. [ 74 ] [ 75 ]  Metadata structures, including controlled vocabularies, reflect the  ontologies  of the systems from which they were created. Often the processes through which cultural objects are described and categorized through metadata in museums do not reflect the perspectives of the maker communities. [ 73 ] [ 77 ] Metadata has been instrumental in the creation of digital information systems and archives within museums and has made it easier for museums to publish digital content online. This has enabled audiences who might not have had access to cultural objects due to geographic or economic barriers to have access to them. [ 74 ]  In the 2000s, as more museums have adopted archival standards and created intricate databases, discussions about  Linked Data  between museum databases have come up in the museum, archival, and library science communities. [ 76 ]  Collection Management Systems (CMS) and  Digital Asset Management  tools can be local or shared systems. [ 75 ]   Digital Humanities  scholars note many benefits of interoperability between museum databases and collections, while also acknowledging the difficulties of achieving such interoperability. [ 76 ] Problems involving metadata in  litigation  in the  United States  are becoming widespread. [ when? ]  Courts have looked at various questions involving metadata, including the  discoverability  of metadata by parties. The Federal Rules of Civil Procedure have specific rules for discovery of electronically stored information, and subsequent case law applying those rules has elucidated on the litigant's duty to produce metadata when litigating in federal court. [ 78 ]  In October 2009, the  Arizona Supreme Court  has ruled that metadata records are  public record . [ 79 ]  Document metadata have proven particularly important in legal environments in which litigation has requested metadata, that can include sensitive information detrimental to a certain party in court. Using  metadata removal tools  to \"clean\" or redact documents can mitigate the risks of unwittingly sending sensitive data. This process partially (see  data remanence ) protects law firms from potentially damaging leaking of sensitive data through  electronic discovery . Opinion polls have shown that 45% of Americans are \"not at all confident\" in the ability of social media sites to ensure their personal data is secure and 40% say that social media sites should not be able to store any information on individuals. 76% of Americans say that they are not confident that the information advertising agencies collect on them is secure and 50% say that online advertising agencies should not be allowed to record any of their information at all. [ 80 ] In Australia, the need to strengthen national security has resulted in the introduction of a new metadata storage law. [ 81 ]  This new law means that both security and policing agencies will be allowed to access up to 2 years of an individual's metadata, with the aim of making it easier to stop any terrorist attacks and serious crimes from happening. Legislative metadata has been the subject of some discussion in  law.gov  forums such as workshops held by the  Legal Information Institute  at the  Cornell Law School  on 22 and 23 March 2010. The documentation for these forums is titled, \"Suggested metadata practices for legislation and regulations\". [ 82 ] A handful of key points have been outlined by these discussions, section headings of which are listed as follows: Australian medical research pioneered the definition of metadata for applications in health care. That approach offers the first recognized attempt to adhere to international standards in medical sciences instead of defining a proprietary standard under the  World Health Organization  (WHO) umbrella. The medical community yet did not approve of the need to follow metadata standards despite research that supported these standards. [ 83 ] Research studies in the fields of  biomedicine  and  molecular biology  frequently yield large quantities of data, including results of  genome  or  meta-genome   sequencing ,  proteomics  data, and even notes or plans created during the course of research itself. [ 84 ]  Each data type involves its own variety of metadata and the processes necessary to produce these metadata. General metadata standards, such as ISA-Tab, [ 85 ]  allow researchers to create and exchange experimental metadata in consistent formats. Specific experimental approaches frequently have their own metadata standards and systems: metadata standards for  mass spectrometry  include  mzML [ 86 ]  and SPLASH, [ 87 ]  while  XML -based standards such as  PDBML [ 88 ]  and SRA XML [ 89 ]  serve as standards for macromolecular structure and sequencing data, respectively. The products of biomedical research are generally realized as peer-reviewed manuscripts and these publications are yet another source of data  (see  #Science ) . A  data warehouse  (DW) is a repository of an organization's electronically stored data. Data warehouses are designed to manage and store the data. Data warehouses differ from  business intelligence  (BI) systems because BI systems are designed to use data to create reports and analyze the information, to provide strategic guidance to management. [ 90 ]  Metadata is an important tool in how data is stored in data warehouses. The purpose of a data warehouse is to house standardized, structured, consistent, integrated, correct, \"cleaned\" and timely data, extracted from various operational systems in an organization. The extracted data are integrated in the data warehouse environment to provide an enterprise-wide perspective. Data are structured in a way to serve the reporting and analytic requirements. The design of structural metadata commonality using a  data modeling  method such as  entity-relationship model  diagramming is important in any data warehouse development effort. They detail metadata on each piece of data in the data warehouse. An essential component of a  data warehouse / business intelligence  system is the metadata and tools to manage and retrieve the metadata.  Ralph Kimball [ 91 ]  describes metadata as the DNA of the data warehouse as metadata defines the elements of the  data warehouse  and how they work together. Kimball  et al. [ 92 ]  refers to 3 main categories of metadata: Technical metadata, business metadata and process metadata. Technical metadata is primarily  definitional , while business metadata and process metadata is primarily  descriptive . The categories sometimes overlap. The  HTML  format used to define web pages allows for the inclusion of a variety of types of metadata, from basic descriptive text, dates and keywords to further advanced metadata schemes such as the  Dublin Core ,  e-GMS , and  AGLS [ 93 ]  standards. Pages and files can also be  geotagged  with  coordinates , categorized or tagged, including collaboratively such as with  folksonomies . When media has  identifiers  set or when such can be generated, information such as file tags and descriptions can be pulled or  scraped  from the Internet – for example about movies. [ 94 ]  Various online databases are aggregated and provide metadata for various data. The collaboratively built  Wikidata  has identifiers not just for media but also abstract concepts, various objects, and other entities, that can be looked up by humans and machines to retrieve useful information and to link knowledge in other knowledge bases and databases. [ 65 ] Metadata may be included in the page's header or in a separate file.  Microformats  allow metadata to be added to on-page data in a way that regular web users do not see, but computers,  web crawlers  and  search engines  can readily access. Many search engines are cautious about using metadata in their ranking algorithms because of exploitation of metadata and the practice of search engine optimization,  SEO , to improve rankings. See the  Meta element  article for further discussion. This cautious attitude may be justified as people, according to Doctorow, [ 95 ]  are not executing care and diligence when creating their own metadata and that metadata is part of a competitive environment where the metadata is used to promote the metadata creators own purposes. Studies show that search engines respond to web pages with metadata implementations, [ 96 ]  and Google has an announcement on its site showing the meta tags that its search engine understands. [ 97 ]  Enterprise search startup  Swiftype  recognizes metadata as a relevance signal that webmasters can implement for their website-specific search engine, even releasing their own extension, known as Meta Tags 2. [ 98 ] In the  broadcast  industry, metadata is linked to audio and video  broadcast media  to: This metadata can be linked to the video media thanks to the  video servers . Most major broadcast sporting events like  FIFA World Cup  or the  Olympic Games  use this metadata to distribute their video content to  TV stations  through  keywords . It is often the host broadcaster [ 99 ]  who is in charge of organizing metadata through its  International Broadcast Centre  and its video servers. This metadata is recorded with the images and entered by metadata operators ( loggers ) who associate in live metadata available in  metadata grids  through  software  (such as  Multicam(LSM)  or  IPDirector  used during the FIFA World Cup or Olympic Games). [ 100 ] [ 101 ] Metadata that describes geographic objects in electronic storage or format (such as datasets, maps, features, or documents with a geospatial component) has a history dating back to at least 1994. This class of metadata is described more fully on the  geospatial metadata  article. Ecological and environmental metadata is intended to document the \"who, what, when, where, why, and how\" of data collection for a particular study. This typically means which organization or institution collected the data, what type of data, which date(s) the data was collected, the rationale for the data collection, and the methodology used for the data collection. Metadata should be generated in a format commonly used by the most relevant science community, such as  Darwin Core ,  Ecological Metadata Language , [ 102 ]  or  Dublin Core . Metadata editing tools exist to facilitate metadata generation (e.g. Metavist, [ 103 ]   Mercury , Morpho [ 104 ] ). Metadata should describe the  provenance  of the data (where they originated, as well as any transformations the data underwent) and how to give credit for (cite) the data products. When first released in 1982, Compact Discs only contained a Table Of Contents (TOC) with the number of tracks on the disc and their length in samples. [ 105 ] [ 106 ]  Fourteen years later in 1996, a revision of the  CD Red Book  standard added  CD-Text  to carry additional metadata. [ 107 ]  But CD-Text was not widely adopted. Shortly thereafter, it became common for personal computers to retrieve metadata from external sources (e.g.  CDDB ,  Gracenote ) based on the TOC. Digital  audio  formats such as  digital audio files  superseded music formats such as  cassette tapes  and  CDs  in the 2000s. Digital audio files could be labeled with more information than could be contained in just the file name. That descriptive information is called the  audio tag  or audio metadata in general. Computer programs specializing in adding or modifying this information are called  tag editors . Metadata can be used to name, describe, catalog, and indicate ownership or copyright for a digital audio file, and its presence makes it much easier to locate a specific audio file within a group, typically through use of a search engine that accesses the metadata. As different digital audio formats were developed, attempts were made to standardize a specific location within the digital files where this information could be stored. As a result, almost all digital audio formats, including  mp3 , broadcast wav, and  AIFF  files, have similar standardized locations that can be populated with metadata. The metadata for compressed and uncompressed digital music is often encoded in the  ID3  tag. Common editors such as  TagLib  support MP3, Ogg Vorbis, FLAC, MPC, Speex, WavPack TrueAudio, WAV, AIFF, MP4, and ASF file formats. With the availability of  cloud  applications, which include those to add metadata to content, metadata is increasingly available over the Internet. Metadata can be stored either  internally , [ 108 ]  in the same file or structure as the data (this is also called  embedded metadata ), or  externally , in a separate file or field from the described data. A data repository typically stores the metadata  detached  from the data but can be designed to support embedded metadata approaches. Each option has advantages and disadvantages: Metadata can be stored in either human-readable or binary form. Storing metadata in a human-readable format such as  XML  can be useful because users can understand and edit it without specialized tools. [ 109 ]  However, text-based formats are rarely optimized for storage capacity, communication time, or processing speed. A binary metadata format enables efficiency in all these respects, but requires special software to convert the binary information into human-readable content. Each relational database system has its own mechanisms for storing metadata. Examples of relational-database metadata include: In database terminology, this set of metadata is referred to as the  catalog . The  SQL  standard specifies a uniform means to access the catalog, called the  information schema , but not all databases implement it, even if they implement other aspects of the SQL standard. For an example of database-specific metadata access methods, see  Oracle metadata . Programmatic access to metadata is possible using APIs such as  JDBC , or SchemaCrawler. [ 110 ] One of the first satirical examinations of the concept of Metadata as we understand it today is American science fiction author  Hal Draper 's short story, \" MS Fnd in a Lbry \" (1961). Here, the knowledge of all Mankind is condensed into an object the size of a desk drawer, however, the magnitude of the metadata (e.g. catalog of catalogs of... , as well as indexes and histories) eventually leads to dire yet humorous consequences for the human race. The story prefigures the modern consequences of allowing metadata to become more important than the real data it is concerned with, and the risks inherent in that eventuality as a cautionary tale."
  },
  {
    "id": 37,
    "title": "Database",
    "content": "In  computing , a  database  is an organized collection of  data  or a type of  data store  based on the use of a  database management system  ( DBMS ), the  software  that interacts with  end users ,  applications , and the database itself to capture and analyze the data. The DBMS additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a  database system . Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database. Small databases can be stored on a  file system , while large databases are hosted on  computer clusters  or  cloud storage . The  design of databases  spans formal techniques and practical considerations, including  data modeling , efficient data representation and storage,  query languages ,  security  and  privacy  of sensitive data, and  distributed computing  issues, including supporting  concurrent  access and  fault tolerance . Computer scientists  may classify database management systems according to the  database models  that they support.  Relational databases  became dominant in the 1980s. These model data as  rows  and  columns  in a series of  tables , and the vast majority use  SQL  for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as  NoSQL , because they use different  query languages . Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of  computer software  that allows  users  to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized. Because of the close relationship between them, the term \"database\" is often used casually to refer to both a database and the DBMS used to manipulate it. Outside the world of professional  information technology , the term  database  is often used to refer to any collection of related data (such as a  spreadsheet  or a card index) as size and usage requirements typically necessitate use of a database management system. [ 1 ] Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups: Both a database and its DBMS conform to the principles of a particular  database model . [ 5 ]  \"Database system\" refers collectively to the database model, database management system, and database. [ 6 ] Physically, database  servers  are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually  multiprocessor  computers, with generous memory and  RAID  disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume  transaction processing environments . DBMSs are found at the heart of most  database applications . DBMSs may be built around a custom  multitasking   kernel  with built-in  networking  support, but modern DBMSs typically rely on a standard  operating system  to provide these functions. [ citation needed ] Since DBMSs comprise a significant  market , computer and storage vendors often take into account DBMS requirements in their own development plans. [ 7 ] Databases and DBMSs can be categorized according to the database model(s) that they support (such as  relational  or  XML ), the type(s) of computer they run on (from a  server cluster  to a  mobile phone ), the  query language (s) used to access the database (such as SQL or  XQuery ), and their internal engineering, which affects performance,  scalability , resilience, and security. The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of  processors ,  computer memory ,  computer storage , and  computer networks . The concept of a database was made possible by the emergence of direct access  storage media  such as  magnetic disks , which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on  magnetic tape . The subsequent development of database technology can be divided into three eras based on data model or structure:  navigational , [ 8 ]  SQL/ relational , and post-relational. The two main early navigational  data models  were the  hierarchical model  and the  CODASYL  model ( network model ). These were characterized by the use of  pointers  (often physical disk addresses) to follow relationships from one record to another. The  relational model , first proposed in 1970 by  Edgar F. Codd , departed from this tradition by insisting that  applications  should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of  entity . Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale  data processing  applications, and as of 2018 [update]  they remain dominant:  IBM Db2 ,  Oracle ,  MySQL , and  Microsoft SQL Server  are the most searched  DBMS . [ 9 ]  The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models. [ citation needed ] Object databases  were developed in the 1980s to overcome the inconvenience of  object–relational impedance mismatch , which led to the coining of the term \"post-relational\" and also the development of hybrid  object–relational databases . The next generation of post-relational databases in the late 2000s became known as  NoSQL  databases, introducing fast  key–value stores  and  document-oriented databases . A competing \"next generation\" known as  NewSQL  databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs. The introduction of the term  database  coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily  batch processing . The  Oxford English Dictionary  cites a 1962 report by the  System Development Corporation  of California as the first to use the term \"data-base\" in a specific technical sense. [ 10 ] As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and  Charles Bachman , author of one such product, the  Integrated Data Store  (IDS), founded the  Database Task Group  within  CODASYL , the group responsible for the creation and standardization of  COBOL . In 1971, the Database Task Group delivered their standard, which generally became known as the  CODASYL approach , and soon a number of commercial products based on this approach entered the market. The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods: Later systems added  B-trees  to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational  API ). However, CODASYL databases were complex and required significant training and effort to produce useful applications. IBM  also had its own DBMS in 1966, known as  Information Management System  (IMS). IMS was a development of software written for the  Apollo program  on the  System/360 . IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973  Turing Award  presentation  The Programmer as Navigator . IMS is classified by IBM as a  hierarchical database . IDMS and  Cincom Systems '  TOTAL  databases are classified as network databases. IMS remains in use as of 2014 [update] . [ 11 ] Edgar F. Codd  worked at IBM in  San Jose, California , in one of their offshoot offices that were primarily involved in the development of  hard disk  systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking  A Relational Model of Data for Large Shared Data Banks . [ 12 ] In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of  linked list  of free-form records as in CODASYL, Codd's idea was to organize the data as a number of \" tables \", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a   primary key  by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of  relational calculus  (from which the model takes its name). Splitting the data into a set of normalized tables (or  relations ) aimed to ensure that each \"fact\" was only stored once, thus simplifying update operations. Virtual tables called  views  could present the data in different ways for different users, but views could not be directly updated. Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based. The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of  first-order predicate calculus ; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit. In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys. For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be  normalized  into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided. As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic. Codd's paper was picked up by two people at Berkeley, Eugene Wong and  Michael Stonebraker . They started a project known as  INGRES  using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to  System R  in a number of ways, including the use of a \"language\" for  data access , known as  QUEL . Over time, INGRES moved to the emerging SQL standard. IBM itself did one test implementation of the relational model,  PRTV , and a production one,  Business System 12 , both now discontinued.  Honeywell  wrote  MRDS  for  Multics , and now there are two new implementations:  Alphora Dataphor  and Rel. Most other DBMS implementations usually called  relational  are actually SQL DBMSs. In 1970, the University of Michigan began development of the  MICRO Information Management System [ 13 ]  based on  D.L. Childs ' Set-Theoretic Data model. [ 14 ] [ 15 ] [ 16 ]  MICRO was used to manage very large data sets by the  US Department of Labor , the  U.S. Environmental Protection Agency , and researchers from the  University of Alberta , the  University of Michigan , and  Wayne State University . It ran on IBM mainframe computers using the  Michigan Terminal System . [ 17 ]  The system remained in production until 1998. In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were  IBM System/38 , the early offering of  Teradata , and the  Britton Lee, Inc.  database machine. Another approach to hardware support for database management was  ICL 's  CAFS  accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like  Netezza  and Oracle ( Exadata ). IBM started working on a prototype system loosely based on Codd's concepts as  System R  in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized  query language  – SQL [ citation needed ]  – had been added. Codd's ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as  SQL/DS , and, later,  Database 2  ( IBM Db2 ). Larry Ellison 's Oracle Database (or more simply,  Oracle ) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it was not until Oracle Version 2 when Ellison beat IBM to market in 1979. [ 18 ] Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as  PostgreSQL . PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary  data store , as do many large companies and financial institutions). In Sweden, Codd's paper was also read and  Mimer SQL  was developed in the mid-1970s at  Uppsala University . In 1984, this project was consolidated into an independent enterprise. Another data model, the  entity–relationship model , emerged in 1976 and gained popularity for  database design  as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a  data modeling  construct for the relational model, and the difference between the two has become irrelevant. [ citation needed ] The 1980s ushered in the age of  desktop computing . The new computers empowered their users with spreadsheets like  Lotus 1-2-3  and database software like  dBASE . The dBASE product was lightweight and easy for any computer user to understand out of the box.  C. Wayne Ratliff , the creator of dBASE, stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\" [ 19 ]  dBASE was one of the top selling software titles in the 1980s and early 1990s. The 1990s, along with a rise in  object-oriented programming , saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as  objects . That is to say that if a person's data were in a database, that person's attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be related to objects and their  attributes  and not to individual fields. [ 20 ]  The term \" object–relational impedance mismatch \" described the inconvenience of translating between programmed objects and database tables.  Object databases  and  object–relational databases  attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as  object–relational mappings  (ORMs) attempt to solve the same problem. XML databases  are a type of structured document-oriented database that allows querying based on  XML  document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records. NoSQL  databases are often very fast, do not require fixed table schemas, avoid join operations by storing  denormalized  data, and are designed to  scale horizontally . In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the  CAP theorem , it is impossible for a  distributed system  to simultaneously provide  consistency , availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called  eventual consistency  to provide both availability and partition tolerance guarantees with a reduced level of data consistency. NewSQL  is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the  ACID  guarantees of a traditional database system. Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see  Enterprise software ). Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized  library  systems,  flight reservation systems , computerized  parts inventory systems , and many  content management systems  that store  websites  as collections of webpages in a database. One way to classify databases involves the type of their contents, for example:  bibliographic , document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases. Connolly and Begg define database management system (DBMS) as a \"software system that enables users to define, create, maintain and control access to the database.\" [ 24 ]  Examples of DBMS's include  MySQL ,  MariaDB ,  PostgreSQL ,  Microsoft SQL Server ,  Oracle Database , and  Microsoft Access . The DBMS acronym is sometimes extended to indicate the underlying  database model , with RDBMS for the  relational , OODBMS for the  object (oriented)  and ORDBMS for the  object–relational model . Other extensions can indicate some other characteristics, such as DDBMS for a distributed database management systems. The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data.  Codd  proposed the following functions and services a fully-fledged general purpose DBMS should provide: [ 25 ] It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities. [ 26 ]  The core part of the DBMS interacting between the database and the application interface sometimes referred to as the  database engine . Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as  embedded databases  the need to target zero-administration is paramount. The large major enterprise DBMSs have tended to increase in size and functionality and have involved up to thousands of human years of development effort throughout their lifetime. [ a ] Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via  terminals  or terminal emulation software. The  client–server architecture  was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a  multitier architecture  incorporating  application servers  and  web servers  with the end user interface via a  web browser  with the database only directly connected to the adjacent tier. [ 28 ] A general-purpose DBMS will provide public  application programming interfaces  (API) and optionally a processor for  database languages  such as  SQL  to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an  email  system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email. External interaction with the database will be via an application program that interfaces with the DBMS. [ 29 ]  This can range from a  database tool  that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information. A  programmer  will  code  interactions to the database (sometimes referred to as a  datasource ) via an  application program interface  (API) or via a  database language . The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a  preprocessor  or a bridging API. Some API's aim to be database independent,  ODBC  being a commonly known example. Other common API's include  JDBC  and  ADO.NET . Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as  sublanguages : Database languages are specific to a particular data model. Notable examples include: A database language may also incorporate features like: Database storage is the container of the physical materialization of a database. It comprises the  internal  (physical)  level  in the database architecture. It also contains all the information needed (e.g.,  metadata , \"data about the data\", and internal  data structures ) to reconstruct the  conceptual level  and  external level  from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future  preservation  and longevity of the database. [ 33 ]  Putting data into permanent storage is generally the responsibility of the  database engine  a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems'  file systems  as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database). Some DBMSs support specifying which  character encoding  was used to store data, so multiple encodings can be used in the same database. Various low-level  database storage structures  are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also  column-oriented  and  correlation databases . Often storage redundancy is employed to increase performance. A common example is storing  materialized views , which consist of frequently needed  external views  or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy. Occasionally a database employs storage redundancy by  database objects  replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated. With  data virtualization , the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach. [ 34 ] Database security  deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program). Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces. This may be managed directly on an individual basis, or by the assignment of individuals and  privileges  to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases. Data security  in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see  physical security ), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see  data encryption ). Change and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic  database audit  later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this in the database. Monitoring can be set up to attempt to detect security breaches. Therefore, organizations must take database security seriously because of the many benefits it provides. Organizations will be safeguarded from security breaches and hacking activities like firewall intrusion, virus spread, and ransom ware. This helps in protecting the company's essential information, which cannot be shared with outsiders at any cause. [ 35 ] Database transactions  can be used to introduce some level of  fault tolerance  and  data integrity  after recovery from a  crash . A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a  lock , etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). The acronym  ACID  describes some ideal properties of a database transaction:  atomicity ,  consistency ,  isolation , and  durability . A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different  total costs of ownership  or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs. After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed  user interfaces  to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.). When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation. After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned ( tuning ) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc. Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state. Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the * Abstract interpretation  framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. [ 36 ]  The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc. Other DBMS features might include: Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as \" DevOps  for database\". [ 37 ] The first task of a database designer is to produce a  conceptual data model  that reflects the structure of the information to be held in the database. A common approach to this is to develop an  entity–relationship model , often with the aid of drawing tools. Another popular approach is the  Unified Modeling Language . A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes. Producing the conceptual data model sometimes involves input from  business processes , or the analysis of  workflow  in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data. Having produced a conceptual data model that users are happy with, the next stage is to translate this into a  schema  that implements the relevant data structures within the database. This process is often called logical database design, and the output is a  logical data model  expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms  data model  and  database model  are often used interchangeably, but in this article we use  data model  for the design of a specific database, and  database model  for the modeling notation used to express that design). The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as  normalization . The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency. The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called  physical database design , and the output is the  physical data model . A key goal during this stage is  data independence , meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS. Another aspect of physical database design is security. It involves both defining  access control  to database objects as well as defining security levels and methods for the data itself. A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner  data  can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format. Common logical data models for databases include: An object–relational database combines the two related structures. Physical data models  include: Other models include: Specialized models are optimized for particular types of data: A database management system provides three views of the database data: While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the  human resources  department. Thus different departments need different  views  of the company's database. The three-level database architecture relates to the concept of  data independence  which was one of the major initial driving forces of the relational model. [ 39 ]  The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance. The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types. Database technology has been an active research topic since the 1960s, both in  academia  and in the research and development groups of companies (for example  IBM Research ). Research activity includes  theory  and development of  prototypes . Notable research topics have included  models , the atomic transaction concept, related  concurrency control  techniques, query languages and  query optimization  methods,  RAID , and more. The database research area has several dedicated  academic journals  (for example,  ACM Transactions on Database Systems -TODS,  Data and Knowledge Engineering -DKE) and annual  conferences  (e.g.,  ACM   SIGMOD , ACM  PODS ,  VLDB ,  IEEE  ICDE)."
  },
  {
    "id": 38,
    "title": "Information overload",
    "content": "Information overload  (also known as  infobesity , [ 1 ] [ 2 ]   infoxication , [ 3 ]  or  information anxiety [ 4 ] ) is the difficulty in understanding an issue and  effectively making decisions  when one has  too much information  (TMI) about that issue, [ 5 ]  and is generally associated with the excessive quantity of daily information. [ 6 ]  The term \"information overload\" was first used as early as 1962 by scholars in management and information studies, including in Bertram Gross' 1964 book,  The Managing of Organizations, [ 7 ] [ 8 ]  and was further popularized by  Alvin Toffler  in his bestselling 1970 book  Future Shock . [ 9 ]  Speier et al. (1999) said that if input exceeds the processing capacity, information overload occurs, which is likely to reduce the quality of the decisions. [ 10 ] In a newer definition, Roetzel (2019) focuses on time and resources aspects. He states that when a decision-maker is given many sets of information, such as complexity, amount, and contradiction, the quality of its decision is decreased because of the individual's limitation of scarce resources to process all the information and optimally make the best decision. [ 11 ] The advent of modern  information technology  has been a primary driver of information overload on multiple fronts: in quantity produced, ease of dissemination, and breadth of the audience reached. Longstanding technological factors have been further intensified by the rise of  social media  including the  attention economy , which facilitates  attention theft . [ 12 ] [ 13 ]  In the age of connective digital technologies,  informatics , the  Internet culture  (or the digital culture), information overload is associated with over-exposure, excessive viewing of information, and input abundance of information and data. Even though information overload is linked to digital cultures and technologies,  Ann Blair  notes that the term itself predates modern technologies, as indications of information overload were apparent when humans began collecting manuscripts, collecting, recording, and preserving information. [ 14 ]  One of the first social scientists to notice the negative effects of information overload was the sociologist  Georg Simmel  (1858–1918), who hypothesized that the overload of sensations in the modern urban world caused city dwellers to become jaded and interfered with their ability to react to new situations. [ 15 ]  The social psychologist  Stanley Milgram  (1933–1984) later used the concept of information overload to explain  bystander behavior . Psychologists have recognized for many years that humans have a limited capacity to store current information in memory. Psychologist  George Armitage Miller  was very influential in this regard, proposing that people can process about seven chunks of information at a time. Miller says that under overload conditions, people become confused and are likely to make poorer decisions based on the information they have received as opposed to making informed ones. A quite early example of the term \"information overload\" can be found in an article by Jacob Jacoby, Donald Speller and Carol Kohn Berning, who conducted an experiment on 192 housewives which was said to confirm the hypothesis that more information about brands would lead to poorer  decision making . Long before that, the concept was introduced by Diderot, although it was not by the term \"information overload\": As long as the centuries continue to unfold, the number of books will grow continually, and one can predict that a time will come when it will be almost as difficult to learn anything from books as from the direct study of the whole universe. It will be almost as convenient to search for some bit of truth concealed in nature as it will be to find it hidden away in an immense multitude of bound volumes. In the internet age, the term \"information overload\" has evolved into phrases such as \"information glut\", \"data smog\", and \"data glut\" ( Data Smog , Shenk, 1997). [ 16 ]   In his abstract, Kazi Mostak Gausul Hoq commented that people often experience an \"information glut\" whenever they struggle with locating information from print, online, or digital sources. [ 17 ]  What was once a term grounded in  cognitive psychology  has evolved into a rich metaphor used outside the world of academia. Information overload has been documented throughout periods where advances in technology have increased a production of information. As early as the 3rd or 4th century BC, people regarded information overload with disapproval. Around this time, in  Ecclesiastes  12:12, the passage revealed the writer's comment \"of making books there is no end\" and in the 1st century AD,  Seneca the Elder  commented, that \"the abundance of books is distraction\". In 1255, the Dominican Vincent of Beauvais, also commented on the flood of information: \"the multitude of books, the shortness of time and the slipperiness of memory.\" [ 14 ]  Similar complaints around the growth of books were also mentioned in China. There were also information enthusiasts. The  Library of Alexandria  was established around the 3rd century BCE or 1st century Rome, which introduced acts of preserving historical artifacts. Museums and libraries established universal grounds of preserving the past for the future, but much like books, libraries were only granted with limited access. Renaissance humanists always had a desire to preserve their writings and observations, [ 14 ]  but were only able to record ancient texts by hand because books were expensive and only the privileged and educated could afford them. Humans experience an overload in information by excessively copying ancient manuscripts and replicating artifacts, creating libraries and museums that have remained in the present. [ 14 ]  Around 1453 AD,  Johannes Gutenberg  invented the  printing press  and this marked another period of information proliferation. As a result of lowering production costs, generation of printed materials ranging from  pamphlets ,  manuscripts  to books were made available to the average person. Following Gutenberg's invention, the introduction of mass printing began in Western Europe. Information overload was often experienced by the affluent, but the circulation of books were becoming rapidly printed and available at a lower cost, allowing the educated to purchase books. Information became recordable, by hand, and could be easily memorized for future storage and accessibility. This era marked a time where inventive methods were established to practice information accumulation. Aside from printing books and passage recording, encyclopedias and alphabetical indexes were introduced, enabling people to save and bookmark information for retrieval. These practices marked both present and future acts of information processing. Swiss scientist  Conrad Gessner  commented on the increasing number of libraries and printed books, [ 14 ]  and was most likely the first academic who discussed the consequences of information overload as he observed how \"unmanageable\" information came to be after the creation of the printing press. [ 18 ] Blair notes that while scholars were elated with the number of books available to them, they also later experienced fatigue with the amount of excessive information that was readily available and overpopulated them.  Scholars  complained about the abundance of information for a variety of reasons, such as the diminishing quality of text as  printers  rushed to print manuscripts and the supply of new information being distracting and difficult to manage. Erasmus, one of the many recognized humanists of the 16th century asked, \"Is there anywhere on earth exempt from these swarms of new books?\". [ 19 ] Many grew concerned with the rise of books in Europe, especially in England, France, and Germany. From 1750 to 1800, there was a 150% increase in the production of books. In 1795, German bookseller and publisher Johann Georg Heinzmann said \"no nation printed as much as the Germans\" and expressed concern about Germans reading ideas and no longer creating original thoughts and ideas. [ 20 ] To combat information overload, scholars developed their own information records for easier and simply archival access and retrieval. Modern Europe compilers used paper and glue to cut specific notes and passages from a book and pasted them to a new sheet for storage.  Carl Linnaeus  developed paper slips, often called his botanical paper slips, from 1767 to 1773, to record his observations. Blair argues that these botanical paper slips gave birth to the \"taxonomical system\" that has endured to the present, influencing both the mass inventions of the index card and the library card catalog. [ 19 ] In his book,  The Information: A History, A Theory, A Flood,  published in 2011, author  James Gleick  notes that engineers began taking note of the concept of information, quickly associated it in a technical sense: information was both quantifiable and measurable. He discusses how information theory was created to first bridge mathematics, engineering, and computing together, creating an information code between the fields. English speakers from Europe often equated \"computer science\" to \" informatique ,  informatica , and  Informatik \". [ 21 ]  This leads to the idea that all information can be saved and stored on computers, even if information experiences entropy. But at the same time, the term information, and its many definitions have changed. [ citation needed ] In the second half of the 20th century, advances in computer and information technology led to the creation of the  Internet . In the modern  Information Age , information overload is experienced as distracting and unmanageable information such as  email spam , email notifications,  instant messages ,  Tweets  and Facebook updates in the context of the work environment. [ 22 ]   Social media  has resulted in \"social information overload\", which can occur on sites like Facebook, and technology is changing to serve our social culture. In today's society, day-to-day activities increasingly involve the technological world where information technology exacerbates the number of interruptions that occur in the work environment. [ 23 ]  Management may be even more disrupted in their decision making, and may result in more poor decisions. Thus, the  PIECES  framework mentions information overload as a potential problem in existing information systems. [ 24 ] As the world moves into a new era of  globalization , an increasing number of people connect to the internet to conduct their own research [ 25 ]  and are given the ability to contribute to publicly accessible data. This has elevated the risk for the spread of misinformation. [ according to whom? ] In a 2018 literature review, Roetzel indicates that information overload can be seen as a virus—spreading through (social) media and news networks. [ 11 ] The latest research hypothesizes that information overload is a multilevel phenomenon, i.e., there are different mechanisms responsible for its emergence at the individual, group, and the whole society levels, however, these levels are interlinked.  [ 26 ] In a piece published by  Slate , Vaughan Bell argues that \"Worries about information overload are as old as information itself\" [ 18 ]  because each generation and century will inevitably experience a significant impact with technology. In the 21st century, Frank Furedi describes how an overload in information is metaphorically expressed as a flood, which is an indication that humanity is being \"drowned\" by the waves of data coming at it. [ 27 ]  This includes how the human brain continues to process information whether digitally or not. Information overload can lead to \"information anxiety\", which is the gap between the information that is understood and the information that it is perceived must be understood. The phenomenon of information overload is connected to the field of  information technology  (IT). IT corporate management implements training to \"improve the productivity of knowledge workers\". Ali F. Farhoomand and Don H. Drury note that employees often experience an overload in information whenever they have difficulty absorbing and assimilating the information they receive to efficiently complete a task because they feel burdened, stressed, and overwhelmed. [ 28 ] At New York's Web 2.0 Expo in 2008,  Clay Shirky 's speech indicated that information overload in the modern age is a consequence of a deeper problem, which he calls \"filter failure\", [ 29 ]  where humans continue to overshare information with each other. This is due to the rapid rise of apps and unlimited wireless access. In the modern  information age , information overload is experienced as distracting and unmanageable information such as  email spam , email notifications,  instant messages ,  Tweets , and Facebook updates in the context of the work environment.  Social media  has resulted in \"social information overload\", which can occur on sites like Facebook, and technology is changing to serve our social culture. As people view increasing amounts of information in the form of news stories, emails, blog posts, Facebook statuses,  Tweets ,  Tumblr  posts and other new sources of information, they become their own editors,  gatekeepers , and aggregators of information. [ 30 ]  Social media platforms create a distraction as users attention spans are challenged once they enter an online platform. One concern in this field is that massive amounts of information can be distracting and negatively impact productivity and  decision-making  and  cognitive control . Another concern is the \"contamination\" of useful information with information that might not be entirely accurate ( information pollution ). The general causes of information overload include: Email remains a major source of information overload, as people struggle to keep up with the rate of incoming messages. As well as filtering out unsolicited commercial messages ( spam ), users also have to contend with the growing use of  email attachments  in the form of lengthy reports, presentations, and media files. [ 31 ] A December 2007  New York Times  blog post described email as \"a $650 billion drag on the economy\", [ 32 ]  and the  New York Times  reported in April 2008 that \"email has become the bane of some people's professional lives\" due to information overload, yet \"none of [the current wave of high-profile Internet startups focused on email] really eliminates the problem of email overload because none helps us prepare replies\". [ 33 ] In January 2011, Eve Tahmincioglu, a writer for  NBC News , wrote an article titled \"It's Time to Deal With That Overflowing Inbox\". Compiling statistics with commentary, she reported that there were 294 billion emails sent each day in 2010, up from 50 billion in 2009. Quoted in the article, workplace productivity expert Marsha Egan stated that people need to differentiate between working on email and sorting through it. This meant that rather than responding to every email right away, users should delete unnecessary emails and sort the others into action or reference folders first. Egan then went on to say \"We are more wired than ever before, and as a result need to be more mindful of managing email or it will end up managing us.\" [ 34 ] The Daily Telegraph  quoted  Nicholas Carr , former executive editor of the  Harvard Business Review  and the author of  The Shallows: What The Internet Is Doing To Our Brains , as saying that email exploits a basic human instinct to search for new information, causing people to become addicted to \"mindlessly pressing levers in the hope of receiving a pellet of social or intellectual nourishment\". His concern is shared by  Eric Schmidt , chief executive of  Google , who stated that \"instantaneous devices\" and the abundance of information people are exposed to through email and other technology-based sources could be having an impact on the thought process, obstructing deep thinking, understanding, impeding the formation of memories and making learning more difficult. This condition of \"cognitive overload\" results in diminished information retaining ability and failing to connect remembrances to experiences stored in the long-term memory, leaving thoughts \"thin and scattered\". [ 35 ]  This is also manifest in the education process. [ 36 ] In addition to email, the  World Wide Web  has provided access to billions of pages of information. In many offices, workers are given unrestricted access to the Web, allowing them to manage their own research. The use of  search engines  helps users to find information quickly. However, information published online may not always be reliable, due to the lack of authority-approval or a compulsory accuracy check before publication. Internet information lacks credibility as the Web's search engines do not have the abilities to filter and manage information and misinformation. [ 37 ]  This results in people having to cross-check what they read before using it for decision-making, which takes up more time. [ citation needed ] Viktor Mayer-Schönberger , author of  Delete: The Virtue of Forgetting in the Digital Age,  argues that everyone can be a \"participant\" on the Internet, where they are all senders and receivers of information. [ 38 ]  On the Internet, trails of information are left behind, allowing other Internet participants to share and exchange information. Information becomes difficult to control on the Internet. The  BBC  reports that \"every day, the information we send and receive online – whether that's checking emails or searching the internet – amount to over 2.5 quintillion bytes of data.\" [ 39 ] Social media  are applications and websites with an online community where users create and share content with each other, and it adds to the problem of information overload because so many people have access to it. [ 40 ]  It presents many different views and outlooks on subject matters so that one may have difficulty taking it all in and drawing a clear conclusion. [ 41 ]  Information overload may not be the core reason for people's anxieties about the amount of information they receive in their daily lives. Instead, information overload can be considered situational. Social media users tend to feel less overloaded by information when using their personal profiles, rather than when their work institutions expect individuals to gather a mass of information. Most people see information through social media in their lives as an aid to help manage their day-to-day activities and not an overload. [ 42 ]  Depending on what social media platform is being used, it may be easier or harder to stay up to date on posts from people. Facebook users who post and read more than others tend to be able to keep up. On the other hand, Twitter users who post and read a lot of tweets still feel like it is too much information (or none of it is interesting enough). [ 11 ]  Another problem with social media is that many people create a living by creating content for either their own or someone else's platform, which can create for creators to publish an overload of content. In the context of searching for information, researchers have identified two forms of information overload:  outcome overload  where there are too many sources of information and  textual overload   where the individual sources are too long. This form of information overload may cause searchers to be less systematic. Disillusionment when a search is more challenging than expected may result in an individual being less able to search effectively. Information overload when searching can result in a  satisficing  strategy. [ 43 ] : 7 Savolainen identifies  filtering  and  withdrawal  as common responses to information. Filtering involves quickly working out whether a particular piece of information, such as an email, can be ignored based on certain criteria. Withdrawal refers to limiting the number of sources of information with which one interacts. They distinguish between \"pull\" and \"push\" sources of information, a \"pull\" source being one where one seeks out relevant information, a \"push\" source one where others decide what information might be interesting. They note that \"pull\" sources can avoid information overload but by only \"pulling\" information one risks missing important information. [ 44 ] There have been many solutions proposed for how to mitigate information overload. Research examining how people seek to control an overloaded environment has shown that people purposefully using different coping strategies. [ 45 ] [ 46 ] [ 47 ]  In general, overload coping strategy consists of two excluding (ignoring and filtering) and two including (customizing and saving) approaches. [ 47 ] [ 46 ]  Excluding approach focuses on managing the quantity of information, while including approach is geared towards complexity management. Johnson advises  discipline  which helps mitigate interruptions and for the elimination of push or notifications. He explains that notifications pull people's attentions away from their work and into social networks and emails. He also advises that people stop using their iPhones as alarm clocks which means that the phone is the first thing that people will see when they wake up leading to people checking their email right away. [ 51 ] Clay Shirky  states: [ 29 ] What we're dealing with now is not the problem of information overload, because we're always dealing (and always have been dealing) with information overload... Thinking about information overload isn't accurately describing the problem; thinking about filter failure is. Consider the use of Internet applications and add-ons such as the  Inbox Pause  add-on for  Gmail . [ 52 ]  This add-on does not reduce the number of emails that people get but it pauses the inbox. Burkeman in his article talks about the feeling of being in control is the way to deal with information overload which might involve self-deception. He advises to fight irrationality with irrationality by using add-ons that allow you to pause your inbox or produce other results. Reducing large amounts of information is key. Dealing with IO from a social network site such as Facebook, a study done by  Humboldt University [ 53 ]  showed some strategies that students take to try and alleviate IO while using Facebook. Some of these strategies included: Prioritizing updates from friends who were physically farther away in other countries, hiding updates from less-prioritized friends, deleting people from their friends list, narrowing the amount of personal information shared, and deactivating the Facebook account. Decision makers performing complex tasks have little if any excess cognitive capacity. Narrowing one's attention as a result of the interruption is likely to result in the loss of information cues, some of which may be relevant to completing the task. Under these circumstances, performance is likely to deteriorate. As the number or intensity of the distractions/interruptions increases, the decision maker's cognitive capacity is exceeded, and performance deteriorates more severely. In addition to reducing the number of possible cues attended to, more severe distractions/interruptions may encourage decision-makers to use heuristics, take shortcuts, or opt for a  satisficing decision , resulting in lower decision accuracy. Some  cognitive scientists  and graphic designers have emphasized the distinction between raw information and information in a form that can be used in thinking. In this view, information overload may be better viewed as organization underload. That is, they suggest that the problem is not so much the volume of information but the fact that it cannot be discerned how to use it well in the raw or biased form it is presented. Authors who have taken this view include graphic artist and architect  Richard Saul Wurman  and statistician and cognitive scientist  Edward Tufte . Wurman uses the term \"information anxiety\" to describe humanity's attitude toward the volume of information in general and their limitations in processing it. [ 55 ]  Tufte primarily focuses on quantitative information and explores ways to organize large complex datasets visually to facilitate clear thinking. Tufte's writing is important in such fields as information design and visual literacy, [ 56 ]  which deal with the visual communication of information. Tufte coined the term \"chartjunk\" to refer to useless, non-informative, or information-obscuring elements of quantitative information displays, such as the use of graphics to overemphasize the importance of certain pieces of data or information. [ 57 ] In a study conducted by Soucek and Moser (2010), [ 58 ]  they investigated what impact a training intervention on how to cope with information overload would have on employees. They found that the training intervention did have a positive impact on IO, especially on those who struggled with work impairment and media usage, and employees who had a higher amount of incoming emails. [ 58 ] Recent research suggests that an \" attention economy \" of sorts will naturally emerge from information overload, [ 59 ]  allowing Internet users greater control over their online experience with particular regard to communication mediums such as email and instant messaging. This could involve some sort of cost being attached to email messages. For example, managers charging a small fee for every email received – e.g. $1.00 – which the sender must pay from their budget. The aim of such charging is to force the sender to consider the necessity of the interruption. However, such a suggestion undermines the entire basis of the popularity of email, namely that emails are free of charge to send. Economics often assumes that people are rational in that they have the knowledge of their preferences and an ability to look for the best possible ways to maximize their preferences. People are seen as selfish and focus on what pleases them. Looking at various parts on their own results in the negligence of the other parts that work alongside it that create the effect of IO. Lincoln suggests possible ways to look at IO in a more holistic approach by recognizing the many possible factors that play a role in IO and how they work together to achieve IO. [ 60 ] It would be impossible for an individual to read all the  academic papers  published in a narrow speciality, even if they spent all their time reading. A response to this is the publishing of  systematic reviews  such as the  Cochrane Reviews . Richard Smith argues that it would be impossible for a general practitioner to read all the literature relevant to every individual patient they consult with and suggests one solution would be an  expert system  for use of doctors while consulting. [ 61 ] Media related to  Information overload  at Wikimedia Commons"
  },
  {
    "id": 39,
    "title": "Search engine",
    "content": "A  search engine  is a  software system  that provides  hyperlinks  to  web pages  and other relevant information on  the Web  in response to a user's  query . The user  inputs  a query within a  web browser  or a  mobile app , and the  search results  are often a list of hyperlinks, accompanied by textual summaries and images. Users also have the option of limiting the search to a specific type of results, such as images, videos, or news. For a search provider, its  engine  is part of a  distributed computing  system that can encompass many  data centers  throughout the world. The speed and accuracy of an engine's response to a query is based on a complex system of  indexing  that is continuously updated by automated  web crawlers . This can include  data mining  the  files  and  databases  stored on  web servers , but some content is  not accessible  to crawlers. There have been many search engines since the dawn of the Web in the 1990s, but  Google Search  became the dominant one in the 2000s and has remained so. It currently has a 91% global market share. [ 1 ] [ 2 ]  The business of  websites  improving their visibility in  search results , known as  marketing  and  optimization , has thus largely focused on Google. In 1945,  Vannevar Bush  described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk. [ 3 ]  He called it a  memex . He described the system in an article titled \" As We May Think \" that was published in  The Atlantic Monthly . [ 4 ]  The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern  hyperlinks . [ 5 ] Link analysis  eventually became a crucial component of search engines through algorithms such as  Hyper Search  and  PageRank . [ 6 ] [ 7 ] The first internet search engines predate the debut of the Web in December 1990:  WHOIS  user search dates back to 1982, [ 8 ]  and the  Knowbot Information Service  multi-network user search was first implemented in 1989. [ 9 ]  The first well documented search engine that searched content files, namely  FTP  files, was  Archie , which debuted on 10 September 1990. [ 10 ] Prior to September 1993, the  World Wide Web  was entirely indexed by hand. There was a list of  webservers  edited by  Tim Berners-Lee  and hosted on the  CERN   webserver . One snapshot of the list in 1992 remains, [ 11 ]  but as more and more web servers went online the central list could no longer keep up. On the  NCSA  site, new servers were announced under the title \"What's New!\". [ 12 ] The first tool used for searching content (as opposed to users) on the  Internet  was  Archie . [ 13 ]  The name stands for \"archive\" without the \"v\". [ 14 ]  It was created by  Alan Emtage , [ 14 ] [ 15 ] [ 16 ] [ 17 ]   computer science  student at  McGill University  in  Montreal, Quebec , Canada. The program downloaded the directory listings of all the files located on public anonymous FTP ( File Transfer Protocol ) sites, creating a searchable  database  of file names; however,  Archie Search Engine  did not index the contents of these sites since the amount of data was so limited it could be readily searched manually. The rise of  Gopher  (created in 1991 by  Mark McCahill  at the  University of Minnesota ) led to two new search programs,  Veronica  and  Jughead . Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \" Archie Search Engine \" was not a reference to the  Archie comic book  series, \" Veronica \" and \" Jughead \" are characters in the series, thus referencing their predecessor. In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand.  Oscar Nierstrasz  at the  University of Geneva  wrote a series of  Perl  scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for  W3Catalog , the web's first primitive search engine, released on September 2, 1993. [ 18 ] In June 1993, Matthew Gray, then at  MIT , produced what was probably the first  web robot , the  Perl -based  World Wide Web Wanderer , and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine  Aliweb  appeared in November 1993. Aliweb did not use a  web robot , but instead depended on being notified by  website administrators  of the existence at each site of an index file in a particular format. JumpStation  (created in December 1993 [ 19 ]  by  Jonathon Fletcher ) used a  web robot  to find web pages and to build its index, and used a  web form  as the interface to its query program. It was thus the first  WWW  resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the  web pages  the crawler encountered. One of the first \"all text\" crawler-based search engines was  WebCrawler , which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any  web page , which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994,  Lycos  (which started at  Carnegie Mellon University ) was launched and became a major commercial endeavor. The first popular search engine on the Web was  Yahoo! Search . [ 20 ]  The first product from  Yahoo! , founded by  Jerry Yang  and  David Filo  in January 1994, was a  Web directory  called  Yahoo! Directory . In 1995, a search function was added, allowing users to search Yahoo! Directory. [ 21 ] [ 22 ]  It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages. Soon after, a number of search engines appeared and vied for popularity. These included  Magellan ,  Excite ,  Infoseek ,  Inktomi ,  Northern Light , and  AltaVista . Information seekers could also browse the directory instead of doing a keyword-based search. In 1996,  Robin Li  developed the  RankDex  site-scoring  algorithm  for search engines results page ranking [ 23 ] [ 24 ] [ 25 ]  and received a US patent for the technology. [ 26 ]  It was the first search engine that used  hyperlinks  to measure the quality of websites it was indexing, [ 27 ]  predating the very similar algorithm patent filed by  Google  two years later in 1998. [ 28 ]   Larry Page  referenced Li's work in some of his U.S. patents for PageRank. [ 29 ]  Li later used his Rankdex technology for the  Baidu  search engine, which was founded by him in China and launched in 2000. In 1996,  Netscape  was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite. [ 30 ] [ 31 ] Google  adopted the idea of selling search terms in 1998 from a small search engine company named  goto.com . This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet. [ citation needed ] Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s. [ 32 ]  Several companies entered the market spectacularly, receiving record gains during their  initial public offerings . Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the  dot-com bubble , a speculation-driven market boom that peaked in March 2000. Around 2000,  Google's search engine  rose to prominence. [ 33 ]  The company achieved better results for many searches with an algorithm called  PageRank , as was explained in the paper  Anatomy of a Search Engine  written by  Sergey Brin  and  Larry Page , the later founders of Google. [ 7 ]  This  iterative algorithm  ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites  Robin Li 's earlier  RankDex  patent as an influence. [ 29 ] [ 25 ]  Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a  web portal . In fact, the Google search engine became so popular that spoof engines emerged such as  Mystery Seeker . By 2000,  Yahoo!  was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and  Overture  (which owned  AlltheWeb  and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions. Microsoft  first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from  Looksmart , blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004,  Microsoft  began a transition to its own search technology, powered by its own  web crawler  (called  msnbot ). Microsoft's rebranded search engine,  Bing , was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which  Yahoo! Search  would be powered by Microsoft Bing technology. As of 2019, [update]  active search engine crawlers include those of Google,  Sogou , Baidu, Bing,  Gigablast ,  Mojeek ,  DuckDuckGo  and  Yandex . A search engine maintains the following processes in near real time: [ 34 ] Web search engines get their information by  web crawling  from site to site. The \"spider\" checks for the standard filename  robots.txt , addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be  indexed  depending on many factors, such as the titles, page content,  JavaScript ,  Cascading Style Sheets  (CSS), headings, or its  metadata  in HTML  meta tags . After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\". [ 36 ] Indexing means associating words and other definable tokens found on web pages to their domain names and  HTML -based fields. The associations are made in a public database, made available for web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible. [ 35 ]  Some of the techniques for indexing, and  caching  are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis. Between visits by the  spider , the  cached  version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a  web proxy  instead. In this case, the page may differ from the search terms indexed. [ 35 ]  The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of  linkrot . Typically when a user enters a  query  into a search engine it is a few  keywords . [ 37 ]  The  index  already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be  weighted  according to information in the indexes. [ 35 ]  Then the top search result item requires the lookup, reconstruction, and markup of the  snippets  showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing. Beyond simple keyword lookups, search engines offer their own  GUI - or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by  filtering  and  weighting  while refining the search results, given the initial pages of the first search results.\nFor example, from 2007 the Google.com search engine has allowed one to  filter  by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range. [ 38 ]  It is also possible to  weight  by date because each page has a modification time. Most search engines support the use of the  Boolean operators  AND, OR and NOT to help end users refine the  search query . Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called  proximity search , which allows users to define the distance between keywords. [ 35 ]  There is also  concept-based searching  where the research involves using statistical analysis on pages containing the words or phrases you search for. The usefulness of a search engine depends on the  relevance  of the  result set  it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to  rank  the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another. [ 35 ]  The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \" inverted index \" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work. Most Web search engines are commercial ventures supported by  advertising  revenue and thus some of them allow advertisers to  have their listings ranked higher  in search results for a fee. Search engines that do not accept money for their search results make money by running  search related ads  alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads. [ 39 ] Local search  is the process that optimizes the efforts of local businesses. They focus on change to make sure all searches are consistent. It is important because many people determine where they plan to go and what to buy based on their searches. [ 40 ] As of January 2022, [update]   Google  is by far the world's most used search engine, with a market share of 90.6%, and the world's other most used search engines were  Bing ,  Yahoo! ,  Baidu ,  Yandex , and  DuckDuckGo . [ 2 ]  In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice. [ 41 ] In Russia,  Yandex  has a market share of 62.6%, compared to Google's 28.3%. And Yandex is the second most used search engine on smartphones in Asia and Europe. [ 42 ]  In China, Baidu is the most popular search engine. [ 43 ]  South Korea's homegrown search portal,  Naver , is used for 62.8% of online searches in the country. [ 44 ]   Yahoo! Japan  and  Yahoo! Taiwan  are the most popular avenues for Internet searches in Japan and Taiwan, respectively. [ 45 ]  China is one of few countries where Google is not in the top three web search engines for market share. Google was previously a top search engine in China, but withdrew after a disagreement with the government over censorship and a cyberattack. But Bing is in top three web search engine with a market share of 14.95%. Baidu is on top with 49.1% market share. [ 46 ] [ citation needed ] Most countries' markets in the European Union are dominated by Google, except for the  Czech Republic , where  Seznam  is a strong competitor. [ 47 ] The search engine  Qwant  is based in  Paris ,  France , where it attracts most of its 50 million monthly registered users from. Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide [ 48 ] [ 49 ]  and the underlying assumptions about the technology. [ 50 ]  These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its  organic search  results), and political processes (e.g., the removal of search results to comply with local laws). [ 51 ]  For example, Google will not surface certain  neo-Nazi  websites in France and Germany, where  Holocaust denial  is illegal. Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results. [ 52 ]  Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries. [ 49 ] Google Bombing  is one example of an attempt to manipulate search results for political, social or commercial reasons. Several scholars have studied the cultural changes triggered by search engines, [ 53 ]  and the representation of certain controversial topics in their results, such as  terrorism in Ireland , [ 54 ]   climate change denial , [ 55 ]  and  conspiracy theories . [ 56 ] There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or  filter bubbles  by  Eli Pariser  in 2011. [ 57 ]  The argument is that search engines and social media platforms use  algorithms  to selectively guess what information a user would like to see, based on information about the user (such as location, past click behaviour and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to  Eli Pariser  users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as  DuckDuckGo . However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble. [ 58 ] [ 59 ] [ 60 ]  On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalisation in search, [ 60 ]  that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets. [ 61 ] [ 59 ] The global growth of the Internet and electronic media in the  Arab  and  Muslim  world during the last decade has encouraged Islamic adherents in  the Middle East  and  Asian sub-continent , to attempt their own search engines, their own filtered search portals that would enable users to perform  safe searches . More than usual  safe search  filters, these Islamic web portals categorizing websites into being either \" halal \" or \" haram \", based on interpretation of  Sharia law .  ImHalal  came online in September 2011.  Halalgoogling  came online in July 2013. These use  haram  filters on the collections from  Google  and  Bing  (and others). [ 62 ] While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like  Muxlim  (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google, [ 63 ]  and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith. [ 64 ] Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a  sitemap , but it is normally only necessary to submit the  home page  of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign. Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's  ranking , because external links are one of the most important factors determining a website's ranking. However, John Mueller of  Google  has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking. [ 65 ] In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as  search engine   spiders . All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and  bookmark web pages  that have not yet been noticed or indexed by web spiders. [ 66 ]  Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful  metric  for  end-users  than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see  Gaming the system ), and both need technical countermeasures to try to deal with this. The first web search engine was  Archie , created in 1990 [ 67 ]  by  Alan Emtage , a student at  McGill University  in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on. The primary method of storing and retrieving files was via the  File Transfer Protocol  (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: Some administrator decides that he wants to make files available from his computer. He sets up a program on his computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, he or she connects to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol. Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them. Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file. Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database. [ 68 ] In 1993, the University of Nevada System Computing Services group developed  Veronica . [ 67 ]  It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges. [ 68 ] The  World Wide Web Wanderer , developed by Matthew Gray in 1993 [ 69 ]  was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database. Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained. In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways. ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth.  The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos. [ 68 ] Excite , initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers. [ 68 ] Excite was the first serious commercial search engine which launched in 1995. [ 70 ]  It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and  InfoSpace  bought Excite for $10 million. Some of the first analysis of web searching was conducted on search logs from Excite [ 71 ] [ 72 ] In April 1994, two Stanford University Ph.D. candidates,  David Filo  and  Jerry Yang , created some pages that became rather popular. They called the collection of pages  Yahoo!  Their official explanation for the name choice was that they considered themselves to be a pair of yahoos. As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory. The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites. At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the  Lycos  search engine. Search engines on the web are sites enriched with facility to search the content stored on other sites.  There is difference in the way various search engines work, but they all perform three basic tasks. [ 73 ] The process begins when a user enters a query statement into the system through the interface provided. There are basically three types of search engines: Those that are powered by robots (called  crawlers ; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two. Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine. Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index. In both cases, when you query a search engine to locate information, you're actually searching through the index that the search engine has created —you are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated. So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the  relevance  of the information in the index to what the user is searching for. One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing. Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking. Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g.  Google ), database or structured data search engines (e.g.  Dieselpoint ), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and  Yahoo! , utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity. Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings. [ 74 ]"
  },
  {
    "id": 40,
    "title": "Relevance (information retrieval)",
    "content": "In  information science  and  information retrieval ,  relevance  denotes how well a retrieved document or set of documents meets the  information need  of the user. Relevance may include concerns such as timeliness, authority or novelty of the result. The concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century. [ citation needed ] The formal study of relevance began in the 20th century with the study of what would later be called  bibliometrics . In the 1930s and 1940s,  S. C. Bradford  used the term \"relevant\" to characterize articles relevant to a subject (cf.,  Bradford's law ). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information. [ 1 ] Since 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between \"relevance to a subject\" or \"topical relevance\" and \"user relevance\". [ 1 ] The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the  Cranfield Experiments  of the early 1960s and culminating in the  TREC  evaluations that continue to this day as the main evaluation framework for information retrieval research. [ 2 ] In order to evaluate how well an  information retrieval  system retrieved topically relevant results, the relevance of retrieved results must be quantified. In  Cranfield -style evaluations, this typically involves assigning a  relevance level  to each retrieved result, a process known as  relevance assessment . Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results,  information retrieval performance measures  can be used to assess the quality of a retrieval system's output. In contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance. [ 3 ]  These studies often focus on aspects of  human-computer interaction  (see also  human-computer information retrieval ). The  cluster hypothesis , proposed by  C. J. van Rijsbergen  in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally. [ 4 ]     The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include: A second interpretation, most notably advanced by  Ellen Voorhees , [ 8 ]     focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include: Local methods require an accurate and appropriate document  similarity measure . The documents which are most relevant are not necessarily those which are most useful to display in the first page of search results.  For example, two duplicate documents might be individually considered quite relevant, but it is only useful to display one of them.  A measure called \"maximal marginal relevance\" (MMR) has been proposed to manage this shortcoming. It considers the relevance of each document only in terms of how much new information it brings given the previous results. [ 13 ] In some cases, a query may have an ambiguous interpretation, or a variety of potential responses.  Providing a diversity of results can be a consideration when evaluating the utility of a result set. [ 14 ]"
  },
  {
    "id": 41,
    "title": "Ranking (information retrieval)",
    "content": "Ranking  of query is one of the fundamental problems in  information retrieval  (IR), [ 1 ]  the scientific/engineering discipline behind  search engines . [ 2 ]  Given a query  q  and a collection  D  of documents that match the query, the problem is to rank, that is, sort, the documents in  D  according to some criterion so that the \"best\" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and  recommender systems . [ 3 ]  A majority of search engines use ranking algorithms to provide users with accurate and  relevant  results. [ 4 ] The notion of page rank dates back to the 1940s and the idea originated in the field of economics. In 1941,  Wassily Leontief  developed an iterative method of valuing a country's sector based on the importance of other sectors that supplied resources to it. In 1965, Charles H Hubbell at the  University of California, Santa Barbara , published a technique for determining the importance of individuals based on the importance of the people who endorse them. [ 5 ] Gabriel Pinski and Francis Narin came up with an approach to rank journals. [ 6 ]  Their rule was that a journal is important if it is cited by other important journals.  Jon Kleinberg , a computer scientist at  Cornell University , developed an almost identical approach to  PageRank  which was called  Hypertext Induced Topic Search  or HITS and it treated  web pages  as \"hubs\" and \"authorities\". Google’s PageRank algorithm was developed in 1998 by Google’s founders  Sergey Brin  and  Larry Page  and it is a key part of Google’s method of ranking web pages in  search results . [ 7 ]  All the above methods are somewhat similar as all of them exploit the structure of links and require an iterative approach. [ 8 ] Ranking functions are evaluated by a variety of means; one of the simplest is determining the  precision  of the first  k  top-ranked results for some fixed  k ; for example, the proportion of the top 10 results that are relevant, on average over many queries. IR models can be broadly divided into three types:  Boolean models  or BIR,  Vector Space Models , and  Probabilistic Models . [ 9 ]  Various comparisons between retrieval models can be found in the literature (e.g.,  [ 10 ] ). Boolean Model or BIR is a simple baseline query model where each query follows the underlying principles of relational algebra with algebraic expressions and where documents are not fetched unless they completely match with each other. Since the query is either fetch the document (1) or doesn’t fetch the document (0), there is no methodology to rank them. Since the Boolean Model only fetches complete matches, it doesn’t address the problem of the documents being partially matched. The  Vector Space Model  solves this problem by introducing vectors of index items each assigned with weights. The weights are ranged from positive (if matched completely or to some extent) to negative (if unmatched or completely oppositely matched) if documents are present. Term Frequency - Inverse Document Frequency ( tf-idf ) is one of the most popular techniques where weights are terms (e.g. words, keywords, phrases etc.) and dimensions is number of words inside corpus. The similarity score between query and document can be found by calculating cosine value between query weight vector and document weight vector using  cosine similarity . Desired documents can be fetched by ranking them according to similarity score and fetched top k documents which has the highest scores or most relevant to query vector. In probabilistic model, probability theory has been used as a principal means for modeling the retrieval process in mathematical terms. The probability model of information retrieval was introduced by Maron and Kuhns in 1960 and further developed by Roberston and other researchers. According to Spack Jones and Willett (1997): The rationale for introducing probabilistic concepts is obvious: IR systems deal with natural language, and this is too far imprecise to enable a system to state with certainty which document will be relevant to a particular query. The model applies the theory of probability to information retrieval (An event has a possibility from 0 percent to 100 percent of occurring). i.e, in probability model, relevance is expressed in terms of probability. Here, documents are ranked in order of decreasing probability of relevance. It takes into the consideration of uncertainty element in the IR process. i.e., uncertainty about whether documents retrieved by the system are relevant to a given query. The probability model intends to estimate and calculate the probability that a document will be relevant to a given query based on some methods. The “event” in this context of information retrieval refers to the probability of relevance between a query and a document. Unlike other IR models, the probability model does not treat relevance as an exact miss-or-match measurement. The model adopts various methods to determine the probability of relevance between queries and documents. Relevance in the probability model is judged according to the similarity between queries and documents. The similarity judgment is further dependent on term frequency. Thus, for a query consisting of only one term (B), the probability that a particular document (Dm) will be judged relevant is the ratio of users who submit query term (B) and consider the document (Dm) to be relevant in relation to the number of users who submitted the term (B). As represented in Maron’s and Kuhn’s model, can be represented as the probability that users submitting a particular query term (B) will judge an individual document (Dm) to be relevant. According to  Gerard Salton  and Michael J. McGill, the essence of this model is that if estimates for the probability of occurrence of various terms in relevant documents can be calculated, then the probabilities that a document will be retrieved, given that it is relevant, or that it is not, can be estimated. [ 11 ] Several experiments have shown that the probabilistic model can yield good results. However, such results have not been sufficiently better than those obtained using the Boolean or Vector Space model. [ 12 ] [ 13 ] The most common measures of evaluation are precision, recall, and f-score. They are computed using unordered sets of documents. These measures must be extended, or new measures must be defined, in order to evaluate the ranked retrieval results that are standard in modern search engines. In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by the top k retrieved documents. For each such set,  precision and recall  values can be plotted to give a precision-recall curve. [ 14 ] Precision measures the exactness of the retrieval process. If the actual set of relevant documents is denoted by I and the retrieved set of documents is denoted by O, then the precision is given by:\n \n \n \n \n \n Precision \n \n = \n \n \n \n \n | \n \n { \n \n I \n \n } \n ∩ \n { \n \n O \n \n } \n \n | \n \n \n \n \n | \n \n { \n \n O \n \n } \n \n | \n \n \n \n \n \n \n {\\displaystyle {\\text{Precision}}={\\frac {|\\{{\\text{I}}\\}\\cap \\{{\\text{O}}\\}|}{|\\{{\\text{O}}\\}|}}} Recall is a measure of completeness of the IR process. If the actual set of relevant documents is denoted by I and the retrieved set of documents is denoted by O, then the recall is given by: \n \n \n \n \n \n Recall \n \n = \n \n \n \n \n | \n \n { \n \n I \n \n } \n ∩ \n { \n \n O \n \n } \n \n | \n \n \n \n \n | \n \n { \n \n I \n \n } \n \n | \n \n \n \n \n \n \n {\\displaystyle {\\text{Recall}}={\\frac {|\\{{\\text{I}}\\}\\cap \\{{\\text{O}}\\}|}{|\\{{\\text{I}}\\}|}}} F1 Score tries to combine the precision and recall measure. It is the harmonic mean of the two. If P is the precision and R is the recall then the F-Score is given by: The  PageRank  algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on the links will arrive at any particular page. PageRank can be calculated for collections of documents of any size. It is assumed in several research papers that the distribution is evenly divided among all documents in the collection at the beginning of the computational process. The PageRank computations require several passes through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value. The formulae is given below: i.e. the PageRank value for a page  u  is dependent on the PageRank values for each page  v  contained in the set  B u  (the set containing all pages linking to page  u ), divided by the amount  L ( v ) of links from page  v . Similar to  PageRank , HITS uses Link Analysis for analyzing the relevance of the pages but only works on small sets of subgraph (rather than entire web graph) and as well as being query dependent. The subgraphs are ranked according to weights in hubs and authorities where pages that rank highest are fetched and displayed. [ 15 ]"
  },
  {
    "id": 42,
    "title": "Information retrieval",
    "content": "Information retrieval  ( IR ) in  computing  and  information science  is the task of identifying and retrieving  information system  resources that are relevant to an  information need .  The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on  full-text  or other content-based indexing. Information retrieval is the  science [ 1 ]  of searching for information in a document, searching for documents themselves, and also searching for the  metadata  that describes data, and for  databases  of texts, images or sounds. Automated information retrieval systems are used to reduce what has been called  information overload . An IR system is a software system that provides access to books, journals and other documents; it also stores and manages those documents.  Web search engines  are the most visible IR applications. An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval, a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of  relevance . An object is an entity that is represented by information in a content collection or  database . User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This  ranking  of results is a key difference of information retrieval searching compared to database searching. [ 2 ] Depending on the  application  the data objects may be, for example, text documents, images, [ 3 ]  audio, [ 4 ]   mind maps [ 5 ]  or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or  metadata . Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query. [ 6 ] there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute The idea of using computers to search for relevant pieces of information was popularized in the article  As We May Think  by  Vannevar Bush  in 1945. [ 7 ]  It would appear that Bush was inspired by patents for a 'statistical machine' – filed by  Emanuel Goldberg  in the 1920s and 1930s – that searched for documents stored on film. [ 8 ]  The first description of a computer searching for information was described by Holmstrom in 1948, [ 9 ]  detailing an early mention of the  Univac  computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy,  Desk Set . In the 1960s, the first large information retrieval research group was formed by  Gerard Salton  at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small  text corpora  such as the Cranfield collection (several thousand documents). [ 7 ]  Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s. In 1992, the US Department of Defense along with the  National Institute of Standards and Technology  (NIST), cosponsored the  Text Retrieval Conference  (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that  scale  to huge corpora. The introduction of  web search engines  has boosted the need for very large scale retrieval systems even further. Areas where information retrieval techniques are employed include (the entries are in alphabetical order within each category): Methods/Techniques in which information retrieval techniques are employed include: In order to effectively retrieve relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model. The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for  Boolean retrieval [ clarification needed ]  or top-k retrieval, include  precision and recall . All measures assume a  ground truth  notion of relevance: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be  ill-posed  and there may be different shades of relevance."
  },
  {
    "id": 43,
    "title": "Mind map",
    "content": "A  mind map  is a   diagram  used to visually organize information into a  hierarchy , showing relationships among pieces of the whole. [ 1 ]  It is often based on a single concept, drawn as an image in the center of a blank page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas  branch out  from those major ideas. Mind maps can also be drawn by hand, either as \"notes\" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available. Mind maps are considered to be a type of  spider diagram . [ 2 ] Although the term \"mind map\" was first popularized by British  popular psychology  author and television personality  Tony Buzan , [ 3 ] [ 4 ]  the use of diagrams that visually \"map\" information using branching and  radial maps  traces back centuries. [ 5 ]  These pictorial methods record knowledge and model systems, and have a long history in learning,  brainstorming ,  memory ,  visual thinking , and  problem solving  by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by  Porphyry of Tyros , a noted thinker of the 3rd century, as he graphically visualized the concept  categories of Aristotle . [ 5 ]  Philosopher  Ramon Llull  (1235–1315) also used such techniques. [ 5 ] Buzan's specific approach, and the introduction of the term \"mind map\", started with a 1974  BBC  TV series he hosted, called  Use Your Head . [ 6 ]  In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure. [ 7 ] Cunningham (2005) conducted a user study in which 80% of the students thought \"mindmapping helped them understand concepts and ideas in science\". [ 10 ]  Other studies also report some subjective positive effects of the use of mind maps. [ 11 ] [ 12 ]  Positive opinions on their effectiveness, however, were much more prominent among students of art and design than in students of computer and information technology, with 62.5% vs 34% (respectively) agreeing that they were able to understand concepts better with mind mapping software. [ 11 ]  Farrand, Hussain, and Hennessy (2002) found that  spider diagrams  (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline). [ 13 ]  This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects' preferred methods of  note taking . A meta study about  concept mapping  concluded that concept mapping is more effective than \"reading text passages, attending lectures, and participating in class discussions\". [ 14 ]  The same study also concluded that concept mapping is slightly more effective \"than other constructive activities such as writing summaries and outlines\". However, results were inconsistent, with the authors noting \"significant heterogeneity was found in most subsets\". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students. Joeran Beel and Stefan Langer conducted a comprehensive analysis of the content of mind maps. [ 15 ]  They analysed 19,379 mind maps from 11,179 users of the mind mapping applications  SciPlore MindMapping  (now  Docear ) and  MindMeister . Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about three words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7,500 words. The study also showed that between different mind mapping applications (Docear vs MindMeister) significant differences exist related to how users create mind maps. There have been some attempts to create mind maps automatically. Brucks & Schommer created mind maps automatically from full-text streams. [ 16 ]  Rothenberger et al. extracted the main story of a text and presented it as mind map. [ 17 ]  There is also a patent application about automatically creating sub-topics in mind maps. [ 18 ] Mind-mapping software  can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites, images and videos. [ 19 ]  It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional  note-taking . [ 13 ] The following dozen examples of mind maps show the range of styles that a mind map may take, from hand-drawn to computer-generated and from mostly text to highly illustrated. Despite their stylistic differences, all of the examples share a  tree structure  that hierarchically connects sub-topics to a main topic."
  },
  {
    "id": 44,
    "title": "As We May Think",
    "content": "\" As We May Think \" is a 1945 essay by  Vannevar Bush  which has been described as visionary and influential, anticipating many aspects of  information society . It was first published in  The Atlantic  in July 1945 and republished in an abridged version in September 1945—before and after the  atomic bombings of Hiroshima and Nagasaki . Bush expresses his concern for the direction of scientific efforts toward destruction, rather than understanding, and explicates a desire for a sort of  collective memory  machine with his concept of the  memex  that would make knowledge more accessible, believing that it would help fix these problems. Through this machine, Bush hoped to transform an information explosion into a knowledge explosion. [ 1 ] The article was a reworked and expanded version of Bush's essay \"Mechanization and the Record\" (1939). Here, he described a machine that would combine lower level technologies to achieve a higher level of organized knowledge (like human  memory  processes). Shortly after the publication of this essay, Bush coined the term \" memex \" in a letter written to the editor of  Fortune  magazine. [ 2 ]  That letter became the body of \"As We May Think\", which added only an introduction and conclusion. As described, Bush's memex was based on what was thought, at the time, to be advanced technology of the future: ultra high resolution  microfilm  reels, coupled to multiple screen viewers and cameras, by  electromechanical  controls. The memex, in essence, reflects a library of collective knowledge stored in a piece of machinery described in his essay as \"a piece of furniture\". [ 3 ]  The  Atlantic  publication of Bush's article was followed by an abridged version in the September 10, 1945 issue of  Life  magazine, accompanied by fanciful illustrations of the proposed memex desk and other devices Bush projected. Bush also discussed other technologies such as  dry photography  and  microphotography  where he elaborates on the potentialities of their future use. For example, Bush states in his essay that: The combination of optical projection and photographic reduction is already producing some results in microfilm for scholarly purposes, and the potentialities are highly suggestive. \"As We May Think\" predicted (to some extent) many kinds of technology invented after its publication, including  hypertext ,  personal computers , the  Internet , the  World Wide Web ,  speech recognition , and  online encyclopedias  such as  Wikipedia : \"Wholly new forms of encyclopedias will appear, ready-made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplified.\" [ 3 ]  Bush envisioned the ability to retrieve several articles or pictures on one screen, with the possibility of writing comments that could be stored and recalled together. He believed people would create links between related articles, thus mapping the thought process and path of each user and saving it for others to experience. Wikipedia is one example of how this vision has in part been realized, allowing elements of an article to reference other related topics. A user's browser history maps the trails of possible paths of interaction, although this is typically available only to the user that created it.\nBush's article also laid the foundation for new media.  Doug Engelbart  came across the essay shortly after its publication, and keeping the memex in mind, he began work that would eventually result in the invention of the  mouse , the  word processor , the  hyperlink  and concepts of new media for which these groundbreaking inventions were merely enabling technologies. [ 1 ] Today, storage has greatly surpassed the level imagined by Vannevar Bush, The  Encyclopædia Britannica  could be reduced to the volume of a matchbox. A library of a million volumes could be compressed into one end of a desk. On the other hand, it still uses methods of indexing of information which Bush described as artificial: When data of any sort are placed in storage, they are filed alphabetically or numerically, and information is found (when it is) by tracing it down from subclass to subclass. It can be in only one place, unless duplicates are used. This description resembles popular  file systems  of modern computer operating systems ( FAT ,  NTFS ,  ext3  when used without hard links and symlinks, etc.), which do not easily enable associative indexing as imagined by Bush. Bush urges that scientists should turn to the massive task of creating more efficient accessibility to our fluctuating  store of knowledge . For years inventions have extended people's physical powers rather than the powers of their mind. He argues that the instruments are at hand which, if properly developed, will give society access to and command over the inherited knowledge of the ages. The perfection of these pacific instruments, he suggests, should be the first objective of our scientists. [ 3 ] Through this process, society would be able to focus and evolve past the existing knowledge rather than looping through infinite calculations. We should be able to pass the tedious  work of numbers  to machines and work on the intricate theory which puts them best to use. If humanity were able to obtain the \"privilege of forgetting the manifold things he does not need to have immediately at hand, with some assurance that he can find them again if proven important\" only then \"will mathematics be practically effective in bringing the  growing knowledge  of atomistic to the useful solution of the advanced problems of chemistry, metallurgy, and biology\". [ 1 ]  To exemplify the importance of this concept, consider the process involved in 'simple' shopping: \"Every time a charge sale is made, there are a number of things to be done. The inventory needs to be revised, the salesman needs to be given credit for the sale, the general accounts need an entry, and most important, the customer needs to be charged.\" [ 1 ]  Due to the convenience of the store's central device which rapidly manage thousands of these transactions, the employees may focus on the essential aspects of the department such as sales and advertising. Indeed, as of today, \"science has provided the swiftest  communication  between individuals; it has provided a record of ideas and has enabled man to manipulate and to make extracts from that record so that knowledge evolves and endures throughout the life of a race rather than of an individual\". [ 1 ]  Improved technology has become an extension of our capabilities, much as how external hard drives function for computers so it may reserve more memory for more practical tasks. Another significant role of practicality in technology is the method of  association  and selection. \"There may be millions of fine thoughts, and the account of the experience on which they are based, all encased within stone walls of acceptable architectural form; but if the scholar can get at only one a week by diligent search, his synthesis are not likely to keep up with the current scene.\" [ 1 ]  Bush believes that the tools available in his time lacked this feature, but noted the emergence and development of such ideas such as the Memex, a cross referencing system. Bush concludes his essay by stating that: The applications of science have built man a well-supplied house, and are teaching him to live healthily therein. They have enabled him to throw masses of people against one another with cruel weapons. They may yet allow him truly to encompass the great record and to grow in the wisdom of race experience. He may perish in conflict before he learns to wield that record for his true good. Yet, in the application of science to the needs and desires of man, it would seem to be a singularly unfortunate stage at which to terminate the process, or to lose hope as to the outcome. Editor's note:  Technologies like  trip hammers  exist that can do physical labor better and faster. Soon, technologies will exist that can help people do intellectual labor better and faster. Introduction:  Many scientists, especially physicists, obtained new duties during  World War II . Now, after the war, they need new peaceful duties. Section 1:  Scientific knowledge has grown considerably, but the way we manage knowledge has remained the same for centuries. We are no longer able to keep up and find relevant information in the flood of information.  Leibniz 's  computer  and  Charles Babbage 's  computer  were both failures because technologies of their times could not produce them cheaply and precisely, but now we have enough technology. Section 2:  Science should not only be a vast store of knowledge, but also be frequently consulted and enhanced. Two kinds of technologies can help: analog information on microfilms, and digital information encoded by electric signals. While they are different, both kinds would be vastly cheaper than traditional printed media. With  instant photography  and  microfilm , it will be cheap to copy and transmit analog information. Microfilm could shrink books and other paper-publications by a linear factor of 100x, or an area factor of 10000x. A library of 1 million books would occupy the volume of 100 books, which can fit on a bookshelf. All the world's books can fit inside a  moving van . [ note 1 ] [ note 2 ]  Production and transmission would cost pennies. [ note 3 ] A possible future device would be a walnut-sized camera strapped to the head of the wearer that can take a photo at the squeeze of a hand, and develop it. The photos can be taken out at the end of a day for further processing. (Illustrated in the header image.) Bush goes into some technical details about instant photography and electric fax machines. In his days,  wet photography  was the most common, yet it takes a long time and is hard to shrink into a small camera. However,  whiteprint  technology [ note 4 ]  might be miniaturized, leading to miniature dry photography. [ note 5 ] Printed material could be transmitted cheaply by digital signals, as demonstrated by  electric fax machines . The sending side uses  photocells  to convert images to electric signals, and on the receiving side,  electric printers  convert the electric signal into electric sparks hitting iodine-impregnated paper, turning it black. [ note 6 ] Section 3:  Not only will it be cheap to transmit and copy digital material, it will also be cheap to convert printed material into digital form. Language is interconvertible with digital signals, as shown by three technologies: While currently Vocoders need human operators, a future Vocoder could  transcribe speech automatically . A future researcher could walk around, take photos with the head-mounted camera, and record sound and speech. The photos and the sounds would have timing information. At the end of the day, this timed record of the day can be processed and reviewed. To study  cosmic rays , physicists built  vacuum tubes [ note 7 ]  that could count at 0.1 MHz. Future electronic computers could operate at least 100 times faster, at 10 MHz.  Herman Hollerith 's  tabulating machine  showed that simple machines programmed by  punched cards  could be commercially valuable. Future computers could perform complex programs according to punched cards or microfilms. Section 4:  Most of the existing computing machines are  tabulating machines ,  arithmetic machines . Some are more advanced, like  tide-predicting machines , and  machines for solving differential and integration equations . Future scientists will delegate even more advanced routine mathematics to machines, just as one would delegate the operation of a car to its engine. By delegating away more routines, scientists can perform creative, intuitive work. [ note 8 ] Section 5:  Scientists and other knowledge workers manipulate data and perform logical inferences. Any routine logical process that a worker performs repeatedly could be programmed into a machine. Normal or even mathematical language is too vague for programming. A \"positional\" [ note 9 ]  logical language would be needed for entering information the machines. Not only will they be for entering information, machines will also help people find information. For example,  punched card sorters  and  telephone exchanges  are both search machines: the sorter can quickly produce a stack of cards listing, for example, all employees who live in  Trenton, New Jersey  and know the  Spanish language , and a telephone exchange can quickly connect to the line specified by a number sequence. Bush proceeds to describe in detail a management system for a  department store , where a salesperson enters customer and product information, which a central machine uses to update inventory, credit sales, adjust accounts, and charge customers, using analog devices such as punched cards, dry photography, microfilms,  Valdemar Poulsen 's  magnetic wire recorder , and so on. Section 6:  Traditional information systems, such as the  library classification  system, are tree-like. At the top are the biggest classes, and each class can have subclasses, and so on. Each item belongs uniquely to a leaf on the tree of information. This is cumbersome, and the human mind does not operate that way, but operates by  association . In human thinking, one traces out a \"trail\" of information. This process can be augmented by the  memex . Like human memory, it retrieves information by association, not by going down a tree of classification. The memex is a machine for individual use, where they could store all their books, records, and communications. The memex looks like a desk. It contains a storage unit for microfilms, sufficient for an individual's lifetime. [ note 10 ]  Microfilms can be bought like books and magazines. Letters, documents, and hand-drawn manuscripts can be  placed on a transparent plate that is then photographed  and converted to microfilm. One can also manually type onto them with a keyboard. Typing on the keyboard, the user can find any microfilm by associative search. Pushing on levers allows users to flip through a microfilmed book, moving forward or backward at variable speeds. The user can open up several microfilms at once, then draw lines and commentaries between them using dry photography or by a  telautograph -like pen. Section 7:  The essence of memex is  associative indexing : the user can make any item associate with any other, so that pulling up the first item automatically pulls up the second. Associations can be chained, building a \"trail\". A trail can be named and later retrieved by typing on the keyboard. Any item can be a part of many trails. Associative indexing can be implemented by coded dots printed on the bottoms of microfilms, and an optical reader can read the printed code and electrically signal the memex to pull up the next item. [ note 11 ] Bush describes a  use scenario , where the user is studying why the  short Turkish bow  was apparently superior to the  English longbow  in  the Crusades . He searches through encyclopedias and textbooks, building a trail of connections. He also branches off another trail through textbooks and handbooks on elasticity. Later, in conversation with a friend about how people resist innovation, he brings up the trail again, and then copies the whole trail out to be installed into his friend's memex. There, the trail is joined into a more general trail about how people resist innovation. Section 8:  Bush envisions a future where memex machines are everywhere. There will be microfilmed encyclopedias with trails already installed. Lawyers, patent attorneys, and other knowledge workers will use the memex to store their associative trails accumulated over their professional life. There will be a new kind of job: \"trail blazers\", who find new and useful trails. Bush expect future technology to be superior than those described in the essay, but he keeps to only known technologies, instead of the possible unknown, [ note 12 ]  to keep the idea of memex practical. More speculatively, since the human nervous system is electrical, future  human-machine interfaces could be purely electrical . \"As We May Think\" has turned out to be a visionary and influential essay. In their introduction to a paper discussing information literacy as a discipline, Johnston and Webber write Bush's paper might be regarded as describing a microcosm of the information society, with the boundaries tightly drawn by the interests and experiences of a major scientist of the time, rather than the more open knowledge spaces of the 21st century. Bush provides a core vision of the importance of information to industrial/scientific society, using the image of an \"information explosion\" arising from the unprecedented demands on scientific production and technological application of World War II. He outlines a version of information science as a key discipline within the practice of scientific and technical knowledge domains. His view encompasses the problems of information overload and the need to devise efficient mechanisms to control and channel information for use. Indeed, Bush was very concerned with  information overload  inhibiting the research efforts of scientists. His scientist, operating under conditions of \"information explosion\" and requiring respite from the tide of scientific documents could be construed as a nascent image of the \"Information Literate Person\" in an information saturated society. There is a growing mountain of research. But there is increased evidence that we are being bogged down today as specialization extends. The investigator is staggered by the findings and conclusions of thousands of other workers. Schools, colleges, health care, government, etc., are all implicated in the distribution and use of information, under similar conditions of \"information explosion\" as Bush's post-war scientists. All these people arguably need some sort of personal \"information control\" in order to function."
  },
  {
    "id": 45,
    "title": "Vannevar Bush",
    "content": "Vannevar Bush  ( / v æ ˈ n iː v ɑːr /   van- NEE -var ; March 11, 1890 – June 28, 1974) was an American engineer, inventor and science administrator, who during  World War II  headed the U.S.  Office of Scientific Research and Development  (OSRD), through which almost all wartime military  R&D  was carried out, including important developments in  radar  and the initiation and early administration of the  Manhattan Project . He emphasized the importance of scientific research to national security and economic well-being, and was chiefly responsible for the movement that led to the creation of the  National Science Foundation . Bush joined the Department of Electrical Engineering at  Massachusetts Institute of Technology  (MIT) in 1919, and founded the company that became the  Raytheon Company  in 1922. Bush became vice president of MIT and dean of the  MIT School of Engineering  in 1932, and president of the  Carnegie Institution of Washington  in 1938. During his career, Bush patented a string of his own inventions. He is known particularly for his engineering work on  analog computers ,  and for the  memex . Starting in 1927, Bush constructed a  differential analyzer , a mechanical analog computer with some digital components that could solve  differential equations  with as many as 18 independent variables. An offshoot of the work at MIT by Bush and others was the beginning of  digital circuit  design theory. The memex, which he began developing in the 1930s (heavily influenced by  Emanuel Goldberg 's \"Statistical Machine\" from 1928) was a hypothetical adjustable  microfilm  viewer with a structure analogous to that of  hypertext . The memex and Bush's 1945  essay \" As We May Think \" influenced generations of computer scientists, who drew inspiration from his vision of the future. Bush was appointed to the  National Advisory Committee for Aeronautics  (NACA) in 1938, and soon became its chairman. As chairman of the  National Defense Research Committee  (NDRC), and later director of OSRD, Bush coordinated the activities of some six thousand leading American scientists in the application of science to warfare. Bush was a well-known policymaker and public intellectual during World War II, when he was in effect the first presidential  science advisor . As head of NDRC and OSRD, he initiated the Manhattan Project, and ensured that it received top priority from the highest levels of government. In  Science, The Endless Frontier , his 1945 report to the president of the United States, Bush called for an expansion of government support for science, and he pressed for the creation of the  National Science Foundation . Vannevar Bush was born in  Everett, Massachusetts , on March 11, 1890. [ 2 ]  He was the third child and only son of Richard Perry Bush, the local  Universalist  pastor, and his wife Emma Linwood (née Paine), the daughter of a prominent Provincetown family. [ 3 ]  He had two older sisters,  Edith  and Reba. He was named after John Vannevar, an old friend of the family who had attended  Tufts College  with Perry. The family moved to  Chelsea, Massachusetts , in 1892, [ 4 ]  and Bush graduated from  Chelsea High School  in 1909. [ 5 ] He then attended  Tufts College , like his father before him. A popular student, he was vice president of his  sophomore  class, and president of his  junior  class. During his  senior  year, he managed the football team. He became a member of the  Alpha Tau Omega  fraternity, and dated Phoebe Clara Davis, who also came from Chelsea. Tufts allowed students to gain a master's degree in four years simultaneously with a  bachelor's degree . For his master's thesis, Bush invented and patented a \"profile tracer\". This was a mapping device for assisting  surveyors  that looked like a lawn mower. It had two bicycle wheels, and a pen that plotted the terrain over which it traveled. It was the first of a string of inventions. [ 6 ] [ 7 ]  On graduation in 1913 he received both Bachelor of Science and Master of Science degrees. [ 8 ] After graduation, Bush worked at  General Electric  (GE) in  Schenectady, New York , for $14 a week. [ 9 ]  As a \"test man,\" he assessed equipment to ensure that it was safe. He transferred to GE's plant in  Pittsfield, Massachusetts , to work on high voltage  transformers , but after a fire broke out at the plant, Bush and the other test men were suspended. He returned to Tufts in October 1914 to teach mathematics, and spent the 1915 summer break working at the  Brooklyn Navy Yard  as an electrical inspector.\nBush was awarded a $1,500 scholarship to study at  Clark University  as a doctoral student of  Arthur Gordon Webster , but Webster wanted Bush to study acoustics, a popular field at the time. Bush preferred to quit rather than study a subject that did not interest him. [ 10 ] Bush subsequently enrolled in the  Massachusetts Institute of Technology  (MIT) electrical engineering program. Spurred by the need for enough financial security to marry, [ 10 ]  he submitted his thesis, entitled  Oscillating-Current Circuits: An Extension of the Theory of Generalized Angular Velocities, with Applications to the Coupled Circuit and the Artificial Transmission Line , [ 11 ]  in April 1916. His adviser,  Arthur Edwin Kennelly , demanded more work from him, but Bush refused, and Kennelly was overruled by the department chairman. Bush received his doctorate in engineering jointly from MIT and  Harvard University . [ 10 ]  He married Phoebe in August 1916. [ 10 ]  They had two sons: Richard Davis Bush and John Hathaway Bush. [ 12 ] Bush accepted a job with Tufts, where he became involved with the American Radio and Research Corporation (AMRAD), which began broadcasting music from the campus on March 8, 1916. The station owner, Harold Power, hired him to run the company's laboratory, at a salary greater than that which Bush drew from Tufts. In 1917, following the United States' entry into World War I, he went to work with the  National Research Council . He attempted to develop a  means of detecting submarines  by measuring the disturbance in the Earth's magnetic field. His device worked as designed, but only from a wooden ship; attempts to get it to work on a metal ship such as a  destroyer  failed. [ 13 ] Bush left Tufts in 1919, although he remained employed by AMRAD, and joined the Department of Electrical Engineering at  Massachusetts Institute of Technology  (MIT), where he worked under  Dugald C. Jackson . In 1922, he collaborated with fellow MIT professor William H. Timbie on  Principles of Electrical Engineering , an introductory textbook. AMRAD's lucrative contracts from World War I had been cancelled, and Bush attempted to reverse the company's fortunes by developing a  thermostatic switch  invented by Al Spencer, an AMRAD technician, on his own time. AMRAD's management was not interested in the device, but had no objection to its sale. Bush found backing from Laurence K. Marshall and  Richard S. Aldrich  to create the Spencer Thermostat Company, which hired Bush as a consultant. The new company soon had revenues in excess of a million dollars. [ 15 ]  It merged with General Plate Company to form Metals & Controls Corporation in 1931, and with  Texas Instruments  in 1959. Texas Instruments sold it to  Bain Capital  in 2006, and it became a separate company again as Sensata Technologies in 2010. [ 16 ] In 1924, Bush and Marshall teamed up with physicist Charles G. Smith, who had invented a  voltage-regulator tube  called the S-tube. The device enabled radios, which had previously required two different types of batteries, to operate from  mains power . Marshall had raised $25,000 to set up the American Appliance Company on July 7, 1922, to build silent refrigerators, with Bush and Smith among its five directors, but changed course and renamed it the  Raytheon Company , to make and market the S-tube. The venture made Bush wealthy, and Raytheon ultimately became a large electronics company and  defense contractor . [ 17 ] [ 15 ] Starting in 1927, Bush constructed a  differential analyzer , an  analog computer  that could solve  differential equations  with as many as 18 independent variables. This invention arose from previous work performed by Herbert R. Stewart, one of Bush's master's students, who at Bush's suggestion created the integraph, a device for solving  first-order differential equations , in 1925. Another student,  Harold Hazen , proposed extending the device to handle  second-order differential equations . Bush immediately realized the potential of such an invention, for these were much more difficult to solve, but also quite common in physics. Under Bush's supervision, Hazen was able to construct the differential analyzer, a table-like array of shafts and pens that mechanically simulated and plotted the desired equation. Unlike earlier designs that were purely mechanical, the differential analyzer had both electrical and mechanical components. [ 18 ]  Among the engineers who made use of the differential analyzer was  General Electric 's  Edith Clarke , who used it to solve problems relating to electric power transmission. [ 19 ]  For developing the differential analyzer, Bush was awarded the  Franklin Institute 's  Louis E. Levy Medal  in 1928. [ 20 ] Bush taught  Boolean algebra ,  circuit theory , and  operational calculus  according to the methods of  Oliver Heaviside  while  Samuel Wesley Stratton  was President of MIT. When  Harold Jeffreys  in Cambridge, England, offered his mathematical treatment in  Operational Methods in Mathematical Physics  (1927), Bush responded with his seminal textbook  Operational Circuit Analysis  (1929) for instructing electrical engineering students. In the preface he wrote: I write as an engineer and do not pretend to be a mathematician. I lean for support, and expect always to lean, upon the mathematician, just as I must lean upon the chemist, the physician, or the lawyer.  Norbert Wiener  has patiently guided me around many a mathematical pitfall ... he has written an appendix to this text on certain mathematical points. I did not know an engineer and a mathematician could have such good times together. I only wish that I could get the real vital grasp of mathematics that he has of the basic principles of physics. Parry Moon  and Stratton were acknowledged, as was M.S. Vallarta who \"wrote the first set of class notes which I used.\" [ 21 ] An offshoot of the work at MIT was the beginning of  digital circuit  design theory by one of Bush's graduate students,  Claude Shannon . [ 22 ]  Working on the analytical engine, Shannon described the application of Boolean algebra to electronic circuits in his landmark master's thesis,  A Symbolic Analysis of Relay and Switching Circuits . [ 23 ]  In 1935, Bush was approached by  OP-20-G , which was searching for an electronic device to aid in  codebreaking . Bush was paid a $10,000 fee to design the Rapid Analytical Machine (RAM). The project went over budget and was not delivered until 1938, when it was found to be unreliable in service. Nonetheless, it was an important step toward creating such a device. [ 24 ] The reform of MIT's administration began in 1930, with the appointment of  Karl T. Compton  as president. Bush and Compton soon clashed over the issue of limiting the amount of outside consultancy by professors, a battle Bush quickly lost, but the two men soon built a solid professional relationship. Compton appointed Bush to the newly created post of vice president in 1932. That year Bush also became the dean of the  MIT School of Engineering . The two positions came with a salary of $12,000 plus $6,000 for expenses per annum. [ 25 ] The companies Bush helped to found and the technologies he brought to the market made him financially secure, so he was able to pursue academic and scientific studies that he felt made the world better in the years before and after World War II. In May 1938, Bush accepted a prestigious appointment as president of the  Carnegie Institution of Washington  (CIW), which had been founded in Washington, D.C. Also known as the Carnegie Institution for Science, it had an endowment of $33 million, and annually spent $1.5 million in research, most of which was carried out at its eight major laboratories. Bush became its president on January 1, 1939, with a salary of $25,000. He was now able to influence research policy in the United States at the highest level, and could informally advise the government on scientific matters. [ 26 ]  Bush soon discovered that the CIW had serious financial problems, and he had to ask the  Carnegie Corporation  for additional funding. [ 27 ] Bush clashed over leadership of the institute with  Cameron Forbes , CIW's chairman of the board, and with his predecessor, John Merriam, who continued to offer unwanted advice. A major embarrassment to them all was  Harry H. Laughlin , the head of the  Eugenics Record Office , whose activities Merriam had attempted to curtail without success. Bush made it a priority to remove him, [ 28 ]  regarding him as a scientific fraud, and one of his first acts was to ask for a review of Laughlin's work. In June 1938, Bush asked Laughlin to retire, offering him an annuity, which Laughlin reluctantly accepted. The Eugenics Record Office was renamed the Genetics Record Office, its funding was drastically cut, and it was closed completely in 1944. [ 27 ]  Senator  Robert Reynolds  attempted to get Laughlin reinstated, but Bush informed the trustees that an inquiry into Laughlin would \"show him to be physically incapable of directing an office, and an investigation of his scientific standing would be equally conclusive.\" [ 29 ] Bush wanted the institute to concentrate on  hard science . He gutted Carnegie's archeology program, setting the field back many years in the United States. He saw little value in the  humanities  and  social sciences , and slashed funding for  Isis , a journal dedicated to the history of science and technology and its cultural influence. [ 27 ]  Bush later explained that \"I have a great reservation about these studies where somebody goes out and interviews a bunch of people and reads a lot of stuff and writes a book and puts it on a shelf and nobody ever reads it.\" [ 30 ] On August 23, 1938, Bush was appointed to the  National Advisory Committee for Aeronautics  (NACA), the predecessor of NASA. [ 26 ]  Its chairman  Joseph Sweetman Ames  became ill, and Bush, as vice chairman, soon had to act in his place. In December 1938, NACA asked for $11 million to establish a new aeronautical research laboratory in  Sunnyvale, California , to supplement the existing  Langley Memorial Aeronautical Laboratory . The California location was chosen for its proximity to some of the largest aviation corporations. This decision was supported by the chief of the  United States Army Air Corps ,  Major General   Henry H. Arnold , and by the head of the navy's  Bureau of Aeronautics ,  Rear Admiral   Arthur B. Cook , who between them were planning to spend $225 million on new aircraft in the year ahead. However, Congress was not convinced of its value, and Bush had to appear before the  Senate Appropriations Committee  on April 5, 1939. It was a frustrating experience for Bush, since he had never appeared before Congress before, and the senators were not swayed by his arguments. Further lobbying was required before funding for the new center, now known as the  Ames Research Center , was finally approved. By this time, war had broken out in Europe, and the inferiority of American aircraft engines was apparent, [ 31 ]  in particular the  Allison V-1710  which performed poorly at high altitudes and had to be removed from the  P-51 Mustang  in favor of the British  Rolls-Royce Merlin  engine. [ 32 ]  The NACA asked for funding to build a third center in Ohio, which became the  Glenn Research Center . Following Ames's retirement in October 1939, Bush became chairman of the NACA, with  George J. Mead  as his deputy. [ 31 ]  Bush remained a member of the NACA until November 1948. [ 33 ] During World War I, Bush had become aware of poor cooperation between civilian scientists and the military. Concerned about the lack of coordination in scientific research and the requirements of defense mobilization, Bush proposed the creation of a general directive agency in the  federal government , which he discussed with his colleagues. He had the secretary of NACA prepare a draft of the proposed  National Defense Research Committee  (NDRC) to be presented to Congress, but after the Germans invaded France in May 1940, Bush decided speed was important and approached President  Franklin D. Roosevelt  directly. Through the President's uncle,  Frederic Delano , Bush managed to set up a meeting with Roosevelt on June 12, 1940, to which he brought a single sheet of paper describing the agency. Roosevelt approved the proposal in 15 minutes, writing \"OK – FDR\" on the sheet. [ 34 ] With Bush as chairman, the NDRC was functioning even before the agency was officially established by order of the  Council of National Defense  on June 27, 1940. The organization operated financially on a hand-to-mouth basis with monetary support from the president's emergency fund. [ 35 ]  Bush appointed four leading scientists to the NDRC:  Karl Taylor Compton  (president of MIT),  James B. Conant  (president of Harvard University),  Frank B. Jewett  (president of the National Academy of Sciences and chairman of the Board of Directors of Bell Laboratories), and  Richard C. Tolman  (dean of the graduate school at Caltech); Rear Admiral  Harold G. Bowen, Sr.  and Brigadier General  George V. Strong  represented the military. The civilians already knew each other well, which allowed the organization to begin functioning immediately. [ 36 ]  The NDRC established itself in the  administration building  at the Carnegie Institution of Washington. [ 37 ]  Each member of the committee was assigned an area of responsibility, while Bush handled coordination. A small number of projects reported to him directly, such as the  S-1 Section . [ 38 ]  Compton's deputy,  Alfred Loomis , said that \"of the men whose death in the Summer of 1940 would have been the greatest calamity for America, the President is first, and Dr. Bush would be second or third.\" [ 39 ] Bush was fond of saying that \"if he made any important contribution to the war effort at all, it would be to get the Army and Navy to tell each other what they were doing.\" [ 40 ]  He established a cordial relationship with  Secretary of War   Henry L. Stimson , and Stimson's assistant,  Harvey H. Bundy , who found Bush \"impatient\" and \"vain\", but said he was \"one of the most important, able men I ever knew\". [ 35 ]  Bush's relationship with the navy was more turbulent. Bowen, the director of the  Naval Research Laboratory  (NRL), saw the NDRC as a bureaucratic rival, and recommended abolishing it. A series of bureaucratic battles ended with the NRL placed under the  Bureau of Ships , and  Secretary of the Navy   Frank Knox  placing an unsatisfactory fitness report in Bowen's personnel file. After the war, Bowen would again try to create a rival to the NDRC inside the navy. [ 41 ] On August 31, 1940, Bush met with  Henry Tizard , and arranged a series of meetings between the NDRC and the  Tizard Mission , a British scientific delegation. At a meeting On September 19, 1940, the Americans described Loomis and Compton's microwave research. They had an experimental 10 cm wavelength  short wave  radar, but admitted that it did not have enough power and that they were at a dead end.  Taffy Bowen  and  John Cockcroft  of the Tizard Mission then produced a  cavity magnetron , a device more advanced than anything the Americans had seen, with a power output of around 10 kW at 10 cm, [ 42 ]  enough to spot the periscope of a surfaced submarine at night from an aircraft. To exploit the invention, Bush decided to create a special laboratory. The NDRC allocated the new laboratory a budget of $455,000 for its first year. Loomis suggested that the lab should be run by the Carnegie Institution, but Bush convinced him that it would best be run by MIT. The  Radiation Laboratory , as it came to be known, tested its airborne radar from an Army  B-18  on March 27, 1941. By mid-1941, it had developed  SCR-584 radar , a mobile radar fire control system for  antiaircraft guns . [ 43 ] In September 1940,  Norbert Wiener  approached Bush with a proposal to build a digital computer. Bush declined to provide NDRC funding for it on the grounds that he did not believe that it could be completed before the end of the war. The supporters of digital computers were disappointed at the decision, which they attributed to a preference for outmoded analog technology. In June 1943, the Army provided $500,000 to build the computer, which became  ENIAC , the first general-purpose electronic computer. Having delayed its funding, Bush's prediction proved correct as ENIAC was not completed until December 1945, after the war had ended. [ 44 ]  His critics saw his attitude as a failure of vision. [ 45 ] On June 28, 1941, Roosevelt established the  Office of Scientific Research and Development  (OSRD) with the signing of Executive Order 8807. [ 46 ]  Bush became director of the OSRD while Conant succeeded him as chairman of the NDRC, which was subsumed into the OSRD. The OSRD was on a firmer financial footing than the NDRC since it received funding from Congress, and had the resources and the authority to develop  weapons and technologies with or without the military. Furthermore, the OSRD had a broader mandate than the NDRC, moving into additional areas such as medical research [ 47 ]  and the mass production of  penicillin  and  sulfa drugs . The organization grew to 850 full-time employees, [ 48 ]  and produced between 30,000 and 35,000 reports. [ 49 ]  The OSRD was involved in some 2,500 contracts, [ 50 ]  worth in excess of $536 million. [ 51 ] Bush's method of management at the OSRD was to direct overall policy, while delegating supervision of divisions to qualified colleagues and letting them do their jobs without interference. He attempted to interpret the mandate of the OSRD as narrowly as possible to avoid overtaxing his office and to prevent duplicating the efforts of other agencies. Bush would often ask: \"Will it help to win a war;  this  war?\" [ 52 ]  Other challenges involved obtaining adequate funds from the president and Congress and determining apportionment of research among government, academic, and industrial facilities. [ 52 ]  His most difficult problems, and also greatest successes, were keeping the confidence of the military, which distrusted the ability of civilians to observe security regulations and devise practical solutions, [ 53 ]  and opposing conscription of young scientists into the armed forces. This became especially difficult as the army's manpower crisis really began to bite in 1944. [ 54 ]  In all, the OSRD requested deferments for some 9,725 employees of OSRD contractors, of which all but 63 were granted. [ 54 ]  In his obituary,  The New York Times  described Bush as \"a master craftsman at steering around obstacles, whether they were technical or political or bull-headed generals and admirals.\" [ 55 ] In August 1940, the NDRC began work on a  proximity fuze , a fuze inside an artillery shell that would explode when it came close to its target. A radar set, along with the batteries to power it, was miniaturized to fit inside a shell, and its glass  vacuum tubes  designed to withstand the 20,000  g-force  of being fired from a gun and 500 rotations per second in flight. [ 56 ]  Unlike normal radar, the proximity fuze sent out a continuous signal rather than short pulses. [ 57 ]  The NDRC created a special Section T chaired by  Merle Tuve  of the CIW, with  Commander   William S. Parsons  as special assistant to Bush and liaison between the NDRC and the Navy's  Bureau of Ordnance  (BuOrd). [ 56 ]  One of CIW staff members that Tuve recruited to Section T in 1940 was  James Van Allen . In April 1942, Bush placed Section T directly under the OSRD, and Parsons in charge. The research effort remained under Tuve but moved to the  Johns Hopkins University 's  Applied Physics Laboratory  (APL), where Parsons was BuOrd's representative. [ 58 ]  In August 1942, a live firing test was conducted with the newly commissioned cruiser  USS  Cleveland ; three  pilotless drones  were shot down in succession. [ 59 ] To preserve the secret of the proximity fuze, its use was initially permitted only over water, where a dud round could not fall into enemy hands. In late 1943, the Army obtained permission to use the weapon over land. The proximity fuze proved particularly effective against the  V-1 flying bomb  over England, and later  Antwerp , in 1944. A version was also developed for use with  howitzers  against ground targets. [ 60 ]  Bush met with the  Joint Chiefs of Staff  in October 1944 to press for its use, arguing that the Germans would be unable to copy and produce it before the war was over. Eventually, the Joint Chiefs agreed to allow its employment from December 25. In response to the German  Ardennes Offensive  on December 16, 1944, the immediate use of the proximity fuze was authorized, and it went into action with deadly effect. [ 61 ]  By the end of 1944, proximity fuzes were coming off the production lines at the rate of 40,000 per day. [ 60 ]  \"If one looks at the proximity fuze program as a whole,\" historian  James Phinney Baxter III  wrote, \"the magnitude and complexity of the effort rank it among the three or four most extraordinary scientific achievements of the war.\" [ 62 ] The German V-1 flying bomb demonstrated a serious omission in OSRD's portfolio: guided missiles. While the OSRD had some success developing unguided rockets, it had nothing comparable to the V-1, the  V-2  or the  Henschel Hs 293  air-to-ship gliding guided bomb. Although the United States trailed the Germans and Japanese in several areas, this represented an entire field that had been left to the enemy. Bush did not seek the advice of  Robert H. Goddard . Goddard would come to be regarded as America's pioneer of rocketry, but many contemporaries regarded him as a crank. Before the war, Bush had gone on the record as saying, \"I don't understand how a serious scientist or engineer can play around with rockets\", [ 63 ]  but in May 1944, he was forced to travel to London to warn General  Dwight Eisenhower  of the danger posed by the V-1 and V-2. [ 64 ]  Bush could only recommend that the launch sites be bombed, which was done. [ 65 ] Bush played a critical role in persuading the United States government to undertake a crash program to create an  atomic bomb . [ 66 ]  When the NDRC was formed, the Committee on Uranium was placed under it, reporting directly to Bush as the Uranium Committee. Bush reorganized the committee, strengthening its scientific component by adding Tuve,  George B. Pegram ,  Jesse W. Beams ,  Ross Gunn  and  Harold Urey . [ 67 ]  When the OSRD was formed in June 1941, the Uranium Committee was again placed directly under Bush. For security reasons, its name was changed to the Section S-1. [ 68 ] Bush met with Roosevelt and Vice President  Henry A. Wallace  on October 9, 1941, to discuss the project. He briefed Roosevelt on  Tube Alloys , the British atomic bomb project and its  Maud Committee , which had concluded that an atomic bomb was feasible, and on the  German nuclear energy project , about which little was known. Roosevelt approved and expedited the atomic program. To control it, he created a Top Policy Group consisting of himself—although he never attended a meeting—Wallace, Bush, Conant, Stimson and the  Chief of Staff of the Army ,  General   George Marshall . [ 69 ]  On Bush's advice, Roosevelt chose the army to run the project rather than the navy, although the navy had shown far more interest in the field, and was already conducting research into atomic energy for powering ships. Bush's negative experiences with the Navy had convinced him that it would not listen to his advice, and could not handle large-scale construction projects. [ 70 ] [ 71 ] In March 1942, Bush sent a report to Roosevelt outlining work by  Robert Oppenheimer  on the  nuclear cross section  of  uranium-235 . Oppenheimer's calculations, which Bush had  George Kistiakowsky  check, estimated that the  critical mass  of a sphere of  Uranium-235  was in the range of 2.5 to 5 kilograms, with a destructive power of around 2,000 tons of TNT. Moreover, it appeared that  plutonium  might be even more  fissile . [ 72 ]  After conferring with Brigadier General  Lucius D. Clay  about the construction requirements, Bush drew up a submission for $85 million in  fiscal year  1943 for four pilot plants, which he forwarded to Roosevelt on June 17, 1942. With the Army on board, Bush moved to streamline oversight of the project by the OSRD, replacing the Section S-1 with a new S-1 Executive Committee. [ 73 ] A week later, on June 23, President Roosevelt sent this one-sentence memo back to Bush:  \"Do you have the money?\" \n [ 74 ] Bush soon became dissatisfied with the dilatory way the project was run, with its indecisiveness over the selection of sites for the pilot plants. He was particularly disturbed at the allocation of an AA-3 priority, which would delay completion of the pilot plants by three months. Bush complained about these problems to Bundy and  Under Secretary of War   Robert P. Patterson . Major General  Brehon B. Somervell , the commander of the army's  Services of Supply , appointed Brigadier General  Leslie R. Groves  as project director in September. Within days of taking over, Groves approved the proposed site at  Oak Ridge, Tennessee , and obtained a AAA priority. At a meeting in Stimson's office on September 23 attended by Bundy, Bush, Conant, Groves, Marshall Somervell and Stimson, Bush put forward his proposal for steering the project by a small committee answerable to the Top Policy Group. The meeting agreed with Bush, and created a Military Policy Committee chaired by him, with Somervell's chief of staff, Brigadier General  Wilhelm D. Styer , representing the army, and Rear Admiral  William R. Purnell  representing the navy. [ 75 ] At the meeting with Roosevelt on October 9, 1941, Bush advocated cooperating with the United Kingdom, and he began corresponding with his British counterpart, Sir  John Anderson . [ 76 ]  But by October 1942, Conant and Bush agreed that a joint project would pose security risks and be more complicated to manage. Roosevelt approved a Military Policy Committee recommendation stating that information given to the British should be limited to technologies that they were actively working on and should not extend to post-war developments. [ 77 ]  In July 1943, on a visit to London to learn about British progress on antisubmarine technology, [ 78 ]  Bush, Stimson, and Bundy met with Anderson,  Lord Cherwell , and  Winston Churchill  at  10 Downing Street . At the meeting, Churchill forcefully pressed for a renewal of interchange, while Bush defended current policy. Only when he returned to Washington did he discover that Roosevelt had agreed with the British. The  Quebec Agreement  merged the two atomic bomb projects, creating the  Combined Policy Committee  with Stimson, Bush and Conant as United States representatives. [ 79 ] Bush appeared on the cover of  Time  magazine on April 3, 1944. [ 80 ]  He toured the  Western Front  in October 1944, and spoke to ordnance officers, but no senior commander would meet with him. He was able to meet with  Samuel Goudsmit  and other members of the  Alsos Mission , who assured him that there was no danger from the German project; he conveyed this assessment to Lieutenant General  Bedell Smith . [ 81 ]  In May 1945, Bush became part of the  Interim Committee  formed to advise the new president,  Harry S. Truman , on nuclear weapons. [ 82 ]  It advised that the atomic bomb should be used against an industrial target in Japan as soon as possible and without warning. [ 83 ]  Bush was present at the  Alamogordo Bombing and Gunnery Range  on July 16, 1945, for the  Trinity nuclear test , the first detonation of an atomic bomb. [ 84 ]  Afterwards, he took his hat off to Oppenheimer in tribute. [ 85 ] Before the end of the Second World War, Bush and Conant had foreseen and sought to avoid a possible  nuclear arms race . Bush proposed international scientific openness and information sharing as a method of self-regulation for the scientific community, to prevent any one political group gaining a scientific advantage. Before nuclear research became public knowledge, Bush used the development of biological weapons as a model for the discussion of similar issues, an \"opening wedge\". He was less successful in promoting his ideas in peacetime with President Harry Truman, than he had been under wartime conditions with Roosevelt. [ 86 ] [ 87 ] In \" As We May Think \", an essay published by the  Atlantic Monthly  in July 1945, Bush wrote: \"This has not been a scientist's war; it has been a war in which all have had a part. The scientists, burying their old professional competition in the demand of a common cause, have shared greatly and learned much. It has been exhilarating to work in effective partnership.\" [ 88 ] Bush introduced the concept of the  memex  during the 1930s, which he imagined as a form of memory augmentation involving a  microfilm -based \"device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.\" [ 88 ]  He wanted the memex to emulate the way the brain links data by association rather than by indexes and traditional, hierarchical storage paradigms, and be easily accessed as \"a future device for individual use ... a sort of mechanized private file and library\" in the shape of a desk. [ 88 ]  The memex was also intended as a tool to study the brain itself. [ 88 ]  The structure of memex is considered a precursor to the World Wide Web. [ 89 ] After thinking about the potential of augmented memory for several years, Bush set out his thoughts at length in \" As We May Think \", predicting that \"wholly new forms of encyclopedias will appear, ready made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplified\". [ 88 ]  \"As We May Think\" was published in the July 1945 issue of  The Atlantic . A few months later,  Life  magazine  published a condensed version of \"As We May Think\", accompanied by several illustrations showing the possible appearance of a memex machine and its companion devices. [ 90 ] Shortly after \"As We May Think\" was originally published,  Douglas Engelbart  read it, and with Bush's visions in mind, commenced work that would later lead to the invention of the  mouse . [ 91 ]   Ted Nelson , who coined the terms \" hypertext \" and \" hypermedia \", was also greatly influenced by Bush's essay. [ 92 ] [ 93 ] \"As We May Think\" has turned out to be a visionary and influential essay. [ 94 ]  In their introduction to a paper discussing information literacy as a discipline, Bill Johnston and Sheila Webber wrote in 2005 that: Bush's paper might be regarded as describing a microcosm of  the information society, with the boundaries tightly drawn by the interests and experiences of a major scientist of the time, rather than the more open knowledge spaces of the 21st century. Bush provides a core vision of the importance of information to industrial / scientific society, using the image of an \"information explosion\" arising from the unprecedented demands on scientific production and technological application of World War II. He outlines a version of information science as a key discipline within the practice of scientific and technical knowledge domains. His view encompasses the problems of information overload and the need to devise efficient mechanisms to control and channel information for use. [ 95 ] Bush was concerned that  information overload  might inhibit the research efforts of scientists. Looking to the future, he predicted a time when \"there is a growing mountain of research. But there is increased evidence that we are being bogged down today as specialization extends. The investigator is staggered by the findings and conclusions of thousands of other workers.\" [ 88 ] The OSRD continued to function actively until some time after the end of hostilities, but by 1946–1947 it had been reduced to a minimal staff charged with finishing work remaining from the war period; Bush was calling for its closure even before the war had ended. During the war, the  OSRD had issued contracts as it had seen fit, with just eight organizations accounting for half of its spending. MIT was the largest to receive funds, with its obvious ties to Bush and his close associates. Efforts to obtain legislation exempting the OSRD from the usual government  conflict of interest  regulations failed, leaving Bush and other OSRD principals open to prosecution. Bush therefore pressed for OSRD to be wound up as soon as possible. [ 96 ] With its dissolution, Bush and others had hoped that an equivalent peacetime government research and development agency would replace the OSRD. Bush felt that basic research was important to  national survival for both military and commercial reasons, requiring continued government support for science and technology; technical superiority could be a  deterrent  to future enemy aggression. In  Science, The Endless Frontier , a July 1945 report to the president, Bush maintained that basic research was \"the pacemaker of technological progress\". \"New products and new processes do not appear full-grown,\" Bush wrote in the report. \"They are founded on new principles and new conceptions, which in turn are painstakingly developed by research in the purest realms of science!\" [ 97 ]  In Bush's view, the \"purest realms\" were the physical and medical sciences; he did not propose funding the  social sciences . [ 98 ]  In  Science, The Endless Frontier , science historian  Daniel Kevles  later wrote, Bush \"insisted upon the principle of Federal patronage for the advancement of knowledge in the United States, a departure that came to govern Federal science policy after World War II.\" [ 99 ] In July 1945, the Kilgore bill was introduced in Congress, proposing the appointment and removal of a single science administrator by the president, with emphasis on applied research, and a patent clause favoring a government monopoly. In contrast, the competing Magnuson bill was similar to Bush's proposal to vest control in a panel of top scientists and civilian administrators with the executive director appointed by them. The Magnuson bill emphasized basic research and protected private patent rights. [ 100 ]  A compromise Kilgore–Magnuson bill of February 1946 passed the Senate but expired in the House because Bush favored a competing bill that was a virtual duplicate of Magnuson's original bill. [ 101 ]  A Senate bill was introduced in February 1947 to create the National Science Foundation (NSF) to replace the OSRD. This bill favored most of the features advocated by Bush, including the controversial administration by an autonomous scientific board. The bill passed the Senate and the House, but was  pocket vetoed  by Truman on August 6, on the grounds that the administrative officers were not properly responsible to either the president or Congress. [ 102 ]  The OSRD was abolished without a successor organization on December 31, 1947. [ 103 ] Without a  National Science Foundation , the military stepped in, with the  Office of Naval Research  (ONR) filling the gap. The war had accustomed many scientists to working without the budgetary constraints imposed by pre-war universities. [ 104 ]  Bush helped create the Joint Research and Development Board (JRDB) of the Army and Navy, of which he was chairman. With passage of the  National Security Act  on July 26, 1947, the JRDB became the Research and Development Board (RDB). Its role was to promote research through the military until a bill creating the National Science Foundation finally became law. [ 105 ]  By 1953, the  Department of Defense  was spending $1.6 billion a year on research; physicists were spending 70 percent of their time on defense related research, and 98 percent of the money spent on physics came from either the Department of Defense or the  Atomic Energy Commission  (AEC), which took over from the Manhattan Project on January 1, 1947. [ 106 ]  Legislation to create the  National Science Foundation  finally passed through Congress and was signed into law by Truman in 1950. [ 107 ] The authority that Bush had as chairman of the RDB was much different from the power and influence he enjoyed as director of OSRD and would have enjoyed in the agency he had hoped would be independent of the Executive branch and Congress. He was never happy with the position and resigned as chairman of the RDB after a year, but remained on the oversight committee. [ 108 ]  He continued to be skeptical about rockets and missiles, writing in his 1949 book,  Modern Arms and Free Men , that  intercontinental ballistic missiles  would not be technically feasible \"for a long time to come ... if ever\". [ 109 ] With Truman as president, men like  John R. Steelman , who was appointed chairman of the President's Scientific Research Board in October 1946, came to prominence. [ 110 ]  Bush's authority, both among scientists and politicians, suffered a rapid decline, though he remained a revered figure. [ 111 ]  In September 1949, he was appointed to head a scientific panel that included Oppenheimer to review the evidence that the Soviet Union had  tested its first atomic bomb . The panel concluded that it had, and this finding was relayed to Truman, who made the public announcement. [ 112 ]  During 1952 Bush was one of five members of the  State Department Panel of Consultants on Disarmament , and led the panel in urging that the United States postpone its planned first test of the  hydrogen bomb  and seek a test ban with the Soviet Union, on the grounds that avoiding a test might forestall development of a catastrophic new weapon and open the way for new arms agreements between the two nations. [ 113 ]  The panel lacked political allies in Washington, however, and the  Ivy Mike  shot went ahead as scheduled. [ 113 ]  Bush was outraged when  a security hearing  stripped Oppenheimer of his security clearance in 1954; he issued a strident attack on Oppenheimer's accusers in  The New York Times .  Alfred Friendly  summed up the feeling of many scientists in declaring that Bush had become \"the Grand Old Man of American science\". [ 114 ] Bush continued to serve on the NACA through 1948 and expressed annoyance with aircraft companies for delaying development of a  turbojet  engine because of the huge expense of research and development as well as retooling from older piston engines. [ 115 ]  He was similarly disappointed with the automobile industry, which showed no interest in his proposals for more fuel-efficient engines.  General Motors  told him that \"even if it were a better engine, [General Motors] would not be interested in it.\" [ 116 ]  Bush likewise deplored trends in advertising. \"Madison Avenue believes\", he said, \"that if you tell the public something absurd, but do it enough times, the public will ultimately register it in its stock of accepted verities.\" [ 117 ] From 1947 to 1962, Bush was on the board of directors for  American Telephone and Telegraph . He retired as president of the Carnegie Institution and returned to Massachusetts in 1955, [ 114 ]  but remained a director of Metals and Controls Corporation from 1952 to 1959, and of  Merck & Co.  1949–1962. [ 118 ]  Bush became chairman of the board at Merck following the death of  George W. Merck , serving until 1962. He worked closely with the company's president,  Max Tishler , although Bush was concerned about Tishler's reluctance to delegate responsibility. Bush distrusted the company's sales organization, but supported Tishler's research and development efforts. [ 119 ]  He was a trustee of Tufts College 1943–1962, of Johns Hopkins University 1943–1955, of the Carnegie Corporation of New York 1939–1950, the Carnegie Institution of Washington 1958–1974, and the George Putnam Fund of Boston 1956–1972, and was a regent of the  Smithsonian Institution  1943–1955. [ 120 ] After suffering a stroke, Bush died in  Belmont, Massachusetts  at the age of 84 from  pneumonia  on June 28, 1974. He was survived by his sons Richard (a surgeon) and John (president of  Millipore Corporation ) and by six grandchildren and his sister Edith. Bush's wife had died in 1969. [ 121 ]  He was buried at South Dennis Cemetery in  South Dennis, Massachusetts , [ 122 ]  after a private funeral service. At a public memorial subsequently held by MIT, [ 123 ]   Jerome Wiesner  declared \"No American has had greater influence in the growth of science and technology than Vannevar Bush\". [ 118 ] In 1980, the National Science Foundation created the  Vannevar Bush Award  to honor his contributions to public service. [ 131 ]  The Vannevar Bush papers are located in several places, with the majority of the collection held at the Library of Congress. Additional papers are held by the MIT Institute Archives and Special Collections, the Carnegie Institution, and the National Archives and Records Administration. [ 132 ] [ 133 ] [ 134 ] \nAs of 2023 [update] , the Vannevar Bush Distinguished Professor is  Michael Levin , an American  developmental  and  synthetic biologist  at  Tufts University . [ 135 ] In the 1947 film  The Beginning or the End , Bush is played by  Jonathan Hale . Bush is played by  Matthew Modine  in  Christopher Nolan 's 2023 film  Oppenheimer . [ 137 ] (complete list of published papers:  Wiesner 1979 , pp. 107–117)."
  },
  {
    "id": 46,
    "title": "Emanuel Goldberg",
    "content": "Emanuel Goldberg   ( Hebrew :  עמנואל גולדברג ;  Yiddish :  עמנואל גאָלדבערג ;  Russian :  Эмануэль Гольдберг ; 31   August 1881 – 13   September 1970) was an Israeli physicist and inventor. He was born in  Moscow  and moved first to  Germany  and later to  Israel . He described himself as \"a chemist by learning, physicist by calling, and a mechanic by birth.\" He contributed a wide range of theoretic and practical advances relating to light and media and was the founding head of  Zeiss Ikon , the famous photographic products company in  Dresden , Germany. His inventions include  microdots , the  Kinamo   movie camera , the  Contax  35 mm camera, a very early  search engine , and equipment for  sensitometry . Goldberg was born in Moscow on 31 August 1881 (19 August 1881 in the  Old Style , Julian calendar, sometimes given in error as 1 September) the son of Grigorii Ignat'evich Goldberg, a distinguished Colonel (Polkovnik) in the  Tsar 's military medical corps and his wife Olga Moiseevna Grodsenka. Earlier interested in engineering, he studied  Chemistry  at the  University of Moscow  and at several German universities, and remained in Germany after 1904 to avoid  antisemitism  in Russia. In 1906 he received a Ph.D. from the  University of Leipzig  for research  at the Institute for Physical Chemistry, led by  Wilhelm Ostwald  on the kinetics of photochemical reactions. After a year as assistant to  Adolf Miethe  in the Photochemistry Laboratory at the  Technical University in Charlottenburg , Berlin, he became head of the photographic department of the Royal Academy of Graphic Arts and Bookcraft, in Leipzig from 1907 to 1917. In 1917, Goldberg was recruited by the  Carl Zeiss  Stiftung to become a director of its photographic products subsidiary  Ica  (Internationale Camera Aktien Gesellschaft) [ 1 ]  in Dresden where he introduced the spring-driven Kinamo movie camera. In 1926 a \"Fusion\" of four leading photographic firms (Contessa, Ernemann, Ica and Goerz) formed Zeiss Ikon under Goldberg's leadership until he was kidnapped by Nazis in 1933 and fled to Paris. After four years working for Zeiss subsidiaries in France, Goldberg moved to  Palestine  in 1937 where he established a laboratory, later called Goldberg Instruments, which became the Electro-Optical Industries (\"El-Op\") in  Rehovot . A photograph taken 1943 by  John Phillips  for  Life Magazine  shows Goldberg in his workshop in Palestine. [ 2 ]  He retired in 1960 but continued his research and died in  Tel Aviv  on 13 September 1970. Goldberg patented improved methods for  electroplating  zinc on iron in 1902 and published numerous technical papers on improved printing techniques, reducing  moiré  effects in  half-tone  printing,  photoengraving  and other topics. In 1910 he became well known for an improved method for making neutral gelatin wedges (\" Goldberg wedge \") that was widely used in  sensitometry  and the  Densograph  [ de ] , an instrument that greatly reduced the labor required to measure the characteristic curves of  photographic emulsions . At Ica, foreseeing a growing market in amateur and semi-professional movies, he designed an extremely compact 35 mm movie camera, the Kinamo, introduced in 1921 with a spring motor attachment added in 1923 to allow flexible handheld filming. Goldberg made films of himself and his family as promotional shorts and, in 1927, a skiing drama,  \"Ein Sprung . . . Ein Traum.\"  The Kinamo was used by  Joris Ivens  and other avant-garde and documentary filmmakers in the late 1920s and early 1930s. In 1925 Goldberg demonstrated and published a technique for making  microdot  (Mikrat nach Goldberg) at a resolution equivalent to the text 50 complete  Bibles  per square inch. This invention has been widely attributed to a mythical \"Professor Zapp\" based on  J. Edgar Hoover 's erroneous article in the April 1946  Reader's Digest , probably a confusion with Kurt Zapp who trained German spies in microdot photography during the  Second World War . In 1937, Goldberg presented a paper at the  World Congress of Universal Documentation  on an early copying camera he had invented. [ 3 ] At Ica and Zeiss Ikon, Goldberg was involved in many innovations and led the design of famous  Contax  35 mm still camera. Goldberg was best known for his extensive studies in sensitometry summarized in his book  Der Aufbau des photographischen Bildes  (1922) and the  \"Goldberg Condition\"  (Goldberg Bedingung), a design principle for high quality reproduction in two stage, negative-positive photographic processes better known in English as \"the gamma rule.\" Goldberg and his former teacher and collaborator  Robert Luther  [ de ]  were instrumental in the acceptance at the International Congress of Photography in Dresden in 1931 of the widely adopted German national  film speed  standard  DIN 4512 . At the same Congress Goldberg introduced his \"Statistical Machine,\" a document search engine that used  photoelectric  cells and  pattern recognition  to search the  metadata  on rolls of  microfilmed  documents (US patent 1,838,389, 29 December 1931). This technology was used in a variant form in 1938 by  Vannevar Bush  in his \"microfilm rapid selector,\" his \"comparator\" (for cryptanalysis), and was the technological basis for the imaginary  Memex  in Bush's influential 1945 essay \"As we may think.\" In Germany Goldberg was noted for his educational displays at exhibitions, served as consultant on  aerial photography  in the  First World War , and was a consultant to the Carl Zeiss firm in Jena. In Palestine and later Israel he was deeply engaged as an advisory to both civilian and military spheres. The apprenticeship scheme that he introduced in Tel Aviv provided advanced technical skills to many who went on to develop the Israeli high tech industry. In 1968, Goldberg was awarded the  Israel Prize , in exact science. [ 4 ] On 28 June 1907 Goldberg married Sophie Posniak (28   August 1886 – 10   December 1968). They had a son, Herbert Goldberg (born 20 November 1914) and a daughter Renate Eva, who later changed her name to Chava (born 19 September 1922). Chava married  Mordechai Gichon  in 1948. Goldberg's great-granddaughter is woodworker  Aspen Gollan . In 1990 Chava Gichon ( Tel-Aviv ) requested restitution of the property in the Oeserstraße 5 in Dresden, which Goldberg as director of Zeiss-Ikon had bought in 1927, but the real estate office did not approve an intended agreement with the owner at that time in August 1995. [ 5 ]"
  },
  {
    "id": 47,
    "title": "UNIVAC",
    "content": "UNIVAC  ( Universal Automatic Computer ) was a line of electronic digital stored-program  computers  starting with the products of the  Eckert–Mauchly Computer Corporation . Later the name was applied to a division of the  Remington Rand  company and successor organizations. The  BINAC , built by the Eckert–Mauchly Computer Corporation, was the first general-purpose computer for commercial use, but it was not a success. The last UNIVAC-badged computer was produced in 1986. J. Presper Eckert  and  John Mauchly  built the  ENIAC  (Electronic Numerical Integrator and Computer) at the  University of Pennsylvania 's  Moore School of Electrical Engineering  between 1943 and 1946. A 1946 patent rights dispute with the university led Eckert and Mauchly to depart the Moore School to form the Electronic Control Company, later renamed  Eckert–Mauchly Computer Corporation  (EMCC), based in  Philadelphia, Pennsylvania .  That company first built a computer called  BINAC  (BINary Automatic Computer) for  Northrop Aviation  (which was little used, or perhaps not at all).  Afterwards began the development of UNIVAC in April 1946. [ 1 ]  UNIVAC was first intended for the  Bureau of the Census , which paid for much of the development, and then was put in production. With the death of EMCC's chairman and chief financial backer  Henry L. Straus  in a plane crash on October 25, 1949, EMCC was sold to typewriter, office machine, electric razor, and gun maker Remington Rand on February 15, 1950. Eckert and Mauchly now reported to  Leslie Groves [ citation needed ] , the retired army general who had previously managed building  The Pentagon  and led the  Manhattan Project . The most famous UNIVAC product was the  UNIVAC I   mainframe computer  of 1951, which became known for predicting the outcome of the U.S. presidential election the following year: this incident is noteworthy because the computer correctly predicted an Eisenhower landslide over  Adlai Stevenson , whereas the final Gallup poll had Eisenhower winning the popular vote 51–49 in a close contest. [ 2 ] The prediction led  CBS 's news boss in New York,  Siegfried Mickelson , to believe the computer was in error, and he refused to allow the prediction to be read. Instead, the crew showed some staged theatrics that suggested the computer was not responsive, and announced it was predicting 8–7 odds for an Eisenhower win (the actual prediction was 100–1 in his favour). When the predictions proved true—Eisenhower defeated Stevenson in a landslide, with UNIVAC coming within 3.5% of his popular vote total and four votes of his Electoral College total— Charles Collingwood , the on-air announcer, announced that they had failed to believe the earlier prediction. [ 3 ] The  United States Army  requested a UNIVAC computer from Congress in 1951. Colonel Wade Heavey explained to the Senate subcommittee that the national mobilization planning involved multiple industries and agencies: \"This is a tremendous calculating process...there are equations that can not be solved by hand or by electrically operated computing machines because they involve millions of relationships that would take a lifetime to figure out.\" Heavey told the subcommittee it was needed to help with mobilization and other issues similar to the  invasion of Normandy  that were based on the relationships of various groups. [ 4 ] The UNIVAC was manufactured at Remington Rand's former Eckert-Mauchly Division plant on W Allegheny Avenue in  Philadelphia, Pennsylvania . [ 5 ] [ 6 ]  Remington Rand also had an engineering research lab in  Norwalk, Connecticut , and later bought  Engineering Research Associates  (ERA) in  St. Paul, Minnesota . [ 5 ]  In 1953 or 1954 Remington Rand merged their Norwalk tabulating machine division, the ERA \"scientific\" computer division, and the UNIVAC \"business\" computer division into a single division under the UNIVAC name. This severely annoyed those who had been with ERA and with the Norwalk laboratory. [ citation needed ] In 1955 Remington Rand merged with  Sperry Corporation  to become Sperry Rand. General  Douglas MacArthur , then the chairman of the Board of Directors of Remington Rand, was chosen to continue in that role in the new company. [ 7 ]   Harry Franklin Vickers , then the President of Sperry Corporation, continued as president and CEO of Sperry Rand. [ 7 ]  The UNIVAC division of Remington Rand was renamed the Remington Rand Univac division of Sperry Rand. [ 5 ]   William Norris  was put in charge as Vice-President and General Manager [ 8 ]  reporting to the President of the Remington Rand Division (of Sperry Rand). [ 9 ] The following is a list of the General Managers/Presidents of the Division. There was a some degree of internal organisation turmoil from the period of the creation of Sperry Rand in 1955 right into the early 1960s. This culminated in the resignation of William Norris in 1957 [ 9 ]  and would continue until the early 1960s with the decentralisation of the former Remington Group and the promotion of UNIVAC to a full division of Sperry Rand. In the 1960s, UNIVAC was one of the eight major American computer companies in an industry then referred to as \" IBM  and the seven dwarfs\" – a play on  Snow White  and the seven dwarfs, with IBM, by far the largest, being cast as Snow White and the other seven as being dwarfs:  Burroughs , Univac,  NCR ,  CDC ,  GE ,  RCA  and  Honeywell . [ 22 ]  In the 1970s, after GE sold its computer business to Honeywell and RCA sold its to Univac, the analogy to the seven dwarfs became less apt and the remaining small firms became known as the \" BUNCH \" ( B urroughs,  U nivac,  N CR,  C ontrol Data, and  H oneywell). In 1977, Sperry Rand purchased  Varian Data Machines  so as to enter the  minicomputer  market. Varian would be renamed as the Sperry UNIVAC Minicomputer Operation, operating as part of the Sperry UNIVAC division. [ 23 ] [ 24 ]  Sperry UNIVAC would continue to market the V77 but never made a significant dent in the minicomputer market. To assist \"corporate identity\" the name was changed to Sperry Univac, along with Sperry Remington,  Sperry New Holland , etc. In 1978, Sperry Rand, a conglomerate of various divisions (computers, typewriters, office furniture, hay balers, manure spreaders, gyroscopes, avionics, radar, electric razors), decided to concentrate solely on its computing interests and all of the unrelated divisions were sold. The company dropped the  Rand  from its title and reverted to Sperry Corporation. In 1981/82 the distinct Sperry UNIVAC branding was dropped and the division was renamed as the Sperry Computer Systems Division. [ 21 ] [ 25 ]  In 1986, Sperry Corporation merged with  Burroughs Corporation  to become  Unisys . After the 1986 merger of Burroughs and Sperry, Unisys evolved from a computer manufacturer to a computer services and  outsourcing  firm, competing at that time in the same marketplace as  IBM ,  Electronic Data Systems  (EDS), and  Computer Sciences Corporation . As of 2021 [update] , Unisys continues to design and manufacture enterprise class computers with the ClearPath server lines. [ 26 ] In the course of its history, UNIVAC produced a number of separate model ranges. One early UNIVAC line of  vacuum tube  computers was based on the ERA 1101 and those models built at ERA were rebadged as UNIVAC 110x; despite the 1100 model numbers, they were not related to the latter 1100/2200 series. The 1103A is credited in the literature as the first computer to have interrupts. The original model range was the  UNIVAC I  (UNIVersal Automatic Computer I), the second commercial computer made in the United States. [ a ]  The main memory consisted of tanks of liquid mercury implementing  delay-line memory , arranged in 1,000 words of 12 alphanumeric characters each. The first machine was delivered on 31 March 1951. The  Remington Rand 409  was a  control panel  programmed  punched card  calculator, designed in 1949, and sold in two models: the UNIVAC 60 (1952) and the UNIVAC 120 (1953). The  UNIVAC File Computer  was first shipped in 1956. It was equipped with between one and ten large drums each holding 180,000 Alphanumeric characters. [ 27 ]   One early application was for an  airline reservations system , [ 28 ]  which was used by  Eastern Air Lines . [ 29 ]  It competed mainly against the  IBM 650  and the  IBM 305 RAMAC  and a total of 130 were manufactured. [ 30 ] The  UNIVAC II  was an improvement to the  UNIVAC I  that UNIVAC first delivered in 1958. The improvements included magnetic (non-mercury)  core memory  of 2,000 to 10,000 words, UNISERVO II tape drives, which could use either the old UNIVAC I metal tapes or the new  PET film  tapes, and some circuits that were  transistorized  (although it was still a  vacuum-tube computer ). It was fully compatible with existing UNIVAC I programs for both code and data. The UNIVAC II also added some instructions to the UNIVAC I's instruction set. The  UNIVAC Solid State  was a 2-address, decimal computer, with memory on a rotating drum with 5,000 signed 10-digit words, aimed at the general-purpose business market. It came in two versions: the Solid State 80 (IBM-Hollerith 80-column cards) and the Solid State 90 (Remington-Rand 90-column cards). This computer used  magnetic logic , not transistors, because the transistors then available had highly variable characteristics and were not sufficiently reliable. Magnetic logic gates were based on magnetic cores with multiple wire windings; unlike vacuum tubes, they were solid-state devices and had a virtually infinite lifetime. The magnetic gates required drive pulses of current produced by a transmitter-type vacuum tube, of a type still used in amateur radio final amplifiers. Thus the Solid State depended, at the heart of its operations, on a vacuum tube, however, only a few tubes were required, instead of thousands, greatly increasing reliability. Sperry Rand began shipment of  UNIVAC III  in 1962, and produced 96 UNIVAC III systems. Unlike the UNIVAC I and UNIVAC II, it was a binary machine as well as maintaining support for all UNIVAC I and UNIVAC II decimal and alphanumeric data formats for backward compatibility. This was the last of the original UNIVAC machines. The  UNIVAC 418  (aka 1219), first shipped in 1962, was an  18-bit  word core memory machine. Over the three different models, more than 392 systems were manufactured. The  UNIVAC 490  was a 30-bit word core memory machine with 16K or 32K words; 4.8 microsecond cycle time. The UNIVAC 1232 was a military version of the 490. [ 31 ] The  UNIVAC 492  is similar to the  UNIVAC 490 , but with  extended memory  to 64K 30-bit words. The  UNIVAC 494  was a 30-bit  word  machine and successor to the UNIVAC 490/492 with faster  CPU  and 131K (later 262K) core memory. Up to 24 I/O channels were available and the system was usually shipped with UNIVAC FH880 or UNIVAC FH432 or FH1782 magnetic drum storage. Basic operating system was OMEGA (successor to REX for the 490) although custom operating systems were also used (e.g. CONTORTS for airline reservations). The  UNIVAC 1050  was an internally programmed computer with up to 32K of six-bit character memory, which was introduced in 1963. It was a one-address machine with 30-bit instructions, had a 4K operating system and was programmed in the PAL assembly language. The 1050 was used extensively by the U.S. Air Force supply system for inventory control (The Standard Base Level Supply System  [ 32 ] [ 33 ] ). The  UNIVAC 1004  was a plug-board programmed punched-card data processing system, introduced in 1962 by UNIVAC. Total memory was 961 characters (6 bits per character) of  core memory . Peripherals were a card reader (400 cards/minute), a card punch (200 cards/minute) using proprietary 90-column, round-hole cards or IBM-compatible, 80-column cards, a drum printer (400 lines/minute) and a Uniservo tape drive. [ 34 ]  The 1004 was also supported as a remote card reader & printer via synchronous communication services. A U.S. Navy (Weapons Station, Concord) 1004 was dedicated to printing from tape as a means of offloading the task from their Solid State 80 mainframe, which produced the tapes. A design for an \"Emulator\" board was available that would allow the plugboard 1004 to run programs read from card decks. The board was made by the customers, not by UNIVAC. [ 35 ]  However, the Emulator made heavy use of the 1004's program-branching reed relays, called selectors, which caused increased failures, later solved by the use of electronic selectors in the follow-on 1005. The  UNIVAC 1005 , an enhanced version of the UNIVAC 1004, was first shipped in February 1966. [ 36 ]  The machine saw extensive use by the  US Army , including the first use of an electronic computer on the battlefield. Additional peripherals were also available including a paper tape reader and a three pocket stacker selectable card read/punch. The machine had a two-stage assembler (SAAL – Single Address Assembly Language) which was its primary assembler; it also had a three-stage card based compiler for a programming language called SARGE. 1005s were used as some nodes on  Autodin . There were actually two versions of the 1005. The Federal Systems (military) version described above and a Commercial Systems version for civilian use. While the two versions shared common memory and peripherals they had two completely different instruction sets. [ citation needed ]    The Commercial Systems version had a three pass assembler and a program generator. The  UNIVAC 1100/2200 series  is a series of compatible 36-bit  transistorized  computer systems initially made by Sperry Rand. The first true member of the series was the 1107, also known as the Thin-Film Computer due to its use of  Thin-film memory  for its Control Memory store (128 registers). Delivery of the 1107 was late and this affected sales; the subsequent 1108 was considerably more successful, and helped to establish the series as viable competitors to the  IBM System/360 . The series continues to be supported today by  Unisys Corporation  as the ClearPath Forward Dorado Series. [ 37 ] The  UNIVAC 9000 series  (9200, 9300, 9400, 9700) was introduced in the mid-1960s to compete with the low end of the IBM 360 series. The 9200 and 9300, which differed in CPU speed and maximum memory capacity (16K for the original 9200 vs 32K for the other variants) implemented the same 16-bit modified subset of the 360 architecture as the  Model 20 , while the UNIVAC 9400 implemented a subset of the full 360 instruction set. This did not violate IBM patents or copyrights; Sperry gained the right to \"clone\" the 360 as settlement of a lawsuit concerning IBM's infringement of Remington Rand's core memory patents. The 9400 was roughly equivalent to the IBM 360/30. The 9000 series used  plated-wire memory , which functioned somewhat like  core memory  but used a non-destructive read. Since the 9000 series was intended as direct competitors to IBM, they used 80-column cards and  EBCDIC  character encoding. Memory capacity started as low as 8K byte primary storage for a batch-configured system. Optionally a disk drive subsystem could be added, with 8414 5 MB disk drives as well as tape drives, using the Uniservo VI. The  UNIVAC Series 90 : The 1107 was the first 36-bit,  word-oriented  machine with an  architecture  close to that which came to be known as that of the \" 1100 Series .\" It ran the  EXEC I  or  EXEC II  operating system,  batch-oriented  second-generation  operating systems , typical of the early to mid-1960s. The 1108 ran EXEC II or  EXEC 8 . EXEC 8 allowed simultaneous handling of real-time applications,  time-sharing , and background batch work.  Transaction Interface Package  (TIP), a transaction-processing environment, allowed programs to be written in  COBOL  whereas similar programs on competing systems were written in assembly language. On later systems, EXEC 8 was renamed  OS 1100  and  OS 2200 , with modern descendants maintaining backwards compatibility. Some more exotic operating systems ran on the 1108 – one of which was RTOS, a more bare-bones system designed to take better advantage of the hardware. The affordable System 80 series of small mainframes ran the OS/3 operating system which originated on the Univac 90/30 (and later 90/25, and 90/40). The  UNIVAC Series 90  first ran with Univac developed OS/9, which was later replaced by RCA's  Virtual Memory Operating System  (VMOS).  RCA originally called this operating system Time Sharing Operating System (TSOS), running on RCA's  Spectra 70  line of virtual memory systems and changed its name to VMOS before the Sperry acquisition of RCA CSD.  After VMOS was ported to the 90/60, Univac renamed it  VS/9 . UNIVAC  has been, over the years, a registered trademark of:"
  },
  {
    "id": 48,
    "title": "Desk Set",
    "content": "Desk Set  (released as  His Other Woman  in the UK) is a 1957 American  romantic comedy  film  starring  Spencer Tracy  and  Katharine Hepburn . Directed by  Walter Lang , the picture's screenplay was written by  Phoebe Ephron  and  Henry Ephron , adapted from the 1955 play of the same name by  William Marchant . Bunny Watson is in charge of the reference library at the Federal Broadcasting Network in  Midtown Manhattan . The reference librarians are responsible for researching facts and answering questions for the general public on all manner of topics, great and small. Bunny has been romantically involved for seven years with rising network executive Mike Cutler, but with no marriage in sight. Methods engineer and  efficiency expert  Richard Sumner is the inventor of EMERAC (\" E lectromagnetic  ME mory and  R esearch  A rithmetical  C alculator\"), nicknamed \"Emmy,\" a powerful early generation  computer  (referred to then as an \"electronic brain\").  He is brought in to see how the library functions, and size it up for installation of one of his massive machines. Despite Bunny's initial intransigence, Richard is surprised and intrigued to discover how stunningly capable and engaging she is. When her staff finds out the computer is coming, they jump to the conclusion they are being replaced. After an innocuous but seemingly salacious situation that Mike walks in on at Bunny's apartment, he recognizes the older Richard has emerged as a romantic rival, and begins to want to commit to Bunny. Bunny's fear of unemployment seems confirmed when she and everyone on her staff receive a  pink \"layoff\" slip  printed out by a similar new EMERAC already installed in payroll. But it turns out to have been a mistake – the machine fired everybody in the company, including the president. The network has kept everything hush-hush to avoid tipping off competitors that a merger was in the works. Rather than replace the research staff, \"Emmy\" was installed to help the employees cope with the extra work. With the threat of displacement out of the way, Richard reveals his romantic interest to Bunny, but she believes that EMERAC will always be his first love. He denies it, but then Bunny puts him to the test, pressing the machine beyond its limits. Richard resists the urge to fix it as long as possible, but finally gives in and forces an emergency shutdown. Bunny then accepts his marriage proposal. In the play, Bunny Watson (played by  Shirley Booth , who was originally intended for the film as well) had only brief, somewhat hostile interactions with Richard Sumner. Screenwriters Phoebe and Henry Ephron (the parents of  Nora Ephron ) built up the role of the efficiency expert and tailored the interactions between him and the researcher to fit Spencer Tracy and Katharine Hepburn. [ 3 ] The exterior shots of the \"Federal Broadcasting Network\" seen in the film is actually the  RCA  Building (now known as the  Comcast  Building) at  30 Rockefeller Plaza  in  Rockefeller Center , the headquarters of  NBC . The character of Bunny Watson was based on Agnes E. Law, a real-life librarian at  CBS  who retired about a year before the film was released. [ 4 ] [ 5 ] This film was the eighth screen pairing of Hepburn and Tracy, after a five-year respite since 1952's  Pat and Mike ,  and was a first for Hepburn and Tracy in several ways: the first non-MGM film the two starred in together, their first color film, and their first  CinemaScope  film. Following  Desk Set  their last film together would be 1967's  Guess Who's Coming to Dinner . The computer referred to as EMERAC is a  homoiophone   metonym  for  ENIAC  (\" E lectronic  N umerical  I ntegrator  A nd  C omputer\"), which was developed in the 1940s and was the first electronic general-purpose computer. Parts of the EMERAC computer, particularly the massive display of moving square lights, would later be seen in various  20th Century Fox  productions including both the  motion picture (1961)  and  TV (1964–1968) versions  of  Voyage to the Bottom of the Sea  and the Edgar Hopper segment of the 1964 film  What a Way to Go! . The researchers furnish incorrect information about the career of baseball player  Ty Cobb . Miss Costello claims his major league career lasted for 21 years, and that he played only for the  Detroit Tigers .  In fact, he played for 24 years—22 with Detroit, and his final two seasons with the  Philadelphia Athletics . There is a well-known \"goof\" in one scene. Mike gives Bunny an arrangement of white carnations, and she inserts one in his lapel's button-hole. At the end of the day, she and Richard leave the office. She is carrying the white carnation arrangement as they enter the elevator. As they exit the building, the carnations are pink. [ 6 ] Bosley Crowther , film critic of  The New York Times , felt the film was \"out of dramatic kilter\", inasmuch as Hepburn was simply too \"formidable\" to convincingly play someone \"scared by a machine\", resulting in \"not much tension in this thoroughly lighthearted film\". [ 7 ] The  New York Post  review was mixed: \"There are such sops to sentiment as Miss Hepburn's willingness to be dragged altarwards by the young head of her department,  Gig Young , who kindly lets her do her most impressive work, and a growing understanding between Hepburn and the rather remote and intellectually Olympian Tracy....Running true to form, the sex narrative follows a predictable pattern, rewarding honest virtue and slapping down the unworthy, and the other, scientific trail is permitted a twist that may surprise any who have found themselves emotionally involved in that timely problem of technological unemployment....'Desk Set,' let us conclude, is a shining piece of machinery brought to a high polish, and, delivered with appropriate performances, flourishes. Affection, though, it cannot inspire.\" [ 8 ] TIME  magazine  wrote: \"At long last, somebody has a kind word for the girls in the research department. The word: one of those electronic brains could do the job much better and with less back chat—and what's more, it would free the girls' energies for the more important job of getting a man....Desk Set has been expanded [from the play] by a sizable pigeonhole, in which [Hepburn and Tracy] intermittently bill and coo....On the whole, the film compares favorably with the play....And though Actress Hepburn tends to wallow in the wake of Shirley Booth...she never quite sinks in the comic scenes, and in the romantic ones she is light enough to ride the champagne splashes of emotion as if she were going over Niagara in a barrel. Spencer Tracy has one wonderful slapstick scene, and Gig Young does very well with a comic style for which he is much beholden to William Holden.\" [ 9 ] The  Philadelphia Inquirer  was critical: \"The middle-aged excesses of Miss Hepburn and Tracy...leave a good deal to be desired. Equipped with an insubstantial vehicle, bogged down by surprisingly flat-footed direction...the stars come close to being embarrassing as they bound through roles involving them in office nonsense about a mechanical brain, a bibulous Christmas party, an innocent, but suspecting, dinner in negligee Katie's rain-bound flat....Marchant's foolish little comedy gains nothing via the Phoebe and Henry Ephron adaptation. Long recitations from \"Hiawatha\" and \"The Curfew Shall Not Ring Tonight,\" plus question-and-answer games...in addition to the repetitiousness of the central idea...turn 'Desk Set's' 104 minutes into an endurance contest for cast and audience.\" [ 10 ] Today the film is seen far more favorably, with the sharpness of the script praised in particular. It has achieved a rare 100% rating on  Rotten Tomatoes  based on 22 reviews, with a  weighted average  of 6.78/10. The site's consensus reads: \" Desk Set  reunites one of cinema's most well-loved pairings for a solidly crafted romantic comedy that charmingly encapsulates their timeless appeal\". [ 11 ]  Dennis Schwartz of Dennis Schwartz Movie Reviews called it an \"inconsequential sex comedy,\" but contended \"the star performers are better than the material they are given to work with\" and that \"the comedy was so cheerful and the banter between the two was so refreshingly smart that it was easy to forgive this bauble for not being as rich as many of the legendary duo's other films together.\" [ 12 ] A  Canadian  radio program,  Bunny Watson , was named for and inspired by Hepburn's character."
  },
  {
    "id": 49,
    "title": "Gerard Salton",
    "content": "Gerard A. \"Gerry\" Salton  (8 March 1927 – 28 August 1995) was a professor of  Computer Science  at  Cornell University . Salton was perhaps the leading computer scientist working in the field of  information retrieval  during his time, and \"the father of Information Retrieval\". [ 2 ]   His group at Cornell developed the  SMART Information Retrieval System , which he initiated when he was at  Harvard . It was the first system to use the now popular  vector space  model for information retrieval. Salton was born Gerhard Anton Sahlmann on in  Nuremberg, Germany . He came to the United States in 1947 and was naturalized in 1952. He received a Bachelor's (1950) and Master's (1952) degree in mathematics from  Brooklyn College , and a Ph.D. from  Harvard  in  applied mathematics  in 1958, the last of  Howard Aiken 's doctoral students, and taught there until 1965, when he joined  Cornell University  and co-founded its department of Computer Science. Salton was perhaps most well known for developing the now widely used  vector space model  for Information Retrieval. [ 3 ]   In this model, both documents and queries are represented as vectors of term counts, and the similarity between a document and a query is given by the cosine between the term vector and the document vector.  In this paper, he also introduced  TF-IDF , or term-frequency-inverse-document frequency, a model in which the score of a term in a document is the ratio of the number of terms in that document divided by the frequency of the number of documents in which that term occurs. (The concept of inverse document frequency, a measure of specificity, had been introduced in 1972 by  Karen Sparck-Jones . [ 4 ] ) Later in life, he became interested in automatic text summarization and analysis, [ 5 ]  as well as automatic hypertext generation. [ 6 ]   He published over 150 research articles and 5 books during his life. Salton was editor-in-chief of the  Communications of the ACM  and the  Journal of the ACM , and chaired  Special Interest Group on Information Retrieval  (SIGIR).  He was an associate editor of the  ACM Transactions on Information Systems . He was an  ACM Fellow  (elected 1995), [ 7 ]  received the  Award of Merit  from the  American Society for Information Science  (1989), and was the first recipient of the SIGIR Award for outstanding contributions to study of Information Retrieval (1983) -- now called the  Gerard Salton Award ."
  },
  {
    "id": 50,
    "title": "Text corpus",
    "content": "In  linguistics  and  natural language processing , a  corpus  ( pl. :  corpora ) or  text corpus  is a dataset, consisting of  natively digital and older, digitalized,  language resources , either annotated or unannotated. Annotated, they have been used in  corpus linguistics  for statistical  hypothesis testing , checking occurrences or validating linguistic rules within a specific language territory. A corpus may contain texts in a single language ( monolingual corpus ) or text data in multiple languages ( multilingual corpus ). In order to make the corpora more useful for doing linguistic research, they are often subjected to a process known as  annotation . An example of annotating a corpus is  part-of-speech tagging , or  POS-tagging , in which information about each word's part of speech (verb, noun, adjective, etc.) is added to the corpus in the form of  tags . Another example is indicating the  lemma  (base) form of each word. When the language of the corpus is not a working language of the researchers who use it,  interlinear glossing  is used to make the annotation bilingual. Some corpora have further  structured  levels of analysis applied. In particular, smaller corpora may be fully  parsed . Such corpora are usually called  Treebanks  or  Parsed Corpora . The difficulty of ensuring that the entire corpus is completely and consistently annotated means that these corpora are usually smaller, containing around one to three million words. Other levels of linguistic structured analysis are possible, including annotations for  morphology ,  semantics  and  pragmatics . Corpora are the main knowledge base in  corpus linguistics . Other notable areas of application include:"
  },
  {
    "id": 51,
    "title": "National Institute of Standards and Technology",
    "content": "The  National Institute of Standards and Technology  ( NIST ) is an agency of the  United States Department of Commerce  whose mission is to promote American innovation and industrial competitiveness. NIST's activities are organized into  physical science  laboratory programs that include  nanoscale science and technology ,  engineering ,  information technology ,  neutron  research, material measurement, and physical measurement.  From 1901 to 1988, the agency was named the  National Bureau of Standards . [ 4 ] The  Articles of Confederation , ratified by the colonies in 1781, provided: The United States in Congress assembled shall also have the sole and exclusive right and power of regulating the alloy and value of coin struck by their own authority, or by that of the respective states—fixing the standards of weights and measures throughout the United States. [ 5 ] Article 1, section 8, of the  Constitution of the United States , ratified in 1789, granted these powers to the new Congress: \"The Congress shall have power ... To coin money, regulate the value thereof, and of foreign coin, and fix the standard of weights and measures\". [ 6 ] In January 1790,  President   George Washington , in his first  annual message to Congress , said, \"Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to.\" [ 7 ] On October 25, 1791, Washington again appealed Congress: A uniformity of the weights and measures of the country is among the important objects submitted to you by the Constitution and if it can be derived from a standard at once invariable and universal, must be no less honorable to the public council than conducive to the public convenience. [ 8 ] In 1821,  President   John Quincy Adams  declared, \"Weights and measures may be ranked among the necessities of life to every individual of human society.\". [ 9 ]  Nevertheless, it was not until 1838 that the United States government adopted a uniform set of standards. [ 6 ] From 1830 until 1901, the role of overseeing weights and measures was carried out by the Office of Standard Weights and Measures, which was part of the Survey of the Coast—renamed the United States Coast Survey in 1836 and the  United States Coast and Geodetic Survey  in 1878—in the  United States Department of the Treasury . [ 10 ] [ 11 ] [ 12 ] In 1901, in response to a bill proposed by Congressman  James H. Southard  (R, Ohio), the  National Bureau of Standards  was founded with the mandate to provide standard weights and measures, and to serve as the national physical laboratory for the United States. Southard had previously sponsored a bill for metric conversion of the United States. [ 13 ] President  Theodore Roosevelt  appointed  Samuel W. Stratton  as the first director. The budget for the first year of operation was $40,000. The Bureau took custody of the copies of the  kilogram  and  meter  bars that were the standards for US measures, and set up a program to provide  metrology  services for United States scientific and commercial users. A laboratory site was constructed in  Washington, DC , and instruments were acquired from the national physical laboratories of Europe. In addition to weights and measures, the Bureau developed instruments for electrical units and for measurement of light. In 1905 a meeting was called that would be the first \"National Conference on Weights and Measures\". Initially conceived as purely a  metrology  agency, the Bureau of Standards was directed by  Herbert Hoover  to set up divisions to develop commercial standards for materials and products. [ 13 ]  Some of these standards were for products intended for government use, but product standards also affected private-sector consumption. Quality standards were developed for products including some types of clothing, automobile brake systems and headlamps,  antifreeze , and electrical safety. During  World War I , the Bureau worked on multiple problems related to war production, even operating its own facility to produce  optical glass  when European supplies were cut off. Between the wars,  Harry Diamond  of the Bureau developed a  blind approach  radio aircraft landing system. During  World War II, military research and development  was carried out, including development of  radio propagation  forecast methods, the  proximity fuze  and the standardized airframe used originally for  Project Pigeon , and shortly afterwards the autonomously radar-guided  Bat  anti-ship guided bomb and the  Kingfisher family  of torpedo-carrying missiles. In 1948, financed by the United States Air Force, the Bureau began design and construction of  SEAC , the Standards Eastern Automatic Computer. The computer went into operation in May 1950 using a combination of  vacuum tubes  and solid-state  diode  logic. About the same time the  Standards Western Automatic Computer , was built at the Los Angeles office of the NBS by  Harry Huskey  and used for research there. A mobile version,  DYSEAC , was built for the Signal Corps in 1954. Due to a changing mission, the \"National Bureau of Standards\" became the \"National Institute of Standards and Technology\" in 1988. [ 10 ]  Following the  September 11, 2001  attacks, under the National Construction Safety Team Act (NCST),  NIST conducted the official investigation into the  collapse of the World Trade Center  buildings. Following the 2021  Surfside condominium building collapse , NIST sent engineers to the site to investigate the cause of the collapse. [ 14 ] In 2019, NIST launched a program named NIST on a Chip to decrease the size of instruments from lab machines to chip size. Applications include aircraft testing, communication with satellites for navigation purposes, and temperature and pressure. [ 15 ] In 2023, the  Biden administration  began plans to create a U.S. AI Safety Institute within NIST to coordinate  AI safety  matters. According to  The Washington Post , NIST is considered \"notoriously underfunded and understaffed\", which could present an obstacle to these efforts. [ 16 ] NIST, known between 1901 and 1988 as the National Bureau of Standards (NBS), is a  measurement standards laboratory , also known as the National Metrological Institute (NMI), which is a non-regulatory agency of the  United States Department of Commerce . The institute's official mission is to: [ 17 ] Promote U.S. innovation and industrial competitiveness by advancing  measurement science ,  standards , and  technology  in ways that enhance economic security and improve our  quality of life . NIST had an operating  budget  for  fiscal year  2007 (October 1, 2006 – September 30, 2007) of about $843.3 million. NIST's 2009 budget was $992 million, and it also received $610 million as part of the  American Recovery and Reinvestment Act . [ 18 ]  NIST employs about 2,900 scientists, engineers, technicians, and support and administrative personnel. About 1,800 NIST associates (guest researchers and engineers from American companies and foreign countries) complement the staff. In addition, NIST partners with 1,400 manufacturing specialists and staff at nearly 350 affiliated centers around the country. NIST publishes the  Handbook 44  that provides the \"Specifications, tolerances, and other technical requirements for weighing and measuring devices\". The  Congress of 1866  made use of the metric system in commerce a legally protected activity through the passage of  Metric Act of 1866 . [ 19 ]  On May 20, 1875, 17 out of 20 countries signed a document known as the  Metric Convention  or the  Treaty of the Meter , which established the  International Bureau of Weights and Measures  under the control of an international committee elected by the  General Conference on Weights and Measures . [ 20 ] NIST is headquartered in  Gaithersburg, Maryland , and operates a facility in  Boulder, Colorado , which was dedicated by  President   Eisenhower  in 1954. [ 21 ] [ 22 ] [ 23 ]  NIST's activities are organized into laboratory programs and extramural programs. Effective October 1, 2010, NIST was realigned by reducing the number of NIST laboratory units from ten to six. [ 24 ]  NIST Laboratories include: [ 25 ] Extramural programs include: NIST's Boulder laboratories are best known for  NIST‑F1 , which houses an  atomic clock . NIST‑F1 serves as the source of the nation's official time. From its measurement of the natural resonance frequency of  cesium —which defines the  second —NIST broadcasts  time signals  via  longwave  radio station  WWVB  near  Fort Collins , Colorado, and  shortwave   radio stations   WWV  and  WWVH , located near Fort Collins and  Kekaha, Hawaii , respectively. [ 33 ] NIST also operates a  neutron  science user facility: the  NIST Center for Neutron Research  (NCNR). The NCNR provides scientists access to a variety of  neutron scattering  instruments, which they use in many research fields (materials science, fuel cells, biotechnology, etc.). The SURF III Synchrotron Ultraviolet Radiation Facility is a source of  synchrotron radiation , in continuous operation since 1961. SURF III now serves as the US national standard for source-based radiometry throughout the generalized optical spectrum. All  NASA -borne, extreme-ultraviolet observation instruments have been calibrated at SURF since the 1970s, and SURF is used for the measurement and characterization of systems for  extreme ultraviolet lithography . The Center for Nanoscale Science and Technology (CNST) performs research in  nanotechnology , both through internal research efforts and by running a user-accessible  cleanroom   nanomanufacturing  facility. This \"NanoFab\" is equipped with tools for  lithographic  patterning and imaging (e.g.,  electron microscopes  and  atomic force microscopes ). NIST has seven standing committees: As part of its mission, NIST supplies industry, academia, government, and other users with over 1,300  Standard Reference Materials  (SRMs). These artifacts are certified as having specific characteristics or component content, used as calibration standards for measuring equipment and procedures, quality control benchmarks for industrial processes, and experimental control samples. NIST publishes the  Handbook 44  each year after the annual meeting of the  National Conference on Weights and Measures  (NCWM). Each edition is developed through cooperation of the  Committee on Specifications and Tolerances  of the NCWM and the  Weights and Measures Division  (WMD) of NIST. The purpose of the book is a partial fulfillment of the statutory responsibility for \"cooperation with the states in securing uniformity of weights and measures laws and methods of inspection\". NIST has been publishing various forms of what is now the  Handbook 44  since 1918 and began publication under the current name in 1949. The 2010 edition conforms to the concept of the primary use of the SI (metric) measurements recommended by the  Omnibus Foreign Trade and Competitiveness Act of 1988 . [ 34 ] [ 35 ] NIST is developing government-wide  identity document  standards for federal employees and contractors to prevent unauthorized persons from gaining access to government buildings and computer systems. [ 36 ] In 2002, the  National Construction Safety Team Act  mandated NIST to conduct an investigation into the  collapse of the World Trade Center  buildings 1 and 2 and the 47-story 7 World Trade Center. The \"World Trade Center Collapse Investigation\", directed by lead investigator Shyam Sunder, [ 37 ]  covered three aspects, including a technical building and  fire safety  investigation to study the factors contributing to the probable cause of the collapses of the WTC Towers (WTC 1 and 2) and WTC 7. NIST also established a research and development program to provide the technical basis for improved building and fire codes, standards, and practices, and a dissemination and technical assistance program to engage leaders of the construction and building community in implementing proposed changes to practices, standards, and codes. NIST also is providing practical guidance and tools to better prepare facility owners, contractors, architects, engineers, emergency responders, and regulatory authorities to respond to future disasters. The investigation portion of the response plan was completed with the release of the final report on 7 World Trade Center on November 20, 2008. The final report on the WTC Towers—including 30 recommendations for improving building and occupant safety—was released on October 26, 2005. [ 38 ] NIST works in conjunction with the  Technical Guidelines Development Committee  of the  Election Assistance Commission  to develop the  Voluntary Voting System Guidelines  for  voting machines  and other election technology. In February 2014 NIST published the  NIST Cybersecurity Framework  that serves as voluntary guidance for organizations to manage and reduce cybersecurity risk. [ 39 ]  It was later amended and Version 1.1 was published in April 2018. [ 40 ]   Executive Order  13800, Strengthening the Cybersecurity of Federal Networks and  Critical Infrastructure , made the Framework mandatory for U.S. federal government agencies. [ 39 ]  An extension to the NIST Cybersecurity Framework is the  Cybersecurity Maturity Model (CMMC)  which was introduced in 2019 (though the origin of CMMC began with Executive Order 13556). [ 41 ] It emphasizes the importance of implementing  Zero-trust architecture (ZTA)  which focuses on protecting resources over the network perimeter. ZTA utilizes zero trust principles which include \"never trust, always verify\", \"assume breach\" and \"least privileged access\" to  safeguard  users, assets, and resources. Since ZTA holds no implicit trust to users within the network perimeter, authentication and authorization are performed at every stage of a digital transaction. This reduces the risk of unauthorized access to resources. [ 42 ] NIST released a draft of the CSF 2.0 for public comment through November 4, 2023. NIST decided to update the framework to make it more applicable to small and medium size enterprises that use the framework, as well as to accommodate the constantly changing nature of cybersecurity. [ 43 ] In August 2024, NIST released a final set of encryption tools designed to withstand the attack of a  quantum computer.  These  post-quantum encryption  standards secure a wide range of electronic information, from confidential email messages to e-commerce transactions that propel the modern economy. [ 44 ] Four scientific researchers at NIST have been awarded  Nobel Prizes  for work in  physics :  William Daniel Phillips  in 1997,  Eric Allin Cornell  in 2001,  John Lewis Hall  in 2005 and  David Jeffrey Wineland  in 2012, which is the largest number for any US government laboratory not accounting for ubiquitous government contracts to state institutions and the private sector. All four were recognized for their work related to  laser cooling  of atoms, which is directly related to the development and advancement of the atomic clock. In 2011,  Dan Shechtman  was awarded the Nobel Prize  in chemistry for his work on  quasicrystals  in the  Metallurgy  Division from 1982 to 1984. In addition,  John Werner Cahn  was awarded the 2011 Kyoto Prize for Materials Science, and the  National Medal of Science  has been awarded to NIST researchers Cahn (1998) and Wineland (2007). Other notable people who have worked at NBS or NIST include: Since 1989, the director of NIST has been a Presidential appointee and is confirmed by the  United States Senate , [ 45 ]  and since that year the average tenure of NIST directors has fallen from 11 years to 2 years in duration. Since the 2011 reorganization of NIST, the director also holds the title of Under Secretary of Commerce for Standards and Technology. Fifteen individuals have officially held the position (in addition to four acting directors who have served on a temporary basis). NIST holds  patents  on behalf of the  Federal government of the United States , [ 46 ]  with at least one of them being custodial to protect public domain use, such as one for a  Chip-scale atomic clock , developed by a NIST team as part of a  DARPA  competition. [ 47 ] In September 2013, both  The Guardian  and  The New York Times  reported that NIST allowed the  National Security Agency  (NSA) to insert a  cryptographically secure pseudorandom number generator  called  Dual EC DRBG  into NIST standard  SP 800-90  that had a  kleptographic   backdoor  that the NSA can use to covertly predict the future outputs of this  pseudorandom number generator  thereby allowing the surreptitious decryption of data. [ 48 ]  Both papers report [ 49 ] [ 50 ]  that the NSA worked covertly to get its own version of SP 800-90 approved for worldwide use in 2006. The whistle-blowing document states that \"eventually, NSA became the sole editor\". The reports confirm suspicions and technical grounds publicly raised by cryptographers in 2007 that the EC-DRBG could contain a  kleptographic  backdoor (perhaps placed in the standard by NSA). [ 51 ] NIST responded to the allegations, stating that \"NIST works to publish the strongest cryptographic standards possible\" and that it uses \"a transparent, public process to rigorously vet our recommended standards\". [ 52 ]  The agency stated that \"there has been some confusion about the standards development process and the role of different organizations in it...The National Security Agency (NSA) participates in the NIST cryptography process because of its recognized expertise. NIST is also required by statute to consult with the NSA.\" [ 53 ]  Recognizing the concerns expressed, the agency reopened the public comment period for the SP800-90 publications, promising that \"if vulnerabilities are found in these or any other NIST standards, we will work with the cryptographic community to address them as quickly as possible\". [ 54 ]  Due to public concern of this  cryptovirology  attack, NIST rescinded the EC-DRBG algorithm from the NIST SP 800-90 standard. [ 55 ] In addition to these journals, NIST (and the National Bureau of Standards before it) has a robust technical reports publishing arm. NIST technical reports are published in several dozen series, which cover a wide range of topics, from computer technology to construction to aspects of standardization including weights, measures and reference data. [ 56 ]  In addition to technical reports, NIST scientists publish many journal and conference papers each year; an database of these, along with more recent technical reports, can be found on the NIST website. [ 57 ]"
  },
  {
    "id": 52,
    "title": "Text Retrieval Conference",
    "content": "The  Text REtrieval Conference  ( TREC ) is an ongoing series of  workshops  focusing on a list of different  information retrieval  (IR) research areas, or  tracks.  It is co-sponsored by the  National Institute of Standards and Technology  (NIST) and the  Intelligence Advanced Research Projects Activity  (part of the office of the  Director of National Intelligence ), and began in 1992 as part of the  TIPSTER Text program . Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale  evaluation  of  text retrieval  methodologies and to increase the speed of lab-to-product  transfer of technology . TREC's evaluation protocols have improved many search technologies.  A 2010 study estimated that \"without TREC, U.S. Internet users would have spent up to 3.15 billion additional hours using web search engines between 1999 and 2009.\" [ 1 ]   Hal Varian  the Chief Economist at  Google  wrote that \"The TREC data revitalized research on information retrieval. Having a standard, widely available, and carefully constructed set of data laid the groundwork for further innovation in this field.\" [ 2 ] Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable  features . Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.Text Retrieval Conference started in 1992, funded by DARPA (US Defense Advanced Research Project) and run by NIST. Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. TREC is overseen by a program committee consisting of representatives from government, industry, and academia. For each TREC, NIST provide a set of documents and questions. Participants run their own retrieval system on the data and return to NIST a list of retrieved top-ranked documents .NIST pools the individual result judges the retrieved documents for correctness and evaluates the results. The TREC cycle ends with a workshop that is a forum for participants to share their experiences. TREC defines relevance as: \"If you were writing a report on the subject of the topic and would use the information contained in the document in the report, then the document is relevant.\" [ 3 ]   Most TREC retrieval tasks use binary relevance: a document is either relevant or not relevant. Some TREC tasks use graded relevance, capturing multiple degrees of relevance. Most TREC collections are too large to perform complete relevance assessment; for these collections it is impossible to calculate the absolute recall for each query. To decide which documents to assess, TREC usually uses a method call pooling. In this method, the top-ranked n documents from each contributing run are aggregated, and the resulting document set is judged completely. In 1992 TREC-1 was held at NIST. The first conference attracted 28 groups of researchers from academia and industry. It demonstrated a wide range of different approaches to the retrieval of text from large document collections .Finally TREC1 revealed the facts that automatic construction of queries from natural language query statements seems to work. Techniques based on natural language processing were no better no worse than those based on vector or probabilistic approach. TREC2 Took place in August 1993. 31 group of researchers participated in this. Two types of retrieval were examined. Retrieval using an ‘ad hoc’ query and retrieval using a ‘routing' query In TREC-3 a small group experiments worked with Spanish language collection and others dealt with interactive query formulation in multiple databases TREC-4 they made even shorter to investigate the problems with very short user statements TREC-5 includes both short and long versions of the topics with the goal of carrying out deeper investigation into which types of techniques work well on various lengths of topics In TREC-6 Three new tracks speech, cross language, high precision information retrieval were introduced. The goal of cross language information retrieval is to facilitate research on system that are able to retrieve relevant document regardless of language of the source document TREC-7 contained seven tracks out of which two were new Query track and very large corpus track. The goal of the query track was to create a large query collection TREC-8 contain seven tracks out of which two –question answering and web tracks were new. The objective of QA query is to explore the possibilities of providing answers to specific natural language queries TREC-9 Includes seven tracks In TREC-10 Video tracks introduced Video tracks design to promote research in content based retrieval from digital video In TREC-11 Novelty tracks introduced. The goal of novelty track is to investigate systems abilities to locate relevant and new information within the ranked set of documents returned by a traditional document retrieval system TREC-12 held in 2003 added three new tracks; Genome track, robust retrieval track, HARD (Highly Accurate Retrieval from Documents)  [ 4 ] New tracks are added as new research needs are identified, this list is current for TREC 2018. [ 5 ] In 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called  NTCIR  ( NII  Test Collection for IR Systems), and in 2000,  CLEF , a European counterpart, specifically vectored towards the study of cross-language information retrieval was launched. Forum for Information Retrieval Evaluation  (FIRE)  started in 2008 with the aim of building a South Asian counterpart for TREC, CLEF, and NTCIR, NIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled. [ 7 ]  The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of  publications . Technology first developed in TREC is now included in many of the world's commercial  search engines .  An independent report by RTII found that \"about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia.\"\n [ 8 ] \n [ 9 ] While one study suggests that the state of the art for ad hoc search did not advance substantially in the decade preceding 2009, [ 10 ]  it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections. The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains. [ when? ] [ citation needed ] TREC systems often provide a baseline for further research.  Examples include: The conference is made up of a varied, international group of researchers and developers. [ 15 ] [ 16 ] [ 17 ]  In 2003, there were 93 groups from both academia and industry from 22 countries participating."
  },
  {
    "id": 53,
    "title": "Scalability",
    "content": "Collective intelligence \n Collective action \n Self-organized criticality \n Herd mentality \n Phase transition \n Agent-based modelling \n Synchronization \n Ant colony optimization \n Particle swarm optimization \n Swarm behaviour Social network analysis \n Small-world networks \n Centrality \n Motifs \n Graph theory \n Scaling \n Robustness \n Systems biology \n Dynamic networks Evolutionary computation \n Genetic algorithms \n Genetic programming \n Artificial life \n Machine learning \n Evolutionary developmental biology \n Artificial intelligence \n Evolutionary robotics Reaction–diffusion systems \n Partial differential equations \n Dissipative structures \n Percolation \n Cellular automata \n Spatial ecology \n Self-replication Conversation theory \n Entropy \n Feedback   \n Goal-oriented \n Homeostasis   \n Information theory \n Operationalization \n Second-order cybernetics \n Self-reference \n System dynamics \n Systems science \n Systems thinking \n Sensemaking \n Variety Ordinary differential equations \n Phase space \n Attractors \n Population dynamics \n Chaos \n Multistability \n Bifurcation Rational choice theory \n Bounded rationality Scalability  is the property of a system to handle a growing amount of work.  One definition for software systems specifies that this may be done by adding resources to the system. [ 1 ] In an  economic  context, a scalable  business model  implies that a company can increase sales given increased resources. For example, a package delivery system is scalable because more packages can be delivered by adding more delivery vehicles. However, if all packages had to first pass through a single warehouse for sorting, the system would not be as scalable, because one warehouse can handle only a limited number of packages. [ 2 ] In computing, scalability is a characteristic of computers, networks,  algorithms ,  networking protocols ,  programs  and applications. An example is a  search engine , which must support increasing numbers of users, and the number of topics it  indexes . [ 3 ]   Webscale  is a computer architectural approach that brings the capabilities of large-scale cloud computing companies into enterprise data centers. [ 4 ] In  distributed systems , there are several definitions according to the authors, some considering the concepts of scalability a sub-part of  elasticity , others as being distinct. According to Marc Brooker: \"a system is scalable in the range where  marginal cost  of additional workload is nearly constant.\"  Serverless  technologies fit this definition but you need to consider total cost of ownership not just the infra cost.  [ 5 ] In mathematics, scalability mostly refers to  closure  under  scalar multiplication . In  industrial engineering  and manufacturing, scalability refers to the capacity of a process, system, or organization to handle a growing workload, adapt to increasing demands, and maintain operational efficiency. A scalable system can effectively manage increased production volumes, new product lines, or expanding markets without compromising quality or performance. In this context, scalability is a vital consideration for businesses aiming to meet customer expectations, remain competitive, and achieve sustainable growth. Factors influencing scalability include the flexibility of the production process, the adaptability of the workforce, and the integration of advanced technologies. By implementing scalable solutions, companies can optimize resource utilization, reduce costs, and streamline their operations. Scalability in industrial engineering and manufacturing enables businesses to respond to fluctuating market conditions, capitalize on emerging opportunities, and thrive in an ever-evolving global landscape. [ citation needed ] The  Incident Command System  (ICS) is used by  emergency response  agencies in the United States. ICS can scale resource coordination from a single-engine roadside brushfire to an interstate wildfire. The first resource on scene establishes command, with authority to order resources and delegate responsibility (managing five to seven officers, who will again delegate to up to seven, and on as the incident grows). As an incident expands, more senior officers assume command. [ 6 ] Scalability can be measured over multiple dimensions, such as: [ 7 ] Resources fall into two broad categories: horizontal and vertical. [ 8 ] Scaling horizontally (out/in) means adding or removing nodes, such as adding a new computer to a distributed software application. An example might involve scaling out from one web server to three.  High-performance computing  applications, such as  seismic analysis  and  biotechnology , scale workloads horizontally to support tasks that once would have required expensive  supercomputers . Other workloads, such as large social networks, exceed the capacity of the largest supercomputer and can only be handled by scalable systems. Exploiting this scalability requires software for efficient resource management and maintenance. [ 7 ] Scaling vertically (up/down) means adding resources to (or removing resources from) a single node, typically involving the addition of CPUs, memory or storage to a single computer. [ 7 ] Benefits to scale-up include avoiding increases management complexity, more sophisticated programming to allocate tasks among resources and handle issues such as throughput, latency, synchronization across nodes.  Moreover some  applications do not scale horizontally . Network function virtualization  defines these terms differently: scaling out/in is the ability to scale by adding/removing resource instances (e.g., virtual machine), whereas scaling up/down is the ability to scale by changing allocated resources (e.g., memory/CPU/storage capacity). [ 9 ] Scalability for databases requires that the database system be able to perform additional work given greater hardware resources, such as additional servers, processors, memory and storage. Workloads have continued to grow and demands on databases have followed suit. Algorithmic innovations include row-level locking and table and index partitioning. Architectural innovations include  shared-nothing  and shared-everything architectures for managing multi-server configurations. In the context of scale-out  data storage , scalability is defined as the maximum storage cluster size which guarantees full data consistency, meaning there is only ever one valid version of stored data in the whole cluster, independently from the number of redundant physical data copies. Clusters which provide \"lazy\" redundancy by updating copies in an asynchronous fashion are called  'eventually consistent' . This type of scale-out design is suitable when availability and responsiveness are rated higher than consistency, which is true for many web file-hosting services or web caches ( if you want the latest version, wait some seconds for it to propagate ). For all classical transaction-oriented applications, this design should be avoided. [ 10 ] Many open-source and even commercial scale-out storage clusters, especially those built on top of standard PC hardware and networks, provide eventual consistency only, such as some NoSQL databases like  CouchDB  and others mentioned above. Write operations invalidate other copies, but often don't wait for their acknowledgements. Read operations typically don't check every redundant copy prior to answering, potentially missing the preceding write operation. The large amount of metadata signal traffic would require specialized hardware and short distances to be handled with acceptable performance (i.e., act like a non-clustered storage device or database). [ citation needed ] Whenever strong data consistency is expected, look for these indicators: [ citation needed ] Indicators for eventually consistent designs (not suitable for transactional applications!) are: [ citation needed ] It is often advised to focus system design on hardware scalability rather than on capacity. It is typically cheaper to add a new node to a system in order to achieve improved performance than to partake in  performance tuning  to improve the capacity that each node can handle. But this approach can have diminishing returns (as discussed in  performance engineering ). For example: suppose 70% of a program can be sped up if parallelized and run on multiple CPUs instead of one. If  \n \n \n \n α \n \n \n {\\displaystyle \\alpha } \n \n  is the fraction of a calculation that is sequential, and  \n \n \n \n 1 \n − \n α \n \n \n {\\displaystyle 1-\\alpha } \n \n  is the fraction that can be parallelized, the maximum  speedup  that can be achieved by using P processors is given according to  Amdahl's Law : Substituting the value for this example, using 4 processors gives Doubling the computing power to 8 processors gives Doubling the processing power has only sped up the process by roughly one-fifth. If the whole problem was parallelizable, the speed would also double. Therefore, throwing in more hardware is not necessarily the optimal approach. In  distributed systems , you can use  Universal Scalability Law  (USL) to model and to optimize scalability of your system. USL is coined by  Neil J. Gunther  and quantifies scalability based on parameters such as contention and coherency. Contention refers to delay due to waiting or queueing for shared resources. Coherence refers to delay for data to become consistent. For example, having a high contention indicates sequential processing that could be parallelized, while having a high coherency suggests excessive dependencies among processes, prompting you to minimize interactions. Also, with help of USL, you can, in advance, calculate the maximum effective capacity of your system: scaling up your system beyond that point is a waste.  [ 11 ] High performance computing  has two common notions of scalability:"
  },
  {
    "id": 54,
    "title": "Digital library",
    "content": "A  digital library  (also called an  online library , an  internet library , a  digital repository ,   a library without walls , or a  digital collection ) is an  online database  of digital objects that can include text, still images, audio, video,  digital documents , or other  digital media  formats or a  library  accessible through the  internet . Objects can consist of  digitized  content like  print  or  photographs , as well as  originally produced digital  content like  word processor  files or  social media  posts. In addition to storing content, digital libraries provide means for organizing, searching, and  retrieving  the content contained in the collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations. [ 1 ]  The digital content may be stored locally, or accessed remotely via computer networks. These information retrieval systems are able to exchange information with each other through  interoperability  and  sustainability . [ 2 ] The early history of digital libraries is not well documented, but several key thinkers are connected to the emergence of the concept. [ 3 ]  Predecessors include  Paul Otlet  and  Henri La Fontaine 's  Mundaneum , an attempt begun in 1895 to gather and systematically catalogue the world's knowledge, with the hope of bringing about world peace. [ 4 ]  The visions of the digital library were largely realized a century later during the great expansion of the Internet. [ 5 ] Vannevar Bush  and  J.C.R. Licklider  are two contributors that advanced this idea into then current technology. Bush had supported research that led to the bomb that was dropped on  Hiroshima . After seeing the disaster, he wanted to create a machine that would show how technology can lead to understanding instead of destruction. This machine would include a desk with two screens, switches and buttons, and a keyboard. [ 6 ]  He named this the \" Memex \". This way individuals would be able to access stored books and files at a rapid speed. In 1956,  Ford Foundation  funded Licklider to analyze how libraries could be improved with technology. Almost a decade later, his book entitled \" Libraries of the Future \" included his vision. He wanted to create a system that would use computers and networks so human knowledge would be accessible for human needs and feedback would be automatic for machine purposes. This system contained three components, the corpus of knowledge, the question, and the answer. Licklider called it a procognitive system. In 1980 the role of the library in an electronic society was the focus of a clinic on library applications of  data processing . Participants included  Frederick Wilfrid Lancaster ,  Derek De Solla Price ,  Gerard Salton , and  Michael Gorman) . [ 7 ] Early projects centered on the creation of an electronic card catalogue known as  Online Public Access Catalog  (OPAC). By the 1980s, the success of these endeavors resulted in OPAC replacing the traditional  card catalog  in many academic, public and special libraries. This permitted libraries to undertake additional rewarding co-operative efforts to support resource sharing and expand access to library materials beyond an individual library. An early example of a digital library is the  Education Resources Information Center  (ERIC), a database of education citations, abstracts and texts that was created in 1964 and made available online through  DIALOG  in 1969. [ 8 ] In 1994, digital libraries became widely visible in the research community due to a $24.4 million  NSF  managed program supported jointly by  DARPA 's Intelligent Integration of Information (I3) program,  NASA , and NSF itself. [ 9 ]  Successful research proposals came from six U.S. universities. [ 10 ]  The universities included  Carnegie Mellon University ,  University of California-Berkeley ,  University of Michigan ,  University of Illinois ,  University of California-Santa Barbara , and  Stanford University . Articles from the projects summarized their progress at their halfway point in May 1996. [ 11 ]  Stanford research, by  Sergey Brin  and  Larry Page , led to the founding of  Google . Early attempts at creating a model for digital libraries included the DELOS  Digital Library Reference Model [ 12 ] [ 13 ]  and the 5S Framework. [ 14 ] [ 15 ] The term  digital library  was first popularized by the  NSF / DARPA / NASA  Digital Libraries Initiative in 1994. [ 16 ]  With the availability of the computer networks the information resources are expected to stay distributed and accessed as needed, whereas in  Vannevar Bush 's essay  As We May Think  (1945) they were to be collected and kept within the researcher's  Memex . The term  virtual library  was initially used interchangeably with  digital library,  but is now primarily used for libraries that are virtual in other senses (such as libraries which aggregate distributed content). In the early days of digital libraries, there was discussion of the similarities and differences among the terms  digital ,  virtual , and  electronic . [ 17 ] A distinction is often made between content that was created in a digital format, known as  born-digital , and information that has been converted from a physical medium, e.g. paper, through  digitization . Not all electronic content is in  digital data  format. The term  hybrid library  is sometimes used for libraries that have both physical collections and electronic collections. For example,  American Memory  is a digital library within the  Library of Congress . Some important digital libraries also serve as long term archives, such as  arXiv  and the  Internet Archive . Others, such as the  Digital Public Library of America , seek to make digital information from various institutions widely accessible online. [ 18 ] Many academic libraries are actively involved in building repositories of their institution's books, papers, theses, and other works that can be digitized or were 'born digital'. Many of these repositories are made available to the general public with few restrictions, in accordance with the goals of  open access , in contrast to the publication of research in commercial journals, where the publishers usually limit access rights. Irrespective of access rights, institutional, truly free, and corporate repositories can be referred to as digital libraries.  Institutional repository  software is designed for archiving, organizing, and searching a library's content. Popular open-source solutions include  DSpace ,  Greenstone Digital Library (GSDL) ,  EPrints ,  Digital Commons , and the  Fedora Commons -based systems  Islandora  and  Samvera . [ 19 ] Legal deposit  is often covered by  copyright  legislation and sometimes by laws specific to legal deposit, and requires that one or more copies of all material published in a country should be submitted for preservation in an institution, typically the  national library . Since the advent of  electronic documents , legislation has had to be amended to cover the new formats, such as the 2016 amendment to the  Copyright Act 1968  in Australia. [ 20 ] [ 21 ] [ 22 ] Since then various types of electronic depositories have been built. The  British Library 's Publisher Submission Portal and the German model at the  Deutsche Nationalbibliothek  have one deposit point for a network of libraries, but public access is only available in the reading rooms in the libraries. The Australian  National edeposit  system has the same features, but also allows for remote access by the general public for most of the content. [ 23 ] Physical  archives  differ from physical libraries in several ways. Traditionally, archives are defined as: The technology used to create digital libraries is even more revolutionary for archives since it breaks down the second and third of these general rules. In other words, \"digital archives\" or \"online archives\" will still generally contain primary sources, but they are likely to be described individually rather than (or in addition to) in groups or collections. Further, because they are digital, their contents are easily reproducible and may indeed have been reproduced from elsewhere. The  Oxford Text Archive  is generally considered to be the oldest digital archive of academic physical primary source materials. Archives differ from libraries in the nature of the materials held. Libraries collect individual published books and serials, or bounded sets of individual items. The books and journals held by libraries are not unique, since multiple copies exist and any given copy will generally prove as satisfactory as any other copy. The material in archives and manuscript libraries are \"the unique records of corporate bodies and the papers of individuals and families\". [ 24 ] A fundamental characteristic of archives is that they have to keep the context in which their records have been created and the network of relationships between them in order to preserve their informative content and provide understandable and useful information over time. The fundamental characteristic of archives resides in their hierarchical organization expressing the context by means of the  archival bond . Archival descriptions are the fundamental means to describe, understand, retrieve and access archival material. At the digital level, archival descriptions are usually encoded by means of the  Encoded Archival Description  XML format. The EAD is a standardized electronic representation of archival description which makes it possible to provide union access to detailed archival descriptions and resources in repositories distributed throughout the world. Given the importance of archives, a dedicated formal model, called NEsted SeTs for Object Hierarchies (NESTOR), [ 25 ]  built around their peculiar constituents, has been defined. NESTOR is based on the idea of expressing the hierarchical relationships between objects through the inclusion property between sets, in contrast to the binary relation between nodes exploited by the tree. NESTOR has been used to formally extend the 5S model to define a digital archive as a specific case of digital library able to take into consideration the peculiar features of archives. A  computer-aided design  library or CAD library is a  cloud  based  repository  of 3D models or parts for  computer-aided design  (CAD),  computer-aided engineering  (CAE),  computer-aided manufacturing  (CAM), or  Building information modeling  (BIM). Examples of CAD libraries are  GrabCAD ,  Sketchup 3D Warehouse ,  Sketchfab ,  McMaster-Carr ,  TurboSquid ,  Chaos Cosmos , [ 26 ]  and  Thingiverse . [ 27 ] [ spam link? ]   The models can be free and  open source  or  proprietary  and have to pay a  subscription  to have access to the CAD library 3D models.   Generative Ai CAD libraries  are being developed using  linked   open data  of  schematics   and  diagrams . [ 28 ] CAD libraries can have assets such as  3D models , materials/ textures ,  bump maps , trees/plants,  HDRIs , and different  Computer graphics lighting  sources to be  rendered . [ 29 ] [ spam link? ] A 2D graphics repository/library are  vector graphics  or  raster graphics  images/ icons  that can be  free use  or  proprietary . [ 30 ] [ spam link? ] The advantages of digital libraries as a means of easily and rapidly accessing books, archives and images of various types are now widely recognized by commercial interests and public bodies alike. [ 31 ] Traditional libraries are limited by storage space; digital libraries have the potential to store much more information, simply because digital information requires very little physical space to contain it. [ 32 ]  As such, the cost of maintaining a digital library can be much lower than that of a traditional library. A physical library must spend large sums of money paying for staff, book maintenance, rent, and additional books. Digital libraries may reduce or, in some instances, do away with these fees. Both types of library require cataloging input to allow users to locate and retrieve material. Digital libraries may be more willing to adopt innovations in technology providing users with improvements in electronic and audio book technology as well as presenting new forms of communication such as wikis and blogs; conventional libraries may consider that providing online access to their OP AC catalog is sufficient. An important advantage to digital conversion is increased accessibility to users. They also increase availability to individuals who may not be traditional patrons of a library, due to geographic location or organizational affiliation. Digital libraries offer a variety of software packages, including those tailored for  kids' educational games . [ 34 ]  Institutional repository software, which focuses primarily on ingest, preservation and access of locally produced documents, particularly locally produced academic outputs, can be found in  Institutional repository software . This software may be proprietary, as is the case with the Library of Congress which uses Digiboard and CTS to manage digital content. [ 35 ] The design and implementation in digital libraries are constructed so computer systems and software can make use of the information when it is exchanged. These are referred to as semantic digital libraries. Semantic libraries are also used to socialize with different communities from a mass of social networks. [ 36 ]  DjDL is a type of semantic digital library. Keywords-based and semantic search are the two main types of searches. A tool is provided in the semantic search that create a group for augmentation and refinement for keywords-based search. Conceptual knowledge used in DjDL is centered around two forms; the subject  ontology  and the set of  concept search  patterns based on the ontology. The three type of ontologies that are associated to this search are  bibliographic ontologies , community-aware ontologies, and subject ontologies. In traditional libraries, the ability to find works of interest is directly related to how well they were cataloged. While cataloging electronic works digitized from a library's existing holding may be as simple as copying or moving a record from the print to the electronic form, complex and born-digital works require substantially more effort. To handle the growing volume of electronic publications, new tools and technologies have to be designed to allow effective automated semantic classification and searching. While  full-text search  can be used for some items, there are many common catalog searches which cannot be performed using full text, including: Most digital libraries provide a search interface which allows resources to be found. These resources are typically  deep web  (or invisible web) resources since they frequently cannot be located by  search engine   crawlers . Some digital libraries create special pages or  sitemaps  to allow search engines to find all their resources. Digital libraries frequently use the  Open Archives Initiative Protocol for Metadata Harvesting  (OAI-PMH) to expose their metadata to other digital libraries, and search engines like  Google Scholar ,  Yahoo!  and  Scirus  can also use OAI-PMH to find these deep web resources. [ 37 ]  As with physical libraries, very relatively little is known about how users actually select books. [ 38 ] There are two general strategies for searching a  federation  of digital libraries: distributed searching and searching previously harvested  metadata . Distributed searching typically involves a client sending multiple search requests in parallel to a number of servers in the federation. The results are gathered, duplicates are eliminated or clustered, and the remaining items are sorted and presented back to the client. Protocols like  Z39.50  are frequently used in distributed searching. A benefit to this approach is that the resource-intensive tasks of indexing and storage are left to the respective servers in the federation. A drawback to this approach is that the search mechanism is limited by the different indexing and ranking capabilities of each database; therefore, making it difficult to assemble a combined result consisting of the most relevant found items. Searching over previously harvested metadata involves searching a locally stored  index  of information that has previously been collected from the libraries in the federation. When a search is performed, the search mechanism does not need to make connections with the digital libraries it is searching—it already has a local representation of the information. This approach requires the creation of an indexing and harvesting mechanism which operates regularly, connecting to all the digital libraries and querying the whole collection in order to discover new and updated resources.  OAI-PMH  is frequently used by digital libraries for allowing metadata to be harvested. A benefit to this approach is that the search mechanism has full control over indexing and ranking algorithms, possibly allowing more consistent results. A drawback is that harvesting and indexing systems are more resource-intensive and therefore expensive. Digital preservation aims to ensure that digital media and information systems are still interpretable into the indefinite future. [ 39 ]  Each necessary component of this must be migrated, preserved or  emulated . [ 40 ]  Typically lower levels of systems ( floppy disks  for example) are emulated, bit-streams (the actual files stored in the disks) are preserved and operating systems are emulated as a  virtual machine . Only where the meaning and content of digital media and information systems are well understood is migration possible, as is the case for office documents. [ 40 ] [ 41 ] [ 42 ]  However, at least one organization, the Wider Net Project, has created an offline digital library, the  eGranary , by reproducing materials on a 6  TB   hard drive . Instead of a bit-stream environment, the digital library contains a built-in  proxy server  and  search engine  so the digital materials can be accessed using an  Internet browser . [ 43 ]  Also, the materials are not preserved for the future. The eGranary is intended for use in places or situations where Internet connectivity is very slow, non-existent, unreliable, unsuitable or too expensive. In the past few years, procedures for  digitizing  books at high speed and comparatively low cost have improved considerably with the result that it is now possible to digitize millions of books per year. [ 44 ]  The Google book-scanning project is also working with libraries to offer digitize books pushing forward on the digitize book realm. Digital libraries are hampered by  copyright  law because, unlike with traditional printed works, the laws of digital copyright are still being formed. The republication of material on the web by libraries may require permission from rights holders, and there is a conflict of interest between libraries and the publishers who may wish to create online versions of their acquired content for commercial purposes. In 2010, it was estimated that twenty-three percent of books in existence were created before 1923 and thus out of copyright. Of those printed after this date, only five percent were still in print as of 2010. [update]  Thus, approximately seventy-two percent of books were not available to the public. [ 45 ] There is a dilution of responsibility that occurs as a result of the distributed nature of digital resources. Complex intellectual property matters may become involved since digital material is not always owned by a library. [ 46 ]  The content is, in many cases,  public domain  or self-generated content only. Some digital libraries, such as  Project Gutenberg , work to digitize out-of-copyright works and make them freely available to the public. An estimate of the number of distinct books still existent in library catalogues from 2000 BC to 1960, has been made. [ 47 ] [ 48 ] The  Fair Use  Provisions  (17 USC § 107)  under the  Copyright Act of 1976  provide specific guidelines under which circumstances libraries are allowed to copy digital resources. Four factors that constitute fair use are \"Purpose of the use, Nature of the work, Amount or substantiality used and Market impact\". [ 49 ] Some digital libraries acquire a license to lend their resources. This may involve the restriction of lending out only one copy at a time for each license, and applying a system of  digital rights management  for this purpose. The  Digital Millennium Copyright Act  of 1998 was an act created in the United States to attempt to deal with the introduction of digital works. This Act incorporates two treaties from the year 1996. It criminalizes the attempt to circumvent measures which limit access to copyrighted materials. It also criminalizes the act of attempting to circumvent access control. [ 50 ]  This act provides an exemption for nonprofit libraries and archives which allows up to three copies to be made, one of which may be digital. This may not be made public or distributed on the web, however. Further, it allows libraries and archives to copy a work if its format becomes obsolete. [ 50 ] Copyright issues persist. As such, proposals have been put forward suggesting that digital libraries be exempt from copyright law. Although this would be very beneficial to the public, it may have a negative economic effect and authors may be less inclined to create new works. [ 51 ] Another issue that complicates matters is the desire of some publishing houses to restrict the use of digit materials such as e-books purchased by libraries. Whereas with printed books, the library owns the book until it can no longer be circulated, publishers want to limit the number of times an e-book can be checked out before the library would need to repurchase that book. \"[HarperCollins] began licensing use of each e-book copy for a maximum of 26 loans. This affects only the most popular titles and has no practical effect on others. After the limit is reached, the library can repurchase access rights at a lower cost than the original price.\" [ 52 ]  While from a publishing perspective, this sounds like a good balance of library lending and protecting themselves from a feared decrease in book sales, libraries are not set up to monitor their collections as such. They acknowledge the increased demand of digital materials available to patrons and the desire of a digital library to become expanded to include best sellers, but publisher licensing may hinder the process. Many digital libraries offer  recommender systems  to reduce  information overload  and help their users discovering relevant literature. Some examples of digital libraries offering recommender systems are  IEEE Xplore ,  Europeana , and  GESIS Sowiport . The recommender systems work mostly based on  content-based filtering  but also other approaches are used such as  collaborative filtering  and citation-based recommendations. [ 53 ]  Beel et al. report that there are more than 90 different recommendation approaches for digital libraries, presented in more than 200  research articles . [ 53 ] Typically, digital libraries develop and maintain their own recommender systems based on existing search and recommendation frameworks such as  Apache Lucene  or  Apache Mahout . Digital libraries, or at least their digital collections, also have brought their own problems and challenges in areas such as: There are many large scale digitisation projects that perpetuate these problems. Large scale digitization projects are underway at  Google , the  Million Book Project , and  Internet Archive . With continued improvements in book handling and presentation technologies such as  optical character recognition  and development of alternative depositories and business models, digital libraries are rapidly growing in popularity. Just as libraries have ventured into audio and video collections, so have digital libraries such as the Internet Archive. In 2016,  Google Books  project received a court victory on proceeding with their book-scanning project that was halted by the Authors' Guild. [ 55 ]  This helped open the road for libraries to work with Google to better reach patrons who are accustomed to computerized information. According to Larry Lannom, Director of Information Management Technology at the nonprofit  Corporation for National Research Initiatives  (CNRI), \"all the problems associated with digital libraries are wrapped up in archiving\". He goes on to state, \"If in 100 years people can still read your article, we'll have solved the problem.\"  Daniel Akst , author of  The Webster Chronicle , proposes that \"the future of libraries—and of information—is digital\".  Peter Lyman  and  Hal Variant , information scientists at the  University of California, Berkeley , estimate that \"the world's total yearly production of print, film, optical, and magnetic content would require roughly 1.5 billion gigabytes of storage\". Therefore, they believe that \"soon it will be technologically possible for an average person to access virtually all recorded information\". [ 56 ] Digital archives are an evolving medium and they develop under various circumstances. Alongside large scale repositories, other digital archiving projects have also evolved in response to needs in research and  research communication  on various institutional levels. For example, during the  COVID-19 pandemic ,  libraries  and higher education institutions have launched digital archiving projects to document life during the pandemic, thus creating a digital, cultural record of  collective memories  from the period. [ 57 ]  Researchers have also utilized digital archiving to create specialized  research databases . These databases compile digital records for use on international and interdisciplinary levels. COVID CORPUS, launched in October 2020, is an example of such a database, built in response to scientific communication needs in light of the pandemic. [ 58 ]  Beyond academia, digital collections have also recently been developed to appeal to a more general audience, as is the case with the Selected General Audience Content of the Internet-First University Press developed by Cornell University. This general-audience database contains specialized research information but is digitally organized for accessibility. [ 59 ]  The establishment of these archives has facilitated specialized forms of digital recordkeeping to fulfill various niches in online,  research-based communication."
  },
  {
    "id": 55,
    "title": "Information filtering system",
    "content": "An  information filtering system  is a system that removes  redundant  or unwanted  information  from an information stream using (semi)automated or computerized methods prior to presentation to a human user. Its main goal is the management of the  information overload  and increment of the  semantic   signal-to-noise ratio . To do this the user's profile is compared to some reference characteristics. These characteristics may originate from the information item (the content-based approach) or the user's social environment (the  collaborative filtering  approach). Whereas in  information transmission   signal processing filters  are used against  syntax -disrupting noise on the bit-level, the methods employed in information filtering act on the semantic level. The range of machine methods employed builds on the same principles as those for  information extraction . A notable application can be found in the field of email  spam filters . Thus, it is not only the  information explosion  that necessitates some form of filters, but also inadvertently or maliciously introduced  pseudo -information. On the presentation level, information filtering takes the form of user-preferences-based  newsfeeds , etc. Recommender systems  and  content discovery platforms  are active information filtering systems that attempt to present to the user information items ( film ,  television ,  music ,  books ,  news ,  web pages ) the user is interested in. These systems add information items to the information flowing towards the user, as opposed to removing information items from the information flow towards the user. Recommender systems typically use  collaborative filtering  approaches or a combination of the collaborative filtering and content-based filtering approaches, although content-based recommender systems do exist. Before the advent of the  Internet , there are already several methods of  filtering information ; for instance, governments may control and restrict the flow of information in a given country by means of formal or informal censorship. On the other hand, we are going to talk about information filters if we refer to newspaper editors and journalists when they provide a service that selects the most valuable information for their clients, readers of books, magazines, newspapers,  radio  listeners and  TV  viewers. This filtering operation is also present in schools and universities where there is a selection of information to provide assistance based on academic criteria to customers of this service, the students. With the advent of the Internet it is possible that anyone can publish anything he wishes at a low-cost. In this way, it increases considerably the less useful information and consequently the quality information is disseminated. With this problem, it began to devise new filtering with which we can get the information required for each specific topic to easily and efficiently. A filtering system of this style consists of several tools that help people find the most valuable information, so the limited time you can dedicate to read / listen / view, is correctly directed to the most interesting and valuable documents. These filters are also used to organize and structure information in a correct and understandable way, in addition to group messages on the mail addressed. These filters are essential in the results obtained of the  search engines  on the Internet. The functions of filtering improves every day to get downloading Web documents and more efficient messages. One of the criteria used in this step is whether the  knowledge  is harmful or not, whether knowledge allows a better understanding with or without the concept. In this case the task of  information filtering  to reduce or eliminate the harmful information with knowledge. A system of learning content consists, in general rules, mainly of three basic stages: Currently the problem is not finding the best way to  filter information , but the way that these systems require to learn independently the information needs of users. Not only because they automate the process of  filtering  but also the construction and adaptation of the filter. Some branches based on it, such as statistics, machine learning, pattern recognition and data mining, are the base for developing information filters that appear and adapt in base to experience. To carry out the learning process, part of the information has to be pre-filtered, which means there are positive and negative examples which we named training data, which can be generated by experts, or via  feedback  from ordinary users. As data is entered, the system includes new rules; if we consider that this data can generalize the training data information, then we have to evaluate the system development and measure the system's ability to correctly predict the categories of new  information . This step is simplified by separating the training data in a new series called \"test data\" that we will use to measure the error rate. As a general rule it is important to distinguish between types of errors (false positives and false negatives). For example, in the case on an aggregator of content for children, it doesn't have the same gravity to allow the passage of information not suitable for them, that shows violence or pornography, than the mistake to discard some appropriated information.\nTo improve the system to lower error rates and have these systems with learning capabilities similar to humans we require development of systems that simulate human cognitive abilities, such as  natural-language understanding , capturing meaning Common and other forms of advanced processing to achieve the semantics of information. Nowadays, there are numerous techniques to develop information filters, some of these reach error rates lower than 10% in various experiments. [ citation needed ]  Among these techniques there are decision trees, support vector machines, neural networks, Bayesian networks, linear discriminants, logistic regression, etc..\nAt present, these techniques are used in different applications, not only in the web context, but in thematic issues as varied as voice recognition, classification of telescopic astronomy or evaluation of financial risk."
  },
  {
    "id": 56,
    "title": "Recommender system",
    "content": "A  recommender system (RecSys) , or a  recommendation system  (sometimes replacing  system  with terms such as  platform ,  engine , or  algorithm ), is a subclass of  information filtering system  that provides suggestions for items that are most pertinent to a particular user. [ 1 ] [ 2 ] [ 3 ]  Recommender systems are particularly useful when an individual needs to choose an item from a potentially overwhelming number of items that a service may offer. [ 1 ] [ 4 ] Typically, the suggestions refer to various  decision-making processes , such as what product to purchase, what music to listen to, or what online news to read. [ 1 ] \nRecommender systems are used in a variety of areas, with commonly recognised examples taking the form of  playlist  generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders. [ 5 ] [ 6 ]  These systems can operate using a single type of input, like music, or multiple inputs within and across platforms like news, books and search queries. There are also popular recommender systems for specific topics like restaurants and  online dating . Recommender systems have also been developed to explore research articles and experts, [ 7 ]  collaborators, [ 8 ]  and financial services. [ 9 ] A  content discovery platform  is an implemented  software  recommendation  platform  which uses recommender system tools. It utilizes user  metadata  in order to discover and recommend appropriate content, whilst reducing ongoing maintenance and development costs. A content discovery platform delivers personalized content to  websites ,  mobile devices  and  set-top boxes . A large range of content discovery platforms currently exist for various forms of content ranging from news articles and  academic journal  articles [ 10 ]  to television. [ 11 ]  As operators compete to be the gateway to home entertainment, personalized television is a key service differentiator. Academic content discovery has recently become another area of interest, with several companies being established to help academic researchers keep up to date with relevant academic content and serendipitously discover new content. [ 10 ] Recommender systems usually make use of either or both  collaborative filtering  and content-based filtering, as well as other systems such as  knowledge-based systems . Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. [ 12 ]  Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties. [ 13 ] The differences between collaborative and content-based filtering can be demonstrated by comparing two early music recommender systems,  Last.fm  and  Pandora Radio . Each type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations. This is an example of the  cold start  problem, and is common in collaborative filtering systems. [ 15 ] [ 16 ] [ 17 ] [ 18 ] [ 19 ] [ 20 ]  Whereas Pandora needs very little information to start, it is far more limited in scope (for example, it can only make recommendations that are similar to the original seed). Recommender systems are a useful alternative to  search algorithms  since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data. Recommender systems have been the focus of several granted patents, [ 21 ] [ 22 ] [ 23 ] [ 24 ] [ 25 ]  and there are more than 50 software libraries [ 26 ]  that support the development of recommender systems including LensKit, [ 27 ] [ 28 ]  RecBole, [ 29 ]  ReChorus [ 30 ]  and RecPack. [ 31 ] Elaine Rich  created the first recommender system in 1979, called Grundy. [ 32 ] [ 33 ]  She looked for a way to recommend users books they might like. Her idea was to create a system that asks users specific questions and classifies them into classes of preferences, or \"stereotypes\", depending on their answers. Depending on users' stereotype membership, they would then get recommendations for books they might like. Another early recommender system, called a \"digital bookshelf\", was described in a 1990 technical report by  Jussi Karlgren  at Columbia University,\n [ 34 ] \nand implemented at scale and worked through in technical reports and publications from 1994 onwards by  Jussi Karlgren , then at  SICS , [ 35 ] [ 36 ] \nand research groups led by  Pattie Maes  at MIT, [ 37 ]  Will Hill at Bellcore, [ 38 ]  and  Paul Resnick , also at MIT, [ 39 ] [ 4 ]  whose work with GroupLens was awarded the 2010  ACM Software Systems Award . Montaner provided the first overview of recommender systems from an intelligent agent perspective. [ 40 ]   Adomavicius  provided a new, alternate overview of recommender systems. [ 41 ]   Herlocker provides an additional overview of evaluation techniques for recommender systems, [ 42 ]  and  Beel  et al. discussed the problems of offline evaluations. [ 43 ]  Beel et al. have also provided literature surveys on available research paper recommender systems and existing challenges. [ 44 ] [ 45 ] One approach to the design of recommender systems that has wide use is  collaborative filtering . [ 46 ]  Collaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past. The system generates recommendations using only information about rating profiles for different users or items. By locating peer users/items with a rating history similar to the current user or item, they generate recommendations using this neighborhood. Collaborative filtering methods are classified as memory-based and model-based. A well-known example of memory-based approaches is the user-based algorithm, [ 47 ]  while that of model-based approaches is  matrix factorization (recommender systems) . [ 48 ] A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an \"understanding\" of the item itself. Many algorithms have been used in measuring user similarity or item similarity in recommender systems. For example, the  k-nearest neighbor  (k-NN) approach [ 49 ]  and the  Pearson Correlation  as first implemented by Allen. [ 50 ] When building a model from a user's behavior, a distinction is often made between explicit and  implicit  forms of  data collection . Examples of explicit data collection include the following: Examples of  implicit data collection  include the following: Collaborative filtering approaches often suffer from three problems:  cold start , scalability, and sparsity. [ 52 ] One of the most famous examples of collaborative filtering is item-to-item collaborative filtering (people who buy x also buy y), an algorithm popularized by  Amazon.com 's recommender system. [ 54 ] Many  social networks  originally used collaborative filtering to recommend new friends, groups, and other social connections by examining the network of connections between a user and their friends. [ 1 ]  Collaborative filtering is still used as part of hybrid systems. Another common approach when designing recommender systems is  content-based filtering . Content-based filtering methods are based on a description of the item and a profile of the user's preferences. [ 55 ] [ 56 ]  These methods are best suited to situations where there is known data on an item (name, location, description, etc.), but not on the user. Content-based recommenders treat recommendation as a user-specific classification problem and learn a classifier for the user's likes and dislikes based on an item's features. In this system, keywords are used to describe the items, and a  user profile  is built to indicate the type of item this user likes. In other words, these algorithms try to recommend items similar to those that a user liked in the past or is examining in the present. It does not rely on a user sign-in mechanism to generate this often temporary profile. In particular, various candidate items are compared with items previously rated by the user, and the best-matching items are recommended. This approach has its roots in  information retrieval  and  information filtering  research. To create a  user profile , the system mostly focuses on two types of information: Basically, these methods use an item profile (i.e., a set of discrete attributes and features) characterizing the item within the system. To abstract the features of the items in the system, an item presentation algorithm is applied. A widely used algorithm is the  tf–idf  representation (also called vector space representation). [ 57 ]  The system creates a content-based profile of users based on a weighted vector of item features. The weights denote the importance of each feature to the user and can be computed from individually rated content vectors using a variety of techniques. Simple approaches use the average values of the rated item vector while other sophisticated methods use machine learning techniques such as  Bayesian Classifiers ,  cluster analysis ,  decision trees , and  artificial neural networks  in order to estimate the probability that the user is going to like the item. [ 58 ] A key issue with content-based filtering is whether the system can learn user preferences from users' actions regarding one content source and use them across other content types. When the system is limited to recommending content of the same type as the user is already using, the value from the recommendation system is significantly less than when other content types from other services can be recommended. For example, recommending news articles based on news browsing is useful. Still, it would be much more useful when music, videos, products, discussions, etc., from different services, can be recommended based on news browsing. To overcome this, most content-based recommender systems now use some form of the hybrid system. Content-based recommender systems can also include opinion-based recommender systems. In some cases, users are allowed to leave text reviews or feedback on the items. These user-generated texts are implicit data for the recommender system because they are potentially rich resources of both feature/aspects of the item and users' evaluation/sentiment to the item. Features extracted from the user-generated reviews are improved  metadata  of items, because as they also reflect aspects of the item like metadata, extracted features are widely concerned by the users. Sentiments extracted from the reviews can be seen as users' rating scores on the corresponding features. Popular approaches of opinion-based recommender system utilize various techniques including  text mining ,  information retrieval ,  sentiment analysis  (see also  Multimodal sentiment analysis ) and  deep learning . [ 59 ] Most recommender systems now use a hybrid approach, combining  collaborative filtering , content-based filtering, and other approaches. There is no reason why several different techniques of the same type could not be hybridized. Hybrid approaches can be implemented in several ways: by making content-based and collaborative-based predictions separately and then combining them; by adding content-based capabilities to a collaborative-based approach (and vice versa); or by unifying the approaches into one model. [ 41 ]  Several studies that empirically compared the performance of the hybrid with the pure collaborative and content-based methods and demonstrated that the hybrid methods can provide more accurate recommendations than pure approaches. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem, as well as the knowledge engineering bottleneck in  knowledge-based  approaches. [ 60 ] Netflix  is a good example of the use of hybrid recommender systems. [ 61 ]  The website makes recommendations by comparing the watching and searching habits of similar users (i.e., collaborative filtering) as well as by offering movies that share characteristics with films that a user has rated highly (content-based filtering). Some hybridization techniques include: These recommender systems use the interactions of a user within a session [ 63 ]  to generate recommendations. Session-based recommender systems are used at YouTube [ 64 ]  and Amazon. [ 65 ]  These are particularly useful when history (such as past clicks, purchases) of a user is not available or not relevant in the current user session. Domains, where session-based recommendations are particularly relevant, include video, e-commerce, travel, music and more. Most instances of session-based recommender systems rely on the sequence of recent interactions within a session without requiring any additional details (historical, demographic) of the user. Techniques for session-based recommendations are mainly based on generative sequential models such as  recurrent neural networks , [ 63 ] [ 66 ]  Transformers, [ 67 ]  and other deep-learning-based approaches. [ 68 ] [ 69 ] The recommendation problem can be seen as a special instance of a reinforcement learning problem whereby the user is the environment upon which the agent, the recommendation system acts upon in order to receive a reward, for instance, a click or engagement by the user. [ 64 ] [ 70 ] [ 71 ]  One aspect of reinforcement learning that is of particular use in the area of recommender systems is the fact that the models or policies can be learned by providing a reward to the recommendation agent. This is in contrast to traditional learning techniques which rely on supervised learning approaches that are less flexible, reinforcement learning recommendation techniques allow to potentially train models that can be optimized directly on metrics of engagement, and user interest. [ 72 ] Multi-criteria recommender systems (MCRS) can be defined as recommender systems that incorporate preference information upon multiple criteria. Instead of developing recommendation techniques based on a single criterion value, the overall preference of user u for the item i, these systems try to predict a rating for unexplored items of u by exploiting preference information on multiple criteria that affect this overall preference value. Several researchers approach MCRS as a multi-criteria decision making (MCDM) problem, and apply MCDM methods and techniques to implement MCRS systems. [ 73 ]  See this chapter [ 74 ]  for an extended introduction. The majority of existing approaches to recommender systems focus on recommending the most relevant content to users using contextual information, yet do not take into account the risk of disturbing the user with unwanted notifications. It is important to consider the risk of upsetting the user by pushing recommendations in certain circumstances, for instance, during a professional meeting, early morning, or late at night. Therefore, the performance of the recommender system depends in part on the degree to which it has incorporated the risk into the recommendation process. One option to manage this issue is  DRARS , a system which models the context-aware recommendation as a  bandit problem . This system combines a content-based technique and a contextual bandit algorithm. [ 75 ] Mobile recommender systems make use of internet-accessing  smartphones  to offer personalized, context-sensitive recommendations. This is a particularly difficult area of research as mobile data is more complex than data that recommender systems often have to deal with. It is heterogeneous, noisy, requires spatial and temporal auto-correlation, and has validation and generality problems. [ 76 ] There are three factors that could affect the mobile recommender systems and the accuracy of prediction results: the context, the recommendation method and privacy. [ 77 ]  Additionally, mobile recommender systems suffer from a transplantation problem – recommendations may not apply in all regions (for instance, it would be unwise to recommend a recipe in an area where all of the ingredients may not be available). One example of a mobile recommender system are the approaches taken by companies such as  Uber  and  Lyft  to generate driving routes for taxi drivers in a city. [ 76 ]  This system uses GPS data of the routes that taxi drivers take while working, which includes location (latitude and longitude), time stamps, and operational status (with or without passengers). It uses this data to recommend a list of pickup points along a route, with the goal of optimizing occupancy times and profits. One of the events that energized research in recommender systems was the  Netflix Prize . From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system. This competition energized the search for new and more accurate algorithms. On 21 September 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team using tiebreaking rules. [ 78 ] The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction. As stated by the winners, Bell et al.: [ 79 ] Predictive accuracy is substantially improved when blending multiple predictors.  Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique.   Consequently, our solution is an ensemble of many methods. Many benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets. Some members from the team that finished second place founded  Gravity R&D , a recommendation engine that's active in the  RecSys community . [ 78 ] [ 80 ]  4-Tell, Inc. created a Netflix project–derived solution for ecommerce websites. A number of privacy issues arose around the dataset offered by Netflix for the Netflix Prize competition. Although the data sets were anonymized in order to preserve customer privacy, in 2007 two researchers from the University of Texas were able to identify individual users by matching the data sets with film ratings on the  Internet Movie Database (IMDb) . [ 81 ]  As a result, in December 2009, an anonymous Netflix user sued Netflix in Doe v. Netflix, alleging that Netflix had violated United States fair trade laws and the  Video Privacy Protection Act  by releasing the datasets. [ 82 ]  This, as well as concerns from the  Federal Trade Commission , led to the cancellation of a second Netflix Prize competition in 2010. [ 83 ] Evaluation is important in assessing the effectiveness of recommendation algorithms. To measure the  effectiveness  of recommender systems, and compare different approaches, three types of  evaluations  are available: user studies,  online evaluations (A/B tests) , and offline evaluations. [ 43 ] The commonly used metrics are the  mean squared error  and  root mean squared error , the latter having been used in the Netflix Prize. The information retrieval metrics such as  precision and recall  or  DCG  are useful to assess the quality of a recommendation method. Diversity, novelty, and coverage are also considered as important aspects in evaluation. [ 84 ]  However, many of the classic evaluation measures are highly criticized. [ 85 ] Evaluating the performance of a recommendation algorithm on a fixed test dataset will always be extremely challenging as it is impossible to accurately predict the reactions of real users to the recommendations. Hence any metric that computes the effectiveness of an algorithm in offline data will be imprecise. User studies are rather a small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge which recommendations are best. In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as  conversion rate  or  click-through rate . Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies. [ 86 ] The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers. [ 87 ] [ 88 ] [ 89 ] [ 43 ]  For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests. [ 89 ] [ 90 ]  A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms. [ 91 ]  Often, results of so-called offline evaluations do not correlate with actually assessed user-satisfaction. [ 92 ]  This is probably because offline training is highly biased toward the highly reachable items, and offline testing data is highly influenced by the outputs of the online recommendation module. [ 87 ] [ 93 ]  Researchers have concluded that the results of offline evaluations should be viewed critically. [ 94 ] Typically, research on recommender systems is concerned with finding the most accurate recommendation algorithms. However, there are a number of factors that are also important. Recommender systems are notoriously difficult to evaluate offline, with some researchers claiming that this has led to a  reproducibility crisis  in recommender systems publications. The topic of reproducibility seems to be a recurrent issue in some Machine Learning publication venues, but does not have a considerable effect beyond the world of scientific publication. In the context of recommender systems a 2019 paper surveyed a small number of hand-picked publications applying deep learning or neural methods to the top-k recommendation problem, published in top conferences (SIGIR, KDD, WWW,  RecSys , IJCAI), has shown that on average less than 40% of articles could be reproduced by the authors of the survey, with as little as 14% in some conferences. The articles considers a number of potential problems in today's research scholarship and suggests improved scientific practices in that area. [ 107 ] [ 108 ] [ 109 ] \nMore recent work on benchmarking a set of the same methods came to qualitatively very different results [ 110 ]  whereby neural methods were found to be among the best performing methods. Deep learning and neural methods for recommender systems have been used in the winning solutions in several recent recommender system challenges, WSDM, [ 111 ]   RecSys Challenge . [ 112 ] \nMoreover, neural and deep learning methods are widely used in industry where they are extensively tested. [ 113 ] [ 64 ] [ 65 ]  The topic of reproducibility is not new in recommender systems. By 2011,  Ekstrand ,  Konstan , et al. criticized that \"it is currently difficult to reproduce and extend recommender systems research results,\" and that evaluations are \"not handled consistently\". [ 114 ]  Konstan and Adomavicius conclude that \"the Recommender Systems research community is facing a crisis where a significant number of papers present results that contribute little to collective knowledge [...] often because the research lacks the [...] evaluation to be properly judged and, hence, to provide meaningful contributions.\" [ 115 ]  As a consequence, much research about recommender systems can be considered as not reproducible. [ 116 ]  Hence, operators of recommender systems find little guidance in the current research for answering the question, which recommendation approaches to use in a recommender systems.  Said  and  Bellogín  conducted a study of papers published in the field, as well as benchmarked some of the most popular frameworks for recommendation and found large inconsistencies in results, even when the same algorithms and data sets were used. [ 117 ]  Some researchers demonstrated that minor variations in the recommendation algorithms or scenarios led to strong changes in the effectiveness of a recommender system. They conclude that seven actions are necessary to improve the current situation: [ 116 ]  \"(1) survey other research fields and learn from them, (2) find a common understanding of reproducibility, (3) identify and understand the determinants that affect reproducibility, (4) conduct more comprehensive experiments (5) modernize publication practices, (6) foster the development and use of recommendation frameworks, and (7) establish best-practice guidelines for recommender-systems research.\" Artificial intelligence  (AI) applications in recommendation systems are the advanced methodologies that leverage AI technologies, to enhance the performance recommendation engines. The AI-based recommender can analyze complex data sets, learning from user behavior, preferences, and interactions to generate highly accurate and personalized content or product suggestions. [ 118 ]  The integration of AI in recommendation systems has marked a significant evolution from traditional recommendation methods. Traditional methods often relied on inflexible algorithms that could suggest items based on general user trends or apparent similarities in content. In comparison, AI-powered systems have the capability to detect patterns and subtle distinctions that may be overlooked by traditional methods. [ 119 ]  These systems can adapt to specific individual preferences, thereby offering recommendations that are more aligned with individual user needs. This approach marks a shift towards more personalized, user-centric suggestions. Recommendation systems widely adopt AI techniques such as  machine learning ,  deep learning , and  natural language processing . [ 120 ]  These advanced methods enhance system capabilities to predict user preferences and deliver personalized content more accurately. Each technique contributes uniquely. The following sections will introduce specific AI models utilized by a recommendation system by illustrating their theories and functionalities. [ citation needed ] Collaborative filtering  (CF) is one of the most commonly used recommendation system algorithms. It generates personalized suggestions for users based on explicit or implicit behavioral patterns to form predictions. [ 121 ]  Specifically, it relies on external feedback such as star ratings, purchasing history and so on to make judgments. CF make predictions about users' preference based on similarity measurements. Essentially, the underlying theory is: \"if user A is similar to user B, and if A likes item C, then it is likely that B also likes item C.\" There are many models available for collaborative filtering. For AI-applied collaborative filtering, a common model is called  K-nearest neighbors . The ideas are as follows: An  artificial neural network  (ANN), is a deep learning model structure which aims to mimic a human brain. They comprise a series of neurons, each responsible for receiving and processing information transmitted from other interconnected neurons. [ 122 ]  Similar to a human brain, these neurons will change activation state based on incoming signals (training input and backpropagated output), allowing the system to adjust activation weights during the network learning phase. ANN is usually designed to be a  black-box  model. Unlike regular machine learning where the underlying theoretical components are formal and rigid, the collaborative effects of neurons are not entirely clear, but modern experiments has shown the predictive power of ANN. ANN is widely used in recommendation systems for its power to utilize various data. Other than feedback data, ANN can incorporate non-feedback data which are too intricate for collaborative filtering to learn, and the unique structure allows ANN to identify extra signal from non-feedback data to boost user experience. [ 120 ]  Following are some examples: Natural language processing is a series of AI algorithms to make natural human language accessible and analyzable to a machine. [ 123 ]  It is a fairly modern technique inspired by the growing amount of textual information. For application in recommendation system, a common case is the Amazon customer review. Amazon will analyze the feedbacks comments from each customer and report relevant data to other customers for reference. The recent years have witnessed the development of various text analysis models, including  latent semantic analysis  (LSA),  singular value decomposition  (SVD),  latent Dirichlet allocation  (LDA), etc. Their uses have consistently aimed to provide customers with more precise and tailored recommendations. An emerging market for content discovery platforms is academic content. [ 124 ] [ 125 ]  Approximately 6000 academic journal articles are published daily, making it increasingly difficult for researchers to balance time management with staying up to date with relevant research. [ 10 ]  Though traditional tools academic search tools such as  Google Scholar  or  PubMed  provide a readily accessible database of journal articles, content recommendation in these cases are performed in a 'linear' fashion, with users setting 'alarms' for new publications based on keywords, journals or particular authors. Google Scholar provides an 'Updates' tool that suggests articles by using a  statistical model  that takes a researchers' authorized paper and citations as input. [ 10 ]  Whilst these recommendations have been noted to be extremely good, this poses a problem with early career researchers which may be lacking a sufficient body of work to produce accurate recommendations. [ 10 ] In contrast to an engagement-based ranking system employed by social media and other digital platforms, a bridging-based ranking optimizes for content that is unifying instead of  polarizing . [ 126 ] [ 127 ]  Examples include  Polis  and Remesh which have been used around the world to help find more consensus around specific political issues. [ 127 ]   Twitter  has also used this approach for managing its  community notes , [ 128 ]  which  YouTube  planned to pilot in 2024. [ 129 ] [ 130 ]  Aviv Ovadya also argues for implementing bridging-based algorithms in major platforms by empowering  deliberative groups  that are representative of the platform's users to control the design and implementation of the algorithm. [ 131 ] As the connected television landscape continues to evolve, search and recommendation are seen as having an even more pivotal role in the discovery of content. [ 132 ]  With  broadband -connected devices, consumers are projected to have access to content from linear broadcast sources as well as  internet television . Therefore, there is a risk that the market could become fragmented, leaving it to the viewer to visit various locations and find what they want to watch in a way that is time-consuming and complicated for them. By using a search and recommendation engine, viewers are provided with a central 'portal' from which to discover content from several sources in just one location."
  },
  {
    "id": 57,
    "title": "Image retrieval",
    "content": "An  image retrieval  system is a computer system used for browsing, searching and retrieving images from a large  database  of digital images. Most traditional and common methods of image retrieval utilize some method of adding  metadata  such as  captioning ,  keywords , title or descriptions to the images so that retrieval can be performed over the annotation words. Manual image annotation is time-consuming, laborious and expensive; to address this, there has been a large amount of research done on automatic image annotation. Additionally, the increase in social  web applications  and the  semantic web  have inspired the development of several web-based image annotation tools. The first microcomputer-based image database retrieval system was developed at  MIT , in the 1990s, by Banireddy Prasaad,  Amar Gupta , Hoo-min Toong, and  Stuart Madnick . [ 1 ] A 2008 survey article documented progresses after 2007. [ 2 ] Image search  is a specialized data search used to find images. To search for images, a user may provide query terms such as keyword, image file/link, or click on some image, and the system will return images \"similar\" to the query. The similarity used for search criteria could be meta tags, color distribution in images, region/shape attributes, etc. It is crucial to understand the scope and nature of image data in order to determine the complexity of image search system design. The design is also largely influenced by factors such as the diversity of user-base and expected user traffic for a search system. Along this dimension, search data can be classified into the following categories: There are evaluation workshops for image retrieval systems aiming to investigate and improve the performance of such systems."
  },
  {
    "id": 58,
    "title": "3D Content Retrieval",
    "content": "A  3D Content Retrieval  system is a computer system for browsing, searching and retrieving three dimensional digital contents (e.g.:  Computer-aided design , molecular biology models, and cultural heritage 3D scenes,  etc.) from a large database of digital images. The most original way of doing 3D content retrieval uses methods to add description text to 3D content files such as the content file name, link text, and the web page title so that related 3D content can be found through text retrieval. Because of the inefficiency of manually annotating 3D files, researchers have investigated ways to automate the  annotation  process and provide a unified standard to create text descriptions for 3D contents. Moreover, the increase in 3D content has demanded and inspired more advanced ways to retrieve 3D information. Thus, shape matching methods for 3D content retrieval have become popular. Shape matching retrieval is based on techniques that compare and contrast similarities between 3D models. Derive a high level description (e.g.: a skeleton) and then find matching results This method describes 3D models by using a skeleton. The skeleton encodes the geometric and topological information in the form of a skeletal graph and uses  graph matching  techniques to match the skeletons and compare them. [ 1 ]  However, this method requires a 2-manifold input model, and it is very sensitive to noise and details. Many of the existing 3D models are created for visualization purposes, while missing the input quality standard for the skeleton method. The skeleton 3D retrieval method needs more time and effort before it can be used widely. Compute a feature vector based on statistics Unlike Skeleton modeling, which requires a high quality standard for the input source, statistical methods do not put restriction on the validity of an input source. Shape  histograms , feature vectors composed of global geo-metic properties such as circularity and eccentricity, and feature vectors created using frequency decomposition of spherical functions are common examples of using statistical methods to describe 3D information. [ 2 ] 2D projection method Some approaches use 2D projections of a 3D model, justified by the assumption that if two objects are similar in 3D, then they should have similar 2D projections in many directions.  Prototypical  Views [ 3 ]  and  Light field  description [ 4 ]  are good examples of 2D projection methods. In Purdue University, researchers led by Professor Karthik Ramani at the Research and Education Center for Information created a 3D search engine called the  3D Engineering Search System  (3DESS). It is designed to find computer-generated engineering parts. The mechanism behind this search engine is that it starts from an algorithm which can transform query drawing to  voxels , then extracts the most important shape information from the voxels by using another algorithm called thinning, and formulates a skeleton of the object’s outlines and  topology . After that, 3DESS will develop a skeletal graph to render the skeleton, using three common topological constructs: loops, edges, and nodes. The processed common constructs graph can reduce the data amount to represent an object, and it is easier to store and index the description in a database. [ 5 ] According to the lead professor, 3DESS can also describe objects using feature vectors, such as volume, surface area, etc. The system processes queries by comparing their feature vectors or skeletal graphs with data stored in the database. When the system retrieves models in response to the query, users can pick whichever object looks more similar to what they want and leave feedback. Challenges associated with 3D shape-based similarity queries With the skeleton modeling 3D retrieval method, figuring out an efficient way to index 3D shape descriptors is very challenging because 3D shape indexing has very strict criteria. The 3D models must be quick to compute, concise to store, easy to index, invariant under similarity transformations, insensitive to noise and small extra features, robust to arbitrary topological degeneracies, and discriminating of shape differences at many scales. 3D search and retrieval with multimodal support challenges In order to make the 3D search interface simple enough for novice users who know little on 3D retrieval input source requirements, a  multimodal  retrieval system, which can take various types of input sources and provide robust query results, is necessary. So far, only a few approaches have been proposed. In Funkhouser et al. (2003), [ 6 ]  the proposed “Princeton 3D search engine” supports 2D sketches, 3D sketches, 3D models and text as queries. In Chen et al. (2003), [ 7 ]  he designed a 3D retrieval system that intakes 2D sketches and retrieves for 3D objects. Recently, Ansary et al. (2007) [ 8 ]  proposed a 3D retrieval framework using 2D photographic images, sketches, and 3D models."
  },
  {
    "id": 59,
    "title": "Music information retrieval",
    "content": "Music information retrieval  ( MIR ) is the  interdisciplinary science  of retrieving  information  from  music . Those involved in MIR may have a background in academic  musicology ,  psychoacoustics ,  psychology ,  signal processing ,  informatics ,  machine learning ,  optical music recognition ,  computational intelligence , or some combination of these. Music information retrieval is being used by businesses and academics to categorize, manipulate and even create music. One of the classical MIR research topics is genre classification, which is categorizing music items into one of the pre-defined genres such as  classical ,  jazz ,  rock , etc.  Mood classification , artist classification, instrument identification, and music tagging are also popular topics. Several  recommender systems  for music already exist, but surprisingly few are based upon MIR techniques, instead of making use of similarity between users or laborious data compilation.  Pandora , for example, uses experts to tag the music with particular qualities such as \"female singer\" or \"strong bassline\". Many other systems find users whose listening history is similar and suggests unheard music to the users from their respective collections. MIR techniques for  similarity in music  are now beginning to form part of such systems. Music source separation is about separating original signals from a mixture  audio signal . Instrument recognition is about identifying the instruments involved in music. Various MIR systems have been developed that can separate music into its component tracks without access to the master copy. In this way, for example, karaoke tracks can be created from normal music tracks, though the process is not yet perfect owing to vocals occupying some of the same  frequency  space as the other instruments. Automatic  music transcription  is the process of converting an audio recording into symbolic notation, such as a score or a  MIDI file . [ 1 ]  This process involves several audio analysis tasks, which may include multi-pitch detection,  onset detection , duration estimation, instrument identification, and the extraction of  harmonic ,  rhythmic  or  melodic  information. This task becomes more difficult with greater numbers of instruments and a greater  polyphony level . The  automatic generation of music  is a goal held by many MIR researchers. Attempts have been made with limited success in terms of human appreciation of the results. Scores  give a clear and logical description of music from which to work, but access to  sheet music , whether digital or otherwise, is often impractical.  MIDI  music has also been used for similar reasons, but some data is lost in the conversion to MIDI from any other format, unless the music was written with the MIDI standards in mind, which is rare.  Digital audio formats  such as  WAV ,  mp3 , and  ogg  are used when the audio itself is part of the analysis. Lossy formats such as mp3 and ogg work well with the human ear but may be missing crucial data for study. Additionally some encodings create artifacts which could be misleading to any automatic analyser. Despite this the ubiquity of the mp3 has meant much research in the field involves these as the source material. Increasingly,  metadata  mined from the web is incorporated in MIR for a more rounded understanding of the music within its cultural context, and this recently consists of analysis of  social tags  for music. Analysis can often require some summarising, [ 2 ]  and for music (as with many other forms of data) this is achieved by  feature extraction , especially when the  audio content  itself is analysed and  machine learning  is to be applied. The purpose is to reduce the sheer quantity of data down to a manageable set of values so that learning can be performed within a reasonable time-frame. One common feature extracted is the  Mel-Frequency Cepstral Coefficient  (MFCC) which is a measure of the  timbre  of a  piece of music . Other features may be employed to represent the  key ,  chords ,  harmonies ,  melody , main  pitch ,  beats per minute  or rhythm in the piece. There are a number of available audio feature extraction tools [ 3 ]   Available here"
  },
  {
    "id": 60,
    "title": "Video search engine",
    "content": "A  video search engine  is a web-based  search engine  which  crawls  the web for  video  content. Some video search engines parse externally hosted content while others allow content to be uploaded and hosted on their own servers. Some engines also allow users to search by video format type and by length of the clip. The video search results are usually accompanied by a  thumbnail  view of the video. Video search engines are computer programs designed to find videos stored on digital devices, either through Internet servers or in storage units from the same computer. These searches can be made through audiovisual  indexing , which can extract information from audiovisual material and record it as metadata, which will be tracked by search engines. The main use of these search engines is the increasing creation of audiovisual content and the need to manage it properly. The digitization of  audiovisual archives  and the establishment of the Internet, has led to large quantities of video files stored in big databases, whose recovery can be very difficult because of the huge volumes of data and the existence of a semantic gap. The search criterion used by each search engine depends on its nature and purpose of the searches. Metadata is information about facts. It could be information about who is the author of the video, creation date, duration, and all the information that could be extracted and included in the same files. Internet is often used in a language called XML to encode metadata, which works very well through the web and is readable by people. Thus, through this information contained in these files is the easiest way to find data of interest to us. In the videos there are two types of metadata, that we can integrate in the video code itself and external metadata from the page where the video is. In both cases we optimize them to make them ideal when indexed. All video formats incorporate their own metadata. The title, description, coding quality or transcription of the content are possible. To review these data exist programs like FLV MetaData Injector, Sorenson Squeeze or Castfire. Each one has some utilities and special specifications. Converting from one format to another can lose much of this data, so check that the new format information is correct. It is therefore advisable to have the video in multiple formats, so all search robots will be able to find and index it. In most cases the same mechanisms must be applied as in the positioning of an image or text content. They are the most important factors when positioning a video, because they contain most of the necessary information. The titles have to be clearly descriptive and should remove every word or phrase that is not useful. It should be descriptive, including keywords that describe the video with no need to see their title or description. Ideally, separate the words by dashes \"-\". On the page where the video is, it should be a list of keywords linked to the microformat \"rel-tag\". These words will be used by search engines as a basis for organizing information. Although not completely standard, there are two formats that store information in a temporal component that is specified, one for subtitles and another for transcripts, which can also be used for subtitles. The formats are SRT or SUB for subtitles and TTXT for transcripts. Speech recognition  consists of a transcript of the speech of the audio track of the videos, creating a text file. In this way and with the help of a phrase extractor can easily search if the video content is of interest. Some search engines apart from using speech recognition to search for videos, also use it to find the specific point of a multimedia file in which a specific word or phrase is located and so go directly to this point. Gaudi (Google Audio Indexing), a project developed by  Google Labs , uses voice recognition technology to locate the exact moment that one or more words have been spoken within an audio, allowing the user to go directly to exact moment that the words were spoken. If the search query matches some videos from YouTube, the positions are indicated by yellow markers, and must pass the mouse over to read the transcribed text. In addition to transcription, analysis can detect different speakers and sometime attribute the speech to an identified name for the speaker. The text recognition can be very useful to recognize characters in the videos through \"chyrons\". As with speech recognizers, there are search engines that allow (through character recognition) to play a video from a particular point. TalkMiner, an example of search of specific fragments from videos by text recognition, analyzes each video once per second looking for identifier signs of a slide, such as its shape and static nature, captures the image of the slide and uses  Optical Character Recognition  (OCR) to detect the words on the slides. Then, these words are indexed in the  search engine  of TalkMiner, which currently offers to users more than 20,000 videos from institutions such as Stanford University, the University of California at Berkeley, and TED. Through the  visual descriptors  we can analyze the frames of a video and extract information that can be scored as metadata. Descriptions are generated automatically and can describe different aspects of the frames, such as color, texture, shape, motion, and the situation. The video analysis can lead to automatic chaptering, using technics such as change of camera angle, identification of audio jingles. By knowing the typical structure of a video document, it is possible to identify starting and ending credits, content parts and beginning and ending of advertising breaks. The usefulness of a search engine depends on the  relevance  of the result set returned. While there may be millions of videos that include a particular word or phrase, some videos may be more relevant, popular or have more authority than others. This arrangement has a lot to do with search engine optimization. Most search engines use different methods to classify the results and provide the best video in the first results. However, most programs allow sorting the results by several criteria. This criterion is more ambiguous and less objective, but sometimes it is the closest to what we want; depends entirely on the searcher and the algorithm that the owner has chosen. That's why it has always been discussed and now that search results are so ingrained into our society it has been discussed even more. This type of management often depends on the number of times that the searched word comes out, the number of viewings of this, the number of pages that link to this content and ratings given by users who have seen it. [ 1 ] This is a criterion based totally on timeline. Results can be sorted according to their seniority in the repository. It can give us an idea of the popularity of each video. This is the length of the video and can give a taste of which video it is. It is common practice in repositories let the users rate the videos, so that a content of quality and relevance will have a high rank on the list of results gaining visibility. This practice is closely related to virtual communities. We can distinguish two basic types of interfaces, some are web pages hosted on servers which are accessed by Internet and searched through the network, and the others are computer programs that search within a private network. Within Internet interfaces we can find repositories that host video files which incorporate a search engine that searches only their own databases, and video searchers without repository that search in sources of external software. Provides accommodation in video files stored on its servers and usually has an integrated search engine that searches through videos uploaded by its users. One of the first web repositories, or at least the most famous are the portals Vimeo, Dailymotion and YouTube. Their searches are often based on reading the metadata tags, titles and descriptions that users assign to their videos. The disposal and order criterion of the results of these searches are usually selectable between the file upload date, the number of viewings or what they call the relevance. Still, sorting criteria are nowadays the main weapon of these websites, because the positioning of videos is important in terms of promotion. [ citation needed ] They are websites specialized in searching videos across the network or certain pre-selected repositories. They work by web spiders that inspect the network in an automated way to create copies of the visited websites, which will then be indexed by search engines, so they can provide faster searches. Sometimes a search engine only searches in audiovisual files stored within a computer or, as it happens in televisions, on a private server where users access through a local area network. These searchers are usually software or rich Internet applications with a very specific search options for maximum speed and efficiency when presenting the results. They are typically used for large databases and are therefore highly focused to satisfy the needs of television companies. An example of this type of software would be the Digition Suite, which apart from being a benchmark in this kind of interfaces is very close to us as for the storage and retrieval files system from the  Corporació Catalana de Mitjans Audiovisuals . [ 2 ] This particular suite and perhaps in its strongest point is that it integrates the entire process of creating, indexing, storing, searching, editing, and a recovery. Once we have a digitized audiovisual content is indexed with different techniques of different level depending on the importance of content and it's stored. The user, when he wants to retrieve a particular file, has to fill a search fields such as program title, issue date, characters who act or the name of the producer, and the robot starts the search. Once the results appear and they arranged according to preferences, the user can play the low quality videos to work as quickly as possible. When he finds the desired content, it is downloaded with good definition, it's edited and reproduced. [ 3 ] Video search has evolved slowly through several basic search formats which exist today and all use  keywords .  The keywords for each search can be found in the title of the media, any text attached to the media and content linked web pages, also defined by authors and users of video hosted resources. Some video search is performed using human powered search, others create technological systems that work automatically to detect what is in the video and match the searchers needs. Many efforts to improve video search including both human powered search as well as writing algorithm that recognize what's inside the video have meant complete redevelopment of search efforts. It is generally acknowledged that speech to text is possible, though recently Thomas Wilde, the new CEO of Everyzing, acknowledged that Everyzing works 70% of the time when there is music, ambient noise or more than one person speaking. If newscast style speaking (one person, speaking clearly, no ambient noise) is available, that can rise to 93%.  (From the Web Video Summit, San Jose, CA, June 27, 2007). Around 40  phonemes  exist in every language with about 400 in all spoken languages. Rather than applying a text search algorithm after speech-to-text processing is completed, some engines use a phonetic search algorithm to find results within the spoken word. Others work by literally listening to the entire podcast and creating a text transcription using a sophisticated speech-to-text process. Once the text file is created, the file can be searched for any number of search words and phrases. It is generally acknowledged that visual search into video does not work well and that no company is using it publicly.  Researchers at UC San Diego and Carnegie Mellon University have been working on the visual search problem for more than 15 years, and admitted at a \"Future of Search\" conference at UC Berkeley in spring 2007 that it was years away from being viable even in simple search. Search that is not affected by the hosting of video, where results are agnostic no matter where the video is located: Search results are modified, or suspect, due to the large hosted video being given preferential treatment in search results:"
  },
  {
    "id": 61,
    "title": "List of search engines",
    "content": "Search engines , including  web search engines ,  selection-based search  engines,  metasearch engines ,  desktop search  tools, and  web portals  and  vertical market  websites have a search facility for  online databases . † Main website is a portal General: Academic materials only: Search engines dedicated to a specific kind of information These search engines work across the  BitTorrent protocol . Desktop search  engines listed on a light purple background are no longer in active development."
  },
  {
    "id": 62,
    "title": "Desktop search",
    "content": "Desktop search  tools search within a user's own  computer files  as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video. A variety of desktop search programs are now available; see  this list  for examples.  Most desktop search programs are standalone applications. Desktop search products are software alternatives to the search software included in the  operating system , helping users sift through desktop files, emails, attachments, and more. [ 1 ] [ 2 ] [ 3 ] Desktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside  unstructured data  — the information stored on a user's PC, the directories (folders) and files they've created on a  network , documents stored in repositories such as corporate  intranets  and a multitude of other locations. [ 4 ]   Moreover, many companies have structured or unstructured information stored in older  file formats  to which they don't have ready access. The sector attracted considerable attention in the late 2004 to early 2005 period from the struggle between Microsoft and Google. [ 5 ] [ 6 ] [ 7 ]  According to market analysts, both companies were attempting to leverage their monopolies (of  web browsers  and  search engines , respectively) to strengthen their dominance. Due to  Google 's complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between  US Justice Department  and  Microsoft  that  Windows Vista Service Pack 1  would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default. [ 8 ]  As of September 2011, Google ended life for  Google Desktop . Most desktop search engines build and maintain an  index database  to improve performance when searching large amounts of  data . Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however:  Voidtools' Everything Search Engine , [ 9 ]  which performs searches over only file names, not contents, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine, [ 10 ]  which performs searches over filenames and files' contents without building any indices.  An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive. Desktop search tools typically collect three types of information about files: Long-term goals for desktop search include the ability to search the  contents of image files , sound files and video by context. [ 11 ] [ 12 ] Indexing Service , a \"base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching\", [ 13 ]  was originally released in August 1996. It was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation. [ 14 ]  This made indexing large amounts of files require extremely powerful hardware and very long wait times. In 2003,  Windows Desktop Search  (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought \"Instant searching\" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters. [ 15 ]  Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed, e.g. if the indexed files amounted to around 100GB, the index size would be 10GB. With the release of  Windows Vista  came  Windows Search   3.1. Unlike its predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the  RAM  and  CPU  requirements were greatly reduced, cutting back indexing times immensely. Windows Search 4.0 is currently running on all PCs with  Windows 7  and up. In 1994 the  AppleSearch  search engine was introduced, allowing users to fully search all documents within their Macintosh computer, including file format types, meta-data on those files, and content within the files. AppleSearch was a  client/server application , and as such required a server separate from the main device in order to function. The biggest issue with AppleSearch were its large resource requirements: \"AppleSearch requires at least a 68040 processor and 5MB of RAM.\" [ 16 ]  At the time, a Macintosh computer with these specifications was priced at approximately $1400; equivalent to $2050 in 2015. [ 17 ]  On top of this, the software itself cost an additional $1400 for a single license. In 1997,  Sherlock  was released alongside Mac OS 8.5. Sherlock (named after the famous fictional detective  Sherlock Holmes ) was integrated into Mac OS's file browser –  Finder . Sherlock extended the desktop search function to the World Wide Web, allowing users to search both locally and externally. Adding additional functions—such as internet access—to Sherlock was relatively simple, as this was done through plugins written as plain text files. Sherlock was included in every release of Mac OS from  Mac OS 8 , before being deprecated and replaced by  Spotlight  and  Dashboard  in  Mac OS X 10.4 Tiger . It was officially removed in  Mac OS X 10.5 Leopard Spotlight  was released in 2005 as part of  Mac OS X 10.4 Tiger . It is a Selection-based search tool, which means the user invokes a query using only the mouse. Spotlight allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage, and uses a built-in calculator and Oxford American Dictionary to offer quick access to small calculations and word definitions. [ 18 ]  While Spotlight initially has a long startup time, this decreases as the hard disk is indexed. As files are added by the user, the index is constantly updated in the background using minimal CPU & RAM resources. There are a wide range of desktop search options for Linux users, depending upon the skill level of the user, their preference to use desktop tools which tightly integrate into their desktop environment, command-shell functionality (often with advanced scripting options), or browser-based users interfaces to locally running software.  In addition, many users create their own indexing from a variety of indexing packages (e.g. one which does extraction and indexing of PDF/DOC/DOCX/ ODT  documents well, another search engine which works ith/ vcard, LDAP, and other directory/contact databases, as well as the conventional  find  and  locate  commands. Ubuntu Linux  didn't have desktop search until release  Feisty Fawn 7.04 . Using  Tracker [ 19 ]  desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. It not only featured the basic features of file format sorting and meta-data matching, but support for searching through emails and instant messages was added. In 2014  Recoll [ 20 ]  was added to Linux distributions, working with other search programs such as Tracker and  Beagle  to provide efficient full text search. This greatly increased the types of queries and file types that Linux desktop searches could handle. A major advantage of Recoll is that it allows for greater customization of what is indexed; Recoll will index the entire hard disk by default, but can be made to index only selected directories, omitting directories that will never need to be searched. [ 21 ] Starting with  KDE4 , the  NEPOMUK  was introduced.  It provided the ability to index a wide range of desktop content, email, and use semantic web technologies (e.g.  RDF ) to annotate the database.  The introduction faced a few glitches, much of which seemed to be based on the  triplestore .  Performance improved (at least for queries) by switching the backend to a stripped-down version of the  Virtuoso  Open Source Edition, however indexing remained a common user complaint.\nBased on user feedback, the Nepomuk indexing and search has been replaced with the Baloo framework [ 22 ]  based on  Xapian . [ 23 ]"
  },
  {
    "id": 63,
    "title": "Enterprise search",
    "content": "Enterprise search  is software technology for searching data sources internal to a company, typically  intranet  and  database  content. The search is generally offered only to users internal to the company. [ 1 ] [ 2 ]  Enterprise search can be contrasted with  web search , which applies search technology to documents on the open web, and  desktop search , which applies search technology to the content on a single computer. Enterprise search systems index data and documents from a variety of sources such as:  file systems ,  intranets ,  document management systems ,  e-mail , and  databases . Many enterprise search systems integrate structured and  unstructured data  in their collections. [ 3 ]  Enterprise search systems also use access controls to enforce a security policy on their users. [ 4 ] Enterprise search can be seen as a type of  vertical search  of an enterprise. In an enterprise search system, content goes through various phases from source repository to search results: Content awareness (or \"content collection\") is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its  APIs . This model is used when real-time indexing is important. In the pull model, the software gathers content from sources using a connector such as a  web crawler  or a  database  connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content. [ 5 ] Content from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve  recall  or  precision . These may include  stemming ,  lemmatization ,  synonym  expansion,  entity extraction ,  part of speech  tagging. As part of processing and analysis,  tokenization  is applied to split the content into  tokens  which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall. The resulting text is stored in an  index , which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and  term frequency . Using a web page, the user issues a  query  to the system. The query consists of any terms the user enters as well as navigational actions such as  faceting  and paging information. The processed query is then compared to the stored index, and the search system returns results (or \"hits\") referencing source documents that match. Some systems are able to present the document as it was indexed."
  },
  {
    "id": 64,
    "title": "Federated search",
    "content": "Federated search  retrieves information from a variety of sources via a search application built on top of one or more search engines. [ 1 ]  A user makes a single query request which is distributed to the  search engines , databases or other query engines participating in the federation. The federated search then aggregates the results that are received from the search engines for presentation to the user. Federated search can be used to integrate disparate information resources within a single large organization (\"enterprise\") or for the entire web. Federated search, unlike distributed search, requires centralized coordination of the searchable resources. This involves both coordination of the queries transmitted to the individual search engines and fusion of the search results returned by each of them. Federated search came about to meet the need of searching multiple disparate content sources with one query.  This allows a user to search multiple databases at once in real time, arrange the results from the various databases into a useful form and then present the results to the user. As such, it is an information aggregation or integration approach - it provides single point access to many information resources, and typically returns the data in a standard or partially homogenized form. Other approaches include constructing an  Enterprise data warehouse ,  Data lake , or  Data hub . Federated Search queries many times in many ways (each source is queried separately) where other approaches import and transform data many times, typically in overnight batch processes. Federated search provides a real-time view of all sources (to the extent they are all online and available). In industrial search engines, such as  LinkedIn , federated search is used to personalize vertical preference for ambiguous queries. [ 2 ]  For instance, when a user issues a query like \"machine learning\" on LinkedIn, he or she could mean to search for people with machine learning skill, jobs requiring machine learning skill or content about the topic. In such cases, federated search could exploit  user intent  (e.g., hiring, job seeking or content consuming) to personalize the vertical order for each individual user. As described by Peter Jacso (2004 [ 3 ] ), federated searching consists of (1) transforming a  query  and broadcasting it to a group of disparate databases or other web resources, with the appropriate syntax, (2) merging the results collected from the databases, (3) presenting them in a succinct and unified format with minimal duplication, and (4) providing a means, performed either automatically or by the portal user, to sort the merged result set. Federated search portals, either commercial or  open access , generally search public access  bibliographic databases , public access Web-based library catalogues ( OPACs ), Web-based search engines like  Google  and/or open-access, government-operated or corporate data collections. These individual information sources send back to the portal's interface a list of results from the search query. The user can review this hit list.  Some portals will merely  screen scrape  the actual database results and not directly allow a user to enter the information source's application. More sophisticated ones will de-dupe the results list by merging and removing duplicates. There are additional features available in many portals, but the basic idea is the same: to improve the accuracy and relevance of individual searches as well as reduce the amount of time required to search for resources. This process allows federated search some key advantages when compared with existing crawler-based search engines.  Federated search need not place any requirements or burdens on owners of the individual information sources, other than handling increased traffic.  Federated searches are inherently as current as the individual information sources, as they are searched in real time. One application of federated searching is the  metasearch engine . However, the metasearch approach does not overcome the shortcomings of the component search engines, such as incomplete indexes. Documents that are not indexed by search engines create what is known as the  deep Web , or invisible Web.  Google Scholar  is one example of many projects trying to address this, by indexing electronic documents that search engines ignore. And the metasearch approach, like the underlying search engine technology, only works with information sources stored in electronic form. One of the main challenges of metasearch, is ensuring that the search query is compatible with the component search engines that are being federated and combined. When the search vocabulary or  data model  of the search system is different from the data model of one or more of the foreign target systems, the query must be translated into each of the foreign target systems.  This can be done using simple data-element translation or may require  semantic translation . For example, if one search engine allows for quoting of exact strings or n-grams and another does not, the query must be translated to be compatible with each search engine. To translate a quoted exact string query, it can be broken down into a set of overlapping  N-grams  that are most likely to give the desired search results in each search engine. Another challenge faced in the implementation of federated search engines is scalability. It is difficult to maintain the performance, the response speed, of a federated search engine as it combines more and more information sources together.  One implementation of federated search that has begun to address this issue is  WorldWideScience , hosted by the  U.S. Department of Energy 's  Office of Scientific and Technical Information .  WorldWideScience [ 4 ]  is composed of more than 40 information sources, several of which are federated search portals themselves.  One such portal is Science.gov [ 5 ]  which itself federates more than 30 information sources representing most of the R&D output of the U.S. Federal government.  Science.gov returns its highest ranked results to WorldWideScience, which then merges and ranks these results with the search returned by the other information sources that comprise WorldWideScience. [ 5 ]  This approach of cascaded federated search enables large number of information sources to be searched via a single query. Another application  Sesam  running in both Norway and Sweden has been built on top of an open sourced platform specialised for federated search solutions. Sesat, [ 6 ]  an acronym for  Sesam Search Application Toolkit , is a platform that provides much of the framework and functionality required for handling parallel and pipelined searches and displaying them elegantly in a user interface, allowing engineers to focus on the index/database configuration tuning. To personalize vertical orders in federated search, LinkedIn search engine [ 2 ]  exploits the searcher's profile and recent activities to infer his or her intent, such as hiring, job seeking and content consuming, then uses the intent, along with many other signals, to rank vertical orders that are personally relevant to the individual searcher. SWIRL Search [ 7 ]  is an open source federated search engine, released under the Apache 2.0 license. It includes pre-built connectors to popular open source search engines, and re-ranks results using cosine vector similarity. Federated searches present a number of significant challenges, as compared with conventional, single-source searches: When federated search is performed against secure data sources, the users' credentials must be passed on\nto each underlying search engine, so that appropriate security is maintained.  If the user has different\nlogin credentials for different systems, there must be a means to map their login ID to each search\nengine's security domain. [ 8 ] Suppose three real-estate sites are searched, each provides a list of hyperlinked city names to click on, to see matches only in each city.  Ideally these facets would be combined into one set, but that presents additional technical challenges. [ 9 ]   The system also needs to understand \"next page\" links if it's going to allow the user to page through the combined results. Some of this challenge of mapping to a common form can be solved if the federated resources support  linked open data  via  RDF . Ontologies (rules) can be added to map results to common forms using that technology. Each web resource has its own notion of relevance score, and may support some sorted results orders. Relevance varies greatly among \"federates\" in the search, so knowing how to interleave results to show the most relevant is difficult or impossible. Federated search may have to restrict itself to the minimal set of query capabilities that are common to all federates. E.g. if Google supports negation and quoted phrases, but science.gov does not, it will be impossible for the federated search to support negated, quoted phrases. As the number of federates (federated sources) grows, the likelihood of one or more slow or offline federates becomes high. The federated search must decide when to consider a federate offline, or wait for a slow response. Response times will be dictated by the slowest federate of the bunch. Development groups should typically not hit live, production systems as they do regular work, much less intensive load testing. Also, some resources are secure, and should not be arbitrarily queried and exposed in development due to privacy and security concerns. Therefore, the development, testing and performance test environments must include installation and configuration for many sub-systems to allow safe, secure testing. For the overall federated system to be HA/DR, every sub-system must be HA/DR. Similarly,  performance modeling  and  capacity planning  for the federated system requires modeling, planning and sometimes expansion of all federates. For all of the above reasons, within an enterprise, a  data hub  or  data lake  may be preferable, or a hybrid approach. Data hubs and lakes simplify development and access, but may incur some time lag before data is available (without special synchronizing logic). On the web, federation is more typical."
  },
  {
    "id": 65,
    "title": "Social search",
    "content": "Social search  is a behavior of retrieving and searching on a social searching engine that mainly searches  user-generated content  such as news, videos and  images  related search queries on  social media  like  Facebook ,  LinkedIn ,  Twitter ,  Instagram  and  Flickr . [ 1 ]  It is an enhanced version of  web search  that combines traditional  algorithms . The idea behind social search is that instead of ranking search results purely based on semantic relevance between a query and the results, a social search system also takes into account social relationships between the results and the searcher. [ 2 ] [ 3 ] [ 4 ]  The social relationships could be in various forms. For example, in  LinkedIn  people search engine, the social relationships include social connections between searcher and each result, whether or not they are in the same industries, work for the same companies, belong the same social groups, and go the same schools, etc. [ 2 ] [ 5 ] Social search may not be demonstrably better than algorithm-driven search. [ 6 ]  In the  algorithmic ranking model  that search engines used in the past, relevance of a site is determined after analyzing the text and content on the page and link structure of the document. In contrast, search results with social search highlight content that was created or touched by other users who are in the  Social Graph  of the person conducting a search. It is a personalized search technology with  online community  filtering to produce highly personalized results. [ 7 ]  Social search takes many forms, ranging from simple shared bookmarks or tagging of content with descriptive labels to more sophisticated approaches that combine human intelligence with computer algorithms. Depending on the feature-set of a particular  search engine , these results may then be saved and added to community search results, further improving the relevance of results for future searches of that keyword. The principle behind social search is that human network oriented results would be more meaningful and relevant for the user, instead of  computer algorithms  deciding the results for specific queries. [ 8 ] [ 9 ] [ 10 ] [ 11 ] Over the years, there have been different studies, researches and some implementations of Social Search.\nIn 2008, there were a few startup companies that focused on ranking search results according to one's  social graph  on  social networks . [ 12 ] [ 13 ]  Companies in the social search space include  Sproose ,  Mahalo ,  Jumper 2.0 ,  Scour ,  Wink ,  Eurekster , and  Delver . Former efforts include  Wikia Search . In 2008, a story on  TechCrunch  showed  Google  potentially adding in a voting mechanism to search results similar to  Digg 's methodology. [ 14 ]  This suggests growing interest in how social groups can influence and potentially enhance the ability of algorithms to find meaningful data for end users. There are also other services like Sentiment that turn search personal by searching within the users' social circles. In 2009, a startup project called HeyStaks ( www.heystaks.com ) developed a web browser plugin \"HayStaks\". HeyStaks applies social search through collaboration in web search as a way that leads to better search results. [ 15 ]  The main motivation for HeyStaks to work on this idea is to provide the user with features that search engines didn't provide at that time. For instance, different searches have indicated that about 70% of the time when user search for something, a friend or a coworker have found it already. Also, studies have shown that approximately, 30% of people who use online search, search for something that they have found before. [ 16 ]  The startup believe that they help avoid these kind of issues by providing a shared and rich search experience through a list of recommendations that get generated based on search results. In October 2009,  Google  rolled out its \"Social Search\"; after a time in  beta , the feature was expanded to multiple languages in May 2011. Before the expansion however in 2010  Bing  and Google were already taking into account re-tweets and Likes when providing search results. [ 17 ]  However, after a search deal with Twitter ended without renewal, Google began to retool its Social Search. In January 2012, Google released \"Search plus Your World\", a further development of Social Search. The feature, which is integrated into Google's regular search as an opt-out feature, pulls references to results from  Google+  profiles. The goal was to deliver better, more relevant and personalized search results with this integration. This integration however had some problems in which Google+ still is not wildly adopted or has much usage among many users. [ 18 ] \nLater on, Google was criticized by  Twitter  for the perceived potential impact of \"Search plus Your World\" upon web publishers, describing the feature's release to the public as a \"bad day for the web\", while Google replied that Twitter refused to allow deep search crawling by Google of Twitter's content. [ 19 ]  By  Google  integrating  Google+ , the company was encouraging users to switch to Google's social networking site in order to improve search results. One famous example occurred when Google showed a link to  Mark Zuckerberg 's dormant Google+ account rather than the active  Facebook  profile. [ 20 ]  In November 2014 these accusations started to die down because Google's Knowledge Graph started to finally show links to Facebook, Twitter, and other social media sites. [ 20 ] In December 2008, Twitter had re-introduced their people search feature. [ 21 ]  While the interface had since changed significantly, it allows you to search either full names or usernames in a straight-forward search engine. In January 2013,  Facebook  announced a new search engine called  Graph Search  still in the beta stages. The goal was to allow users to prioritize results that were popular with their social circle over the general internet. Facebook's Graph search utilized Facebook's user generated content to target users. [ 18 ] Although there have been different researches and studies in social search, social media networks have not vested enough interest in working with  search engines .  LinkedIn  for example has taken steps to improve its own individual search functions in order to stray users from external search engines. Even  Microsoft  started working with Twitter in order to integrate some tweets into  Bing 's search results in November 2013. Yet Twitter has its own search engine which points out how much value their data has and why they would like to keep it in house. [ 22 ]  In the end though social search will never be truly comprehensive of the subjects that matter to people unless users opt to be completely public with their information. [ 23 ] Social discovery is the use of social preferences and personal information to predict what content will be desirable to the user. [ 24 ]  Technology is used to discover new people and sometimes new experiences shopping, meeting friends or even traveling. [ 25 ]   The discovery of new people is often in real-time, enabled by  mobile apps . However, social discovery is not limited to meeting people in real-time, it also leads to sales and revenue for companies via social media. [ 26 ]   An example of retail would be the addition of social sharing with music, through the iTunes music store. There is a social component to discovering new music  [ 27 ]  Social discovery is at the basis of  Facebook 's profitability, generating ad revenue by targeting the ads to users using the social connections to enhance the commercial appeal. [ 24 ] A social search engine in an aspect can be thought of as a search engine that provides an answer for a question from another answer by identifying a person in the answer. That can happen by retrieving a user submitted query and determining that the query is related to the question; and provides an answer, including the link to the resource, as part of search results that are responsive to the query. [ 28 ] Few social search engines depend only on online communities. Depending on the feature-set of a particular search engine, these results may then be saved and added to community search results, further improving the relevance of results for future searches of that keyword.\nSocial search engines are considered a part of Web 2.0 because they use the collective filtering of online communities to elevate particularly interesting or relevant content using tagging. These descriptive tags add to the meta data embedded in Web pages, theoretically improving the results for particular keywords over time. A user will generally see suggested tags for a particular search term, indicating tags that have previously been added. An implementation of a social search engine is  Aardvark . Aardvark is a social search engine that is based on the \"village paradigm\" which is about connecting the user who has a question with friends or friends of friends whom can answer his or her question. [ 29 ]  In Aadvark, a user ask a question in different ways that mostly involves online ways such as instant messaging, email, web input or other non-online ways such as text message or voice. The Aardvark algorithm forwards the question to someone in the asker extended social network who has the highest probability in knowing the answer to the question. Aadvark was obtained by  Google  in 2010 and abandoned later in 2011. Potential drawbacks to social search lie in its open structure, as is the case with other tagged databases. As these are trust-based networks, unintentional or malicious misuse of tags in this context can lead to imprecise search results.\nThere are number of social search engines that mainly based on tracking user information to order to provide related search results. Examples of this types are  Smashfuse ,  SocialMention ,  Topsy  and Social Searcher, [ 30 ]  originally linked to Facebook. [ 31 ]  Other versions of social engines have been launched, including  Google Coop ,  Eurekster ,  Sproose ,  Rollyo ,  Anoox  and Yahoo's  MyWeb2.0 . Confirmed to be in testing, a new  Facebook  app feature called ' Add a Link ' lets users see popular articles they might want to include in their status updates and comments by entering a search query. The results appear to comprise articles that have been well-shared by other Facebook users, with the most recently published given priority over others. The option certainly makes it easier for users to add links without manually searching their News Feed or resorting to a  Google  query. This new app reduce users' reliance on  Google Search . [ 32 ] Twitter  announced it is replacing its 'Discover' tab with ' Tailored Trends '. The new Tailored Trends feature, besides showing Twitter trends, will give a short description of each topic. Since trends tend to be abbreviations without context, a description will make it more clear what a trend is about. The new trends experience may also include how many Tweets have been sent and whether a topic is trending up or down. [ 33 ] [ 34 ] Google may be falling behind in terms of social search, but in reality they see the potential and importance of this technology with  Web 3.0  and  web semantics . The importance of social media lies within how  Semantic search  works. Semantic search understands much more, including where you are, the time of day, your past history, and many other factors including social connections, and social signals. The first step in order to achieve this will be to teach algorithms to understand the relationship between things. [ 35 ] However this is not possible unless social media sites decide to work with search engines, which is difficult since everyone would like to be the main toll bridge to the internet. As we continue on, and more articles are referred by social media sites, the main concern becomes what good is a search engine without the data of users. One development that seeks to redefine search is the combination of  distributed search  with social search. The goal is a basic search service whose operation is controlled and maintained by the community itself. This would largely work like  Peer to Peer networks  in which users provide the data they seems appropriate. Since the data used by search engines belongs to the user they should have absolute control over it. The infrastructure required for a search engine is already available in the form of thousands of idle desktops and extensive residential broadband access. [ 36 ] Despite the advantages of  distributed search , it shares several same security concerns as the traditionally centralized case. The security concerns can be classified into three categories:  data privacy ,  data integrity  and secure social search. Data privacy protection is defined as the way users can fully control their data and manage its accessibility. The solutions for data privacy include information substitution, attributed based  encryption  and identity based broadcast encryption. The data integrity is defined as the protection of data from unauthorized or improper modifications and deletions. The solutions for data integrity are  digital signature , hash chaining and embedded signing key. The solutions for secure social search are  blind signature ,  zero knowledge proof  and resource handler. [ 37 ] [ 38 ] Another issue related to both distributed and centralized search is how to more accurately understand  user intent  from observed multimedia data. The solutions are based on how to effectively and efficiently leverage social media and search engine. A potential method is to derive a user-image  interest graph  from social media, and then re-rank image search results by integrating social relevance from the user-image interest graph and visual relevance from general search engines. [ 39 ] [ 40 ] Besides above engineering explorations, a more fundamental and potential method is to develop social search systems based on the understanding of related neural mechanisms. Search problems scale from individuals to societies, however, recent trends across disciplines indicate that the formal properties of these problems share similar structures and, often, similar solutions. Moreover, internal search (e.g., memory search) shows similar characteristics to external search (e.g., spatial foraging), including shared neural mechanisms consistent with a common evolutionary origin across species. For search scenarios, organisms must detect – and climb – noisy, long-range environmental (e.g., temperature, salinity, resource) gradients. Here,  social interactions  can provide substantial additional benefit by allowing individuals, simply through grouping, to average their imperfect estimates of temporal and spatial cues (the so-called ‘ wisdom-of-crowds ’ effect). Due to the investment necessary to obtain personal information, however, this again sets the scene for producers (searchers) to be exploited by others. [ 41 ]"
  },
  {
    "id": 66,
    "title": "Geographic information retrieval",
    "content": "Geographic information retrieval (GIR)  or  geographical information retrieval  systems are  search tools  for  searching the Web ,  enterprise  documents, and  mobile local search  that combine traditional text-based queries with location querying, such as a  map  or  placenames .  Like traditional  information retrieval  systems, GIR systems index text and information from  structured  and  unstructured   documents , and also augment those indices with  geographic information . The development and engineering of GIR systems aims to build systems that can reliably answer queries that include a geographic dimension, such as \"What wars were fought in Greece?\" or \"restaurants in Beirut\". [ 1 ]   Semantic similarity  and  word-sense disambiguation  are important components of GIR. [ 2 ]  To identify place names, GIR systems often rely on  natural language processing [ 3 ]  or other  metadata  to associate text  documents  with locations.  Such  georeferencing ,  geotagging , and  geoparsing  tools often need databases of location names, known as  gazetteers . [ 4 ] [ 5 ] [ 6 ] [ 7 ] GIR involves extracting and resolving the meaning of locations in unstructured text.  This is known as  geoparsing . [ 5 ]  After identifying mentions of places and locations in text, a GIR system indexes this information for search and retrieval. GIR systems can commonly be broken down into the following stages:  geoparsing , text and geographic indexing, data storage, geographic relevance ranking with respect to a geographic query and browsing results commonly with a map interface. Some GIR systems separate text indexing from geographic indexing, which enables the use of generic  database joins , [ 8 ]  or multi-stage filtering, [ 9 ]  and others combine them for efficiency. [ 10 ] GIR must manage several forms of uncertainty, including  semantic ambiguity  of mentions of places in  natural language  text and position precision. [ 11 ] The study of GIR systems has a rich history dating back to the 1970s and possibly earlier.  See Ray Larson’s book  Geographic information retrieval and spatial browsing [ 20 ]  for references to much of the pre- Web  literature on GIR. In 2005 the  Cross-Language Evaluation Forum  added a geographic track, GeoCLEF. GeoCLEF was the first  TREC -style evaluation forum for GIR systems and provided participants a chance to compare systems. [ 21 ] GIR has many applications in  geoweb ,  neogeography , and  mobile local search  and has been a focus of many conferences, including the  ESRI  Users Conferences and  O'Reilly ’s Where 2.0 conferences. [ 22 ] [ 23 ] This  computer science  article is a  stub . You can help Wikipedia by  expanding it . This  computational linguistics -related article is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 67,
    "title": "Software engineering",
    "content": "Software engineering  is a field within  computer science  focused on designing, developing, testing, and maintening of software applications. It involves applying  engineering principles  and  computer programming  expertise to develop software systems that meet user needs. [ 1 ] [ 2 ] [ 3 ] [ 4 ] The terms  programmer  and  coder  overlap  software engineer , but they imply only the construction aspect of typical software engineer workload. [ 5 ] A software engineer applies a  software development process , [ 1 ] [ 6 ]  which involves defining,  implementing ,  testing ,  managing , and  maintaining  software systems and, creating and modifying the development process. Beginning in the 1960s, software engineering was recognized as a separate field of  engineering . The development of software engineering was seen as a struggle. Problems included software that was over budget, exceeded deadlines, required extensive  debugging  and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968,  NATO  held the first software engineering conference where issues related to software were addressed. Guidelines and best practices for the development of software were established. [ 7 ] The origins of the term  software engineering  have been attributed to various sources. The term appeared in a list of services offered by companies in the June 1965 issue of \"Computers and Automation\" [ 8 ]  and was used more formally in the August 1966 issue of  Communications of the ACM  (Volume 9, number 8) in \"President's Letter to the ACM Membership\" by Anthony A. Oettinger. [ 9 ] [ 10 ]  It is also associated with the title of a NATO conference in 1968 by Professor  Friedrich L. Bauer . [ 11 ]   Margaret Hamilton  described the discipline of \"software engineering\" during the Apollo missions to give what they were doing legitimacy. [ 12 ]  At the time there was perceived to be a \" software crisis \". [ 13 ] [ 14 ] [ 15 ]  The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of \"Software Engineering\" with the Plenary Sessions' keynotes of  Frederick Brooks [ 16 ]  and  Margaret Hamilton . [ 17 ] In 1984, the  Software Engineering Institute  (SEI) was established as a federally funded research and development center headquartered on the campus of  Carnegie Mellon University  in  Pittsburgh, Pennsylvania , United States. [ 18 ] \n Watts Humphrey  founded the SEI Software Process Program, aimed at understanding and managing the software engineering process. [ 18 ]  The Process Maturity Levels introduced became the  Capability Maturity Model Integration  for Development (CMMI-DEV), which defined how the US Government evaluates the abilities of a software development team. Modern, generally accepted best-practices for software engineering have been collected by the  ISO/IEC JTC 1/SC 7  subcommittee and published as the  Software Engineering Body of Knowledge  (SWEBOK). [ 6 ]  Software engineering is considered one of the major  computing  disciplines. [ 19 ] Notable definitions of software engineering include: The term has also been used less formally: Margaret Hamilton  promoted the term \"software engineering\" during her work on the  Apollo program . The term \"engineering\" was used to acknowledge that the work should be taken just as seriously as other contributions toward the advancement of technology. Hamilton details her use of the term: When I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new \"term\" per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right. [ 29 ] Individual commentators have disagreed sharply on how to define  software engineering  or its legitimacy as an engineering discipline.  David Parnas  has said that software engineering is, in fact, a form of engineering. [ 30 ] [ 31 ]   Steve McConnell  has said that it is not, but that it should be. [ 32 ]   Donald Knuth  has said that programming is an art and a science. [ 33 ]   Edsger W. Dijkstra  claimed that the terms  software engineering  and  software engineer  have been misused in the United States. [ 34 ] Requirements engineering  is about elicitation, analysis, specification, and validation of  requirements  for  software . Software requirements can be  functional ,  non-functional  or domain. Functional requirements describe expected behaviors (i.e. outputs). Non-functional requirements specify issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects. [ 35 ] Software design  is the process of making high-level plans for the software. Design is sometimes divided into levels: Software construction typically involves  programming  (a.k.a. coding),  unit testing ,  integration testing , and  debugging  so as to implement the design. [ 1 ]   [ 6 ] “Software testing is related to, but different from, ... debugging”. [ 6 ] \nTesting during this phase is generally performed by the programmer and with the purpose to verify that the code behaves as designed and to know when the code is ready for the next level of testing. [ citation needed ] Software testing  is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the software under test. [ 1 ] [ 6 ] When described separately from construction, testing typically is performed by  test engineers  or  quality assurance  instead of the programmers who wrote it.  It is performed at the  system level  and is considered an aspect of  software quality . Program analysis is the process of analyzing computer programs with respect to an aspect such as  performance ,  robustness , and  security . Software maintenance  refers to supporting the software after release. It may include but is not limited to:  error correction , optimization, deletion of  unused  and discarded features, and enhancement of existing features. [ 1 ] [ 6 ] Usually, maintenance takes up 40% to 80% of project cost. [ 37 ] Knowledge of  computer programming  is a prerequisite for becoming a software engineer. In 2004, the  IEEE Computer Society  produced the  SWEBOK , which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience. [ 38 ] \nMany software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the  IEEE Computer Society  and the  Association for Computing Machinery , and updated in 2014. [ 19 ]  A number of universities have Software Engineering degree programs; as of 2010 [update] , there were 244 Campus  Bachelor of Software Engineering  programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States. In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to real-world tasks that typical software engineers encounter every day. Similar experience can be gained through  military service  in software engineering. Half of all practitioners today have  degrees  in  computer science ,  information systems , or  information technology . [ citation needed ]  A small but growing number of practitioners have software engineering degrees. In 1987, the  Department of Computing  at  Imperial College London  introduced the first three-year software engineering  bachelor's degree  in the world; in the following year, the  University of Sheffield  established a similar program. [ 39 ]  In 1996, the  Rochester Institute of Technology  established the first software engineering bachelor's degree program in the United States; however, it did not obtain  ABET  accreditation until 2003, the same year as  Rice University ,  Clarkson University ,  Milwaukee School of Engineering , and  Mississippi State University . [ 40 ]  In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering. [ citation needed ] Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees,  SE2004 , was defined by a steering committee between 2001 and 2004 with funding from the  Association for Computing Machinery  and the  IEEE Computer Society . As of 2004 [update] , about 50 universities in the U.S. offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering  master's degree  was established at  Seattle University  in 1979. Since then, graduate software engineering degrees have been made available from many more universities. Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the  Canadian Council of Professional Engineers  has recognized several software engineering programs. In 1998, the US  Naval Postgraduate School  (NPS) established the first  doctorate  program in Software Engineering in the world. [ citation needed ]  Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at  California State University, Fullerton .  Steve McConnell  opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers. [ 41 ]   ETS  (École de technologie supérieure) University and  UQAM  (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge ( SWEBOK ), which has become an ISO standard describing the body of knowledge covered by a software engineer. [ 6 ] Legal  requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, [ 42 ]  and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the  European Engineer  (EUR ING) professional title. Software Engineers can also become professionally qualified as a  Chartered Engineer  through the  British Computer Society . In the United States, the  NCEES  began offering a  Professional Engineer  exam for Software Engineering in 2013, thereby allowing Software Engineers to be licensed and recognized. [ 43 ]  NCEES ended the exam after April 2019 due to lack of participation. [ 44 ]  Mandatory licensing is currently still largely debated, and perceived as controversial. [ 45 ] [ 46 ] The  IEEE Computer Society  and the  ACM , the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's  Guide to the Software Engineering Body of Knowledge – 2004 Version , or  SWEBOK , defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014. [ 6 ]  The IEEE also promulgates a \"Software Engineering Code of Ethics\". [ 47 ] There are an estimated 26.9 million professional software engineers in the world as of 2022, up from 21 million in 2016. [ 48 ] [ 49 ] Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as  freelancers . Some organizations have specialists to perform each of the tasks in the  software development process . Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire  interns , often university or college students during a summer break, or  externships . Specializations include  analysts ,  architects ,  developers ,  testers ,  technical support ,  middleware analysts ,  project managers ,  software product managers ,  educators , and  researchers . Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. [ 50 ]  Potential injuries in these occupations are possible because like other workers who spend long periods  sitting  in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort,  Thrombosis ,  Obesity , and hand and wrist problems such as  carpal tunnel syndrome . [ 51 ] The  U. S. Bureau of Labor Statistics  (BLS) counted 1,365,500 software developers holding jobs in the  U.S.  in 2018. [ 52 ]  Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees. [ 53 ]  The BLS estimates from 2023 to 2033 that computer software engineering would increase by 17%. [ 54 ]  This is down from the 2022 to 2032 BLS estimate of 25% for software engineering. [ 54 ] [ 55 ]  And, is further down from their 30% 2010 to 2020 BLS estimate. [ 56 ]  Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries. [ 57 ] [ 50 ]  In addition, the BLS Job Outlook for Computer Programmers, the  U.S. Bureau of Labor Statistics  (BLS) Occupational Outlook predicts a decline of -7 percent from 2016 to 2026, a further decline of -9 percent from 2019 to 2029, a decline of -10 percent from 2021 to 2031. [ 57 ]  and then a decline of -11 percent from 2022 to 2032. [ 57 ]  Since computer programming can be done from anywhere in the world, companies sometimes hire programmers in countries where wages are lower. [ 57 ] [ 58 ] [ 59 ]  Furthermore, the ratio of women in many software fields has also been declining over the years as compared to other engineering fields. [ 60 ]  Then there is the additional concern that recent advances in  Artificial Intelligence  might impact the demand for future generations of Software Engineers. [ 61 ] [ 62 ] [ 63 ] [ 64 ] [ 65 ] [ 66 ] [ 67 ]  However, this trend may change or slow in the future as many current software engineers in the U.S. market flee the profession or  age out  of the market in the next few decades. [ 57 ] The  Software Engineering Institute  offers certifications on specific topics like  security , process improvement and  software architecture . [ 68 ]   IBM ,  Microsoft  and other companies also sponsor their own certification examinations. Many  IT   certification  programs are oriented toward specific technologies, and managed by the vendors of these technologies. [ 69 ]  These certification programs are tailored to the institutions that would employ people who use these technologies. Broader certification of general software engineering skills is available through various professional societies. As of 2006 [update] , the  IEEE  had certified over 575 software professionals as a  Certified Software Development Professional  (CSDP). [ 70 ]  In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). [ 71 ]  The  ACM  had a professional certification program in the early 1980s, [ citation needed ]  which was discontinued due to lack of interest. The ACM and the  IEEE Computer Society  together examined the possibility of licensing of software engineers as Professional Engineers in the 1990s,\nbut eventually decided that such licensing was inappropriate for the professional industrial practice of software engineering. [ 45 ]  John C. Knight and  Nancy G. Leveson  presented a more balanced analysis of the licensing issue in 2002. [ 46 ] In the U.K. the  British Computer Society  has developed a legally recognized professional certification called  Chartered IT Professional (CITP) , available to fully qualified members ( MBCS ). Software engineers may be eligible for membership of the  British Computer Society  or  Institution of Engineering and Technology  and so qualify to be considered for  Chartered Engineer  status through either of those institutions. In Canada the  Canadian Information Processing Society  has developed a legally recognized professional certification called  Information Systems Professional (ISP) . [ 72 ]  In Ontario, Canada, Software Engineers who graduate from a  Canadian Engineering Accreditation Board (CEAB)  accredited program, successfully complete PEO's ( Professional Engineers Ontario ) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the  Professional Engineers Ontario  and can become Professional Engineers P.Eng. [ 73 ]  The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license. The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the  developed world  avoid education related to software engineering because of the fear of  offshore outsourcing  (importing software products or services from other countries) and of being displaced by  foreign visa workers . [ 74 ]  Although statistics do not currently show a threat to software engineering itself; a related career,  computer programming  does appear to have been affected. [ 75 ]  Nevertheless, the ability to smartly leverage offshore and near-shore resources via the  follow-the-sun  workflow has improved the overall operational capability of many organizations. [ 76 ]  When North Americans leave work, Asians are just arriving to work. When Asians are leaving work, Europeans arrive to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns. While global outsourcing has several advantages, global – and generally distributed – development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). [ 77 ]  Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas. There are various prizes in the field of software engineering: Some call for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field. [ 81 ] Some claim that the concept of software engineering is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters. [ 82 ] Some claim that a core issue with software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a \"theoretical environment.\" [ 82 ] Edsger Dijkstra , a founder of many of the concepts in software development today, rejected the idea of \"software engineering\" up until his death in 2002, arguing that those terms were poor analogies for what he called the \"radical novelty\" of  computer science : A number of these phenomena have been bundled under the name \"Software Engineering\". As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter \"How to program if you cannot.\" [ 83 ]"
  },
  {
    "id": 68,
    "title": "Legal information retrieval",
    "content": "Legal information retrieval  is the science of  information retrieval  applied to legal text, including  legislation ,  case law , and scholarly works. [ 1 ]  Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means. [ 2 ]  Legal information retrieval is a part of the growing field of  legal informatics . In a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used  boolean search  methods (exact matches of specified terms) on full text legal documents have been shown to have an average  recall rate  as low as 20 percent, [ 3 ]  meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents. [ 3 ]  This may result in failing to retrieve important or  precedential  cases. In some jurisdictions this may be especially problematic, as legal professionals are  ethically  obligated to be reasonably informed as to relevant legal documents. [ 4 ] Legal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high  recall rate ) and reducing the number of irrelevant documents (a high  precision rate ). This is a difficult task, as the legal field is prone to  jargon , [ 5 ]   polysemes [ 6 ]  (words that have different meanings when used in a legal context), and constant change. Techniques used to achieve these goals generally fall into three categories:  boolean  retrieval, manual classification of legal text, and  natural language processing  of legal text. Application of standard  information retrieval  techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent  taxonomy . [ 7 ]  Instead, the law is generally filled with open-ended terms, which may change over time. [ 7 ]  This can be especially true in  common law  countries, where each decided case can subtly change the meaning of a certain word or phrase. [ 8 ] Legal information systems  must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term \"worker\" has four different meanings: [ 9 ] It also has the common meaning: Though the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results. Even if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case. [ 10 ]  Case decisions from senior or  superior courts  may be more relevant than those from  lower courts , even where the lower court's decision contains more discussion of the relevant facts. [ 10 ]  The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case). [ 10 ]  An information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority. Additionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not. [ 10 ]  He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions. [ 10 ] Overcoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day [ 2 ] ), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data. [ 2 ] [ 11 ] Boolean searches , where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented but overcome few of the problems discussed above. The recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's  recall rate  to be roughly 20%, and its precision rate to be roughly 79%. [ 3 ]  Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals. [ 12 ] In order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an  ontology  to classify the texts, based on the way a legal professional might think about them. [ 13 ]  These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as  Westlaw 's “Natural Language” [ 14 ]  or  LexisNexis ' Headnote [ 15 ]  searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers [ 14 ]  or Lexis' Headnotes. [ 15 ]  Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted). [ 13 ] These systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text. [ 16 ]  In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals. [ 17 ]  The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction. [ 18 ] The major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts. [ 16 ] [ 19 ]  As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable. [ 20 ] In order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries. [ 2 ] [ 21 ] [ 22 ]  Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ  Natural Language Processing  (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal  ontology . Though multiple systems have been postulated, [ 2 ] [ 21 ] [ 22 ]  few have reported results. One system, “SMILE,” which attempted to automatically extract classifications from case texts, resulted in an  f-measure  (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0). [ 23 ]  This is probably much lower than an acceptable rate for general usage. [ 23 ] [ 24 ] Despite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems. [ 25 ] [ 26 ] In the mid-90s the Room 5 case law retrieval project used citation mining for summaries and ranked its search results based on citation type and count.  This slightly pre-dated the  Page Rank  algorithm at Stanford which was also a citation-based ranking.  Ranking of results was based as much on jurisdiction as on number of references. [ 27 ]"
  },
  {
    "id": 69,
    "title": "Vertical search",
    "content": "A  vertical search  engine is distinct from a general  web search engine , in that it focuses on a specific segment of online content. They are also called specialty or topical search engines. The vertical content area may be based on topicality, media type, or genre of content. Common verticals include shopping, the automotive industry, legal information, medical information, scholarly literature, job search and travel. Examples of vertical search engines include the  Library of Congress ,  Mocavo ,  Nuroa ,  Trulia , and  Yelp . In contrast to general web search engines, which attempt to  index  large portions of the  World Wide Web  using a  web crawler , vertical search engines typically use a  focused crawler  which attempts to index only relevant web pages to a pre-defined topic or set of topics. Some vertical search sites focus on individual verticals, while other sites include multiple vertical searches within one search engine. Vertical search offers several potential benefits over general search engines: Vertical search can be viewed as similar to  enterprise search  where the domain of focus is the enterprise, such as a company, government or other organization. In 2013, consumer price comparison websites with integrated vertical search engines such as  FindTheBest  drew large rounds of venture capital funding, indicating a growth trend for these applications of vertical search technology. [ 1 ] [ 2 ] Domain-specific verticals focus on a specific topic.   John Battelle  describes this in his book  The Search  (2005): Domain-specific search solutions focus on one area of knowledge, creating customized search experiences, that because of the domain's limited corpus and clear relationships between concepts, provide extremely relevant results for searchers. [ 3 ] Any general search engine would be indexing all the pages and searches in a breadth-first manner to collect documents. The spidering in domain-specific search engines more efficiently searches a small subset of documents by focusing on a particular set. Spidering accomplished with a reinforcement-learning framework has been found to be three times more efficient than  breadth-first search . [ 4 ] In early 2014, the Defense Advanced Research Projects Agency ( DARPA ) released a statement on their website outlining the preliminary details of the \"Memex program\", which aims at developing new search technologies overcoming some limitations of text-based search. [ 5 ]   DARPA wants the Memex technology developed in this research to be usable for search engines that can search for information on the  Deep Web  – the part of the Internet that is largely unreachable by commercial search engines like  Google  or  Yahoo . DARPA's website describes that \"The goal is to invent better methods for interacting with and sharing information, so users can quickly and thoroughly organize and search subsets of information relevant to their individual interests\". [ 6 ]  As reported in a 2015  Wired  article, the search technology being developed in the Memex program \"aims to shine a light on the  dark web  and uncover patterns and relationships in online data to help law enforcement and others track illegal activity\". [ 7 ]  DARPA intends for the program to replace the centralized procedures used by commercial search engines, stating that the \"creation of a new domain-specific indexing and search paradigm will provide mechanisms for improved content discovery, information extraction, information retrieval, user collaboration, and extension of current search capabilities to the deep web, the dark web, and nontraditional (e.g. multimedia) content\". [ 8 ]  In their description of the program, DARPA explains the program's name as a tribute to Bush's original Memex invention, which served as an inspiration. [ 5 ] In April 2015, it was announced parts of Memex would be open sourced. [ 9 ]  Modules were available for download. [ 8 ]"
  },
  {
    "id": 70,
    "title": "Adversarial information retrieval",
    "content": "Adversarial information retrieval  ( adversarial IR ) is a topic in  information retrieval  related to strategies for working with a data source where some portion of it has been manipulated maliciously.  Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation. On the Web, the predominant form of such manipulation is  search engine spamming  (also known as spamdexing), which involves employing various techniques to disrupt the activity of  web search engines , usually for financial gain. Examples of spamdexing are  link-bombing ,  comment  or  referrer spam ,  spam blogs  (splogs), malicious tagging.   Reverse engineering  of  ranking algorithms ,  click fraud , [ 1 ]  and  web content filtering  may also be considered forms of adversarial  data manipulation . [ 2 ] Topics related to Web spam (spamdexing): Other topics: The term \"adversarial information retrieval\" was first coined in 2000 by  Andrei Broder  (then Chief Scientist at  Alta Vista ) during the Web plenary session at the  TREC -9 conference. [ 3 ]"
  },
  {
    "id": 71,
    "title": "Automatic summarization",
    "content": "Automatic summarization  is the process of shortening a set of data computationally, to create a subset (a  summary ) that represents the most important or relevant information within the original content.  Artificial intelligence   algorithms  are commonly developed and employed to achieve this, specialized for different types of data. Text  summarization is usually implemented by  natural language processing  methods, designed to locate the most informative sentences in a given document. [ 1 ]  On the other hand, visual content can be summarized using  computer vision  algorithms.  Image  summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection. [ 2 ] [ 3 ] [ 4 ]  Video summarization algorithms identify and extract from the original video content the most important frames ( key-frames ), and/or the most important video segments ( key-shots ), normally in a temporally ordered fashion. [ 5 ] [ 6 ] [ 7 ] [ 8 ]  Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of  video synopsis  algorithms, where  new  video frames are being synthesized based on the original video content. In 2022  Google Docs  released an automatic summarization feature. [ 9 ] There are two general approaches to automatic summarization:  extraction  and  abstraction . Here, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail. [ 10 ]  Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome). [ 11 ] Abstractive summarization methods generate new text that did not exist in the original text. [ 12 ]  This has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by  paraphrasing  sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both  natural language processing  and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge. \"Paraphrasing\" is even more difficult to apply to images and videos, which is why most summarization systems are extractive. Approaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate. There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is  generic summarization , which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is   query relevant summarization , sometimes called  query-based summarization , which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs. An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a  cluster  of articles on the same topic). This problem is called  multi-document summarization . A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary. Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images. [ 13 ]  A summary in this context is useful to show the most representative images of results in an  image collection exploration  system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured. At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the  core-set . These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank,  Submodular set function ,  Determinantal point process , maximal marginal relevance (MMR) etc. The task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text. [ 14 ]  In the case of  research articles , many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.\nConsider the example text from a news article: A keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep  understanding of the text , which makes it difficult for a computer system.\nKeyphrases have many applications. They can enable document browsing by providing a short summary, improve  information retrieval  (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a  full-text search ), and be employed in generating index entries for a large text corpus. Depending on the different literature and the definition of key terms, words or phrases,  keyword extraction  is a highly related theme. Beginning with the work of Turney, [ 15 ]  many researchers have approached keyphrase extraction as a  supervised machine learning  problem.\nGiven a document, we construct an example for each  unigram ,  bigram , and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a  binary classification  for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\nAfter training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases.\nKeyphrase extractors are generally evaluated using  precision and recall . Precision measures how\nmany of the proposed keyphrases are actually correct. Recall measures how many of the true\nkeyphrases your system proposed. The two measures can be combined in an F-score, which is the\nharmonic mean of the two ( F  = 2 PR /( P  +  R ) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization. Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision. We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various Boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper. In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number. Once examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees,  Naive Bayes , and rule induction. In the case of Turney's GenEx algorithm, a  genetic algorithm  is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases. Another keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of  training data . Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate.\nUnsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm [ 16 ]  exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that  PageRank  selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from  social networks . In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties. Thus the algorithm is easily portable to new domains and languages. TextRank is a general purpose  graph -based ranking algorithm for  NLP . Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or  lexical   similarity  between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to  eigenvalue  1 (i.e., the  stationary distribution  of the  random walk  on the graph). The vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step. Edges are created based on word  co-occurrence  in this application of TextRank. Two vertices are connected by an edge if the  unigrams  appear within a window of size N in the original text. N is typically around 2–10. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text  cohesion \" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader. Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text. It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\". In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below. Like keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units—whole sentences instead of words and phrases. Supervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 evaluation only considers unigrams. During the DUC 2001 and 2002 evaluation workshops,  TNO  developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a  Naive Bayes classifier  and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a  maximum entropy  (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain. A promising approach is adaptive document/text summarization. [ 17 ]  It involves first recognizing the text genre and then applying summarization algorithms optimized for this genre. Such software has been created. [ 18 ] The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \" centroid \" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence. A more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank [ 19 ]  is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task. In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document. The edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses  cosine similarity  of  TF-IDF  vectors, TextRank uses a very similar measure based on the number of words two sentences have in common ( normalized  by the sentences' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous  similarity scores  as weights. In both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary. It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ( MEAD ) that combines the LexRank score (stationary probability) with other features like sentence position and length using a  linear combination  with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary. Unlike TextRank, LexRank has been applied to multi-document summarization. Multi-document summarization  is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the  news aggregators  performing the next step down the road of coping with  information overload . Multi-document summarization may also be done in response to a question. [ 20 ] [ 11 ] Multi-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.  [ dubious  –  discuss ] Multi-document extractive summarization faces a problem of redundancy. Ideally, we want to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). For example, in a set of news articles about some event, each article is likely to have many similar sentences. To address this issue, LexRank applies a heuristic post-processing step that adds sentences in rank order, but discards sentences that are too similar to ones already in the summary. This method is called Cross-Sentence Information Subsumption (CSIS). These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. Its importance also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to an arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain. A related method is Maximal Marginal Relevance (MMR), [ 21 ]  which uses a general-purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on  absorbing Markov chain  random walks (a random walk where certain states end the walk). The algorithm is called GRASSHOPPER. [ 22 ]  In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization). The state of the art results for multi-document summarization are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07. [ 23 ]  Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04. [ 24 ] A new method for multi-lingual multi-document summarization that avoids redundancy generates ideograms to represent the meaning of each sentence in each document, then evaluates similarity by comparing ideogram shape and position. It does not use word frequency, training or preprocessing. It uses two user-supplied parameters: equivalence (when are two sentences to be considered equivalent?) and relevance (how long is the desired summary?). The idea of a  submodular set function  has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of  coverage ,  information ,  representation  and  diversity . Moreover, several important  combinatorial optimization  problems occur as special instances of submodular optimization. For example, the  set cover problem  is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which  cover  a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the  facility location problem  is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a  determinantal point process  to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem. While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple  greedy algorithm  admits a constant factor guarantee. [ 25 ]  Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems. Submodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012 [ 26 ]  shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011, [ 27 ]  shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems. [ citation needed ] Submodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show [ 28 ]  that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015 [ 29 ]  show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets. [ 30 ] Specific applications of automatic summarization include: The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries. Evaluation can be intrinsic or extrinsic, [ 36 ]  and inter-textual or intra-textual. [ 37 ] Intrinsic evaluation assesses the summaries directly, while extrinsic evaluation evaluates how the summarization system affects the completion of some other task. Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc. Intra-textual evaluation assess the output of a specific summarization system, while inter-textual evaluation focuses on contrastive analysis of outputs of several summarization systems. Human judgement often varies greatly in what it considers a \"good\" summary, so creating an automatic evaluation process is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive, as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning  coherence  and coverage. The most common way to evaluate summaries is  ROUGE  (Recall-Oriented Understudy for Gisting Evaluation). It is very common for summarization and translation systems in  NIST 's Document Understanding Conferences. [2]  ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references. It calculates  n-gram  overlaps between automatically generated summaries and previously written human summaries. It is recall-based to encourage inclusion of all important topics in summaries. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is the fraction of unigrams that appear in both the reference summary and the automatic summary out of all unigrams in the reference summary. If there are multiple reference summaries, their scores are averaged. A high level of overlap should indicate a high degree of shared concepts between the two summaries. ROUGE cannot determine if the result is coherent, that is if sentences flow together in a sensibly. High-order n-gram ROUGE measures help to some degree. Another unsolved problem is  Anaphor resolution . Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization. [ 38 ] Domain-independent summarization techniques apply sets of general features to identify information-rich text segments. Recent research focuses on domain-specific summarization using knowledge specific to the text's domain, such as medical knowledge and ontologies for summarizing medical texts. [ 39 ] The main drawback of the evaluation systems so far is that we need a reference summary (for some methods, more than one), to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be made to create corpora of texts and their corresponding summaries. Furthermore, some methods require manual annotation of the summaries (e.g. SCU in the Pyramid Method). Moreover, they all perform a quantitative evaluation with regard to different similarity metrics. The first publication in the area dates back to 1957  [ 40 ]  ( Hans Peter Luhn ), starting with a statistical technique. Research increased significantly in 2015.  Term frequency–inverse document frequency  had been used by 2016. Pattern-based summarization was the most powerful option for multi-document summarization found by 2016. In the following year it was surpassed by  latent semantic analysis  (LSA) combined with  non-negative matrix factorization  (NMF). Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity. By 2020, the field was still very active and research is shifting towards abstractive summation and real-time summarization. [ 41 ] Recently the rise of  transformer models  replacing more traditional  RNN  ( LSTM ) have provided a flexibility in the mapping of text sequences to text sequences of a different type, which is well suited to automatic summarization. This includes models such as T5 [ 42 ]  and Pegasus. [ 43 ]"
  },
  {
    "id": 72,
    "title": "Multi-document summarization",
    "content": "Multi-document summarization  is an automatic procedure aimed at  extraction of information  from multiple texts written about the same topic. The resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the  news aggregators  performing the next step down the road of coping with  information overload . Multi-document summarization creates information reports that are both concise and comprehensive.\nWith different opinions being put together & outlined, every topic is described from multiple perspectives within a single document.\nWhile the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should in theory contain the required information, hence limiting the need for accessing original files to cases when refinement is required. In practice, it is hard to summarize multiple documents with conflicting views and biases. In fact, it is almost impossible to achieve clear  extractive summarization  of documents with conflicting views.   Abstractive summarization  is the preferred venue in this case. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. The difficulties remain, if doing automatic extractive summaries of documents with conflicting views. The multi-document summarization task is more complex than  summarizing a single document , even a long one. The difficulty arises from thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and concision. The Document Understanding Conferences, [ 1 ]  conducted annually by  NIST , have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge. An ideal multi-document summarization system not only shortens the source texts, but also presents information organized around the key aspects to represent diverse views. Success produces an overview of a given topic. Such text compilations should also follow basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows: The latter point deserves an additional note. Care is taken to ensure that the automatic overview shows: The multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available. As auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face  copyright  issues in relation to the  fair use  copyright concept."
  },
  {
    "id": 73,
    "title": "Compound-term processing",
    "content": "Compound-term processing,  in  information-retrieval , is search result matching on the basis of  compound terms . Compound terms are built by combining two or more simple terms; for example, \"triple\" is a single word term, but \"triple heart bypass\" is a compound term. Compound-term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? Using this technique, a search for  survival rates following a triple heart bypass in elderly people  will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a  concept search , which itself uses compound-term processing. This will extract the key concepts automatically (in this case \"survival rates\", \"triple heart bypass\" and \"elderly people\") and use these concepts to select the most relevant documents. In August 2003,  Concept Searching Limited  introduced the idea of using statistical compound-term processing. [ 1 ] CLAMOUR is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information and statistics. CLAMOUR appears to use a linguistic approach, rather than one based on  statistical modelling . [ 2 ] Techniques for probabilistic weighting of single word terms date back to at least 1976 in the landmark publication by  Stephen E. Robertson  and  Karen Spärck Jones . [ 3 ]  Robertson stated that the assumption of word independence is not justified and exists as a matter of mathematical convenience. His objection to the term independence is not a new idea, dating back to at least 1964 when H. H. Williams stated that \"[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience\". [ 4 ] In 2004, Anna Lynn Patterson filed patents on \"phrase-based searching in an information retrieval system\" [ 5 ]  to which  Google  subsequently acquired the rights. [ 6 ] Statistical compound-term processing is more adaptable than the process described by Patterson. Her process is targeted at searching the  World Wide Web  where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to  enterprise search  applications where such  a priori  knowledge is not available. Statistical compound-term processing is also more adaptable than the linguistic approach taken by the CLAMOUR project, which must consider the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent. Compound-term processing allows information-retrieval applications, such as  search engines , to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous. Early search engines looked for documents containing the words entered by the user into the search box . These are known as  keyword search  engines.  Boolean search  engines add a degree of sophistication by allowing the user to specify additional requirements. For example, \"Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen\" uses the operators \"NEAR\", \"AND\", \"OR\" and \"NOT\" to specify that these words must follow certain requirements. A  phrase search  is simpler to use, but requires that the exact phrase specified appear in the results."
  },
  {
    "id": 74,
    "title": "Cross-language information retrieval",
    "content": "Cross-language information retrieval  ( CLIR ) is a subfield of  information retrieval  dealing with retrieving information written in a language different from the language of the user's query. [ 1 ]  \nThe term \"cross-language information retrieval\" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term \"multilingual information retrieval\" refers more generally both to technology for retrieval of multilingual collections and to technology which has been moved to handle material in one language to another. The term Multilingual Information Retrieval (MLIR) involves the study of systems that accept queries for information in various languages and return objects (text, and other media) of various languages, translated into the user's language. Cross-language information retrieval refers more specifically to the use case where users formulate their information need in one language and the system retrieves relevant documents in another. To do so, most CLIR systems use various translation techniques. CLIR techniques can be classified into different categories based on different translation resources: [ 2 ] CLIR systems have improved so much that the most accurate multi-lingual and cross-lingual  adhoc information retrieval  systems today are nearly as effective as monolingual systems. [ 3 ]  Other related information access tasks, such as  media monitoring ,  information filtering  and routing,  sentiment analysis , and  information extraction  require more sophisticated models and typically more processing and analysis of the information items of interest. Much of that processing needs to be aware of the specifics of the target languages it is deployed in. Mostly, the various mechanisms of  variation in human language  pose coverage challenges for information retrieval systems: texts in a collection may treat a topic of interest but use terms or expressions which do not match the expression of information need given by the user. This can be true even in a mono-lingual case, but this is especially true in cross-lingual information retrieval, where users may know the target language only to some extent. The benefits of CLIR technology for users with poor to moderate competence in the target language has been found to be greater than for those who are fluent. [ 4 ]  Specific technologies in place for CLIR services include  morphological analysis  to handle  inflection , decompounding or compound splitting to handle  compound terms , and translations mechanisms to translate a query from one language to another. The first workshop on CLIR was held in Zürich during the SIGIR-96 conference. [ 5 ]  Workshops have been held yearly since 2000 at the meetings of the  Cross Language Evaluation Forum  (CLEF). Researchers also convene at the annual  Text Retrieval Conference  (TREC) to discuss their findings regarding different systems and methods of information retrieval, and the conference has served as a point of reference for the CLIR subfield. [ 6 ]  Early CLIR experiments were conducted at TREC-6, held at the  National Institute of Standards and Technology  (NIST) on November 19–21, 1997. [ 7 ] Google Search  had a cross-language search feature that was removed in 2013. [ 8 ] This  computational linguistics -related article is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 75,
    "title": "Document classification",
    "content": "Document classification  or  document categorization  is a problem in  library science ,  information science  and  computer science . The task is to assign a  document  to one or more  classes  or  categories . This may be done \"manually\" (or \"intellectually\") or  algorithmically . The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification. The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified,  text classification  is implied. Documents may be classified according to their  subjects  or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach. Content-based classification  is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned. [ 1 ]  In automatic classification it could be the number of times given words appears in a document. Request-oriented classification  (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks themself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p. 230 [ 2 ] ). Request-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library.  It is probably better, however, to understand request-oriented classification as  policy-based classification : The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach. Sometimes a distinction is made between assigning documents to classes (\"classification\") versus assigning  subjects  to documents (\" subject indexing \") but as  Frederick Wilfrid Lancaster  has argued, this distinction is not fruitful. \"These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p. 21 [ 3 ] ). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a  thesaurus  and vice versa (cf., Aitchison, 1986, [ 4 ]  2004; [ 5 ]  Broughton, 2008; [ 6 ]  Riesthuis & Bliedung, 1991 [ 7 ] ). Therefore, the act of labeling a document (say by assigning a term from a  controlled vocabulary  to a document) is at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents). In other words, labeling a document is the same as assigning it to the class of documents indexed under that label. Automatic document classification tasks can be divided into three sorts:  supervised document classification  where some external mechanism (such as human feedback) provides information on the correct classification for documents,  unsupervised document classification  (also known as  document clustering ), where the classification must be done entirely without reference to external information, and  semi-supervised document classification , [ 8 ]  where parts of the documents are labeled by the external mechanism. There are several software products under various license models available. [ 9 ] [ 10 ] [ 11 ] [ 12 ] [ 13 ] [ 14 ] Automatic document classification techniques include: Classification techniques have been applied to"
  },
  {
    "id": 76,
    "title": "Anti-spam techniques",
    "content": "‹The  template   How-to  is being  considered for merging .› Various  anti-spam techniques  are used to prevent  email spam  (unsolicited bulk email). No technique is a complete solution to the spam problem, and each has  trade-offs  between incorrectly rejecting legitimate email ( false positives ) as opposed to not rejecting all spam email ( false negatives ) – and the associated costs in time, effort, and cost of wrongfully obstructing good mail. [ 1 ] Anti-spam techniques can be broken into four broad categories: those that require actions by individuals, those that can be automated by email administrators, those that can be automated by email senders and those employed by researchers and law enforcement officials. There are a number of techniques that individuals can use to restrict the availability of their email addresses, with the goal of reducing their chance of receiving spam. Sharing an email address only among a  limited group of correspondents is one way to limit the chance that the address will be \"harvested\" and targeted by spam. Similarly, when forwarding messages to a number of recipients who don't know one another, recipient addresses can be put in the  \" bcc: field \" so that each recipient does not get a list of the other recipients' email addresses. Email addresses posted on  webpages ,  Usenet  or  chat rooms  are vulnerable to  e-mail address harvesting . [ 2 ]  Address munging is the practice of disguising an  e-mail address  to prevent it from being automatically collected in this way, but still allow a human reader to reconstruct the original: an email address such as, \"no-one@example.com\", might be written as \"no-one at example dot com\", for instance. A related technique is to display all or part of the email address as an image, or as jumbled text with the order of characters restored using  CSS . A common piece of advice is to not to reply to spam messages [ 3 ]  as spammers may simply regard responses as confirmation that an email address is valid. Similarly, many spam messages contain web links or addresses which the user is directed to follow to be removed from the spammer's mailing list – and these should be treated as dangerous. In any case, sender addresses are often forged in spam messages, so that responding to spam may result in failed deliveries – or may reach completely innocent third parties. Businesses and individuals sometimes avoid publicising an email address by asking for contact to come via a \"contact form\" on a webpage – which then typically forwards the information via email. Such forms, however, are sometimes inconvenient to users, as they are not able to use their preferred email client, risk entering a faulty reply address, and are typically not notified about delivery problems. Further, contact forms have the drawback that they require a website with the appropriate technology. In some cases contact forms also send the message to the email address given by the user. This allows the contact form to be used for sending spam, which may incur email deliverability problems from the site once the spam is reported and the sending IP is blacklisted. Many modern mail programs incorporate  web browser  functionality, such as the display of  HTML , URLs, and images. Avoiding or disabling this feature does not help avoid spam. It may, however, be useful to avoid some problems if a user opens a spam message: offensive images, obfuscated hyperlinks, being tracked by  web bugs ,  being targeted by  JavaScript  or attacks upon security vulnerabilities in the HTML renderer. Mail clients which do not automatically download and display HTML, images or attachments have fewer risks, as do clients who have been configured to not display these by default. An email user may sometimes need to give an address to a site without complete assurance that the site owner will not use it for sending spam. One way to mitigate the risk is to provide a  disposable  email address — an address which the user can disable or abandon which forwards email to a real account. A number of services provide disposable address forwarding. Addresses can be manually disabled, can expire after a given time interval, or can expire after a certain number of messages have been forwarded.\nDisposable email addresses can be used by users to track whether a site owner has disclosed an address, or had a  security breach . [ 4 ] Systems that use \"ham passwords\" ask unrecognised senders to include in their email a password that demonstrates that the email message is a \"ham\" (not spam) message. Typically the email address and ham password would be described on a web page, and the ham password would be included in the subject line of an email message (or appended to the \"username\" part of the email address using the \" plus addressing \" technique). Ham passwords are often combined with filtering systems which let through only those messages that have identified themselves as \"ham\". [ 5 ] Tracking down a spammer's ISP and reporting the offense can lead to the spammer's service being terminated [ 6 ]  and criminal prosecution. [ 7 ]   Unfortunately, it can be difficult to track down the spammer, and while there are some online tools such as  SpamCop  and Network Abuse Clearinghouse to assist, they are not always accurate. Historically, reporting spam in this way has not played a large part in abating spam, since the spammers simply move their operation to another URL, ISP or network of IP addresses. In many countries consumers may also report unwanted and deceptive commercial email to the authorities, e.g. in the US to the  US Federal Trade Commission  (FTC), [ 8 ]  or similar agencies in other countries. [ 9 ] There are now a large number of applications, appliances, services, and software systems that email administrators can use to reduce the load of spam on their systems and mailboxes. In general these attempt to reject (or \"block\"), the majority of spam email outright at the SMTP connection stage. If they do accept a message, they will typically then analyze the content further – and may decide to  \"quarantine\" any categorised as spam. A number of systems have been developed that allow domain name owners to identify email as authorized. Many of these systems use the DNS to list sites authorized to send email on their behalf. After many other proposals,  SPF ,  DKIM  and  DMARC  are all now widely supported with growing adoption. [ 10 ] [ 11 ] [ 12 ]  While not directly attacking spam, these systems make it much harder to  spoof addresses , a common technique of spammers - but also used in  phishing , and other types of fraud via email. A method which may be used by internet service providers, by specialized services or enterprises to combat spam is to require unknown senders to pass various tests before their messages are delivered. These strategies are termed \"challenge/response systems\". Checksum-based filter  exploits the fact that the messages are sent in bulk, that is that they will be identical with small variations. Checksum-based filters strip out everything that might vary between messages, reduce what remains to a  checksum , and look that checksum up in a database such as the   Distributed Checksum Clearinghouse  which collects the checksums of messages that email recipients consider to be spam (some people have a button on their email client which they can click to nominate a message as being spam); if the checksum is in the database, the message is likely to be spam. To avoid being detected in this way, spammers will sometimes insert unique invisible gibberish known as  hashbusters  into the middle of each of their messages, to make each message have a unique checksum. Some email servers expect to never communicate with particular countries from which they receive a great deal of spam.  Therefore, they use country-based filtering – a technique that blocks email from certain countries.  This technique is based on country of origin determined by the sender's IP address rather than any trait of the sender. There are large number of free and commercial DNS-based Blacklists, or  DNSBLs  which allow a mail server to quickly look up the IP of an incoming mail connection - and reject it if it is listed there. Administrators can choose from scores of DNSBLs, each of which reflects different policies: some list sites known to emit spam; others list  open mail relays  or proxies; others list ISPs known to support spam. Most spam/phishing messages contain an URL that they entice victims into clicking on.  Thus, a popular technique since the early 2000s consists of extracting URLs from messages and looking them up in databases such as  Spamhaus ' Domain Block List (DBL),  SURBL , and URIBL. [ 13 ] Many spammers use poorly written software or are unable to comply with the standards because they do not have legitimate control of the computer they are using to send spam ( zombie computer ).  By setting tighter limits on the deviation from RFC standards that the  MTA  will accept, a mail administrator can reduce spam significantly - but this also runs the risk of rejecting mail from older or poorly written or configured servers. Greeting delay  – A sending server is required to wait until it has received the SMTP greeting banner before it sends any data. A deliberate pause can be introduced by receiving servers to allow them to detect and deny any spam-sending applications that do not wait to receive this banner. Temporary rejection  – The  greylisting  technique is built on the fact that the  SMTP  protocol allows for temporary rejection of incoming messages. Greylisting temporarily rejects all messages from unknown senders or mail servers – using the standard 4xx error codes. [ 14 ]  All compliant MTAs will proceed to retry delivery later, but many spammers and spambots will not. The downside is that all legitimate messages from first-time senders will experience a delay in delivery. HELO/EHLO checking  –  RFC   5321  says that an SMTP server \"MAY verify that the domain name argument in the EHLO command actually corresponds to the IP address of the client. However, if the verification fails, the server MUST NOT refuse to accept a message on that basis.\" Systems can, however, be configured to Invalid pipelining  – Several SMTP commands are allowed to be placed in one network packet and \"pipelined\".  For example, if an email is sent with a CC: header, several SMTP \"RCPT TO\" commands might be placed in a single packet instead of one packet per \"RCPT TO\" command.  The SMTP protocol, however, requires that errors be checked and everything is synchronized at certain points.  Many spammers will send everything in a single packet since they do not care about errors and it is more efficient.  Some MTAs will detect this invalid pipelining and reject email sent this way. Nolisting  – The email servers for any given domain are specified in a prioritized list, via the  MX records . The  nolisting  technique is simply the adding of an MX record pointing to a non-existent server as the \"primary\" (i.e. that with the lowest preference value) – which means that an initial mail contact will always fail. Many spam sources do not retry on failure, so the spammer will move on to the next victim; legitimate email servers should retry the next higher numbered MX, and normal email will be delivered with only a brief delay. Quit detection  – An SMTP connection should always be closed with a QUIT command. Many spammers skip this step because their spam has already been sent and taking the time to properly close the connection takes time and bandwidth. Some MTAs are capable of detecting whether or not the connection is closed correctly and use this as a measure of how trustworthy the other system is. Another approach is simply creating an imitation MTA that gives the appearance of being an open mail relay, or an imitation TCP/IP proxy server that gives the appearance of being an open proxy. Spammers who probe systems for open relays and proxies will find such a host and attempt to send mail through it, wasting their time and resources, and potentially, revealing information about themselves and the origin of the spam they are sending to the entity that operates the honeypot. Such a system may simply discard the spam attempts, submit them to  DNSBLs , or store them for analysis by the entity operating the honeypot that may enable identification of the spammer for blocking. SpamAssassin , Policyd-weight and others use some or all of the various tests for spam, and assign a numerical score to each test.  Each message is scanned for these patterns, and the applicable scores tallied up. If the total is above a fixed value, the message is rejected or flagged as spam. By ensuring that no single spam test by itself can flag a message as spam, the false positive rate can be greatly reduced. Outbound spam protection involves scanning email traffic as it exits a network, identifying spam messages and then taking an action such as blocking the message or shutting off the source of the traffic. While the primary impact of  spam  is on spam recipients, sending networks also experience financial costs, such as wasted bandwidth, and the risk of having their IP addresses blocked by receiving networks. Outbound spam protection not only stops spam, but also lets system administrators track down spam sources on their network and remediate them – for example, clearing malware from machines which have become infected with a  virus  or are participating in a  botnet . The PTR DNS records in the reverse DNS can be used for a number of things, including: Content filtering techniques rely on the specification of lists of words or  regular expressions  disallowed in mail messages. Thus, if a site receives spam advertising \"herbal Viagra\", the administrator might place this phrase in the filter configuration. The mail server would then reject any message containing the phrase. Header filtering looks at the header of the email which contains information about the origin, destination and content of the message. Although spammers will often  spoof  fields in the header in order to hide their identity, or to try to make the email look more legitimate than it is, many of these spoofing methods can be detected, and any violation of, e.g.,  RFC   5322 ,  7208 , standards on how the header is to be formed can also serve as a basis for rejecting the message. Since a large percentage of spam has forged and invalid sender (\"from\") addresses, some spam can be detected by checking that this \"from\" address is valid.  A mail server can try to verify the sender address by making an SMTP connection back to the mail exchanger for the address, as if it were creating a bounce, but stopping just before any email is sent. Callback verification has various drawbacks:  (1) Since nearly all spam has forged  return addresses , nearly all callbacks are to innocent third party mail servers that are unrelated to the spam; (2) When the spammer uses a  trap address  as his sender's address. If the receiving MTA tries to make the callback using the trap address in a MAIL FROM command, the receiving MTA's IP address will be blacklisted; (3) Finally, the standard VRFY and EXPN commands [ 16 ]  used to verify an address have been so exploited by spammers that few mail administrators enable them, leaving the receiving SMTP server no effective way to validate the sender's email address. [ 17 ] SMTP proxies allow combating spam in real time, combining sender's behavior controls, providing legitimate users immediate feedback, eliminating a need for quarantine. Spamtrapping is the seeding of an email address so that spammers can find it, but normal users can not. If the email address is used then the sender must be a spammer and they are black listed. As an example, if the email address \"spamtrap@example.org\" is placed in the source HTML of a web site in a way that it isn't displayed on the web page, human visitors to the website would not see it. Spammers, on the other hand, use web page scrapers and bots to harvest email addresses from HTML source code - so they would find this address. When the spammer later sends to the address the spamtrap knows this is highly likely to be a spammer and can take appropriate action. Statistical, or Bayesian, filtering once set up requires no administrative maintenance per se: instead, users mark messages as  spam  or  nonspam  and the filtering software learns from these judgements. Thus, it is matched to the  end user's  needs, and as long as users consistently mark/tag the emails, can respond quickly to changes in spam content. Statistical filters typically also look at message headers, considering not just the content but also peculiarities of the transport mechanism of the email. Software programs that implement statistical filtering include  Bogofilter ,  DSPAM ,  SpamBayes ,  ASSP ,  CRM114 , the email programs  Mozilla  and  Mozilla Thunderbird ,  Mailwasher , and later revisions of  SpamAssassin . A  tarpit  is any server software which intentionally responds extremely slowly to client commands. By running a tarpit which treats acceptable mail normally and known spam slowly or which appears to be an open mail relay, a site can slow down the rate at which spammers can inject messages into the mail facility. Depending on the server and internet speed, a tarpit can slow an attack by a factor of around 500. [ 18 ]  Many systems will simply disconnect if the server doesn't respond quickly, which will eliminate the spam.  However, a few legitimate email systems will also not deal correctly with these delays. The fundamental idea is to slow the attack so that the perpetrator has to waste time without any significant success. [ 19 ] An organization can successfully deploy a tarpit if it is able to define the range of addresses, protocols, and ports for deception. [ 20 ]  The process involves a router passing the supported traffic to the appropriate server while those sent by other contacts are sent to the tarpit. [ 20 ]  Examples of tarpits include the Labrea tarpit, Honeyd, [ 21 ]  SMTP tarpits, and IP-level tarpits. Measures to protect against spam can cause collateral damage. This includes: There are a variety of techniques that email senders use to try to make sure that they do not send spam.  Failure to control the amount of spam sent, as judged by email receivers, can often cause even legitimate email to be blocked and for the sender to be put on  DNSBLs . Since spammer's accounts are frequently disabled due to violations of abuse policies, they are constantly trying to create new accounts.  Due to the damage done to an ISP's reputation when it is the source of spam, many ISPs and web email providers use  CAPTCHAs  on new accounts to verify that it is a real human registering the account, and not an automated spamming system.  They can also verify that credit cards are not stolen before accepting new customers, check  the Spamhaus Project  ROKSO list, and do other background checks. A malicious person can easily attempt to subscribe another user to a  mailing list  — to harass them, or to make the company or organisation appear to be spamming. To prevent this, all modern mailing list management programs (such as  GNU Mailman ,  LISTSERV ,  Majordomo , and  qmail 's ezmlm) support \"confirmed opt-in\" by default. Whenever an email address is presented for subscription to the list, the software will send a confirmation message to that address. The confirmation message contains no advertising content, so it is not construed to be spam itself, and the address is not added to the live mail list unless the recipient responds to the confirmation message. Email senders typically now do the same type of anti-spam checks on email coming from their users and customers as for inward email coming from the rest of the Internet. This protects their reputation, which could otherwise be harmed in the case of infection by spam-sending malware. If a receiving server initially fully accepts an email, and only later determines that the message is spam or to a non-existent recipient, it will generate a  bounce message  back to the supposed sender. However, if (as is often the case with spam), the sender information on the incoming email was forged to be that of an unrelated third party then this bounce message is  backscatter spam . For this reason it is generally preferable for most rejection of incoming email to happen during the SMTP connection stage, with a 5xx error code, while the sending server is still connected. In this case then the  sending  server will report the problem to the real sender cleanly. Firewalls  and  routers  can be programmed to not allow  SMTP  traffic (TCP port 25) from machines on the network that are not supposed to run  Mail Transfer Agents  or send email. [ 22 ]   This practice is somewhat controversial when ISPs block home users, especially if the ISPs do not allow the blocking to be turned off upon request.  Email can still be sent from these computers to designated  smart hosts  via port 25 and to other smart hosts via the email submission port 587. Network address translation  can be used to intercept all port 25 (SMTP) traffic and direct it to a mail server that enforces rate limiting and egress spam filtering.  This is commonly done in hotels, [ 23 ]  but it can cause  email privacy  problems, as well making it impossible to use  STARTTLS  and  SMTP-AUTH  if the port 587 submission port isn't used. Machines that suddenly start sending lots of email may well have become  zombie computers .  By limiting the rate that email can be sent around what is typical for the computer in question, legitimate email can still be sent, but large spam runs can be slowed down until manual investigation can be done. [ 24 ] By monitoring spam reports from places such as  spamcop ,  AOL 's feedback loop, and Network Abuse Clearinghouse, the domain's abuse@ mailbox, etc., ISPs can often learn of problems before they seriously damage the ISP's reputation and have their mail servers blacklisted. Both malicious software and human spam senders often use forged FROM addresses when sending spam messages. Control may be enforced on SMTP servers to ensure senders can only use their correct email address in the FROM field of outgoing messages. In an email users database each user has a record with an email address. The SMTP server must check if the email address in the FROM field of an outgoing message is the same address that belongs to the user's credentials, supplied for SMTP authentication. If the FROM field is forged, an SMTP error will be returned to the email client (e.g. \"You do not own the email address you are trying to send from\"). Most ISPs and  webmail  providers have either an  Acceptable Use Policy  (AUP) or a  Terms of Service  (TOS) agreement that discourages spammers from using their system and allows the spammer to be terminated quickly for violations. From 2000 onwards, many countries enacted specific legislation to criminalize spamming, and appropriate  legislation  and  enforcement  can have a significant impact on spamming activity. [ 25 ]  Where legislation provides specific text that bulk emailers must include, this also makes \"legitimate\" bulk email easier to identify. Increasingly, anti-spam efforts have led to co-ordination between law enforcement, researchers, major consumer financial service companies and  Internet service providers  in monitoring and tracking email spam,  identity theft  and  phishing  activities and gathering evidence for criminal cases. [ 26 ] Analysis of the sites being  spamvertised  by a given piece of spam can often be followed up with domain registrars with good results. [ 27 ] Several approaches have been proposed to improve the email system. Since spamming is facilitated by the fact that large volumes of email are very inexpensive to send, one proposed set of solutions would require that senders pay some cost in order to send email, making it prohibitively expensive for spammers. Anti-spam activist   Daniel Balsam  attempts to make spamming less profitable by bringing lawsuits against spammers. [ 28 ] Artificial intelligence techniques can be deployed for filtering spam emails, such as artificial neural networks algorithms and Bayesian filters. These methods use probabilistic methods to train the networks, such as examination of the concentration or frequency of words seen in the spam versus legitimate email contents. [ 29 ] Channel email is a new proposal for sending email that attempts to distribute anti-spam activities by forcing verification (probably using  bounce messages  so back-scatter does not occur) when the first email is sent for new contacts. Spam is the subject of several research conferences, including:"
  },
  {
    "id": 77,
    "title": "Question answering",
    "content": "Question answering  ( QA ) is a  computer science  discipline within the fields of  information retrieval  and  natural language processing  (NLP) that is concerned with building systems that automatically answer  questions  that are posed by humans in a  natural language . [ 1 ] A question-answering implementation, usually a computer program, may construct its answers by querying a structured  database  of knowledge or information, usually a  knowledge base . More commonly, question-answering systems can pull answers from an unstructured collection of natural language documents. Some examples of natural language document collections used for question answering systems include: Question-answering research attempts to develop ways of answering a wide range of question types, including fact, list,  definition , how, why, hypothetical, semantically constrained, and cross-lingual questions. Another way to categorize question-answering systems is by the technical approach used. There are a number of different types of QA systems, including Rule-based systems use a set of rules to determine the correct answer to a question. Statistical systems use statistical methods to find the most likely answer to a question. Hybrid systems use a combination of rule-based and statistical methods. Two early question answering systems were BASEBALL [ 4 ]  and LUNAR. [ 5 ]  BASEBALL answered questions about Major League Baseball over a period of one year [ ambiguous ] . LUNAR answered questions about the geological analysis of rocks returned by the Apollo Moon missions. Both question answering systems were very effective in their chosen domains. LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain that were posed by people untrained on the system. Further restricted-domain question answering systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to  ELIZA  and  DOCTOR , the first  chatterbot  programs. SHRDLU  was a successful question-answering program developed by  Terry Winograd  in the late 1960s and early 1970s. It simulated the operation of a robot in a toy world (the \"blocks world\"), and it offered the possibility of asking the robot questions about the state of the world. The strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program. In the 1970s,  knowledge bases  were developed that targeted narrower domains of knowledge. The question answering systems developed to interface with these  expert systems  produced  more repeatable [ clarification needed ]  and valid responses to questions within an area of knowledge. These expert systems closely resembled modern question answering systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized  knowledge bases , whereas many modern question answering systems rely on statistical processing of a large, unstructured, natural language text corpus. The 1970s and 1980s saw the development of comprehensive theories in  computational linguistics , which led to the development of ambitious projects in text comprehension and question answering. One example was the Unix Consultant (UC), developed by  Robert Wilensky  at  U.C. Berkeley  in the late 1980s. The system answered questions pertaining to the  Unix  operating system. It had a comprehensive, hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a  text-understanding  system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning. Specialized natural-language question answering systems have been developed, such as EAGLi for health and life scientists. [ 6 ] QA systems are used in a variety of applications, including As of 2001 [update] , question-answering systems typically included a  question classifier  module that determined the type of question and the type of answer. [ 7 ] Different types of question-answering systems employ different architectures. For example, modern open-domain question answering systems may use a retriever-reader architecture. The retriever is aimed at retrieving relevant documents related to a given question, while the reader is used to infer the answer from the retrieved documents. Systems such as  GPT-3 , T5, [ 8 ]  and BART [ 9 ]  use an end-to-end [ jargon ]  architecture in which a transformer-based [ jargon ]  architecture stores large-scale textual data in the underlying parameters. Such models can answer questions without accessing any external knowledge sources. Question answering is dependent on a good search  corpus ; without documents containing the answer, there is little any question answering system can do. Larger collections generally mean better question answering performance, unless the question domain is orthogonal to the collection.  Data redundancy  in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents, [ 10 ]  leading to two benefits: Some question answering systems rely heavily on  automated reasoning . [ 11 ] [ 12 ] In  information retrieval , an open-domain question answering system tries to return an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. [ 13 ]  The system finds answers by using a combination of techniques from  computational linguistics ,  information retrieval , and  knowledge representation . The system takes a  natural language  question as an input rather than a set of keywords, for example: \"When is the national day of China?\" It then transforms this input sentence into a query in its  logical form . Accepting natural language questions makes the system more user-friendly, but harder to implement, as there are a variety of question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task; the entire answer extraction process relies on finding the correct question type and hence the correct answer type. Keyword  extraction  is the first step in identifying the input question type. [ 14 ]  In some cases, words clearly indicate the question type, e.g., \"Who\", \"Where\", \"When\", or \"How many\"—these words might suggest to the system that the answers should be of type \"Person\", \"Location\", \"Date\", or \"Number\", respectively.  POS (part-of-speech) tagging  and syntactic parsing techniques can also determine the answer type. In the example above, the subject is \"Chinese National Day\", the predicate is \"is\" and the adverbial modifier is \"when\", therefore the answer type is \"Date\". Unfortunately, some interrogative words like \"Which\", \"What\", or \"How\" do not correspond to unambiguous answer types: Each can represent more than one type. In situations like this, other words in the question need to be considered. A lexical dictionary such as  WordNet  can be used for understanding the context. Once the system identifies the question type, it uses an  information retrieval  system to find a set of documents that contain the correct keywords. A  tagger  and  NP/Verb Group chunker  can verify whether the correct entities and relations are mentioned in the found documents. For questions such as \"Who\" or \"Where\", a  named-entity recogniser  finds relevant \"Person\" and \"Location\" names from the retrieved documents.  Only the relevant paragraphs are selected for ranking. [ clarification needed ] A  vector space model  can classify the candidate answers. Check [ who? ]  if the answer is of the correct type as determined in the question type analysis stage. An inference technique can validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate—the more and the closer the better. The answer is then translated by parsing into a compact and meaningful representation. In the previous example, the expected output answer is \"1st Oct.\" An open-source, math-aware, question answering system called  MathQA , based on  Ask Platypus  and  Wikidata , was published in 2018. [ 15 ]  MathQA takes an English or Hindi natural language question as input and returns a mathematical formula retrieved from Wikidata as a succinct answer, translated into a computable form that allows the user to insert values for the variables. The system retrieves names and values of variables and common constants from Wikidata if those are available. It is claimed that the system outperforms a commercial computational mathematical knowledge engine on a test set. [ 15 ]  MathQA is hosted by Wikimedia at  https://mathqa.wmflabs.org/ . In 2022, it was extended to answer 15 math question types. [ 16 ] MathQA methods need to combine natural and formula language. One possible approach is to perform supervised annotation via  Entity Linking . The \"ARQMath Task\" at  CLEF  2020 [ 17 ]  was launched to address the problem of linking newly posted questions from the platform Math  Stack Exchange  to existing ones that were already answered by the community. Providing hyperlinks to already answered, semantically related questions helps users to get answers earlier but is a challenging problem because semantic relatedness is not trivial. [ 18 ]  The lab was motivated by the fact that 20% of mathematical queries in general-purpose search engines are expressed as well-formed questions. [ 19 ]  The challenge contained two separate sub-tasks. Task 1: \"Answer retrieval\" matching old post answers to newly posed questions, and Task 2: \"Formula retrieval\" matching old post formulae to new questions. Starting with the domain of mathematics, which involves formula language, the goal is to later extend the task to other domains (e.g., STEM disciplines, such as chemistry, biology, etc.), which employ other types of special notation (e.g., chemical formulae). [ 17 ] [ 18 ] The inverse of mathematical question answering—mathematical question generation—has also been researched. The PhysWikiQuiz physics question generation and test engine retrieves mathematical formulae from Wikidata together with semantic information about their constituting identifiers (names and values of variables). [ 20 ]  The formulae are then rearranged to generate a set of formula variants. Subsequently, the variables are substituted with random values to generate a large number of different questions suitable for individual student tests. PhysWikiquiz is hosted by Wikimedia at  https://physwikiquiz.wmflabs.org/ . Question answering systems have been extended in recent [ may be outdated as of April 2023 ]  years to encompass additional domains of knowledge [ 21 ]  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, [ 22 ]  and video. [ 23 ]  Current question answering research topics include: In 2011,  Watson , a question answering computer system developed by  IBM , competed in two exhibition matches of  Jeopardy!  against  Brad Rutter  and  Ken Jennings , winning by a significant margin. [ 32 ] \n Facebook Research  made their  DrQA  system [ 33 ]  available under an  open source license . This system uses  Wikipedia  as knowledge source. [ 2 ]  The  open source  framework Haystack by  deepset  combines open-domain question answering with generative question answering and supports the  domain adaptation [ clarification needed ]  of the  underlying [ clarification needed ]   language models  for  industry use cases [ vague ] .\n [ 34 ] [ 35 ]"
  },
  {
    "id": 78,
    "title": "Set (mathematics)",
    "content": "In  mathematics , a  set  is a collection of different [ 1 ]  things; [ 2 ] [ 3 ] [ 4 ]  these things are called  elements  or  members  of the set and are typically  mathematical objects  of any kind: numbers, symbols, points in space, lines, other geometrical shapes, variables, or even other sets. [ 5 ]   A set may have a finite number of elements or be an  infinite set . There is a unique set with no elements, called the  empty set ; a set with a single element is a  singleton . Sets are uniquely characterized by their elements; this means that two sets that have precisely the same elements are  equal  (they are the same set). [ 6 ]  This property is called  extensionality . In particular, this implies that there is only one empty set. Sets are ubiquitous in modern mathematics. Indeed,  set theory , more specifically  Zermelo–Fraenkel set theory , has been the standard way to provide rigorous  foundations  for all branches of mathematics since the first half of the 20th century. [ 5 ] Mathematical texts commonly denote sets by  capital letters [ 7 ] [ 5 ]  in  italic , such as  A ,  B ,  C . [ 8 ]  A set may also be called a  collection  or  family , especially when its elements are themselves sets. Roster  or  enumeration notation  defines a set by listing its elements between  curly brackets , separated by commas: [ 9 ] [ 10 ] [ 11 ] [ 12 ] This notation was introduced by  Ernst Zermelo  in 1908. [ 13 ]  In a set, all that matters is whether each element is in it or not, so the ordering of the elements in roster notation is irrelevant (in contrast, in a  sequence , a  tuple , or a  permutation  of a set, the ordering of the terms matters). For example,  {2, 4, 6}  and  {4, 6, 4, 2}  represent the same set. [ 14 ] [ 8 ] [ 15 ] For sets with many elements, especially those following an implicit pattern, the list of members can be abbreviated using an  ellipsis  ' ... '. [ 16 ] [ 17 ]  For instance, the set of the first thousand positive integers may be specified in roster notation as An  infinite set  is a set with an infinite number of elements. If the pattern of its elements is obvious, an infinite set can be given in roster notation, with an ellipsis placed at the end of the list, or at both ends, to indicate that the list continues forever. For example, the set of  nonnegative integers  is and the set of all  integers  is Another way to define a set is to use a rule to determine what the elements are: Such a definition is called a  semantic description . [ 18 ] [ 19 ] Set-builder notation specifies a set as a selection from a larger set, determined by a condition on the elements. [ 19 ] [ 20 ] [ 21 ]  For example, a set  F  can be defined as follows: F \n = \n { \n n \n ∣ \n n \n \n  is an integer, and  \n \n 0 \n ≤ \n n \n ≤ \n 19 \n } \n . \n \n \n {\\displaystyle F=\\{n\\mid n{\\text{ is an integer, and }}0\\leq n\\leq 19\\}.} In this notation, the  vertical bar  \"|\" means \"such that\", and the description can be interpreted as \" F  is the set of all numbers  n  such that  n  is an integer in the range from 0 to 19 inclusive\". Some authors use a  colon  \":\" instead of the vertical bar. [ 22 ] Philosophy  uses specific terms to classify types of definitions: If  B  is a set and  x  is an element of  B , this is written in shorthand as  x  ∈  B , which can also be read as \" x  belongs to  B \", or \" x  is in  B \". [ 23 ]  The statement \" y  is not an element of  B \" is written as  y  ∉  B , which can also be read as \" y  is not in  B \". [ 24 ] [ 25 ] For example, with respect to the sets  A  = {1, 2, 3, 4} ,  B  = {blue, white, red} , and  F  = { n  |  n  is an integer, and 0 ≤  n  ≤ 19} , The  empty set  (or  null set ) is the unique set that has no members. It is denoted  ∅ ,  \n \n \n \n ∅ \n \n \n {\\displaystyle \\emptyset } \n \n , { }, [ 26 ] [ 27 ]   ϕ , [ 28 ]  or  ϕ . [ 29 ] A  singleton set  is a set with exactly one element; such a set may also be called a  unit set . [ 6 ]  Any such set can be written as { x }, where  x  is the element.\nThe set { x } and the element  x  mean different things; Halmos [ 30 ]  draws the analogy that a box containing a hat is not the same as the hat. If every element of set  A  is also in  B , then  A  is described as being a  subset of B , or  contained in B , written  A  ⊆  B , [ 31 ]  or  B  ⊇  A . [ 32 ]  The latter notation may be read  B contains A ,  B includes A , or  B is a superset of A . The  relationship  between sets established by ⊆ is called  inclusion  or  containment . Two sets are equal if they contain each other:  A  ⊆  B  and  B  ⊆  A  is equivalent to  A  =  B . [ 20 ] If  A  is a subset of  B , but  A  is not equal to  B , then  A  is called a  proper subset  of  B . This can be written  A  ⊊  B . Likewise,  B  ⊋  A  means  B is a proper superset of A , i.e.  B  contains  A , and is not equal to  A . A third pair of operators ⊂ and ⊃ are used differently by different authors: some authors use  A  ⊂  B  and  B  ⊃  A  to mean  A  is any subset of  B  (and not necessarily a proper subset), [ 33 ] [ 24 ]  while others reserve  A  ⊂  B  and  B  ⊃  A  for cases where  A  is a proper subset of  B . [ 31 ] Examples: The empty set is a subset of every set, [ 26 ]  and every set is a subset of itself: [ 33 ] An  Euler diagram  is a graphical representation of a collection of sets; each set is depicted as a planar region enclosed by a loop, with its elements inside. If  A  is a subset of  B , then the region representing  A  is completely inside the region representing  B . If two sets have no elements in common, the regions do not overlap. A  Venn diagram , in contrast, is a graphical representation of  n  sets in which the  n  loops divide the plane into  2 n  zones such that for each way of selecting some of the  n  sets (possibly all or none), there is a zone for the elements that belong to all the selected sets and none of the others. For example, if the sets are  A ,  B , and  C , there should be a zone for the elements that are inside  A  and  C  and outside  B  (even if such elements do not exist). There are sets of such mathematical importance, to which mathematicians refer so frequently, that they have acquired special names and notational conventions to identify them. Many of these important sets are represented in mathematical texts using bold (e.g.  \n \n \n \n \n Z \n \n \n \n {\\displaystyle \\mathbf {Z} } \n \n ) or  blackboard bold  (e.g.  \n \n \n \n \n Z \n \n \n \n {\\displaystyle \\mathbb {Z} } \n \n ) typeface. [ 34 ]  These include Each of the above sets of numbers has an infinite number of elements. Each is a subset of the sets listed below it. Sets of positive or negative numbers are sometimes denoted by superscript plus and minus signs, respectively. For example,  \n \n \n \n \n \n Q \n \n \n + \n \n \n \n \n {\\displaystyle \\mathbf {Q} ^{+}} \n \n  represents the set of positive rational numbers. A  function  (or  mapping ) from a set  A  to a set  B  is a rule that assigns to each \"input\" element of  A  an \"output\" that is an element of  B ; more formally, a function is a special kind of  relation , one that relates each element of  A  to  exactly one  element of  B . A function is called An injective function is called an  injection , a surjective function is called a  surjection , and a bijective function is called a  bijection  or  one-to-one correspondence . The cardinality of a set  S , denoted  | S | , is the number of members of  S . [ 35 ]  For example, if  B  = {blue, white, red} , then  | B | = 3 . Repeated members in roster notation are not counted, [ 36 ] [ 37 ]  so  | {blue, white, red, blue, white} | = 3 , too. More formally, two sets share the same cardinality if there exists a bijection between them. The cardinality of the empty set is zero. [ 38 ] The list of elements of some sets is endless, or  infinite . For example, the set  \n \n \n \n \n N \n \n \n \n {\\displaystyle \\mathbb {N} } \n \n  of  natural numbers  is infinite. [ 20 ]  In fact, all the special sets of numbers mentioned in the section above are infinite. Infinite sets have  infinite cardinality . Some infinite cardinalities are greater than others. Arguably one of the most significant results from set theory is that the set of  real numbers  has greater cardinality than the set of natural numbers. [ 39 ]  Sets with cardinality less than or equal to that of  \n \n \n \n \n N \n \n \n \n {\\displaystyle \\mathbb {N} } \n \n  are called  countable sets ; these are either finite sets or  countably infinite sets  (sets of the same cardinality as  \n \n \n \n \n N \n \n \n \n {\\displaystyle \\mathbb {N} } \n \n ); some authors use \"countable\" to mean \"countably infinite\". Sets with cardinality strictly greater than that of  \n \n \n \n \n N \n \n \n \n {\\displaystyle \\mathbb {N} } \n \n  are called  uncountable sets . However, it can be shown that the cardinality of a  straight line  (i.e., the number of points on a line) is the same as the cardinality of any  segment  of that line, of the entire  plane , and indeed of any  finite-dimensional   Euclidean space . [ 40 ] The continuum hypothesis, formulated by Georg Cantor in 1878, is the statement that there is no set with cardinality strictly between the  cardinality of the natural numbers  and the cardinality of a straight line. [ 41 ]  In 1963,  Paul Cohen  proved that the continuum hypothesis is  independent  of the axiom system ZFC consisting of  Zermelo–Fraenkel set theory  with the  axiom of choice . [ 42 ]  (ZFC is the most widely-studied version of axiomatic set theory.) The power set of a set  S  is the set of all subsets of  S . [ 20 ]  The  empty set  and  S  itself are elements of the power set of  S , because these are both subsets of  S . For example, the power set of  {1, 2, 3}  is  {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}} . The power set of a set  S  is commonly written as  P ( S )  or  2 S . [ 20 ] [ 43 ] [ 8 ] If  S  has  n  elements, then  P ( S )  has  2 n  elements. [ 44 ]  For example,  {1, 2, 3}  has three elements, and its power set has  2 3  = 8  elements, as shown above. If  S  is infinite (whether  countable  or  uncountable ), then  P ( S )  is uncountable. Moreover, the power set is always strictly \"bigger\" than the original set, in the sense that any attempt to pair up the elements of  S  with the elements of  P ( S )  will leave some elements of  P ( S )  unpaired. (There is never a  bijection  from  S  onto  P ( S ) .) [ 45 ] A  partition of a set   S  is a set of nonempty subsets of  S , such that every element  x  in  S  is in exactly one of these subsets. That is, the subsets are  pairwise disjoint  (meaning any two sets of the partition contain no element in common), and the  union  of all the subsets of the partition is  S . [ 46 ] [ 47 ] Suppose that a  universal set   U  (a set containing all elements being discussed) has been fixed, and that  A  is a subset of  U . Given any two sets  A  and  B , Examples: The operations above satisfy many identities.  For example, one of  De Morgan's laws  states that  ( A  ∪  B )′ =  A ′ ∩  B ′  (that is, the elements outside the union of  A  and  B  are the elements that are outside  A   and  outside  B ). The cardinality of  A  ×  B  is the product of the cardinalities of  A  and  B . This is an elementary fact when  A  and  B  are finite.  When one or both are infinite, multiplication of cardinal numbers is defined to make this true. The power set of any set becomes a  Boolean ring  with symmetric difference as the addition of the ring and intersection as the multiplication of the ring. Sets are ubiquitous in modern mathematics. For example,  structures  in  abstract algebra , such as  groups ,  fields  and  rings , are sets  closed  under one or more operations. One of the main applications of naive set theory is in the construction of  relations . A relation from a  domain   A  to a  codomain   B  is a subset of the Cartesian product  A  ×  B . For example, considering the set  S  = {rock, paper, scissors}  of shapes in the  game  of the same name, the relation \"beats\" from  S  to  S  is the set  B  = {(scissors,paper), (paper,rock), (rock,scissors)} ; thus  x  beats  y  in the game if the pair  ( x , y )  is a member of  B . Another example is the set  F  of all pairs  ( x ,  x 2 ) , where  x  is real. This relation is a subset of  R  ×  R , because the set of all squares is subset of the set of all real numbers. Since for every  x  in  R , one and only one pair  ( x ,...)  is found in  F , it is called a  function . In functional notation, this relation can be written as  F ( x ) =  x 2 . The inclusion–exclusion principle is a technique for counting the elements in a union of two finite sets in terms of the sizes of the two sets and their intersection. It can be expressed symbolically as\n \n \n \n \n \n | \n \n A \n ∪ \n B \n \n | \n \n = \n \n | \n \n A \n \n | \n \n + \n \n | \n \n B \n \n | \n \n − \n \n | \n \n A \n ∩ \n B \n \n | \n \n . \n \n \n {\\displaystyle |A\\cup B|=|A|+|B|-|A\\cap B|.} A more general form of the principle gives the cardinality of any finite union of finite sets:\n \n \n \n \n \n \n \n \n \n | \n \n \n A \n \n 1 \n \n \n ∪ \n \n A \n \n 2 \n \n \n ∪ \n \n A \n \n 3 \n \n \n ∪ \n … \n ∪ \n \n A \n \n n \n \n \n \n | \n \n = \n \n \n \n ( \n \n \n | \n \n A \n \n 1 \n \n \n | \n \n + \n \n | \n \n A \n \n 2 \n \n \n | \n \n + \n \n | \n \n A \n \n 3 \n \n \n | \n \n + \n … \n \n | \n \n A \n \n n \n \n \n | \n \n \n ) \n \n \n \n \n \n \n \n \n \n − \n \n ( \n \n \n | \n \n \n A \n \n 1 \n \n \n ∩ \n \n A \n \n 2 \n \n \n \n | \n \n + \n \n | \n \n \n A \n \n 1 \n \n \n ∩ \n \n A \n \n 3 \n \n \n \n | \n \n + \n … \n \n | \n \n \n A \n \n n \n − \n 1 \n \n \n ∩ \n \n A \n \n n \n \n \n \n | \n \n \n ) \n \n \n \n \n \n \n \n \n \n + \n … \n \n \n \n \n \n \n \n \n + \n \n \n ( \n \n − \n 1 \n \n ) \n \n \n n \n − \n 1 \n \n \n \n ( \n \n | \n \n \n A \n \n 1 \n \n \n ∩ \n \n A \n \n 2 \n \n \n ∩ \n \n A \n \n 3 \n \n \n ∩ \n … \n ∩ \n \n A \n \n n \n \n \n \n | \n \n ) \n \n . \n \n \n \n \n \n \n {\\displaystyle {\\begin{aligned}\\left|A_{1}\\cup A_{2}\\cup A_{3}\\cup \\ldots \\cup A_{n}\\right|=&\\left(\\left|A_{1}\\right|+\\left|A_{2}\\right|+\\left|A_{3}\\right|+\\ldots \\left|A_{n}\\right|\\right)\\\\&{}-\\left(\\left|A_{1}\\cap A_{2}\\right|+\\left|A_{1}\\cap A_{3}\\right|+\\ldots \\left|A_{n-1}\\cap A_{n}\\right|\\right)\\\\&{}+\\ldots \\\\&{}+\\left(-1\\right)^{n-1}\\left(\\left|A_{1}\\cap A_{2}\\cap A_{3}\\cap \\ldots \\cap A_{n}\\right|\\right).\\end{aligned}}} The concept of a set emerged in mathematics at the end of the 19th century. [ 48 ]  The German word for set,  Menge , was coined by  Bernard Bolzano  in his work  Paradoxes of the Infinite . [ 49 ] [ 50 ] [ 51 ] Georg Cantor , one of the founders of set theory,   gave the following definition at the beginning of his  Beiträge zur Begründung der transfiniten Mengenlehre : [ 52 ] [ 1 ] A set is a gathering together into a whole of definite, distinct objects of our perception or our thought—which are called elements of the set. Bertrand Russell  introduced the distinction between a set and a  class  (a set is a class, but some classes, such as the class of all sets, are not sets; see  Russell's paradox ): [ 53 ] When mathematicians deal with what they call a manifold, aggregate,  Menge ,  ensemble , or some equivalent name, it is  common, especially where the number of terms involved is finite, to regard the object in question (which is in fact a class) as defined by the enumeration of its terms, and as consisting possibly of a single term, which in that case  is  the class. The foremost property of a set is that it can have elements, also called  members . Two sets are  equal  when they have the same elements. More precisely, sets  A  and  B  are equal if every element of  A  is an element of  B , and every element of  B  is an element of  A ; this property is called the  extensionality  of sets . [ 23 ]  As a consequence, e.g.  {2, 4, 6}  and  {4, 6, 4, 2}  represent the same set. Unlike sets,  multisets  can be distinguished by the number of occurrences of an element; e.g.  [2, 4, 6]  and  [4, 6, 4, 2]  represent different multisets, while  [2, 4, 6]  and  [6, 4, 2]  are equal.  Tuples  can even be distinguished by element order; e.g.  (2, 4, 6)  and  (6, 4, 2)  represent different tuples. The simple concept of a set has proved enormously useful in mathematics, but  paradoxes  arise if no restrictions are placed on how sets can be constructed: Naïve set theory  defines a set as any  well-defined  collection of distinct elements, but problems arise from the vagueness of the term  well-defined . In subsequent efforts to resolve these paradoxes since the time of the original formulation of naïve set theory, the properties of sets have been defined by  axioms .  Axiomatic set theory  takes the concept of a set as a  primitive notion . [ 54 ]  The purpose of the axioms is to provide a basic framework from which to deduce the truth or falsity of particular  mathematical propositions  (statements) about sets, using  first-order logic . According to  Gödel's incompleteness theorems  however, it is not possible to use first-order logic to prove any such particular axiomatic set theory is free from paradox. [ 55 ]"
  },
  {
    "id": 79,
    "title": "Boolean model of information retrieval",
    "content": "The (standard)  Boolean model of information retrieval  ( BIR ) [ 1 ]  is a classical  information retrieval  (IR) model and, at the same time, the first and most-adopted one. [ 2 ]  The BIR is based on  Boolean logic  and classical  set theory  in that both the documents to be searched and the user's query are conceived as sets of terms (a  bag-of-words model ). Retrieval is based on whether or not the documents contain the query terms and whether they satisfy the boolean conditions described by the query. An  index term  is a word or expression ,  which may be  stemmed , describing or characterizing a document,  such as a keyword given for a journal article. Let \n \n \n \n T \n = \n { \n \n t \n \n 1 \n \n \n , \n \n t \n \n 2 \n \n \n , \n   \n … \n , \n   \n \n t \n \n n \n \n \n } \n \n \n {\\displaystyle T=\\{t_{1},t_{2},\\ \\ldots ,\\ t_{n}\\}} \n \n be the set of all such index terms. A  document  is any subset of  \n \n \n \n T \n \n \n {\\displaystyle T} \n \n . Let \n \n \n \n D \n = \n { \n \n D \n \n 1 \n \n \n , \n   \n … \n   \n , \n \n D \n \n n \n \n \n } \n \n \n {\\displaystyle D=\\{D_{1},\\ \\ldots \\ ,D_{n}\\}} \n \n be the set of all documents. T \n \n \n {\\displaystyle T} \n \n  is a series of words or small phrases (index terms). Each of those words or small phrases are named  \n \n \n \n \n t \n \n n \n \n \n \n \n {\\displaystyle t_{n}} \n \n , where  \n \n \n \n n \n \n \n {\\displaystyle n} \n \n  is the number of the term in the series/list. You can think of   \n \n \n \n T \n \n \n {\\displaystyle T} \n \n  as \"Terms\" and  \n \n \n \n \n t \n \n n \n \n \n \n \n {\\displaystyle t_{n}} \n \n  as \"index term  n \". The words or small phrases (index terms  \n \n \n \n \n t \n \n n \n \n \n \n \n {\\displaystyle t_{n}} \n \n ) can exist in documents. These documents then form a series/list  \n \n \n \n D \n \n \n {\\displaystyle D} \n \n  where each individual documents are called  \n \n \n \n \n D \n \n n \n \n \n \n \n {\\displaystyle D_{n}} \n \n . These documents ( \n \n \n \n \n D \n \n n \n \n \n \n \n {\\displaystyle D_{n}} \n \n ) can contain words or small phrases (index terms  \n \n \n \n \n t \n \n n \n \n \n \n \n {\\displaystyle t_{n}} \n \n ) such as  \n \n \n \n \n D \n \n 1 \n \n \n \n \n {\\displaystyle D_{1}} \n \n   could  contain the terms  \n \n \n \n \n t \n \n 1 \n \n \n \n \n {\\displaystyle t_{1}} \n \n and  \n \n \n \n \n t \n \n 2 \n \n \n \n \n {\\displaystyle t_{2}} \n \n  from  \n \n \n \n T \n \n \n {\\displaystyle T} \n \n . There is an example of this in the following section. Index terms generally want to represent words which have more meaning to them and corresponds to what the content of an article or document could talk about. Terms like \"the\" and \"like\" would appear in nearly all documents whereas \"Bayesian\" would only be a small fraction of documents. Therefor, rarer terms like \"Bayesian\" are a better choice to be selected in the  \n \n \n \n T \n \n \n {\\displaystyle T} \n \n  sets. This relates to  Entropy (information theory) . There are multiple types of operations that can be applied to index terms used in queries to make them more generic and more relevant. One such is  Stemming . A  query  is a Boolean expression  \n \n \n \n Q \n \n \n {\\textstyle Q} \n \n  in normal form: \n \n \n \n Q \n = \n ( \n \n W \n \n 1 \n \n \n   \n ∨ \n   \n \n W \n \n 2 \n \n \n   \n ∨ \n   \n ⋯ \n ) \n ∧ \n   \n ⋯ \n   \n ∧ \n   \n ( \n \n W \n \n i \n \n \n   \n ∨ \n   \n \n W \n \n i \n + \n 1 \n \n \n   \n ∨ \n   \n ⋯ \n ) \n \n \n {\\displaystyle Q=(W_{1}\\ \\lor \\ W_{2}\\ \\lor \\ \\cdots )\\land \\ \\cdots \\ \\land \\ (W_{i}\\ \\lor \\ W_{i+1}\\ \\lor \\ \\cdots )} \n \n where  \n \n \n \n \n W \n \n i \n \n \n \n \n {\\textstyle W_{i}} \n \n  is true for  \n \n \n \n \n D \n \n j \n \n \n \n \n {\\displaystyle D_{j}} \n \n  when  \n \n \n \n \n t \n \n i \n \n \n ∈ \n \n D \n \n j \n \n \n \n \n {\\displaystyle t_{i}\\in D_{j}} \n \n . (Equivalently,  \n \n \n \n Q \n \n \n {\\textstyle Q} \n \n  could be expressed in   disjunctive normal form .) Any  \n \n \n \n Q \n \n \n {\\displaystyle Q} \n \n  queries are a selection of index terms ( \n \n \n \n \n t \n \n n \n \n \n \n \n {\\displaystyle t_{n}} \n \n  or  \n \n \n \n \n W \n \n n \n \n \n \n \n {\\displaystyle W_{n}} \n \n ) picked from a set  \n \n \n \n T \n \n \n {\\displaystyle T} \n \n  of terms which are combined using  Boolean operators  to form a set of conditions. These conditions are then applied to a set  \n \n \n \n D \n \n \n {\\displaystyle D} \n \n  of documents which contain the same index terms ( \n \n \n \n \n t \n \n n \n \n \n \n \n {\\displaystyle t_{n}} \n \n ) from the set  \n \n \n \n T \n \n \n {\\displaystyle T} \n \n . We seek to find the set of documents that satisfy  \n \n \n \n Q \n \n \n {\\textstyle Q} \n \n .  This operation is called  retrieval  and consists of the following two steps: Let the set of original (real) documents be, for example where D \n \n 1 \n \n \n \n \n {\\textstyle D_{1}} \n \n  = \"Bayes' principle: The principle that, in estimating a parameter, one should initially assume that each possible value has equal probability (a uniform prior  distribution).\" D \n \n 2 \n \n \n \n \n {\\textstyle D_{2}} \n \n  = \" Bayesian decision theory : A mathematical theory of decision-making which presumes utility and probability functions, and according to which the act to be chosen is the Bayes act, i.e. the one with highest subjective expected utility. If one had unlimited time and calculating power with which to make every decision, this procedure would be the best way to make any decision.\" D \n \n 3 \n \n \n \n \n {\\textstyle D_{3}} \n \n  = \"Bayesian  epistemology : A philosophical theory which holds that the epistemic status of a proposition (i.e. how well proven or well established it is) is best measured by a probability and that the proper way to revise this probability is given by Bayesian conditionalisation or similar procedures. A Bayesian epistemologist would use probability to define, and explore the relationship between, concepts such as epistemic status, support or explanatory power.\" Let the set  \n \n \n \n T \n \n \n {\\textstyle T} \n \n  of terms be: T \n = \n { \n \n t \n \n 1 \n \n \n = \n \n Bayes' principle \n \n , \n \n t \n \n 2 \n \n \n = \n \n probability \n \n , \n \n t \n \n 3 \n \n \n = \n \n decision-making \n \n , \n \n t \n \n 4 \n \n \n = \n \n Bayesian epistemology \n \n } \n \n \n {\\displaystyle T=\\{t_{1}={\\text{Bayes' principle}},t_{2}={\\text{probability}},t_{3}={\\text{decision-making}},t_{4}={\\text{Bayesian epistemology}}\\}} Then, the set  \n \n \n \n D \n \n \n {\\textstyle D} \n \n  of documents is as follows: where  \n \n \n \n \n \n \n \n \n D \n \n 1 \n \n \n \n \n \n = \n { \n \n probability \n \n , \n   \n \n Bayes' principle \n \n } \n \n \n \n \n \n D \n \n 2 \n \n \n \n \n \n = \n { \n \n probability \n \n , \n   \n \n decision-making \n \n } \n \n \n \n \n \n D \n \n 3 \n \n \n \n \n \n = \n { \n \n probability \n \n , \n   \n \n Bayesian epistemology \n \n } \n \n \n \n \n \n \n {\\displaystyle {\\begin{aligned}D_{1}&=\\{{\\text{probability}},\\ {\\text{Bayes' principle}}\\}\\\\D_{2}&=\\{{\\text{probability}},\\ {\\text{decision-making}}\\}\\\\D_{3}&=\\{{\\text{probability}},\\ {\\text{Bayesian epistemology}}\\}\\end{aligned}}} Let the query  \n \n \n \n Q \n \n \n {\\textstyle Q} \n \n  be (\"probability\" AND \"decision-making\"): Q \n = \n \n probability \n \n ∧ \n \n decision-making \n \n \n \n {\\displaystyle Q={\\text{probability}}\\land {\\text{decision-making}}} \n \n Then to retrieve the relevant documents: This means that the original document  \n \n \n \n \n D \n \n 2 \n \n \n \n \n {\\displaystyle D_{2}} \n \n  is the answer to  \n \n \n \n Q \n \n \n {\\textstyle Q} \n \n . If there is more than one document with the same representation (the same subset of index terms  \n \n \n \n \n t \n \n n \n \n \n \n \n {\\displaystyle t_{n}} \n \n ), every such document is retrieved. Such documents are indistinguishable in the BIR (in other words, equivalent). From a pure formal mathematical point of view, the BIR is straightforward. From a practical point of view, however, several further problems should be solved that relate to algorithms and data structures, such as, for example, the choice of terms (manual or automatic selection or both),  stemming ,  hash tables ,  inverted file  structure, and so on. [ 4 ] Another possibility is to use  hash sets . Each document is represented by a hash table which contains every single term of that document. Since hash table size increases and decreases in real time with the addition and removal of terms, each document will occupy much less space in memory. However, it will have a slowdown in performance because the operations are more complex than with  bit vectors . On the worst-case performance can degrade from O( n ) to O( n 2 ). On the average case, the performance slowdown will not be that much worse than bit vectors and the space usage is much more efficient. Each document can be summarized by  Bloom filter  representing the set of words in that document, stored in a fixed-length bitstring, called a signature.\nThe signature file contains one such  superimposed code  bitstring for every document in the collection.\nEach query can also be summarized by a  Bloom filter  representing the set of words in the query, stored in a bitstring of the same fixed length.\nThe query bitstring is tested against each signature. [ 5 ] [ 6 ] [ 7 ] The signature file approached is used in  BitFunnel . An inverted index file contains two parts:\na vocabulary containing all the terms used in the collection,\nand for each distinct term an inverted index that lists every document that mentions that term. [ 5 ] [ 6 ]"
  },
  {
    "id": 80,
    "title": "Extended Boolean model",
    "content": "The  Extended Boolean model  was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in  information retrieval . The Boolean model doesn't consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the  Vector Space Model  with the properties of  Boolean algebra  and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the  Standard Boolean model  it wasn't. [ 1 ] Thus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing.  Other research has shown that  relevance feedback  and  query expansion  can be integrated with extended Boolean query processing. In the  Extended Boolean model , a document is represented as a vector (similarly to in the vector model). Each  i   dimension  corresponds to a separate term associated with the document. The weight of term  K x  associated with document  d j  is measured by its normalized  Term frequency  and can be defined as: w \n \n x \n , \n j \n \n \n = \n \n f \n \n x \n , \n j \n \n \n ∗ \n \n \n \n I \n d \n \n f \n \n x \n \n \n \n \n m \n a \n \n x \n \n i \n \n \n I \n d \n \n f \n \n i \n \n \n \n \n \n \n \n {\\displaystyle w_{x,j}=f_{x,j}*{\\frac {Idf_{x}}{max_{i}Idf_{i}}}} where  Idf x  is  inverse document frequency  and  f x,j  the term frequency for term x in document j. The weight vector associated with document  d j  can be represented as: v \n \n \n \n d \n \n j \n \n \n \n \n = \n [ \n \n w \n \n 1 \n , \n j \n \n \n , \n \n w \n \n 2 \n , \n j \n \n \n , \n … \n , \n \n w \n \n i \n , \n j \n \n \n ] \n \n \n {\\displaystyle \\mathbf {v} _{d_{j}}=[w_{1,j},w_{2,j},\\ldots ,w_{i,j}]} Considering the space composed of two terms  K x  and  K y  only, the corresponding term weights are  w 1  and  w 2 . [ 2 ]   Thus, for query  q or  = ( K x  ∨  K y ) , we can calculate the similarity with the following formula: s \n i \n m \n ( \n \n q \n \n o \n r \n \n \n , \n d \n ) \n = \n \n \n \n \n \n w \n \n 1 \n \n \n 2 \n \n \n + \n \n w \n \n 2 \n \n \n 2 \n \n \n \n 2 \n \n \n \n \n \n {\\displaystyle sim(q_{or},d)={\\sqrt {\\frac {w_{1}^{2}+w_{2}^{2}}{2}}}} For query  q and  = ( K x  ∧  K y ) , we can use: s \n i \n m \n ( \n \n q \n \n a \n n \n d \n \n \n , \n d \n ) \n = \n 1 \n − \n \n \n \n \n ( \n 1 \n − \n \n w \n \n 1 \n \n \n \n ) \n \n 2 \n \n \n + \n ( \n 1 \n − \n \n w \n \n 2 \n \n \n \n ) \n \n 2 \n \n \n \n 2 \n \n \n \n \n \n {\\displaystyle sim(q_{and},d)=1-{\\sqrt {\\frac {(1-w_{1})^{2}+(1-w_{2})^{2}}{2}}}} We can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances. This can be done using  P-norms  which extends the notion of distance to include p-distances, where  1 ≤  p  ≤ ∞  is a new parameter. [ 3 ] : \n \n \n \n s \n i \n m \n ( \n \n q \n \n o \n r \n \n \n , \n \n d \n \n j \n \n \n ) \n = \n \n \n \n \n \n w \n \n 1 \n \n \n p \n \n \n + \n \n w \n \n 2 \n \n \n p \n \n \n + \n . \n . \n . \n . \n + \n \n w \n \n t \n \n \n p \n \n \n \n t \n \n \n p \n \n \n \n \n \n {\\displaystyle sim(q_{or},d_{j})={\\sqrt[{p}]{\\frac {w_{1}^{p}+w_{2}^{p}+....+w_{t}^{p}}{t}}}} Consider the query  q  = ( K 1  ∧  K 2 ) ∨  K 3 . The similarity between query  q  and document  d  can be computed using the formula: s \n i \n m \n ( \n q \n , \n d \n ) \n = \n \n \n \n \n ( \n 1 \n − \n \n \n \n ( \n \n \n \n ( \n 1 \n − \n \n w \n \n 1 \n \n \n \n ) \n \n p \n \n \n + \n ( \n 1 \n − \n \n w \n \n 2 \n \n \n \n ) \n \n p \n \n \n \n 2 \n \n \n \n \n p \n \n \n \n ) \n \n ) \n \n p \n \n \n + \n \n w \n \n 3 \n \n \n p \n \n \n \n 2 \n \n \n p \n \n \n \n \n \n {\\displaystyle sim(q,d)={\\sqrt[{p}]{\\frac {(1-{\\sqrt[{p}]{({\\frac {(1-w_{1})^{p}+(1-w_{2})^{p}}{2}}}}))^{p}+w_{3}^{p}}{2}}}} Lee and Fox [ 4 ]  compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC.\nUsing P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively. \nThe P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even  Fuzzy retrieval  techniques. The  Standard Boolean model  is still the most efficient."
  },
  {
    "id": 81,
    "title": "Fuzzy retrieval",
    "content": "Fuzzy retrieval  techniques are based on the  Extended Boolean model  and the  Fuzzy set  theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the  P-norms  algorithm. In fuzzy-set theory, an element has a varying degree of membership, say  d A , to a given set  A  instead of the traditional membership choice (is an element/is not an element). \nIn MMM [ 1 ]  each index term has a fuzzy set associated with it. A document's weight with respect to an index term  A  is considered to be the degree of membership of the document in the fuzzy set associated with  A . The degree of membership for union and intersection are defined as follows in Fuzzy set theory: According to this, documents that should be retrieved for a query of the form  A or B , should be in the fuzzy set associated with the union of the two sets  A  and  B . Similarly, the documents that should be retrieved for a query of the form  A and B , should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the  or  query to be  max(d A , d B )  and the similarity of the document to the  and  query to be  min(d A , d B ) . The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the  min  and  max  document weights. Given a document  D  with index-term weights  d A1 , d A2 , ..., d An  for terms  A 1 , A 2 , ..., A n , and the queries: Q or  = (A 1  or A 2  or ... or A n ) \n Q and  = (A 1  and A 2  and ... and A n ) the query-document similarity in the MMM model is computed as follows: SlM(Q or , D) = C or1  * max(d A1 , d A2 , ..., d An ) + C or2  * min(d A1 , d A2 , ..., d An ) \n SlM(Q and , D) = C and1  * min(d A1 , d A2 , ..., d An ) + C and2  * max(d A1 , d A2  ..., d An ) where  C or1 , C or2  are \"softness\" coefficients for the  or  operator, and  C and1 , C and2  are softness coefficients for the  and  operator. Since we would like to give the maximum of the document weights more importance while considering an  or  query and the minimum more importance while considering an  and  query, generally we have  C or1  > C or2  and C and1  > C and2 . For simplicity it is generally assumed that  C or1  = 1 - C or2  and  C and1  = 1 - C and2 . Lee and Fox [ 2 ]  experiments indicate that the best performance usually occurs with  C and1  in the range [0.5, 0.8] and with  C or1  > 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the  Standard Boolean model . The  Paice  model [ 3 ]  is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity: where  r  is a constant coefficient and  w di  is arranged in ascending order for  and  queries and descending order for  or  queries. When n = 2 the Paice model shows the same behavior as the MMM model. The experiments of Lee and Fox [ 2 ]  have shown that setting the  r  to 1.0 for  and  queries and 0.7 for  or  queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of  min  or  max  of a set of term weights each time an  and  or  or  clause is considered, which can be done in  O(n) . The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an  and  clause or an  or  clause is being considered. This requires at least an  0(n log n)  sorting algorithm. A good deal of floating point calculation is needed too. Lee and Fox [ 2 ]  compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement: These are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three. In 2005,  Kang  et al. [ 4 ]  have devised a fuzzy retrieval system indexed by concept identification. If we look at documents on a pure  Tf-idf  approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document. \nThey report improvements over Paice and P-norm on the average  precision and recall  for the Top-5 retrieved documents. Zadrozny [ 5 ]  revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by: The proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval."
  },
  {
    "id": 82,
    "title": "Vector space model",
    "content": "Vector space model  or  term vector model  is an algebraic model for representing text documents (or more generally, items) as  vectors  such that the distance between vectors represents the relevance between the documents. It is used in  information filtering ,  information retrieval ,  indexing  and relevancy rankings.  Its first use was in the  SMART Information Retrieval System . [ 1 ] In this section we consider a particular vector space model based on the  bag-of-words  representation. Documents and queries are represented as vectors. Each  dimension  corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is  tf-idf  weighting (see the example below). The definition of  term  depends on the application. Typically terms are single words,  keywords , or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the  corpus ). Vector operations can be used to compare documents with queries. [ 2 ] Candidate documents from the corpus can be retrieved and ranked using a variety of methods.  Relevance   rankings  of documents in a keyword search can be calculated, using the assumptions of  document similarities  theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as a vector with same dimension as the vectors that represent the other documents. In practice, it is easier to calculate the  cosine  of the angle between the vectors, instead of the angle itself: Where  \n \n \n \n \n \n d \n \n 2 \n \n \n \n ⋅ \n \n q \n \n \n \n {\\displaystyle \\mathbf {d_{2}} \\cdot \\mathbf {q} } \n \n  is the intersection (i.e. the  dot product ) of the document (d 2  in the figure to the right) and the query (q in the figure) vectors,  \n \n \n \n \n ‖ \n \n \n d \n \n 2 \n \n \n \n ‖ \n \n \n \n {\\displaystyle \\left\\|\\mathbf {d_{2}} \\right\\|} \n \n  is the norm of vector d 2 , and  \n \n \n \n \n ‖ \n \n q \n \n ‖ \n \n \n \n {\\displaystyle \\left\\|\\mathbf {q} \\right\\|} \n \n  is the norm of vector q. The  norm  of a vector is calculated as such: Using the cosine the similarity between document  d j  and query  q  can be calculated as: As all vectors under consideration by this model are element-wise nonnegative, a cosine value of zero means that the query and document vector are  orthogonal  and have no match (i.e. the query term does not exist in the document being considered). See  cosine similarity  for further information. [ 2 ] In the classic vector space model proposed by  Salton , Wong and Yang  [ 3 ]  the term-specific weights in the document vectors are products of local and global parameters. The model is known as  term frequency-inverse document frequency  model. The weight vector for document  d  is  \n \n \n \n \n \n v \n \n \n d \n \n \n = \n [ \n \n w \n \n 1 \n , \n d \n \n \n , \n \n w \n \n 2 \n , \n d \n \n \n , \n … \n , \n \n w \n \n N \n , \n d \n \n \n \n ] \n \n T \n \n \n \n \n {\\displaystyle \\mathbf {v} _{d}=[w_{1,d},w_{2,d},\\ldots ,w_{N,d}]^{T}} \n \n , where and The vector space model has the following advantages over the  Standard Boolean model : Most of these advantages are a consequence of the difference in the density of the document collection representation between Boolean and term frequency-inverse document frequency approaches. When using Boolean weights, any document lies in a vertex in a n-dimensional  hypercube . Therefore, the possible document representations are  \n \n \n \n \n 2 \n \n n \n \n \n \n \n {\\displaystyle 2^{n}} \n \n  and the maximum Euclidean distance between pairs is  \n \n \n \n \n \n n \n \n \n \n \n {\\displaystyle {\\sqrt {n}}} \n \n . As documents are added to the document collection, the region defined by the hypercube's vertices become more populated and hence denser. Unlike Boolean, when a document is added using term frequency-inverse document frequency weights, the inverse document frequencies of the terms in the new document decrease while that of the remaining terms increase. In average, as documents are added, the region where documents lie expands regulating the density of the entire collection representation. This behavior models the original motivation of Salton and his colleagues that a document collection represented in a low density region could yield better retrieval results. The vector space model has the following limitations: Many of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as  singular value decomposition  and  lexical databases  such as  WordNet . Models based on and extending the vector space model include: The following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them."
  },
  {
    "id": 83,
    "title": "Generalized vector space model",
    "content": "The  Generalized vector space model  is a generalization of the  vector space model  used in  information retrieval . Wong  et al. [ 1 ]  presented an analysis of the problems that the pairwise orthogonality assumption of the  vector space model  (VSM) creates. From here they extended the VSM to the generalized vector space model (GVSM). GVSM introduces term to term correlations, which deprecate the pairwise orthogonality assumption. More specifically, the factor considered a new space, where each term vector  t i  was expressed as a linear combination of  2 n  vectors  m r  where  r = 1...2 n . For a document  d k  and a query  q  the similarity function now becomes: where  t i  and  t j  are now vectors of a  2 n  dimensional space. Term correlation  \n \n \n \n \n t \n \n i \n \n \n ⋅ \n \n t \n \n j \n \n \n \n \n {\\displaystyle t_{i}\\cdot t_{j}} \n \n  can be implemented in several ways. For an example, Wong et al. uses the term occurrence frequency matrix obtained from automatic indexing as input to their algorithm. The term occurrence  and the output is the term correlation between any pair of index terms. There are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model: Recently Tsatsaronis [ 2 ]  focused on the first approach. They measure semantic relatedness ( SR ) using a thesaurus ( O ) like  WordNet . It considers the path length, captured by compactness ( SCM ), and the path depth, captured by semantic path elaboration ( SPE ).\nThey estimate the  \n \n \n \n \n t \n \n i \n \n \n ⋅ \n \n t \n \n j \n \n \n \n \n {\\displaystyle t_{i}\\cdot t_{j}} \n \n  inner product by: t \n \n i \n \n \n ⋅ \n \n t \n \n j \n \n \n = \n S \n R \n ( \n ( \n \n t \n \n i \n \n \n , \n \n t \n \n j \n \n \n ) \n , \n ( \n \n s \n \n i \n \n \n , \n \n s \n \n j \n \n \n ) \n , \n O \n ) \n \n \n {\\displaystyle t_{i}\\cdot t_{j}=SR((t_{i},t_{j}),(s_{i},s_{j}),O)} where  s i  and  s j  are senses of terms  t i  and  t j  respectively, maximizing  \n \n \n \n S \n C \n M \n ⋅ \n S \n P \n E \n \n \n {\\displaystyle SCM\\cdot SPE} \n \n . Building also on the first approach, Waitelonis et al. [ 3 ]  have computed semantic relatedness from  Linked Open Data  resources including  DBpedia  as well as the  YAGO taxonomy .\nThereby they exploits taxonomic relationships among semantic entities in documents and queries after  named entity linking ."
  },
  {
    "id": 84,
    "title": "Topic-based vector space model",
    "content": "The  Topic-based Vector Space Model (TVSM) [ 1 ]  (literature:  [1] ) extends the  vector space model  of  information retrieval  by removing the constraint that the term-vectors be orthogonal. The assumption of orthogonal terms is incorrect regarding natural languages which causes problems with synonyms and strong related terms. This facilitates the use of stopword lists, stemming and thesaurus in TVSM.\nIn contrast to the  generalized vector space model  the TVSM does not depend on concurrence-based similarities between terms. The basic premise of TVSM is the existence of a  d  dimensional space  R  with only positive axis intercepts, i.e.  R in R +  and  d in N + . Each dimension of  R  represents a fundamental topic. A term vector  t  has a specific weight for a certain  R . To calculate these weights assumptions are made taking into account the document contents. Ideally important terms will have a high weight and stopwords and irrelevants terms to the topic will have a low weight. The TVSM document model is obtained as a sum of term vectors representing terms in the document. The similarity between two documents  Di  and  Dj  is defined as the scalar product of document vectors. The enhancement of the Enhanced Topic-based Vector Space Model (eTVSM) [ 2 ]  (literature:  [2] ) is a proposal on how to derive term vectors from an   Ontology . Using a synonym Ontology created from  WordNet  Kuropka shows good results for document similarity. If a trivial Ontology is used the results are similar to Vector Space model."
  },
  {
    "id": 85,
    "title": "Latent semantic analysis",
    "content": "Latent semantic analysis  ( LSA ) is a technique in  natural language processing , in particular  distributional semantics , of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text (the  distributional hypothesis ).  A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called  singular value decomposition  (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Documents are then compared by  cosine similarity  between any two columns.  Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents. [ 1 ] An information retrieval technique using latent semantic structure was patented in 1988 [ 2 ]  by  Scott Deerwester ,  Susan Dumais ,  George Furnas ,  Richard Harshman ,  Thomas Landauer ,  Karen Lochbaum  and  Lynn Streeter . In the context of its application to  information retrieval , it is sometimes called  latent semantic indexing  ( LSI ). [ 3 ] LSA can use a  document-term matrix  which describes the occurrences of terms in documents; it is a  sparse matrix  whose rows correspond to  terms  and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is  tf-idf  (term frequency–inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance. This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used. After the construction of the occurrence matrix, LSA finds a  low-rank approximation [ 5 ]  to the  term-document matrix . There could be various reasons for these approximations: The consequence of the rank lowering is that some dimensions are combined and depend on more than one term: This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also partially mitigates the problem with  polysemy , since components of polysemous words that point in the \"right\" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense. Let  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n  be a matrix where element  \n \n \n \n ( \n i \n , \n j \n ) \n \n \n {\\displaystyle (i,j)} \n \n  describes the occurrence of term  \n \n \n \n i \n \n \n {\\displaystyle i} \n \n  in document  \n \n \n \n j \n \n \n {\\displaystyle j} \n \n  (this can be, for example, the frequency).  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n  will look like this: Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document: Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term: Now the  dot product   \n \n \n \n \n \n \n t \n \n \n \n i \n \n \n T \n \n \n \n \n \n t \n \n \n \n p \n \n \n \n \n {\\displaystyle {\\textbf {t}}_{i}^{T}{\\textbf {t}}_{p}} \n \n  between two term vectors gives the  correlation  between the terms over the set of documents. The  matrix product   \n \n \n \n X \n \n X \n \n T \n \n \n \n \n {\\displaystyle XX^{T}} \n \n  contains all these dot products. Element  \n \n \n \n ( \n i \n , \n p \n ) \n \n \n {\\displaystyle (i,p)} \n \n  (which is equal to element  \n \n \n \n ( \n p \n , \n i \n ) \n \n \n {\\displaystyle (p,i)} \n \n ) contains the dot product  \n \n \n \n \n \n \n t \n \n \n \n i \n \n \n T \n \n \n \n \n \n t \n \n \n \n p \n \n \n \n \n {\\displaystyle {\\textbf {t}}_{i}^{T}{\\textbf {t}}_{p}} \n \n  ( \n \n \n \n = \n \n \n \n t \n \n \n \n p \n \n \n T \n \n \n \n \n \n t \n \n \n \n i \n \n \n \n \n {\\displaystyle ={\\textbf {t}}_{p}^{T}{\\textbf {t}}_{i}} \n \n ). Likewise, the matrix  \n \n \n \n \n X \n \n T \n \n \n X \n \n \n {\\displaystyle X^{T}X} \n \n  contains the dot products between all the document vectors, giving their correlation over the terms:  \n \n \n \n \n \n \n d \n \n \n \n j \n \n \n T \n \n \n \n \n \n d \n \n \n \n q \n \n \n = \n \n \n \n d \n \n \n \n q \n \n \n T \n \n \n \n \n \n d \n \n \n \n j \n \n \n \n \n {\\displaystyle {\\textbf {d}}_{j}^{T}{\\textbf {d}}_{q}={\\textbf {d}}_{q}^{T}{\\textbf {d}}_{j}} \n \n . Now, from the theory of linear algebra, there exists a decomposition of  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n  such that  \n \n \n \n U \n \n \n {\\displaystyle U} \n \n  and  \n \n \n \n V \n \n \n {\\displaystyle V} \n \n  are  orthogonal matrices  and  \n \n \n \n Σ \n \n \n {\\displaystyle \\Sigma } \n \n  is a  diagonal matrix . This is called a  singular value decomposition  (SVD): The matrix products giving us the term and document correlations then become Since  \n \n \n \n Σ \n \n Σ \n \n T \n \n \n \n \n {\\displaystyle \\Sigma \\Sigma ^{T}} \n \n  and  \n \n \n \n \n Σ \n \n T \n \n \n Σ \n \n \n {\\displaystyle \\Sigma ^{T}\\Sigma } \n \n  are diagonal we see that  \n \n \n \n U \n \n \n {\\displaystyle U} \n \n  must contain the  eigenvectors  of  \n \n \n \n X \n \n X \n \n T \n \n \n \n \n {\\displaystyle XX^{T}} \n \n , while  \n \n \n \n V \n \n \n {\\displaystyle V} \n \n  must be the eigenvectors of  \n \n \n \n \n X \n \n T \n \n \n X \n \n \n {\\displaystyle X^{T}X} \n \n . Both products have the same non-zero eigenvalues, given by the non-zero entries of  \n \n \n \n Σ \n \n Σ \n \n T \n \n \n \n \n {\\displaystyle \\Sigma \\Sigma ^{T}} \n \n , or equally, by the non-zero entries of  \n \n \n \n \n Σ \n \n T \n \n \n Σ \n \n \n {\\displaystyle \\Sigma ^{T}\\Sigma } \n \n . Now the decomposition looks like this: The values  \n \n \n \n \n σ \n \n 1 \n \n \n , \n … \n , \n \n σ \n \n l \n \n \n \n \n {\\displaystyle \\sigma _{1},\\dots ,\\sigma _{l}} \n \n  are called the singular values, and  \n \n \n \n \n u \n \n 1 \n \n \n , \n … \n , \n \n u \n \n l \n \n \n \n \n {\\displaystyle u_{1},\\dots ,u_{l}} \n \n  and  \n \n \n \n \n v \n \n 1 \n \n \n , \n … \n , \n \n v \n \n l \n \n \n \n \n {\\displaystyle v_{1},\\dots ,v_{l}} \n \n  the left and right singular vectors.\nNotice the only part of  \n \n \n \n U \n \n \n {\\displaystyle U} \n \n  that contributes to  \n \n \n \n \n \n \n t \n \n \n \n i \n \n \n \n \n {\\displaystyle {\\textbf {t}}_{i}} \n \n  is the  \n \n \n \n i \n \n \n 'th \n \n \n \n \n {\\displaystyle i{\\textrm {'th}}} \n \n  row.\nLet this row vector be called  \n \n \n \n \n \n \n \n \n t \n \n ^ \n \n \n \n \n i \n \n \n T \n \n \n \n \n {\\displaystyle {\\hat {\\textrm {t}}}_{i}^{T}} \n \n .\nLikewise, the only part of  \n \n \n \n \n V \n \n T \n \n \n \n \n {\\displaystyle V^{T}} \n \n  that contributes to  \n \n \n \n \n \n \n d \n \n \n \n j \n \n \n \n \n {\\displaystyle {\\textbf {d}}_{j}} \n \n  is the  \n \n \n \n j \n \n \n 'th \n \n \n \n \n {\\displaystyle j{\\textrm {'th}}} \n \n  column,  \n \n \n \n \n \n \n \n \n d \n \n ^ \n \n \n \n \n j \n \n \n \n \n {\\displaystyle {\\hat {\\textrm {d}}}_{j}} \n \n .\nThese are  not  the eigenvectors, but  depend  on  all  the eigenvectors. It turns out that when you select the  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  largest singular values, and their corresponding singular vectors from  \n \n \n \n U \n \n \n {\\displaystyle U} \n \n  and  \n \n \n \n V \n \n \n {\\displaystyle V} \n \n , you get the rank  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  approximation to  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n  with the smallest error ( Frobenius norm ). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a \"semantic space\". The row \"term\" vector  \n \n \n \n \n \n \n \n \n t \n \n ^ \n \n \n \n \n i \n \n \n T \n \n \n \n \n {\\displaystyle {\\hat {\\textbf {t}}}_{i}^{T}} \n \n  then has  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  entries mapping it to a lower-dimensional space. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the \"document\" vector  \n \n \n \n \n \n \n \n \n d \n \n ^ \n \n \n \n \n j \n \n \n \n \n {\\displaystyle {\\hat {\\textbf {d}}}_{j}} \n \n  is an approximation in this lower-dimensional space. We write this approximation as You can now do the following: To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents: Note here that the inverse of the diagonal matrix  \n \n \n \n \n Σ \n \n k \n \n \n \n \n {\\displaystyle \\Sigma _{k}} \n \n  may be found by inverting each nonzero value within the matrix. This means that if you have a query vector  \n \n \n \n q \n \n \n {\\displaystyle q} \n \n , you must do the translation  \n \n \n \n \n \n \n \n q \n \n ^ \n \n \n \n = \n \n Σ \n \n k \n \n \n − \n 1 \n \n \n \n U \n \n k \n \n \n T \n \n \n \n \n q \n \n \n \n \n {\\displaystyle {\\hat {\\textbf {q}}}=\\Sigma _{k}^{-1}U_{k}^{T}{\\textbf {q}}} \n \n  before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors: The new low-dimensional space typically can be used to: Synonymy and polysemy are fundamental problems in  natural language processing : LSA has been used to assist in performing  prior art  searches for  patents . [ 9 ] The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of  free recall  and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the  Semantic Proximity Effect . [ 10 ] When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall. [ 11 ] Another model, termed  Word Association Spaces  (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs. [ 12 ] The  SVD  is typically computed using large matrix methods (for example,  Lanczos methods ) but may also be computed incrementally and with greatly reduced resources via a  neural network -like approach, which does not require the large, full-rank matrix to be held in memory. [ 13 ] \nA fast, incremental, low-memory, large-matrix SVD algorithm has been developed. [ 14 ]  MATLAB [ 15 ]  and Python [ 16 ]  implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.\nIn recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality. [ 17 ] Some of LSA's drawbacks include: In semantic hashing  [ 21 ]  documents are mapped to memory addresses by means of a  neural network  in such a way that semantically similar documents are located at nearby addresses.  Deep neural network  essentially builds a  graphical model  of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than  locality sensitive hashing , which is the fastest current method.  [ clarification needed ] Latent semantic indexing  ( LSI ) is an indexing and retrieval method that uses a mathematical technique called  singular value decomposition  (SVD) to identify patterns in the relationships between the  terms  and  concepts  contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a  body of text  by establishing associations between those terms that occur in similar  contexts . [ 22 ] LSI is also an application of  correspondence analysis , a multivariate statistical technique developed by  Jean-Paul Benzécri [ 23 ]  in the early 1970s, to a  contingency table  built from word counts in documents. Called \" latent semantic  indexing\" because of its ability to correlate  semantically  related terms that are  latent  in a collection of text, it was first applied to text at  Bellcore  in the late 1980s.   The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don’t share a specific word or words with the search criteria. LSI helps overcome synonymy by increasing  recall , one of the most problematic constraints of  Boolean keyword queries  and vector space models. [ 18 ]   Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of  information retrieval  systems. [ 24 ]   As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant. LSI is also used to perform automated  document categorization .  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text. [ 25 ]     Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories. [ 26 ]    LSI uses  example  documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents. Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text. Because it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic  concept searching  and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages. [ citation needed ] LSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space. For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations. [ 27 ] LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.). [ 28 ]    This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data. Text does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text. LSI has proven to be a useful solution to a number of conceptual matching problems. [ 29 ] [ 30 ]   The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information. [ 31 ] LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a  Singular Value Decomposition  on the matrix, and using the matrix to identify the concepts contained in the text. LSI begins by constructing a term-document matrix,  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n , to identify the occurrences of the  \n \n \n \n m \n \n \n {\\displaystyle m} \n \n  unique terms within a collection of  \n \n \n \n n \n \n \n {\\displaystyle n} \n \n  documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell,  \n \n \n \n \n a \n \n i \n j \n \n \n \n \n {\\displaystyle a_{ij}} \n \n , initially representing the number of times the associated term appears in the indicated document,  \n \n \n \n \n t \n \n f \n \n i \n j \n \n \n \n \n \n {\\displaystyle \\mathrm {tf_{ij}} } \n \n .  This matrix is usually very large and very sparse. Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell,  \n \n \n \n \n a \n \n i \n j \n \n \n \n \n {\\displaystyle a_{ij}} \n \n  of  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n , to be the product of a local term weight,  \n \n \n \n \n l \n \n i \n j \n \n \n \n \n {\\displaystyle l_{ij}} \n \n , which describes the relative frequency of a term in a document, and a global weight,  \n \n \n \n \n g \n \n i \n \n \n \n \n {\\displaystyle g_{i}} \n \n , which describes the relative frequency of the term within the entire collection of documents. Some common local weighting functions [ 33 ]  are defined in the following table. Some common global weighting functions are defined in the following table. Empirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets. [ 34 ]   In other words, each entry  \n \n \n \n \n a \n \n i \n j \n \n \n \n \n {\\displaystyle a_{ij}} \n \n  of  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  is computed as: A rank-reduced,  singular value decomposition  is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI. [ 35 ]    It computes the term and document vector spaces by approximating the single term-frequency matrix,  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n , into three other matrices— an  m  by  r   term-concept vector matrix  \n \n \n \n T \n \n \n {\\displaystyle T} \n \n , an  r  by  r  singular values matrix  \n \n \n \n S \n \n \n {\\displaystyle S} \n \n , and a  n  by  r  concept-document vector matrix,  \n \n \n \n D \n \n \n {\\displaystyle D} \n \n , which satisfy the following relations: A \n ≈ \n T \n S \n \n D \n \n T \n \n \n \n \n {\\displaystyle A\\approx TSD^{T}} T \n \n T \n \n \n T \n = \n \n I \n \n r \n \n \n \n \n D \n \n T \n \n \n D \n = \n \n I \n \n r \n \n \n \n \n {\\displaystyle T^{T}T=I_{r}\\quad D^{T}D=I_{r}} S \n \n 1 \n , \n 1 \n \n \n ≥ \n \n S \n \n 2 \n , \n 2 \n \n \n ≥ \n … \n ≥ \n \n S \n \n r \n , \n r \n \n \n > \n 0 \n \n \n S \n \n i \n , \n j \n \n \n = \n 0 \n \n \n where \n \n \n i \n ≠ \n j \n \n \n {\\displaystyle S_{1,1}\\geq S_{2,2}\\geq \\ldots \\geq S_{r,r}>0\\quad S_{i,j}=0\\;{\\text{where}}\\;i\\neq j} In the formula,  A  is the supplied  m  by  n  weighted matrix of term frequencies in a collection of text where  m  is the number of unique terms, and  n  is the number of documents.   T  is a computed  m  by  r  matrix of term vectors where  r  is the rank of  A —a measure of its unique dimensions  ≤ min( m,n ) .   S  is a computed  r  by  r  diagonal matrix of decreasing singular values, and  D  is a computed  n  by  r  matrix of document vectors. The SVD is then  truncated  to reduce the rank by keeping only the largest  k  «  r  diagonal entries in the singular value matrix  S ,\nwhere  k  is typically on the order 100 to 300 dimensions.\nThis effectively reduces the term and document vector matrix sizes to  m  by  k  and  n  by  k  respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of  A .  This reduced set of matrices is often denoted with a modified formula such as: Efficient LSI algorithms only compute the first  k  singular values and term and document vectors as opposed to computing a full SVD and then truncating it. Note that this rank reduction is essentially the same as doing  Principal Component Analysis  (PCA) on the matrix  A , except that PCA subtracts off the means.  PCA loses the sparseness of the  A  matrix, which can make it infeasible for large lexicons. The computed  T k  and  D k  matrices define the term and document vector spaces, which with the computed singular values,  S k , embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors. The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the  A = T S D T  equation into the equivalent  D = A T  T S −1  equation, a new vector,  d , for a query or for a new document can be created by computing a new column in  A  and then multiplying the new column by  T S −1 .  The new column in  A  is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document. A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors. The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called  folding in .  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in  [ 14 ] ) is needed. It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome. LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization. [ 36 ]    Below are some other ways in which LSI is being used: LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003. [ 51 ] Early challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques. [ 52 ]   However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source  gensim  software package. [ 53 ] Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents). [ 54 ]    However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection. [ 55 ]  Checking the proportion of variance retained, similar to  PCA  or  factor analysis , to determine the optimal dimensionality is not suitable for LSI. Using a synonym test or prediction of missing words are two possible methods to find the correct dimensionality. [ 56 ]  When LSI topics are used as features in supervised learning methods, one can use prediction error measurements to find the ideal dimensionality. Due to its cross-domain applications in  Information Retrieval ,  Natural Language Processing  (NLP),  Cognitive Science  and  Computational Linguistics , LSA has been implemented to support many different kinds of applications."
  },
  {
    "id": 86,
    "title": "Latent semantic analysis",
    "content": "Latent semantic analysis  ( LSA ) is a technique in  natural language processing , in particular  distributional semantics , of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text (the  distributional hypothesis ).  A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called  singular value decomposition  (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Documents are then compared by  cosine similarity  between any two columns.  Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents. [ 1 ] An information retrieval technique using latent semantic structure was patented in 1988 [ 2 ]  by  Scott Deerwester ,  Susan Dumais ,  George Furnas ,  Richard Harshman ,  Thomas Landauer ,  Karen Lochbaum  and  Lynn Streeter . In the context of its application to  information retrieval , it is sometimes called  latent semantic indexing  ( LSI ). [ 3 ] LSA can use a  document-term matrix  which describes the occurrences of terms in documents; it is a  sparse matrix  whose rows correspond to  terms  and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is  tf-idf  (term frequency–inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance. This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used. After the construction of the occurrence matrix, LSA finds a  low-rank approximation [ 5 ]  to the  term-document matrix . There could be various reasons for these approximations: The consequence of the rank lowering is that some dimensions are combined and depend on more than one term: This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also partially mitigates the problem with  polysemy , since components of polysemous words that point in the \"right\" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense. Let  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n  be a matrix where element  \n \n \n \n ( \n i \n , \n j \n ) \n \n \n {\\displaystyle (i,j)} \n \n  describes the occurrence of term  \n \n \n \n i \n \n \n {\\displaystyle i} \n \n  in document  \n \n \n \n j \n \n \n {\\displaystyle j} \n \n  (this can be, for example, the frequency).  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n  will look like this: Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document: Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term: Now the  dot product   \n \n \n \n \n \n \n t \n \n \n \n i \n \n \n T \n \n \n \n \n \n t \n \n \n \n p \n \n \n \n \n {\\displaystyle {\\textbf {t}}_{i}^{T}{\\textbf {t}}_{p}} \n \n  between two term vectors gives the  correlation  between the terms over the set of documents. The  matrix product   \n \n \n \n X \n \n X \n \n T \n \n \n \n \n {\\displaystyle XX^{T}} \n \n  contains all these dot products. Element  \n \n \n \n ( \n i \n , \n p \n ) \n \n \n {\\displaystyle (i,p)} \n \n  (which is equal to element  \n \n \n \n ( \n p \n , \n i \n ) \n \n \n {\\displaystyle (p,i)} \n \n ) contains the dot product  \n \n \n \n \n \n \n t \n \n \n \n i \n \n \n T \n \n \n \n \n \n t \n \n \n \n p \n \n \n \n \n {\\displaystyle {\\textbf {t}}_{i}^{T}{\\textbf {t}}_{p}} \n \n  ( \n \n \n \n = \n \n \n \n t \n \n \n \n p \n \n \n T \n \n \n \n \n \n t \n \n \n \n i \n \n \n \n \n {\\displaystyle ={\\textbf {t}}_{p}^{T}{\\textbf {t}}_{i}} \n \n ). Likewise, the matrix  \n \n \n \n \n X \n \n T \n \n \n X \n \n \n {\\displaystyle X^{T}X} \n \n  contains the dot products between all the document vectors, giving their correlation over the terms:  \n \n \n \n \n \n \n d \n \n \n \n j \n \n \n T \n \n \n \n \n \n d \n \n \n \n q \n \n \n = \n \n \n \n d \n \n \n \n q \n \n \n T \n \n \n \n \n \n d \n \n \n \n j \n \n \n \n \n {\\displaystyle {\\textbf {d}}_{j}^{T}{\\textbf {d}}_{q}={\\textbf {d}}_{q}^{T}{\\textbf {d}}_{j}} \n \n . Now, from the theory of linear algebra, there exists a decomposition of  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n  such that  \n \n \n \n U \n \n \n {\\displaystyle U} \n \n  and  \n \n \n \n V \n \n \n {\\displaystyle V} \n \n  are  orthogonal matrices  and  \n \n \n \n Σ \n \n \n {\\displaystyle \\Sigma } \n \n  is a  diagonal matrix . This is called a  singular value decomposition  (SVD): The matrix products giving us the term and document correlations then become Since  \n \n \n \n Σ \n \n Σ \n \n T \n \n \n \n \n {\\displaystyle \\Sigma \\Sigma ^{T}} \n \n  and  \n \n \n \n \n Σ \n \n T \n \n \n Σ \n \n \n {\\displaystyle \\Sigma ^{T}\\Sigma } \n \n  are diagonal we see that  \n \n \n \n U \n \n \n {\\displaystyle U} \n \n  must contain the  eigenvectors  of  \n \n \n \n X \n \n X \n \n T \n \n \n \n \n {\\displaystyle XX^{T}} \n \n , while  \n \n \n \n V \n \n \n {\\displaystyle V} \n \n  must be the eigenvectors of  \n \n \n \n \n X \n \n T \n \n \n X \n \n \n {\\displaystyle X^{T}X} \n \n . Both products have the same non-zero eigenvalues, given by the non-zero entries of  \n \n \n \n Σ \n \n Σ \n \n T \n \n \n \n \n {\\displaystyle \\Sigma \\Sigma ^{T}} \n \n , or equally, by the non-zero entries of  \n \n \n \n \n Σ \n \n T \n \n \n Σ \n \n \n {\\displaystyle \\Sigma ^{T}\\Sigma } \n \n . Now the decomposition looks like this: The values  \n \n \n \n \n σ \n \n 1 \n \n \n , \n … \n , \n \n σ \n \n l \n \n \n \n \n {\\displaystyle \\sigma _{1},\\dots ,\\sigma _{l}} \n \n  are called the singular values, and  \n \n \n \n \n u \n \n 1 \n \n \n , \n … \n , \n \n u \n \n l \n \n \n \n \n {\\displaystyle u_{1},\\dots ,u_{l}} \n \n  and  \n \n \n \n \n v \n \n 1 \n \n \n , \n … \n , \n \n v \n \n l \n \n \n \n \n {\\displaystyle v_{1},\\dots ,v_{l}} \n \n  the left and right singular vectors.\nNotice the only part of  \n \n \n \n U \n \n \n {\\displaystyle U} \n \n  that contributes to  \n \n \n \n \n \n \n t \n \n \n \n i \n \n \n \n \n {\\displaystyle {\\textbf {t}}_{i}} \n \n  is the  \n \n \n \n i \n \n \n 'th \n \n \n \n \n {\\displaystyle i{\\textrm {'th}}} \n \n  row.\nLet this row vector be called  \n \n \n \n \n \n \n \n \n t \n \n ^ \n \n \n \n \n i \n \n \n T \n \n \n \n \n {\\displaystyle {\\hat {\\textrm {t}}}_{i}^{T}} \n \n .\nLikewise, the only part of  \n \n \n \n \n V \n \n T \n \n \n \n \n {\\displaystyle V^{T}} \n \n  that contributes to  \n \n \n \n \n \n \n d \n \n \n \n j \n \n \n \n \n {\\displaystyle {\\textbf {d}}_{j}} \n \n  is the  \n \n \n \n j \n \n \n 'th \n \n \n \n \n {\\displaystyle j{\\textrm {'th}}} \n \n  column,  \n \n \n \n \n \n \n \n \n d \n \n ^ \n \n \n \n \n j \n \n \n \n \n {\\displaystyle {\\hat {\\textrm {d}}}_{j}} \n \n .\nThese are  not  the eigenvectors, but  depend  on  all  the eigenvectors. It turns out that when you select the  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  largest singular values, and their corresponding singular vectors from  \n \n \n \n U \n \n \n {\\displaystyle U} \n \n  and  \n \n \n \n V \n \n \n {\\displaystyle V} \n \n , you get the rank  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  approximation to  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n  with the smallest error ( Frobenius norm ). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a \"semantic space\". The row \"term\" vector  \n \n \n \n \n \n \n \n \n t \n \n ^ \n \n \n \n \n i \n \n \n T \n \n \n \n \n {\\displaystyle {\\hat {\\textbf {t}}}_{i}^{T}} \n \n  then has  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  entries mapping it to a lower-dimensional space. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the \"document\" vector  \n \n \n \n \n \n \n \n \n d \n \n ^ \n \n \n \n \n j \n \n \n \n \n {\\displaystyle {\\hat {\\textbf {d}}}_{j}} \n \n  is an approximation in this lower-dimensional space. We write this approximation as You can now do the following: To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents: Note here that the inverse of the diagonal matrix  \n \n \n \n \n Σ \n \n k \n \n \n \n \n {\\displaystyle \\Sigma _{k}} \n \n  may be found by inverting each nonzero value within the matrix. This means that if you have a query vector  \n \n \n \n q \n \n \n {\\displaystyle q} \n \n , you must do the translation  \n \n \n \n \n \n \n \n q \n \n ^ \n \n \n \n = \n \n Σ \n \n k \n \n \n − \n 1 \n \n \n \n U \n \n k \n \n \n T \n \n \n \n \n q \n \n \n \n \n {\\displaystyle {\\hat {\\textbf {q}}}=\\Sigma _{k}^{-1}U_{k}^{T}{\\textbf {q}}} \n \n  before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors: The new low-dimensional space typically can be used to: Synonymy and polysemy are fundamental problems in  natural language processing : LSA has been used to assist in performing  prior art  searches for  patents . [ 9 ] The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of  free recall  and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the  Semantic Proximity Effect . [ 10 ] When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall. [ 11 ] Another model, termed  Word Association Spaces  (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs. [ 12 ] The  SVD  is typically computed using large matrix methods (for example,  Lanczos methods ) but may also be computed incrementally and with greatly reduced resources via a  neural network -like approach, which does not require the large, full-rank matrix to be held in memory. [ 13 ] \nA fast, incremental, low-memory, large-matrix SVD algorithm has been developed. [ 14 ]  MATLAB [ 15 ]  and Python [ 16 ]  implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.\nIn recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality. [ 17 ] Some of LSA's drawbacks include: In semantic hashing  [ 21 ]  documents are mapped to memory addresses by means of a  neural network  in such a way that semantically similar documents are located at nearby addresses.  Deep neural network  essentially builds a  graphical model  of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than  locality sensitive hashing , which is the fastest current method.  [ clarification needed ] Latent semantic indexing  ( LSI ) is an indexing and retrieval method that uses a mathematical technique called  singular value decomposition  (SVD) to identify patterns in the relationships between the  terms  and  concepts  contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a  body of text  by establishing associations between those terms that occur in similar  contexts . [ 22 ] LSI is also an application of  correspondence analysis , a multivariate statistical technique developed by  Jean-Paul Benzécri [ 23 ]  in the early 1970s, to a  contingency table  built from word counts in documents. Called \" latent semantic  indexing\" because of its ability to correlate  semantically  related terms that are  latent  in a collection of text, it was first applied to text at  Bellcore  in the late 1980s.   The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don’t share a specific word or words with the search criteria. LSI helps overcome synonymy by increasing  recall , one of the most problematic constraints of  Boolean keyword queries  and vector space models. [ 18 ]   Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of  information retrieval  systems. [ 24 ]   As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant. LSI is also used to perform automated  document categorization .  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text. [ 25 ]     Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories. [ 26 ]    LSI uses  example  documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents. Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text. Because it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic  concept searching  and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages. [ citation needed ] LSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space. For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations. [ 27 ] LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.). [ 28 ]    This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data. Text does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text. LSI has proven to be a useful solution to a number of conceptual matching problems. [ 29 ] [ 30 ]   The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information. [ 31 ] LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a  Singular Value Decomposition  on the matrix, and using the matrix to identify the concepts contained in the text. LSI begins by constructing a term-document matrix,  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n , to identify the occurrences of the  \n \n \n \n m \n \n \n {\\displaystyle m} \n \n  unique terms within a collection of  \n \n \n \n n \n \n \n {\\displaystyle n} \n \n  documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell,  \n \n \n \n \n a \n \n i \n j \n \n \n \n \n {\\displaystyle a_{ij}} \n \n , initially representing the number of times the associated term appears in the indicated document,  \n \n \n \n \n t \n \n f \n \n i \n j \n \n \n \n \n \n {\\displaystyle \\mathrm {tf_{ij}} } \n \n .  This matrix is usually very large and very sparse. Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell,  \n \n \n \n \n a \n \n i \n j \n \n \n \n \n {\\displaystyle a_{ij}} \n \n  of  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n , to be the product of a local term weight,  \n \n \n \n \n l \n \n i \n j \n \n \n \n \n {\\displaystyle l_{ij}} \n \n , which describes the relative frequency of a term in a document, and a global weight,  \n \n \n \n \n g \n \n i \n \n \n \n \n {\\displaystyle g_{i}} \n \n , which describes the relative frequency of the term within the entire collection of documents. Some common local weighting functions [ 33 ]  are defined in the following table. Some common global weighting functions are defined in the following table. Empirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets. [ 34 ]   In other words, each entry  \n \n \n \n \n a \n \n i \n j \n \n \n \n \n {\\displaystyle a_{ij}} \n \n  of  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  is computed as: A rank-reduced,  singular value decomposition  is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI. [ 35 ]    It computes the term and document vector spaces by approximating the single term-frequency matrix,  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n , into three other matrices— an  m  by  r   term-concept vector matrix  \n \n \n \n T \n \n \n {\\displaystyle T} \n \n , an  r  by  r  singular values matrix  \n \n \n \n S \n \n \n {\\displaystyle S} \n \n , and a  n  by  r  concept-document vector matrix,  \n \n \n \n D \n \n \n {\\displaystyle D} \n \n , which satisfy the following relations: A \n ≈ \n T \n S \n \n D \n \n T \n \n \n \n \n {\\displaystyle A\\approx TSD^{T}} T \n \n T \n \n \n T \n = \n \n I \n \n r \n \n \n \n \n D \n \n T \n \n \n D \n = \n \n I \n \n r \n \n \n \n \n {\\displaystyle T^{T}T=I_{r}\\quad D^{T}D=I_{r}} S \n \n 1 \n , \n 1 \n \n \n ≥ \n \n S \n \n 2 \n , \n 2 \n \n \n ≥ \n … \n ≥ \n \n S \n \n r \n , \n r \n \n \n > \n 0 \n \n \n S \n \n i \n , \n j \n \n \n = \n 0 \n \n \n where \n \n \n i \n ≠ \n j \n \n \n {\\displaystyle S_{1,1}\\geq S_{2,2}\\geq \\ldots \\geq S_{r,r}>0\\quad S_{i,j}=0\\;{\\text{where}}\\;i\\neq j} In the formula,  A  is the supplied  m  by  n  weighted matrix of term frequencies in a collection of text where  m  is the number of unique terms, and  n  is the number of documents.   T  is a computed  m  by  r  matrix of term vectors where  r  is the rank of  A —a measure of its unique dimensions  ≤ min( m,n ) .   S  is a computed  r  by  r  diagonal matrix of decreasing singular values, and  D  is a computed  n  by  r  matrix of document vectors. The SVD is then  truncated  to reduce the rank by keeping only the largest  k  «  r  diagonal entries in the singular value matrix  S ,\nwhere  k  is typically on the order 100 to 300 dimensions.\nThis effectively reduces the term and document vector matrix sizes to  m  by  k  and  n  by  k  respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of  A .  This reduced set of matrices is often denoted with a modified formula such as: Efficient LSI algorithms only compute the first  k  singular values and term and document vectors as opposed to computing a full SVD and then truncating it. Note that this rank reduction is essentially the same as doing  Principal Component Analysis  (PCA) on the matrix  A , except that PCA subtracts off the means.  PCA loses the sparseness of the  A  matrix, which can make it infeasible for large lexicons. The computed  T k  and  D k  matrices define the term and document vector spaces, which with the computed singular values,  S k , embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors. The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the  A = T S D T  equation into the equivalent  D = A T  T S −1  equation, a new vector,  d , for a query or for a new document can be created by computing a new column in  A  and then multiplying the new column by  T S −1 .  The new column in  A  is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document. A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors. The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called  folding in .  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in  [ 14 ] ) is needed. It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome. LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization. [ 36 ]    Below are some other ways in which LSI is being used: LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003. [ 51 ] Early challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques. [ 52 ]   However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source  gensim  software package. [ 53 ] Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents). [ 54 ]    However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection. [ 55 ]  Checking the proportion of variance retained, similar to  PCA  or  factor analysis , to determine the optimal dimensionality is not suitable for LSI. Using a synonym test or prediction of missing words are two possible methods to find the correct dimensionality. [ 56 ]  When LSI topics are used as features in supervised learning methods, one can use prediction error measurements to find the ideal dimensionality. Due to its cross-domain applications in  Information Retrieval ,  Natural Language Processing  (NLP),  Cognitive Science  and  Computational Linguistics , LSA has been implemented to support many different kinds of applications."
  },
  {
    "id": 87,
    "title": "Bayes' theorem",
    "content": "Bayes' theorem  (alternatively  Bayes' law  or  Bayes' rule , after  Thomas Bayes ) gives a mathematical rule for inverting  conditional probabilities , allowing us to find the probability of a cause given its effect. [ 1 ]  For example, if the risk of developing health problems is known to increase with age, Bayes' theorem allows the risk to an individual of a known age to be assessed more accurately by conditioning it relative to their age, rather than assuming that the individual is typical of the population as a whole. Based on Bayes law both the prevalence of a disease in a given population and the error rate of an infectious disease test have to be taken into account to evaluate the meaning of a positive test result correctly and avoid the  base-rate fallacy . One of the many applications of Bayes' theorem is  Bayesian inference , a particular approach to  statistical inference , where it is used to invert the probability of  observations  given a model configuration (i.e., the  likelihood function ) to obtain the probability of the model configuration given the observations (i.e., the  posterior probability ). Bayes' theorem is named after the Reverend  Thomas Bayes  ( / b eɪ z / ), also a statistician and philosopher. Bayes used conditional probability to provide an algorithm (his Proposition 9) that uses evidence to calculate limits on an unknown parameter. His work was published in 1763 as  An Essay Towards Solving a Problem in the Doctrine of Chances . Bayes studied how to compute a distribution for the probability parameter of a  binomial distribution  (in modern terminology). On Bayes's death his family transferred his papers to a friend, the minister, philosopher, and mathematician  Richard Price . Over two years, Richard Price significantly edited the unpublished manuscript, before sending it to a friend who read it aloud at the  Royal Society  on 23 December 1763. [ 2 ]  Price edited [ 3 ]  Bayes's major work \"An Essay Towards Solving a Problem in the Doctrine of Chances\" (1763), which appeared in  Philosophical Transactions , [ 4 ]  and contains Bayes' theorem. Price wrote an introduction to the paper which provides some of the philosophical basis of  Bayesian statistics  and chose one of the two solutions offered by Bayes. In 1765, Price was elected a Fellow of the Royal Society in recognition of his work on the legacy of Bayes. [ 5 ] [ 6 ]   On 27 April a letter sent to his friend  Benjamin Franklin  was read out at the Royal Society, and later published, where Price applies this work to population and computing 'life-annuities'. [ 7 ] Independently of Bayes,  Pierre-Simon Laplace  in 1774, and later in his 1812  Théorie analytique des probabilités , used conditional probability to formulate the relation of an updated  posterior probability  from a prior probability, given evidence. He reproduced and extended Bayes's results in 1774, apparently unaware of Bayes's work. [ note 1 ] [ 8 ]  The  Bayesian interpretation  of probability was developed mainly by Laplace. [ 9 ] About 200 years later,  Sir Harold Jeffreys  put Bayes's algorithm and Laplace's formulation on an  axiomatic  basis, writing in a 1973 book that Bayes' theorem \"is to the theory of probability what the  Pythagorean theorem  is to geometry\". [ 10 ] Stephen Stigler  used a Bayesian argument to conclude that Bayes' theorem was discovered by  Nicholas Saunderson , a blind English mathematician, some time before Bayes; [ 11 ] [ 12 ]  that interpretation, however, has been disputed. [ 13 ] \nMartyn Hooper [ 14 ]  and Sharon McGrayne [ 15 ]  have argued that  Richard Price 's contribution was substantial: By modern standards, we should refer to the Bayes–Price rule. Price discovered Bayes's work, recognized its importance, corrected it, contributed to the article, and found a use for it. The modern convention of employing Bayes's name alone is unfair but so entrenched that anything else makes little sense. [ 15 ] Bayes' theorem is stated mathematically as the following equation: [ 16 ] P \n ( \n A \n | \n B \n ) \n = \n \n \n \n P \n ( \n B \n | \n A \n ) \n P \n ( \n A \n ) \n \n \n P \n ( \n B \n ) \n \n \n \n \n \n {\\displaystyle P(A\\vert B)={\\frac {P(B\\vert A)P(A)}{P(B)}}} where  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  and  \n \n \n \n B \n \n \n {\\displaystyle B} \n \n  are  events  and  \n \n \n \n P \n ( \n B \n ) \n ≠ \n 0 \n \n \n {\\displaystyle P(B)\\neq 0} \n \n . Bayes' theorem may be derived from the definition of  conditional probability : where  \n \n \n \n P \n ( \n A \n ∩ \n B \n ) \n \n \n {\\displaystyle P(A\\cap B)} \n \n  is the probability of both A and B being true. Similarly, Solving for  \n \n \n \n P \n ( \n A \n ∩ \n B \n ) \n \n \n {\\displaystyle P(A\\cap B)} \n \n  and substituting into the above expression for  \n \n \n \n P \n ( \n A \n | \n B \n ) \n \n \n {\\displaystyle P(A\\vert B)} \n \n  yields Bayes' theorem: For two continuous  random variables   X  and  Y , Bayes' theorem may be analogously derived from the definition of  conditional density : Therefore, Let  \n \n \n \n \n P \n \n Y \n \n \n x \n \n \n \n \n {\\displaystyle P_{Y}^{x}} \n \n  be the conditional distribution of  \n \n \n \n Y \n \n \n {\\displaystyle Y} \n \n  given  \n \n \n \n X \n = \n x \n \n \n {\\displaystyle X=x} \n \n  and let  \n \n \n \n \n P \n \n X \n \n \n \n \n {\\displaystyle P_{X}} \n \n  be the distribution of  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n . The joint distribution is then  \n \n \n \n \n P \n \n X \n , \n Y \n \n \n ( \n d \n x \n , \n d \n y \n ) \n = \n \n P \n \n Y \n \n \n x \n \n \n ( \n d \n y \n ) \n \n P \n \n X \n \n \n ( \n d \n x \n ) \n \n \n {\\displaystyle P_{X,Y}(dx,dy)=P_{Y}^{x}(dy)P_{X}(dx)} \n \n . The conditional distribution  \n \n \n \n \n P \n \n X \n \n \n y \n \n \n \n \n {\\displaystyle P_{X}^{y}} \n \n  of  \n \n \n \n X \n \n \n {\\displaystyle X} \n \n   given  \n \n \n \n Y \n = \n y \n \n \n {\\displaystyle Y=y} \n \n  is then determined by P \n \n X \n \n \n y \n \n \n ( \n A \n ) \n = \n E \n ( \n \n 1 \n \n A \n \n \n ( \n X \n ) \n \n | \n \n Y \n = \n y \n ) \n \n \n {\\displaystyle P_{X}^{y}(A)=E(1_{A}(X)|Y=y)} Existence and uniqueness of the needed  conditional expectation  is a consequence of the  Radon–Nikodym theorem . This was formulated by  Kolmogorov  in his famous book from 1933. Kolmogorov underlines the importance of conditional probability by writing \"I wish to call attention to  ... and especially the theory of conditional probabilities and conditional expectations ...\" in the Preface. [ 17 ]  The Bayes theorem determines the posterior distribution from the prior distribution. Uniqueness requires continuity assumptions. [ 18 ]  Bayes' theorem can be generalized to include improper prior distributions such as the uniform distribution on the real line. [ 19 ]  Modern  Markov chain Monte Carlo  methods have boosted the importance of Bayes' theorem including cases with improper priors. [ 20 ] Bayes' rule and computing  conditional probabilities  provide a solution method for a number of popular puzzles, such as the  Three Prisoners problem , the  Monty Hall problem , the  Two Child problem  and the  Two Envelopes problem . Suppose, a particular test for whether someone has been using cannabis is 90%  sensitive , meaning the  true positive rate  (TPR) = 0.90.  Therefore, it leads to 90% true positive results (correct identification of drug use) for cannabis users. The test is also 80%  specific , meaning  true negative rate  (TNR) = 0.80.  Therefore, the test correctly identifies 80% of non-use for non-users, but also generates 20% false positives, or  false positive rate  (FPR) = 0.20, for non-users. Assuming 0.05  prevalence , meaning 5% of people use cannabis, what is the  probability  that a random person who tests positive is really a cannabis user? The  Positive predictive value  (PPV) of a test is the proportion of persons who are actually positive out of all those testing positive, and can be calculated from a sample as: If sensitivity, specificity, and prevalence are known, PPV can be calculated using Bayes theorem.  Let  \n \n \n \n P \n ( \n \n User \n \n | \n \n Positive \n \n ) \n \n \n {\\displaystyle P({\\text{User}}\\vert {\\text{Positive}})} \n \n  mean \"the probability that someone is a cannabis user given that they test positive,\" which is what is meant by PPV.  We can write: The denominator  \n \n \n \n P \n ( \n \n Positive \n \n ) \n = \n P \n ( \n \n Positive \n \n | \n \n User \n \n ) \n P \n ( \n \n User \n \n ) \n + \n P \n ( \n \n Positive \n \n | \n \n Non-user \n \n ) \n P \n ( \n \n Non-user \n \n ) \n \n \n {\\displaystyle P({\\text{Positive}})=P({\\text{Positive}}\\vert {\\text{User}})P({\\text{User}})+P({\\text{Positive}}\\vert {\\text{Non-user}})P({\\text{Non-user}})} \n \n  \nis a direct application of the  Law of Total Probability . In this case, it says that the probability that someone tests positive is the probability that a user tests positive, times the probability of being a user, plus the probability that a non-user tests positive, times the probability of being a non-user. This is true because the classifications user and non-user form a  partition of a set , namely the set of people who take the drug test. This combined with the definition of  conditional probability  results in the above statement. In other words, even if someone tests positive, the probability that they are a cannabis user is only 19%—this is because in this group, only 5% of people are users, and most positives are false positives coming from the remaining 95%. If 1,000 people were tested: The 1,000 people thus yields 235 positive tests, of which only 45 are genuine drug users, about 19%. The importance of  specificity  can be seen by showing that even if sensitivity is raised to 100% and specificity remains at 80%, the probability of someone testing positive really being a cannabis user only rises from 19% to 21%, but if the sensitivity is held at 90% and the specificity is increased to 95%, the probability rises to 49%. Even if 100% of patients with pancreatic cancer have a certain symptom, when someone has the same symptom, it does not mean that this person has a 100% chance of getting pancreatic cancer. Assuming the incidence rate of pancreatic cancer is 1/100000, while 10/99999 healthy individuals have the same symptoms worldwide, the probability of having pancreatic cancer given the symptoms is only 9.1%, and the other 90.9% could be \"false positives\" (that is, falsely said to have cancer; \"positive\" is a confusing term when, as here, the test gives bad news). Based on incidence rate, the following table presents the corresponding numbers per 100,000 people. Which can then be used to calculate the probability of having cancer when you have the symptoms: A factory produces items using three machines—A, B, and C—which account for 20%, 30%, and 50% of its output respectively. Of the items produced by machine A, 5% are defective; similarly, 3% of machine B's items and 1% of machine C's are defective. If a randomly selected item is defective, what is the probability it was produced by machine C? Once again, the answer can be reached without using the formula by applying the conditions to a hypothetical number of cases. For example, if the factory produces 1,000 items, 200 will be produced by Machine A, 300 by Machine B, and 500 by Machine C. Machine A will produce 5% × 200 = 10 defective items, Machine B 3% × 300 = 9, and Machine C 1% × 500 = 5, for a total of 24. Thus, the likelihood that a randomly selected defective item was produced by machine C is 5/24 (~20.83%). This problem can also be solved using Bayes' theorem: Let  X i  denote the event that a randomly chosen item was made by the  i   th  machine (for  i  = A,B,C). Let  Y  denote the event that a randomly chosen item is defective. Then, we are given the following information: If the item was made by the first machine, then the probability that it is defective is 0.05; that is,  P ( Y  |  X A ) = 0.05. Overall, we have To answer the original question, we first find  P (Y). That can be done in the following way: Hence, 2.4% of the total output is defective. We are given that  Y  has occurred, and we want to calculate the conditional\nprobability of  X C . By Bayes' theorem, Given that the item is defective, the probability that it was made by machine C is 5/24. Although machine C produces half of the total output, it produces a much smaller fraction of the defective items. Hence the knowledge that the item selected was defective enables us to replace the prior probability  P ( X C ) = 1/2 by the smaller posterior probability  P (X C  |  Y ) = 5/24. The interpretation of Bayes' rule depends on the  interpretation of probability  ascribed to the terms. The two predominant interpretations are described below. In the  Bayesian (or epistemological) interpretation , probability measures a \"degree of belief\". Bayes' theorem links the degree of belief in a proposition before and after accounting for evidence. For example, suppose it is believed with 50% certainty that a coin is twice as likely to land heads than tails. If the coin is flipped a number of times and the outcomes observed, that degree of belief will probably rise or fall, but might even remain the same, depending on the results.  For proposition  A  and evidence  B , For more on the application of Bayes' theorem under the Bayesian interpretation of probability, see  Bayesian inference . In the  frequentist interpretation , probability measures a \"proportion of outcomes\". For example, suppose an experiment is performed many times.  P ( A ) is the proportion of outcomes with property  A  (the prior)  and  P ( B ) is the proportion with property  B .  P ( B  |  A ) is the proportion of outcomes with property  B   out of  outcomes with property  A , and  P ( A  |  B ) is the proportion of those with  A   out of  those with  B  (the posterior). The role of Bayes' theorem is best visualized with tree diagrams. The two diagrams partition the same outcomes by  A  and  B  in opposite orders, to obtain the inverse probabilities. Bayes' theorem links the different partitionings. An  entomologist  spots what might, due to the pattern on its back, be a rare  subspecies  of  beetle . A full 98% of the members of the rare subspecies have the pattern, so   P (Pattern | Rare) = 98%.  Only 5% of members of the common subspecies have the pattern. The rare subspecies is 0.1% of the total population. How likely is the beetle having the pattern to be rare: what is  P (Rare | Pattern)? From the extended form of Bayes' theorem (since any beetle is either rare or common), For events  A  and  B , provided that  P ( B ) ≠ 0, In many applications, for instance in  Bayesian inference , the event  B  is fixed in the discussion, and we wish to consider the impact of its having been observed on our belief in various possible events  A . In such a situation the denominator of the last expression, the probability of the given evidence  B , is fixed; what we want to vary is  A . Bayes' theorem then shows that the posterior probabilities are  proportional  to the numerator, so the last equation becomes: In words, the posterior is proportional to the prior times the likelihood. [ 21 ] If events  A 1 ,  A 2 , ..., are mutually exclusive and exhaustive, i.e., one of them is certain to occur but no two can occur together,  we can determine the proportionality constant by using the fact that their probabilities must add up to one. For instance, for a given event  A , the event  A  itself and its complement ¬ A  are exclusive and exhaustive. Denoting the constant of proportionality by  c  we have Adding these two formulas we deduce that or Another form of Bayes' theorem for two competing statements or hypotheses is: For an epistemological interpretation: For proposition  A  and evidence or background  B , [ 22 ] Often, for some  partition  { A j } of the  sample space , the  event space  is given in terms of  P ( A j ) and  P ( B  |  A j ). It is then useful to compute  P ( B ) using the  law of total probability : P \n ( \n B \n ) \n = \n \n ∑ \n \n j \n \n \n P \n ( \n B \n ∩ \n \n A \n \n j \n \n \n ) \n , \n \n \n {\\displaystyle P(B)=\\sum _{j}P(B\\cap A_{j}),} Or (using the multiplication rule for conditional probability), [ 23 ] In the special case where  A  is a  binary variable : Consider a  sample space  Ω generated by two  random variables   X  and  Y  with known probability distributions. In principle, Bayes' theorem applies to the events  A  = { X  =  x } and  B  = { Y  =  y }. However, terms become 0 at points where either variable has finite  probability density . To remain useful, Bayes' theorem can be formulated in terms of the relevant densities (see  Derivation ). If  X  is continuous and  Y  is discrete, where each  \n \n \n \n f \n \n \n {\\displaystyle f} \n \n  is a density function. If  X  is discrete and  Y  is continuous, If both  X  and  Y  are continuous, A continuous event space is often conceptualized in terms of the numerator terms. It is then useful to eliminate the denominator using the  law of total probability . For  f Y ( y ), this becomes an integral: Bayes' theorem in  odds form  is: where is called the  Bayes factor  or  likelihood ratio .  The odds between two events is simply the ratio of the probabilities of the two events. Thus Thus,  the rule says that the posterior odds are the prior odds times the  Bayes factor , or in other words,  the posterior is proportional to the prior times the likelihood. In the special case that  \n \n \n \n \n A \n \n 1 \n \n \n = \n A \n \n \n {\\displaystyle A_{1}=A} \n \n  and  \n \n \n \n \n A \n \n 2 \n \n \n = \n ¬ \n A \n \n \n {\\displaystyle A_{2}=\\neg A} \n \n , one writes  \n \n \n \n O \n ( \n A \n ) \n = \n O \n ( \n A \n : \n ¬ \n A \n ) \n = \n P \n ( \n A \n ) \n \n / \n \n ( \n 1 \n − \n P \n ( \n A \n ) \n ) \n \n \n {\\displaystyle O(A)=O(A:\\neg A)=P(A)/(1-P(A))} \n \n , and uses a similar abbreviation for the Bayes factor and for the conditional odds. The odds on  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  is by definition the odds for and against  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n . Bayes' rule can then be written in the abbreviated form or, in words, the posterior odds on  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  equals the prior odds on  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  times the likelihood ratio for  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  given information  \n \n \n \n B \n \n \n {\\displaystyle B} \n \n . In short,   posterior odds equals prior odds times likelihood ratio . For example, if a medical test has a  sensitivity  of 90% and a  specificity  of 91%, then the positive Bayes factor is  \n \n \n \n \n Λ \n \n + \n \n \n = \n P \n ( \n \n True Positive \n \n ) \n \n / \n \n P \n ( \n \n False Positive \n \n ) \n = \n 90 \n % \n \n / \n \n ( \n 100 \n % \n − \n 91 \n % \n ) \n = \n 10 \n \n \n {\\displaystyle \\Lambda _{+}=P({\\text{True Positive}})/P({\\text{False Positive}})=90\\%/(100\\%-91\\%)=10} \n \n . Now, if the  prevalence  of this disease is 9.09%, and if we take that as the prior probability, then the prior odds is about 1:10. So after receiving a positive test result, the posterior odds of actually having the disease becomes 1:1, which means that the posterior probability of having the disease is 50%. If a second test is performed in serial testing, and that also turns out to be positive, then the posterior odds of actually having the disease becomes 10:1, which means a posterior probability of about 90.91%. The negative Bayes factor can be calculated to be 91%/(100%-90%)=9.1, so if the second test turns out to be negative, then the posterior odds of actually having the disease is 1:9.1, which means a posterior probability of about 9.9%. The example above can also be understood with more solid numbers: Assume the patient taking the test is from a group of 1000 people, where 91 of them actually have the disease (prevalence of 9.1%). If all these 1000 people take the medical test, 82 of those with the disease will get a true positive result (sensitivity of 90.1%), 9 of those with the disease will get a false negative result ( false negative rate  of 9.9%), 827 of those without the disease will get a true negative result (specificity of 91.0%), and 82 of those without the disease will get a false positive result (false positive rate of 9.0%). Before taking any test, the patient's odds for having the disease is 91:909. After receiving a positive result, the patient's odds for having the disease is which is consistent with the fact that there are 82 true positives and 82 false positives in the group of 1000 people. Where the  conditional probability   \n \n \n \n P \n ( \n A \n | \n B \n ) \n \n \n {\\displaystyle P(A\\vert B)} \n \n  is  defined , it can be seen to capture the implication  \n \n \n \n B \n → \n A \n \n \n {\\displaystyle B\\to A} \n \n . The probabilistic calculus then mirrors or even generalizes various logical inference rules. Beyond, for example, assigning binary truth values, here one assigns probability values to statements. The assertion of  \n \n \n \n B \n → \n A \n \n \n {\\displaystyle B\\to A} \n \n  is captured by the assertion  \n \n \n \n P \n ( \n A \n | \n B \n ) \n = \n 1 \n \n \n {\\displaystyle P(A\\vert B)=1} \n \n , i.e. that the conditional probability take the extremal probability value  \n \n \n \n 1 \n \n \n {\\displaystyle 1} \n \n . Likewise, the assertion of a negation of an implication is captured by the assignment of  \n \n \n \n 0 \n \n \n {\\displaystyle 0} \n \n . [ 1 ] \nSo for example, if  \n \n \n \n P \n ( \n A \n ) \n = \n 1 \n \n \n {\\displaystyle P(A)=1} \n \n  then (if it is defined) also  \n \n \n \n P \n ( \n A \n | \n B \n ) \n = \n 1 \n \n \n {\\displaystyle P(A\\vert B)=1} \n \n , which entails  \n \n \n \n A \n → \n ( \n B \n → \n A \n ) \n \n \n {\\displaystyle A\\to (B\\to A)} \n \n , the  implication introduction  in logic. Similarly, as the product of two probabilities equalling  \n \n \n \n 1 \n \n \n {\\displaystyle 1} \n \n  necessitates that both factors are also  \n \n \n \n 1 \n \n \n {\\displaystyle 1} \n \n , one finds that Bayes' theorem entails  \n \n \n \n \n \n ( \n \n \n A \n ∧ \n ( \n A \n → \n B \n ) \n \n \n ) \n \n \n ↔ \n \n \n ( \n \n \n B \n ∧ \n ( \n B \n → \n A \n ) \n \n \n ) \n \n \n \n \n {\\displaystyle {\\big (}A\\land (A\\to B){\\big )}\\leftrightarrow {\\big (}B\\land (B\\to A){\\big )}} \n \n , which now also includes  modus ponens . For positive values  \n \n \n \n P \n ( \n A \n ) \n \n \n {\\displaystyle P(A)} \n \n , if it equals  \n \n \n \n P \n ( \n B \n ) \n \n \n {\\displaystyle P(B)} \n \n , then the two conditional probabilities are equal as well, and vice versa. Note that this mirrors the generally valid  \n \n \n \n ( \n A \n ↔ \n B \n ) \n ↔ \n \n \n ( \n \n \n ( \n A \n → \n B \n ) \n ↔ \n ( \n B \n → \n A \n ) \n \n \n ) \n \n \n \n \n {\\displaystyle (A\\leftrightarrow B)\\leftrightarrow {\\big (}(A\\to B)\\leftrightarrow (B\\to A){\\big )}} \n \n . On the other hand, reasoning about either of the probabilities equalling  \n \n \n \n 0 \n \n \n {\\displaystyle 0} \n \n   classically  entails the following contrapositive form of the above:  \n \n \n \n \n \n ( \n \n \n ¬ \n B \n ∨ \n ¬ \n ( \n B \n → \n A \n ) \n \n \n ) \n \n \n ↔ \n \n \n ( \n \n \n ¬ \n A \n ∨ \n ¬ \n ( \n A \n → \n B \n ) \n \n \n ) \n \n \n \n \n {\\displaystyle {\\big (}\\neg B\\lor \\neg (B\\to A){\\big )}\\leftrightarrow {\\big (}\\neg A\\lor \\neg (A\\to B){\\big )}} \n \n . Bayes' theorem with negated  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  gives Ruling out the extremal case  \n \n \n \n P \n ( \n ¬ \n A \n ) \n = \n 0 \n \n \n {\\displaystyle P(\\neg A)=0} \n \n  (i.e.  \n \n \n \n P \n ( \n A \n ) \n = \n 1 \n \n \n {\\displaystyle P(A)=1} \n \n ), one has  \n \n \n \n P \n ( \n B \n | \n ¬ \n A \n ) \n = \n P \n ( \n B \n ) \n ⋅ \n \n \n \n \n 1 \n − \n P \n ( \n A \n | \n B \n ) \n \n \n 1 \n − \n P \n ( \n A \n ) \n \n \n \n \n \n \n {\\displaystyle P(B\\vert \\neg A)=P(B)\\cdot {\\tfrac {1-P(A\\vert B)}{1-P(A)}}} \n \n  and in particular Ruling out also the extremal case  \n \n \n \n P \n ( \n B \n ) \n = \n 0 \n \n \n {\\displaystyle P(B)=0} \n \n , one finds they attain the maximum  \n \n \n \n 1 \n \n \n {\\displaystyle 1} \n \n  simultaneously: which (at least when having ruled out  explosive  antecedents) captures the classical  contraposition  principle Bayes' theorem represents a special case of deriving inverted conditional opinions in  subjective logic  expressed as: where  \n \n \n \n \n \n \n ϕ \n ~ \n \n \n \n \n \n {\\displaystyle {\\widetilde {\\phi }}} \n \n  denotes the operator for inverting conditional opinions. The argument  \n \n \n \n ( \n \n ω \n \n B \n | \n A \n \n \n S \n \n \n , \n \n ω \n \n B \n | \n ¬ \n A \n \n \n S \n \n \n ) \n \n \n {\\displaystyle (\\omega _{B\\vert A}^{S},\\omega _{B\\vert \\lnot A}^{S})} \n \n  denotes a pair of binomial conditional opinions given by source  \n \n \n \n S \n \n \n {\\displaystyle S} \n \n , and the argument  \n \n \n \n \n a \n \n A \n \n \n \n \n {\\displaystyle a_{A}} \n \n  denotes the  prior probability  (aka. the  base rate ) of  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n . The pair of derivative inverted conditional opinions is denoted  \n \n \n \n ( \n \n ω \n \n A \n \n \n \n \n | \n \n ~ \n \n \n \n B \n \n \n S \n \n \n , \n \n ω \n \n A \n \n \n \n \n | \n \n ~ \n \n \n \n ¬ \n B \n \n \n S \n \n \n ) \n \n \n {\\displaystyle (\\omega _{A{\\tilde {|}}B}^{S},\\omega _{A{\\tilde {|}}\\lnot B}^{S})} \n \n . The conditional opinion  \n \n \n \n \n ω \n \n A \n | \n B \n \n \n S \n \n \n \n \n {\\displaystyle \\omega _{A\\vert B}^{S}} \n \n  generalizes the probabilistic conditional  \n \n \n \n P \n ( \n A \n | \n B \n ) \n \n \n {\\displaystyle P(A\\vert B)} \n \n , i.e. in addition to assigning a probability the source  \n \n \n \n S \n \n \n {\\displaystyle S} \n \n  can assign any subjective opinion to the conditional statement  \n \n \n \n ( \n A \n | \n B \n ) \n \n \n {\\displaystyle (A\\vert B)} \n \n . A binomial subjective opinion  \n \n \n \n \n ω \n \n A \n \n \n S \n \n \n \n \n {\\displaystyle \\omega _{A}^{S}} \n \n  is the belief in the truth of statement  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  with degrees of epistemic uncertainty, as expressed by source  \n \n \n \n S \n \n \n {\\displaystyle S} \n \n . Every subjective opinion has a corresponding projected probability  \n \n \n \n P \n ( \n \n ω \n \n A \n \n \n S \n \n \n ) \n \n \n {\\displaystyle P(\\omega _{A}^{S})} \n \n . The application of Bayes' theorem to projected probabilities of opinions is a  homomorphism , meaning that Bayes' theorem can be expressed in terms of projected probabilities of opinions: Hence, the subjective Bayes' theorem represents a generalization of Bayes' theorem. [ 24 ] A version of Bayes' theorem for 3 events [ 25 ]  results from the addition of a third event  \n \n \n \n C \n \n \n {\\displaystyle C} \n \n , with  \n \n \n \n P \n ( \n C \n ) \n > \n 0 \n , \n \n \n {\\displaystyle P(C)>0,} \n \n  on which all probabilities are conditioned: Using the  chain rule And, on the other hand The desired result is obtained by identifying both expressions and solving for  \n \n \n \n P \n ( \n A \n | \n B \n ∩ \n C \n ) \n \n \n {\\displaystyle P(A\\vert B\\cap C)} \n \n . In genetics, Bayes' rule can be used to estimate the probability of an individual having a specific genotype. Many people seek to approximate their chances of being affected by a genetic disease or their likelihood of being a carrier for a recessive gene of interest. A Bayesian analysis can be done based on family history or  genetic testing , in order to predict whether an individual will develop a disease or pass one on to their children. Genetic testing and prediction is a common practice among couples who plan to have children but are concerned that they may both be carriers for a disease, especially within communities with low genetic variance. [ 26 ] Example of a Bayesian analysis table for a female individual's risk for a disease based on the knowledge that the disease is present in her siblings but not in her parents or any of her four children. Based solely on the status of the subject's siblings and parents, she is equally likely to be a carrier as to be a non-carrier (this likelihood is denoted by the Prior Hypothesis). However, the probability that the subject's four sons would all be unaffected is 1/16 ( 1 ⁄ 2 ⋅ 1 ⁄ 2 ⋅ 1 ⁄ 2 ⋅ 1 ⁄ 2 ) if she is a carrier, about 1 if she is a non-carrier (this is the Conditional Probability). The Joint Probability reconciles these two predictions by multiplying them together. The last line (the Posterior Probability) is calculated by dividing the Joint Probability for each hypothesis by the sum of both joint probabilities. [ 27 ] Parental genetic testing can detect around 90% of known disease alleles in parents that can lead to carrier or affected status in their child. Cystic fibrosis is a heritable disease caused by an autosomal recessive mutation on the CFTR gene, [ 28 ]  located on the q arm of chromosome 7. [ 29 ] Bayesian analysis of a female patient with a family history of cystic fibrosis (CF), who has tested negative for CF, demonstrating how this method was used to determine her risk of having a child born with CF: Because the patient is unaffected, she is either homozygous for the wild-type allele, or heterozygous. To establish prior probabilities, a Punnett square is used, based on the knowledge that neither parent was affected by the disease but both could have been carriers: Homozygous for the wild- type allele (a non-carrier) Heterozygous (a CF carrier) Homozygous for the wild- type allele (a non-carrier) Heterozygous (a CF carrier) (affected by cystic fibrosis) Given that the patient is unaffected, there are only three possibilities. Within these three, there are two scenarios in which the patient carries the mutant allele. Thus the prior probabilities are  2 ⁄ 3  and  1 ⁄ 3 . Next, the patient undergoes genetic testing and tests negative for cystic fibrosis. This test has a 90% detection rate, so the conditional probabilities of a negative test are 1/10 and 1.  Finally, the joint and posterior probabilities are calculated as before. After carrying out the same analysis on the patient's male partner (with a negative test result), the chances of their child being affected is equal to the product of the parents' respective posterior probabilities for being carriers times the chances that two carriers will produce an affected offspring ( 1 ⁄ 4 ). Bayesian analysis can be done using phenotypic information associated with a genetic condition, and when combined with genetic testing this analysis becomes much more complicated. Cystic fibrosis, for example, can be identified in a fetus through an ultrasound looking for an echogenic bowel, meaning one that appears brighter than normal on a scan. This is not a foolproof test, as an echogenic bowel can be present in a perfectly healthy fetus. Parental genetic testing is very influential in this case, where a phenotypic facet can be overly influential in probability calculation. In the case of a fetus with an echogenic bowel, with a mother who has been tested and is known to be a CF carrier, the posterior probability that the fetus actually has the disease is very high (0.64). However, once the father has tested negative for CF, the posterior probability drops significantly (to 0.16). [ 27 ] Risk factor calculation is a powerful tool in genetic counseling and reproductive planning, but it cannot be treated as the only important factor to consider. As above, incomplete testing can yield falsely high probability of carrier status, and testing can be financially inaccessible or unfeasible when a parent is not present."
  },
  {
    "id": 88,
    "title": "Binary Independence Model",
    "content": "The  Binary Independence Model  ( BIM ) [ 1 ] [ 2 ]  in  computing  and  information science  is a probabilistic  information retrieval  technique. The model makes some simple assumptions to make the estimation of document/query similarity probable and feasible. The Binary Independence Assumption is the that documents are  binary vectors . That is, only the presence or absence of terms in documents are recorded. Terms are  independently  distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.\nThe representation is an ordered set of  Boolean  variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector  d  = ( x 1 , ...,  x m )  where  x t =1  if term  t  is present in the document  d  and  x t =0  if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.\n\"Independence\" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the \"naive\" assumption of a  Naive Bayes classifier , where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a  Vector space model  by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms. The probability  \n \n \n \n P \n ( \n R \n \n | \n \n d \n , \n q \n ) \n \n \n {\\displaystyle P(R|d,q)} \n \n  that a document is relevant derives from the probability of relevance of the terms vector of that document  \n \n \n \n P \n ( \n R \n \n | \n \n x \n , \n q \n ) \n \n \n {\\displaystyle P(R|x,q)} \n \n . By using the  Bayes rule  we get: where  \n \n \n \n P \n ( \n x \n \n | \n \n R \n = \n 1 \n , \n q \n ) \n \n \n {\\displaystyle P(x|R=1,q)} \n \n  and  \n \n \n \n P \n ( \n x \n \n | \n \n R \n = \n 0 \n , \n q \n ) \n \n \n {\\displaystyle P(x|R=0,q)} \n \n  are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is  x .\nThe exact probabilities can not be known beforehand, so estimates from statistics about the collection of documents must be used. P \n ( \n R \n = \n 1 \n \n | \n \n q \n ) \n \n \n {\\displaystyle P(R=1|q)} \n \n  and  \n \n \n \n P \n ( \n R \n = \n 0 \n \n | \n \n q \n ) \n \n \n {\\displaystyle P(R=0|q)} \n \n  indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query  q . If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.\nSince a document is either relevant or nonrelevant to a query we have that: Given a binary query and the  dot product  as the similarity function between a document and a query, the problem is to assign weights to the\nterms in the query such that the retrieval effectiveness will be high. Let  \n \n \n \n \n p \n \n i \n \n \n \n \n {\\displaystyle p_{i}} \n \n  and  \n \n \n \n \n q \n \n i \n \n \n \n \n {\\displaystyle q_{i}} \n \n  be the probability that a relevant document and an irrelevant document has the  i th  term respectively. Yu and  Salton , [ 1 ]  who first introduce BIM, propose that the weight of the  i th  term is an increasing function of  \n \n \n \n \n Y \n \n i \n \n \n = \n \n \n \n \n p \n \n i \n \n \n ∗ \n ( \n 1 \n − \n \n q \n \n i \n \n \n ) \n \n \n ( \n 1 \n − \n \n p \n \n i \n \n \n ) \n ∗ \n \n q \n \n i \n \n \n \n \n \n \n \n {\\displaystyle Y_{i}={\\frac {p_{i}*(1-q_{i})}{(1-p_{i})*q_{i}}}} \n \n . Thus, if  \n \n \n \n \n Y \n \n i \n \n \n \n \n {\\displaystyle Y_{i}} \n \n  is higher than  \n \n \n \n \n Y \n \n j \n \n \n \n \n {\\displaystyle Y_{j}} \n \n , the weight\nof term  i  will be higher than that of term  j . Yu and Salton [ 1 ]  showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted.  Robertson  and  Spärck Jones [ 2 ]  later showed that if the  i th  term is assigned the weight of  \n \n \n \n log \n ⁡ \n \n Y \n \n i \n \n \n \n \n {\\displaystyle \\log Y_{i}} \n \n , then optimal retrieval effectiveness is obtained under the Binary Independence Assumption. The Binary Independence Model was introduced by Yu and Salton. [ 1 ]  The name Binary Independence Model was coined by Robertson and Spärck Jones [ 2 ]  who used the log-odds probability of the  probabilistic relevance model  to derive  \n \n \n \n log \n ⁡ \n \n Y \n \n i \n \n \n \n \n {\\displaystyle \\log Y_{i}} \n \n  where the log-odds probability is shown to be rank equivalent to the probability of relevance (i.e.,  \n \n \n \n P \n ( \n R \n \n | \n \n d \n , \n q \n ) \n \n \n {\\displaystyle P(R|d,q)} \n \n ) by Luk, [ 3 ]  obeying the probability ranking principle. [ 4 ]"
  },
  {
    "id": 89,
    "title": "Probabilistic relevance model",
    "content": "The  probabilistic relevance model [ 1 ] [ 2 ]  was devised by  Stephen E. Robertson  and  Karen Spärck Jones  as a framework for   probabilistic models  to come. It is a formalism of  information retrieval  useful to derive  ranking functions  used by  search engines  and   web search engines  in order to rank matching documents according to their  relevance  to a given search query. It is a theoretical model estimating the probability that a document  d j  is relevant to a query  q . The model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query  q . Such an ideal answer set is called  R  and should maximize the overall probability of relevance to that user. The prediction is that documents in this set  R  are relevant to the query, while documents not present in the set are non-relevant. s \n i \n m \n ( \n \n d \n \n j \n \n \n , \n q \n ) \n = \n \n \n \n P \n ( \n R \n \n | \n \n \n \n \n \n d \n → \n \n \n \n \n j \n \n \n ) \n \n \n P \n ( \n \n \n \n R \n ¯ \n \n \n \n \n | \n \n \n \n \n \n d \n → \n \n \n \n \n j \n \n \n ) \n \n \n \n \n \n {\\displaystyle sim(d_{j},q)={\\frac {P(R|{\\vec {d}}_{j})}{P({\\bar {R}}|{\\vec {d}}_{j})}}} There are some limitations to this framework that need to be addressed by further development: To address these and other concerns, other models have been developed from the probabilistic relevance framework, among them the  Binary Independence Model  from the same author. The best-known derivatives of this framework are the  Okapi (BM25)  weighting scheme and its multifield refinement, BM25F."
  },
  {
    "id": 90,
    "title": "Okapi BM25",
    "content": "In  information retrieval ,  Okapi BM25  ( BM  is an abbreviation of  best matching ) is a  ranking function  used by  search engines  to estimate the  relevance  of documents to a given search query. It is based on the  probabilistic retrieval framework  developed in the 1970s and 1980s by  Stephen E. Robertson ,  Karen Spärck Jones , and others. The name of the actual ranking function is  BM25 . The fuller name,  Okapi BM25 , includes the name of the first system to use it, which was the Okapi information retrieval system, implemented at  London 's  City University [ 1 ]  in the 1980s and 1990s. BM25 and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent  TF-IDF -like retrieval functions used in document retrieval. [ 2 ] BM25 is a  bag-of-words  retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. It is a family of scoring functions with slightly different components and parameters. One of the most prominent instantiations of the function is as follows. Given a query  Q , containing keywords  \n \n \n \n \n q \n \n 1 \n \n \n , \n . \n . \n . \n , \n \n q \n \n n \n \n \n \n \n {\\displaystyle q_{1},...,q_{n}} \n \n , the BM25 score of a document  D  is: where  \n \n \n \n f \n ( \n \n q \n \n i \n \n \n , \n D \n ) \n \n \n {\\displaystyle f(q_{i},D)} \n \n  is the number of times that the keyword  \n \n \n \n \n q \n \n i \n \n \n \n \n {\\displaystyle q_{i}} \n \n  occurs in the document  D ,  \n \n \n \n \n | \n \n D \n \n | \n \n \n \n {\\displaystyle |D|} \n \n  is the length of the document  D  in words, and  avgdl  is the average document length in the text collection from which documents are drawn.  \n \n \n \n \n k \n \n 1 \n \n \n \n \n {\\displaystyle k_{1}} \n \n  and  b  are free parameters, usually chosen, in absence of an advanced optimization, as  \n \n \n \n \n k \n \n 1 \n \n \n ∈ \n [ \n 1.2 \n , \n 2.0 \n ] \n \n \n {\\displaystyle k_{1}\\in [1.2,2.0]} \n \n  and  \n \n \n \n b \n = \n 0.75 \n \n \n {\\displaystyle b=0.75} \n \n . [ 3 ]   \n \n \n \n \n IDF \n \n ( \n \n q \n \n i \n \n \n ) \n \n \n {\\displaystyle {\\text{IDF}}(q_{i})} \n \n  is the IDF ( inverse document frequency ) weight of the query term  \n \n \n \n \n q \n \n i \n \n \n \n \n {\\displaystyle q_{i}} \n \n . It is usually computed as: where   N  is the total number of documents in the collection, and  \n \n \n \n n \n ( \n \n q \n \n i \n \n \n ) \n \n \n {\\displaystyle n(q_{i})} \n \n  is the number of documents containing  \n \n \n \n \n q \n \n i \n \n \n \n \n {\\displaystyle q_{i}} \n \n . There are several interpretations for IDF and slight variations on its formula. In the original BM25 derivation, the IDF component is derived from the  Binary Independence Model . Here is an interpretation from information theory. Suppose a query term  \n \n \n \n q \n \n \n {\\displaystyle q} \n \n  appears in  \n \n \n \n n \n ( \n q \n ) \n \n \n {\\displaystyle n(q)} \n \n  documents. Then a randomly picked document  \n \n \n \n D \n \n \n {\\displaystyle D} \n \n  will contain the term with probability  \n \n \n \n \n \n \n n \n ( \n q \n ) \n \n N \n \n \n \n \n {\\displaystyle {\\frac {n(q)}{N}}} \n \n  (where  \n \n \n \n N \n \n \n {\\displaystyle N} \n \n  is again the cardinality of the set of documents in the collection). Therefore, the  information  content of the message \" \n \n \n \n D \n \n \n {\\displaystyle D} \n \n  contains  \n \n \n \n q \n \n \n {\\displaystyle q} \n \n \" is: Now suppose we have two query terms  \n \n \n \n \n q \n \n 1 \n \n \n \n \n {\\displaystyle q_{1}} \n \n  and  \n \n \n \n \n q \n \n 2 \n \n \n \n \n {\\displaystyle q_{2}} \n \n . If the two terms occur in documents entirely independently of each other, then the probability of seeing both  \n \n \n \n \n q \n \n 1 \n \n \n \n \n {\\displaystyle q_{1}} \n \n  and  \n \n \n \n \n q \n \n 2 \n \n \n \n \n {\\displaystyle q_{2}} \n \n  in a randomly picked document  \n \n \n \n D \n \n \n {\\displaystyle D} \n \n  is: and the information content of such an event is: With a small variation, this is exactly what is expressed by the IDF component of BM25."
  },
  {
    "id": 91,
    "title": "Uncertain inference",
    "content": "Uncertain inference  was first described by  C. J. van Rijsbergen [ 1 ]  as a way to formally define a query and document relationship in  Information retrieval . This formalization is a  logical implication  with an attached measure of uncertainty. Rijsbergen proposes that the measure of  uncertainty  of a document  d  to a query  q  be the probability of its logical implication, i.e.: A user's query can be interpreted as a set of assertions about the desired document. It is the system's task to  infer , given a particular document, if the query assertions are true. If they are, the document is retrieved.\nIn many cases the contents of documents are not sufficient to assert the queries. A  knowledge base  of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as  plausible inference . The  plausibility  of an inference  \n \n \n \n d \n → \n q \n \n \n {\\displaystyle d\\to q} \n \n  is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.\nSince  d  and  q  are both generated by users, they are error prone; thus  \n \n \n \n d \n → \n q \n \n \n {\\displaystyle d\\to q} \n \n  is uncertain. This will affect the plausibility of a given query. By doing this it accomplishes two things: Multimedia  documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties. Uncertain inference generalizes the notions of  autoepistemic logic , where truth values are either known or unknown, and when known, they are true or false. If we have a query of the form: where A, B and C are query assertions, then for a document D we want the probability: If we transform this into the  conditional probability   \n \n \n \n P \n ( \n ( \n A \n ∧ \n B \n ∧ \n C \n ) \n \n | \n \n D \n ) \n \n \n {\\displaystyle P((A\\wedge B\\wedge C)|D)} \n \n  and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities. Croft and Krovetz [ 2 ]  applied uncertain inference to an information retrieval system for office documents they called  OFFICER . In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed. Probabilistic logic networks  is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability. Markov logic networks  allow uncertain inference to be performed; uncertainties are computed using the  maximum entropy principle , in analogy to the way that  Markov chains  describe the uncertainty of  finite state machines ."
  },
  {
    "id": 92,
    "title": "Language model",
    "content": "A  language model  is a probabilistic  model  of a natural language. [ 1 ]  In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘ Shannon -style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text. [ 2 ] Language models are useful for a variety of tasks, including  speech recognition [ 3 ]  (helping prevent predictions of low-probability (e.g. nonsense) sequences),  machine translation , [ 4 ]   natural language generation  (generating more human-like text),  optical character recognition ,  route optimization , [ 5 ]   handwriting recognition , [ 6 ]   grammar induction , [ 7 ]  and  information retrieval . [ 8 ] [ 9 ] Large language models , currently their most advanced form, are a combination of larger datasets (frequently using words  scraped  from the public  internet ),  feedforward neural networks , and  transformers . They have superseded  recurrent neural network -based models, which had previously superseded the pure statistical models, such as  word  n -gram language model . A  word  n -gram language model  is a purely statistical model of language. It has been superseded by  recurrent neural network –based models, which have been superseded by  large language models . [ 10 ]  It is based on an assumption that the probability of the next word in a sequence depends only on a fixed size window of previous words. If only one previous word is considered, it is called a bigram model; if two words, a trigram model; if  n  − 1 words, an  n -gram model. [ 11 ]  Special tokens are introduced to denote the start and end of a sentence  \n \n \n \n ⟨ \n s \n ⟩ \n \n \n {\\displaystyle \\langle s\\rangle } \n \n  and  \n \n \n \n ⟨ \n \n / \n \n s \n ⟩ \n \n \n {\\displaystyle \\langle /s\\rangle } \n \n . Maximum entropy  language models encode the relationship between a word and the  n -gram history using feature functions. The equation is P \n ( \n \n w \n \n m \n \n \n ∣ \n \n w \n \n 1 \n \n \n , \n … \n , \n \n w \n \n m \n − \n 1 \n \n \n ) \n = \n \n \n 1 \n \n Z \n ( \n \n w \n \n 1 \n \n \n , \n … \n , \n \n w \n \n m \n − \n 1 \n \n \n ) \n \n \n \n exp \n ⁡ \n ( \n \n a \n \n T \n \n \n f \n ( \n \n w \n \n 1 \n \n \n , \n … \n , \n \n w \n \n m \n \n \n ) \n ) \n \n \n {\\displaystyle P(w_{m}\\mid w_{1},\\ldots ,w_{m-1})={\\frac {1}{Z(w_{1},\\ldots ,w_{m-1})}}\\exp(a^{T}f(w_{1},\\ldots ,w_{m}))} where  \n \n \n \n Z \n ( \n \n w \n \n 1 \n \n \n , \n … \n , \n \n w \n \n m \n − \n 1 \n \n \n ) \n \n \n {\\displaystyle Z(w_{1},\\ldots ,w_{m-1})} \n \n  is the  partition function ,  \n \n \n \n a \n \n \n {\\displaystyle a} \n \n  is the parameter vector, and  \n \n \n \n f \n ( \n \n w \n \n 1 \n \n \n , \n … \n , \n \n w \n \n m \n \n \n ) \n \n \n {\\displaystyle f(w_{1},\\ldots ,w_{m})} \n \n  is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain  n -gram. It is helpful to use a prior on  \n \n \n \n a \n \n \n {\\displaystyle a} \n \n  or some form of  regularization . The log-bilinear model is another example of an exponential language model. Skip-gram language model is an attempt at overcoming the data sparsity problem that the preceding model (i.e. word  n -gram language model) faced. Words represented in an embedding vector were not necessarily consecutive anymore, but could leave gaps that are  skipped  over. [ 12 ] Formally, a  k -skip- n -gram is a length- n  subsequence where the components occur at distance at most  k  from each other. For example, in the input text: the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences In skip-gram model, semantic relations between words are represented by  linear combinations , capturing a form of  compositionality . For example, in some such models, if  v  is the function that maps a word  w  to its  n -d vector representation, then v \n ( \n \n k \n i \n n \n g \n \n ) \n − \n v \n ( \n \n m \n a \n l \n e \n \n ) \n + \n v \n ( \n \n f \n e \n m \n a \n l \n e \n \n ) \n ≈ \n v \n ( \n \n q \n u \n e \n e \n n \n \n ) \n \n \n {\\displaystyle v(\\mathrm {king} )-v(\\mathrm {male} )+v(\\mathrm {female} )\\approx v(\\mathrm {queen} )} Continuous representations or  embeddings of words  are produced in  recurrent neural network -based language models (known also as  continuous space language models ). [ 15 ]  Such continuous space embeddings help to alleviate the  curse of dimensionality , which is the consequence of the number of possible sequences of words increasing  exponentially  with the size of the vocabulary, furtherly causing a data sparsity problem. Neural networks avoid this problem by representing words as non-linear combinations of weights in a neural net. [ 16 ] A  large language model  (LLM) is a type of computational  model  designed for  natural language processing  tasks such as language  generation . As language models, LLMs acquire these abilities by  learning statistical relationships  from vast amounts of text during a  self-supervised  and  semi-supervised  training process. [ 17 ] Although sometimes matching human performance, it is not clear whether they are plausible  cognitive models . At least for recurrent neural networks, it has been shown that they sometimes learn patterns that humans do not, but fail to learn patterns that humans typically do. [ 21 ] Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language-oriented tasks. Other, less established, quality tests examine the intrinsic character of a language model or compare two such models. Since language models are typically intended to be dynamic and to learn from data they see, some proposed models investigate the rate of learning, e.g., through inspection of learning curves. [ 22 ] Various data sets have been developed for use in evaluating language processing systems. [ 23 ]  These include:"
  },
  {
    "id": 93,
    "title": "Divergence-from-randomness model",
    "content": "In the field of  information retrieval ,  divergence from randomness  ( DFR ), is a  generalization  of one of the very first models, Harter's  2-Poisson indexing-model . [ 1 ]  It is one type of  probabilistic   model . It is used to test the amount of  information  carried in  documents . The 2-Poisson model is based on the  hypothesis  that the level of documents is related to a set of documents that contains words that occur in relatively greater extent than in the rest of the documents. It is not a 'model', but a framework for  weighting  terms using  probabilistic methods , and it has a special  relationship  for term  weighting  based on the notion of  elite Term weights are being treated as the standard of whether a specific word is in that set or not. Term weights are  computed  by measuring the  divergence  between a term distribution produced by a  random process  and the actual term  distribution . Divergence from  randomness models  set up by instantiating the three main components of the framework: first selecting a basic randomness model, then applying the first  normalization  and at last normalizing the  term frequencies . The basic models are from the following tables. The divergence from  randomness  is based on this idea: \"The more the divergence of the within-document term-frequency from its frequency within the collection, the more the information carried by the word t in document d. In other words, the term-weight is  inversely related  to the probability of term-frequency within the document d obtained by a model M of randomness.\" [ 1 ] weight \n \n ( \n t \n \n | \n \n d \n ) \n = \n k \n \n \n Prob \n \n \n M \n \n \n ( \n t \n ∈ \n d \n \n | \n \n \n Collection \n \n ) \n \n \n {\\displaystyle {\\text{weight}}(t|d)=k{\\text{Prob}}_{M}(t\\in d|{\\text{Collection}})} \n \n  (Formula 1) It is possible that we use different  URN  models to choose the appropriate model M of randomness. In Information Retrieval, there are documents instead of URNs, and terms instead of colors. There are several ways to choose M, each of these has a basic divergence from randomness model to support it. [ 1 ] When a specific rare term cannot be found in a document, then in that document the term has approximately zero probability of being informative. On the other hand, if a rare term occurs frequently in a document, therefore it can have a very high, near 100% probability to be informative for the topic that mentioned by the document. Applying to Ponte and Croft's  language model  can also provide further data. A risk component is considered in the DFR. Logically speaking, if the term-frequency in the document is relatively high, then inversely the risk for the term of not being informative is relatively small. If Formula 1 gives a high value, then there is a minimal risk that it has the negative effect of showing small information gain. As a result, the weight of Formula one is organized 1 to only consider the portion of which is the amount of information gained with the term. The more the term occurs in the elite set, the less term-frequency is due to randomness, and thus the smaller the associated risk is. We use two models to compute the information-gain with a term within a document: the Laplace L model and the ratio of two  Bernoulli 's processes B. [ 2 ] Before using the within-document frequency tf of a term, the document-length dl is normalized to a standard length sl. Therefore, the term-frequencies tf are recalculated with the respect to the standard document-length, that is: tfn represents the normalized term frequency. Another version of the normalization formula is the following: Normalization 2 is usually considered to be more flexible, since there is no fixed value for c. Utility-Theoretic Indexing developed by Cooper and Maron is a theory of indexing based on utility theory. To reflect the value for documents that is expected by the users, index terms are assigned to documents. Also, Utility-Theoretic Indexing is related an \"event space\" in the statistical word. There are several basic spaces  Ω  in the Information Retrieval. A really simple basic space Ω can be the set V of terms t, which is called the vocabulary of the document collection. Due to Ω=V is the set of all mutually exclusive events, Ω can also be the certain event with probability: [ 3 ] Thus P, the probability  distribution , assigns probabilities to all sets of terms for the vocabulary. Notice that the basic problem of Information Retrieval is to find an estimate for P(t). Estimates are computed on the basis of sampling and the experimental text collection furnishes the samples needed for the estimation. Now we run into the main concern which is how do we treat two arbitrary but  heterogeneous  pieces of texts appropriately. Paragons like a chapter in a  Science Magazine  and an article from a sports newspaper as the other. They can be considered as two different samples since those aiming at different population. The relationship of the document with the experiments is made by the way in which the sample space is chosen. In IR, term experiment, or trial, is used here with a technical meaning rather than a common sense. For example, a document could be an experiment which means the document is a sequence of outcomes t∈V, or just a sample of a population. We will talk about the event of observing a number Xt =tf of occurrences of a given word t in a sequence of experiments. In order to introduce this event space, we should introduce the product of the probability spaces associated with the experiments of the sequence. We could introduce our sample space to associate a point with possible configurations of the outcomes. The one-to-one correspondence for sample space can be defined as: Where ld is the number of trials of the experiment or in this example, the length of a document. We can assume that each outcome may or may not depend on the outcomes of the previous experiments. If the experiments are designed so that an outcome is influencing the next outcomes, then the probability distribution on V is different at each trial. But, more commonly, in order to establish the simpler case when the probability space is invariant in IR, the term independence assumption is often made. Therefore, all possible configurations ofΩ=Vld are considered equiprobable. Considering this assumption, we can consider each document a  Bernoulli process . The probability spaces of the product are invariant and the probability of a given sequence is the product of the probabilities at each trial. Consequently, if p=P(t) is the prior probability that the outcome is t and the number of experiments is ld we obtain the probability of  \n \n \n \n \n X \n \n t \n \n \n = \n t \n f \n \n \n {\\displaystyle X_{t}=tf} \n \n  is equal to: Which is the sum of the probability of all possible configurations having tf outcomes out of ld. P(Xt=tf|p) is a probability distribution because Already considering the hypothesis of having a single sample, we need to consider that we have several samples, for example, a collection D of documents. The situation of having a collection of N documents is abstractly equivalent to the scheme of placing a certain number Tot of V colored types of balls in a collection of N cells. For each term t∈V a possible configuration of ball placement satisfies the equations: And the condition Where Ft is the number of balls of the same color t to be distributed in the N cells. We have thus changed the basic space. The outcome of our experiment will be the documents d in which the ball will be placed. Also, we will have a lot of possible configurations consistent with the number of colored balls. The divergence from Randomness Model is based on the Bernoulli model and its limiting forms, the hypergeometric distribution,  Bose-Einstein statistics  and its limiting forms, the compound of the binomial distribution with the beta distribution, and the fat-tailed distribution. Divergence from randomness model shows a unifying framework that has the potential constructing a lot of different effective models of IR. Proximity can be handled within divergence from randomness to consider the number of occurrences of a pair of query terms within a window of pre-defined size. To specify, the DFR Dependence Score Modifier DSM implements both the pBiL and pBiL2 models, which calculate the randomness divided by the document's length, rather than the statistics of the pair in the corpus the pair in the corpus. Let t be a term and c be a collection. Let the term occur in tfc=nL(t,c)=200 locations, and in df(t,c)=nL(t,c)=100 documents. The expected average term frequency is avgtf(t,c)=200/100=2; this is the average over the documents in which the term occurs. \nLet N.D(c)=1000 be the total amounts of documents. The term's occurrence is 10% in the documents: P.D(t|c)=100/1000. The expected average term frequency is 200/1000=1/5, and this is the average over all documents. The term frequency is shown as Kt =0,...,6. The following table show the column nD is the number of Documents that contains kt occurrence of t, shown as nD(t,c,kt). Another column nL is the number of Locations at which the term occurs follows by this equation: nL=kt*nD. The columns to the right show the observed and Poisson probabilities. \nP obs,elite(Kt) is the observed probability over all documents. P Poisson, all, lambda(Kt) is the Poisson probability, where lambda(t,c)=nL(t,c)/N D(c)=0.20 is the Poisson parameter. The table illustrates how the observed probability is different from the Poisson probability. P Poisson(1) is greater than P obs(1), whereas for kt>1.the observed probabilities are greater than the Poisson probabilities. There is more mass in the tail of the observed distribution than the Poisson distribution assumes. \nMoreover, the columns to the right illustrate the usage of the elite documents instead of all documents. Here, the single event probability is based on the locations of elite documents only."
  },
  {
    "id": 94,
    "title": "Latent Dirichlet allocation",
    "content": "In  natural language processing ,  latent  Dirichlet  allocation  ( LDA ) is a  Bayesian network  (and, therefore, a  generative statistical model ) for modeling automatically extracted topics in textual corpora. The LDA is an example of a Bayesian  topic model . In this, observations (e.g., words) are collected into documents, and each word's presence is attributable to one of the document's topics. Each document will contain a small number of topics. In the context of  population genetics , LDA was proposed by  J. K. Pritchard ,  M. Stephens  and  P. Donnelly  in 2000. [ 1 ] [ 2 ] LDA was applied in  machine learning  by  David Blei ,  Andrew Ng  and  Michael I. Jordan  in 2003. [ 3 ] In evolutionary biology and bio-medicine, the model is used to detect the presence of structured genetic variation in a group of individuals. The model assumes that  alleles  carried by individuals under study have origin in various extant or past populations. The model and various inference algorithms allow scientists to estimate the allele frequencies in those source populations and the origin of alleles carried by individuals under study. The source populations can be interpreted ex-post in terms of various evolutionary scenarios. \nIn  association studies , detecting the presence of genetic structure is considered a necessary preliminary step to avoid  confounding . In clinical psychology research, LDA has been used to identify common themes of self-images experienced by young people in social situations. [ 4 ]  Other social scientists have used LDA to examine large sets of topical data from discussions on social media (e.g., tweets about prescription drugs). [ 5 ] Additionally,  supervised Latent Dirichlet Allocation with covariates (SLDAX)  has been specifically developed to combine latent topics identified in texts with other manifest variables. This approach allows for the integration of text data as predictors in statistical regression analyses, improving the accuracy of mental health predictions. One of the main advantages of SLDAX over traditional two-stage approaches is its ability to avoid biased estimates and incorrect standard errors, allowing for a more accurate analysis of psychological texts. [ 6 ] [ 7 ] In the field of social sciences, LDA has proven to be useful for analyzing large datasets, such as social media discussions. For instance, researchers have used LDA to investigate tweets discussing socially relevant topics, like the use of prescription drugs. By analyzing these large text corpora, it is possible to uncover patterns and themes that might otherwise go unnoticed, offering valuable insights into public discourse and perception in real time. [ 8 ] [ 9 ] In the context of  computational musicology , LDA has been used to discover tonal structures in different corpora. [ 10 ] One application of LDA in  machine learning  - specifically,  topic discovery , a subproblem in  natural language processing  – is to discover topics in a collection of documents, and then automatically classify any individual document within the collection in terms of how \"relevant\" it is to each of the discovered topics. A  topic  is considered to be a set of terms (i.e., individual words or phrases) that, taken together, suggest a shared theme. For example, in a document collection related to pet animals, the terms  dog ,  spaniel ,  beagle ,  golden retriever ,  puppy ,  bark , and  woof  would suggest a  DOG_related  theme, while the terms  cat ,  siamese ,  Maine coon ,  tabby ,  manx ,  meow ,  purr , and  kitten  would suggest a  CAT_related  theme. There may be many more topics in the collection – e.g., related to diet, grooming, healthcare, behavior, etc. that we do not discuss for simplicity's sake. (Very common, so called  stop words  in a language  – e.g., \"the\", \"an\", \"that\", \"are\", \"is\", etc., – would not discriminate between topics and are usually filtered out by pre-processing before LDA is performed. Pre-processing also converts terms to their \"root\" lexical forms – e.g., \"barks\", \"barking\", and \"barked\" would be converted to \"bark\".) If the document collection is sufficiently large, LDA will discover such sets of terms (i.e., topics) based upon the co-occurrence of individual terms, though the task of assigning a meaningful label to an individual topic (i.e., that all the terms are DOG_related) is up to the user, and often requires specialized knowledge (e.g., for collection of technical documents). The LDA approach assumes that: When LDA machine learning is employed, both sets of probabilities are computed during the training phase, using  Bayesian  methods and an  Expectation Maximization  algorithm. LDA is a generalization of older approach of  probabilistic latent semantic analysis  (pLSA), The pLSA model is equivalent to LDA under a uniform Dirichlet prior distribution. [ 11 ] \npLSA relies on only the first two assumptions above and does not care about the remainder. \nWhile both methods are similar in principle and require the user to specify the number of topics to be discovered before the start of training (as with  K-means clustering ) LDA has the following advantages over pLSA: With  plate notation , which is often used to represent  probabilistic graphical models  (PGMs), the dependencies among the many variables can be captured concisely. The boxes are \"plates\" representing replicates, which are repeated entities. The outer plate represents documents, while the inner plate represents the repeated word positions in a given document; each position is associated with a choice of topic and word. The variable names are defined as follows: The fact that W is grayed out means that words  \n \n \n \n \n w \n \n i \n j \n \n \n \n \n {\\displaystyle w_{ij}} \n \n  are the only  observable variables , and the other variables are  latent variables .\nAs proposed in the original paper, [ 3 ]  a sparse Dirichlet prior can be used to model the topic-word distribution, following the intuition that the probability distribution over words in a topic is skewed, so that only a small set of words have high probability. The resulting model is the most widely applied variant of LDA today. The plate notation for this model is shown on the right, where  \n \n \n \n K \n \n \n {\\displaystyle K} \n \n  denotes the number of topics and  \n \n \n \n \n φ \n \n 1 \n \n \n , \n … \n , \n \n φ \n \n K \n \n \n \n \n {\\displaystyle \\varphi _{1},\\dots ,\\varphi _{K}} \n \n   are   \n \n \n \n V \n \n \n {\\displaystyle V} \n \n -dimensional vectors storing the parameters of the Dirichlet-distributed topic-word distributions ( \n \n \n \n V \n \n \n {\\displaystyle V} \n \n  is the number of words in the vocabulary). It is helpful to think of the entities represented by  \n \n \n \n θ \n \n \n {\\displaystyle \\theta } \n \n  and  \n \n \n \n φ \n \n \n {\\displaystyle \\varphi } \n \n  as matrices created by decomposing the original document-word matrix that represents the corpus of documents being modeled. In this view,  \n \n \n \n θ \n \n \n {\\displaystyle \\theta } \n \n  consists of rows defined by documents and columns defined by topics, while  \n \n \n \n φ \n \n \n {\\displaystyle \\varphi } \n \n  consists of rows defined by topics and columns defined by words. Thus,  \n \n \n \n \n φ \n \n 1 \n \n \n , \n … \n , \n \n φ \n \n K \n \n \n \n \n {\\displaystyle \\varphi _{1},\\dots ,\\varphi _{K}} \n \n   refers to a set of rows, or vectors, each of which is a distribution over words, and  \n \n \n \n \n θ \n \n 1 \n \n \n , \n … \n , \n \n θ \n \n M \n \n \n \n \n {\\displaystyle \\theta _{1},\\dots ,\\theta _{M}} \n \n refers to a set of rows, each of which is a distribution over topics. To actually infer the topics in a corpus, we imagine a generative process whereby the documents are created, so that we may infer, or reverse engineer, it. We imagine the generative process as follows. Documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over all the words. LDA assumes the following generative process for a corpus  \n \n \n \n D \n \n \n {\\displaystyle D} \n \n  consisting of  \n \n \n \n M \n \n \n {\\displaystyle M} \n \n  documents each of length  \n \n \n \n \n N \n \n i \n \n \n \n \n {\\displaystyle N_{i}} \n \n : 1. Choose  \n \n \n \n \n θ \n \n i \n \n \n ∼ \n Dir \n ⁡ \n ( \n α \n ) \n \n \n {\\displaystyle \\theta _{i}\\sim \\operatorname {Dir} (\\alpha )} \n \n , where  \n \n \n \n i \n ∈ \n { \n 1 \n , \n … \n , \n M \n } \n \n \n {\\displaystyle i\\in \\{1,\\dots ,M\\}} \n \n  and\n \n \n \n \n \n D \n i \n r \n \n ( \n α \n ) \n \n \n {\\displaystyle \\mathrm {Dir} (\\alpha )} \n \n  is a  Dirichlet distribution  with a symmetric parameter  \n \n \n \n α \n \n \n {\\displaystyle \\alpha } \n \n  which typically is sparse ( \n \n \n \n α \n < \n 1 \n \n \n {\\displaystyle \\alpha <1} \n \n ) 2. Choose  \n \n \n \n \n φ \n \n k \n \n \n ∼ \n Dir \n ⁡ \n ( \n β \n ) \n \n \n {\\displaystyle \\varphi _{k}\\sim \\operatorname {Dir} (\\beta )} \n \n , where  \n \n \n \n k \n ∈ \n { \n 1 \n , \n … \n , \n K \n } \n \n \n {\\displaystyle k\\in \\{1,\\dots ,K\\}} \n \n  and  \n \n \n \n β \n \n \n {\\displaystyle \\beta } \n \n  typically is sparse 3. For each of the word positions  \n \n \n \n i \n , \n j \n \n \n {\\displaystyle i,j} \n \n , where  \n \n \n \n i \n ∈ \n { \n 1 \n , \n … \n , \n M \n } \n \n \n {\\displaystyle i\\in \\{1,\\dots ,M\\}} \n \n , and  \n \n \n \n j \n ∈ \n { \n 1 \n , \n … \n , \n \n N \n \n i \n \n \n } \n \n \n {\\displaystyle j\\in \\{1,\\dots ,N_{i}\\}} (Note that  multinomial distribution  here refers to the  multinomial  with only one trial, which is also known as the  categorical distribution .) The lengths  \n \n \n \n \n N \n \n i \n \n \n \n \n {\\displaystyle N_{i}} \n \n  are treated as independent of all the other data generating variables ( \n \n \n \n w \n \n \n {\\displaystyle w} \n \n  and  \n \n \n \n z \n \n \n {\\displaystyle z} \n \n ). The subscript is often dropped, as in the plate diagrams shown here. A formal description of LDA is as follows: We can then mathematically describe the random variables as follows: Learning the various distributions (the set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) is a problem of  statistical inference . The original paper by Pritchard et al. [ 1 ]  used approximation of the posterior distribution by Monte Carlo simulation. \nAlternative proposal of inference techniques include  Gibbs sampling . [ 12 ] The original ML paper used a  variational Bayes  approximation of the  posterior distribution . [ 3 ] A direct optimization of the likelihood with a block relaxation algorithm proves to be a fast alternative to MCMC. [ 13 ] In practice, the optimal number of populations or topics is not known beforehand. It can be estimated by approximation of the posterior distribution with  reversible-jump Markov chain Monte Carlo . [ 14 ] Alternative approaches include   expectation propagation . [ 15 ] Recent research has been focused on speeding up the inference of latent Dirichlet allocation to support the capture of a massive number of topics in a large number of documents. The update equation of the collapsed Gibbs sampler mentioned in the earlier section has a natural sparsity within it that can be taken advantage of. Intuitively, since each document only contains a subset of topics  \n \n \n \n \n K \n \n d \n \n \n \n \n {\\displaystyle K_{d}} \n \n , and a word also only appears in a subset of topics  \n \n \n \n \n K \n \n w \n \n \n \n \n {\\displaystyle K_{w}} \n \n , the above update equation could be rewritten to take advantage of this sparsity. [ 16 ] In this equation, we have three terms, out of which two are sparse, and the other is small. We call these terms  \n \n \n \n a \n , \n b \n \n \n {\\displaystyle a,b} \n \n  and  \n \n \n \n c \n \n \n {\\displaystyle c} \n \n  respectively. Now, if we normalize each term by summing over all the topics, we get: Here, we can see that  \n \n \n \n B \n \n \n {\\displaystyle B} \n \n  is a summation of the topics that appear in document  \n \n \n \n d \n \n \n {\\displaystyle d} \n \n , and  \n \n \n \n C \n \n \n {\\displaystyle C} \n \n  is also a sparse summation of the topics that a word  \n \n \n \n w \n \n \n {\\displaystyle w} \n \n  is assigned to across the whole corpus.  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  on the other hand, is dense but because of the small values of  \n \n \n \n α \n \n \n {\\displaystyle \\alpha } \n \n  &  \n \n \n \n β \n \n \n {\\displaystyle \\beta } \n \n , the value is very small compared to the two other terms. Now, while sampling a topic, if we sample a random variable uniformly from  \n \n \n \n s \n ∼ \n U \n ( \n s \n \n | \n \n ∣ \n A \n + \n B \n + \n C \n ) \n \n \n {\\displaystyle s\\sim U(s|\\mid A+B+C)} \n \n , we can check which bucket our sample lands in. Since  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  is small, we are very unlikely to fall into this bucket; however, if we do fall into this bucket, sampling a topic takes  \n \n \n \n O \n ( \n K \n ) \n \n \n {\\displaystyle O(K)} \n \n  time (same as the original Collapsed Gibbs Sampler). However, if we fall into the other two buckets, we only need to check a subset of topics if we keep a record of the sparse topics. A topic can be sampled from the  \n \n \n \n B \n \n \n {\\displaystyle B} \n \n  bucket in  \n \n \n \n O \n ( \n \n K \n \n d \n \n \n ) \n \n \n {\\displaystyle O(K_{d})} \n \n  time, and a topic can be sampled from the  \n \n \n \n C \n \n \n {\\displaystyle C} \n \n  bucket in  \n \n \n \n O \n ( \n \n K \n \n w \n \n \n ) \n \n \n {\\displaystyle O(K_{w})} \n \n  time where  \n \n \n \n \n K \n \n d \n \n \n \n \n {\\displaystyle K_{d}} \n \n  and  \n \n \n \n \n K \n \n w \n \n \n \n \n {\\displaystyle K_{w}} \n \n  denotes the number of topics assigned to the current document and current word type respectively. Notice that after sampling each topic, updating these buckets is all basic  \n \n \n \n O \n ( \n 1 \n ) \n \n \n {\\displaystyle O(1)} \n \n  arithmetic operations. Following is the derivation of the equations for  collapsed Gibbs sampling , which means  \n \n \n \n φ \n \n \n {\\displaystyle \\varphi } \n \n s and  \n \n \n \n θ \n \n \n {\\displaystyle \\theta } \n \n s will be integrated out. For simplicity, in this derivation the documents are all assumed to have the same length  \n \n \n \n \n N \n \n \n \n \n \n {\\displaystyle N_{}} \n \n . The derivation is equally valid if the document lengths vary. According to the model, the total probability of the model is: where the bold-font variables denote the vector version of the variables. First,  \n \n \n \n \n φ \n \n \n \n {\\displaystyle {\\boldsymbol {\\varphi }}} \n \n  and  \n \n \n \n \n θ \n \n \n \n {\\displaystyle {\\boldsymbol {\\theta }}} \n \n  need to be integrated out. All the  \n \n \n \n θ \n \n \n {\\displaystyle \\theta } \n \n s are independent to each other and the same to all the  \n \n \n \n φ \n \n \n {\\displaystyle \\varphi } \n \n s. So we can treat each  \n \n \n \n θ \n \n \n {\\displaystyle \\theta } \n \n  and each  \n \n \n \n φ \n \n \n {\\displaystyle \\varphi } \n \n  separately. We now focus only on the  \n \n \n \n θ \n \n \n {\\displaystyle \\theta } \n \n  part. We can further focus on only one  \n \n \n \n θ \n \n \n {\\displaystyle \\theta } \n \n  as the following: Actually, it is the hidden part of the model for the  \n \n \n \n \n j \n \n t \n h \n \n \n \n \n {\\displaystyle j^{th}} \n \n  document. Now we replace the probabilities in the above equation by the true distribution expression to write out the explicit equation. Let  \n \n \n \n \n n \n \n j \n , \n r \n \n \n i \n \n \n \n \n {\\displaystyle n_{j,r}^{i}} \n \n  be the number of word tokens in the  \n \n \n \n \n j \n \n t \n h \n \n \n \n \n {\\displaystyle j^{th}} \n \n  document with the same word symbol (the  \n \n \n \n \n r \n \n t \n h \n \n \n \n \n {\\displaystyle r^{th}} \n \n  word in the vocabulary) assigned to the  \n \n \n \n \n i \n \n t \n h \n \n \n \n \n {\\displaystyle i^{th}} \n \n  topic. So,  \n \n \n \n \n n \n \n j \n , \n r \n \n \n i \n \n \n \n \n {\\displaystyle n_{j,r}^{i}} \n \n  is three dimensional. If any of the three dimensions is not limited to a specific value, we use a parenthesized point  \n \n \n \n ( \n ⋅ \n ) \n \n \n {\\displaystyle (\\cdot )} \n \n  to\ndenote. For example,  \n \n \n \n \n n \n \n j \n , \n ( \n ⋅ \n ) \n \n \n i \n \n \n \n \n {\\displaystyle n_{j,(\\cdot )}^{i}} \n \n  denotes the number of word tokens in the  \n \n \n \n \n j \n \n t \n h \n \n \n \n \n {\\displaystyle j^{th}} \n \n  document assigned to the  \n \n \n \n \n i \n \n t \n h \n \n \n \n \n {\\displaystyle i^{th}} \n \n  topic. Thus, the right most part of the above equation can be rewritten as: So the  \n \n \n \n \n θ \n \n j \n \n \n \n \n {\\displaystyle \\theta _{j}} \n \n  integration formula can be changed to: The equation inside the integration has the same form as the  Dirichlet distribution . According to the  Dirichlet distribution , Thus, Now we turn our attention to the  \n \n \n \n \n φ \n \n \n \n {\\displaystyle {\\boldsymbol {\\varphi }}} \n \n  part. Actually, the derivation of the  \n \n \n \n \n φ \n \n \n \n {\\displaystyle {\\boldsymbol {\\varphi }}} \n \n  part is very similar to the  \n \n \n \n \n θ \n \n \n \n {\\displaystyle {\\boldsymbol {\\theta }}} \n \n  part. Here we only list the steps of the derivation: For clarity, here we write down the final equation with both  \n \n \n \n \n ϕ \n \n \n \n {\\displaystyle {\\boldsymbol {\\phi }}} \n \n  and  \n \n \n \n \n θ \n \n \n \n {\\displaystyle {\\boldsymbol {\\theta }}} \n \n  integrated out: The goal of Gibbs Sampling here is to approximate the distribution of  \n \n \n \n P \n ( \n \n Z \n \n ∣ \n \n W \n \n ; \n α \n , \n β \n ) \n \n \n {\\displaystyle P({\\boldsymbol {Z}}\\mid {\\boldsymbol {W}};\\alpha ,\\beta )} \n \n . Since  \n \n \n \n P \n ( \n \n W \n \n ; \n α \n , \n β \n ) \n \n \n {\\displaystyle P({\\boldsymbol {W}};\\alpha ,\\beta )} \n \n  is invariable for any of Z, Gibbs Sampling equations can be derived from  \n \n \n \n P \n ( \n \n Z \n \n , \n \n W \n \n ; \n α \n , \n β \n ) \n \n \n {\\displaystyle P({\\boldsymbol {Z}},{\\boldsymbol {W}};\\alpha ,\\beta )} \n \n  directly. The key point is to derive the following conditional probability: where  \n \n \n \n \n Z \n \n ( \n m \n , \n n \n ) \n \n \n \n \n {\\displaystyle Z_{(m,n)}} \n \n  denotes the  \n \n \n \n Z \n \n \n {\\displaystyle Z} \n \n  hidden variable of the  \n \n \n \n \n n \n \n t \n h \n \n \n \n \n {\\displaystyle n^{th}} \n \n  word token in the  \n \n \n \n \n m \n \n t \n h \n \n \n \n \n {\\displaystyle m^{th}} \n \n  document. And further we assume that the word\nsymbol of it is the  \n \n \n \n \n v \n \n t \n h \n \n \n \n \n {\\displaystyle v^{th}} \n \n  word in the vocabulary.  \n \n \n \n \n \n Z \n \n − \n ( \n m \n , \n n \n ) \n \n \n \n \n \n {\\displaystyle {\\boldsymbol {Z_{-(m,n)}}}} \n \n  denotes all the  \n \n \n \n Z \n \n \n {\\displaystyle Z} \n \n s but  \n \n \n \n \n Z \n \n ( \n m \n , \n n \n ) \n \n \n \n \n {\\displaystyle Z_{(m,n)}} \n \n . Note that Gibbs Sampling needs only to sample a value for  \n \n \n \n \n Z \n \n ( \n m \n , \n n \n ) \n \n \n \n \n {\\displaystyle Z_{(m,n)}} \n \n , according to the above probability, we do not need the exact value of but the ratios among the probabilities that  \n \n \n \n \n Z \n \n ( \n m \n , \n n \n ) \n \n \n \n \n {\\displaystyle Z_{(m,n)}} \n \n  can take value. So, the above equation can be simplified as: Finally, let  \n \n \n \n \n n \n \n j \n , \n r \n \n \n i \n , \n − \n ( \n m \n , \n n \n ) \n \n \n \n \n {\\displaystyle n_{j,r}^{i,-(m,n)}} \n \n  be the same meaning as  \n \n \n \n \n n \n \n j \n , \n r \n \n \n i \n \n \n \n \n {\\displaystyle n_{j,r}^{i}} \n \n  but with the  \n \n \n \n \n Z \n \n ( \n m \n , \n n \n ) \n \n \n \n \n {\\displaystyle Z_{(m,n)}} \n \n  excluded. The above equation can be further simplified leveraging the property of  gamma function . We first split the summation and then merge it back to obtain a  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n -independent summation, which could be dropped: Note that the same formula is derived in the article on the  Dirichlet-multinomial distribution , as part of a more general discussion of integrating  Dirichlet distribution  priors out of a  Bayesian network . Topic modeling is a classic solution to the problem of  information retrieval  using linked data and semantic web technology. [ 17 ]  Related models and techniques are, among others,  latent semantic indexing ,  independent component analysis ,  probabilistic latent semantic indexing ,  non-negative matrix factorization , and  Gamma-Poisson distribution . The LDA model is highly modular and can therefore be easily extended. The main field of interest is modeling relations between topics. This is achieved by using another distribution on the simplex instead of the Dirichlet. The Correlated Topic Model [ 18 ]  follows this approach, inducing a correlation structure between topics by using the  logistic normal distribution  instead of the Dirichlet. Another extension is the hierarchical LDA (hLDA), [ 19 ]  where topics are joined together in a hierarchy by using the nested  Chinese restaurant process , whose structure is learnt from data. LDA can also be extended to a corpus in which a document includes two types of information (e.g., words and names), as in the  LDA-dual model . [ 20 ] \nNonparametric extensions of LDA include the  hierarchical Dirichlet process  mixture model, which allows the number of topics to be unbounded and learnt from data. As noted earlier, pLSA is similar to LDA. The LDA model is essentially the Bayesian version of pLSA model. The Bayesian formulation tends to perform better on small datasets because Bayesian methods can avoid overfitting the data. For very large datasets, the results of the two models tend to converge. One difference is that pLSA uses a variable  \n \n \n \n d \n \n \n {\\displaystyle d} \n \n  to represent a document in the training set. So in pLSA, when presented with a document the model has not seen before, we fix  \n \n \n \n Pr \n ( \n w \n ∣ \n z \n ) \n \n \n {\\displaystyle \\Pr(w\\mid z)} \n \n —the probability of words under topics—to be that learned from the training set and use the same EM algorithm to infer  \n \n \n \n Pr \n ( \n z \n ∣ \n d \n ) \n \n \n {\\displaystyle \\Pr(z\\mid d)} \n \n —the topic distribution under  \n \n \n \n d \n \n \n {\\displaystyle d} \n \n . Blei argues that this step is cheating because you are essentially refitting the model to the new data. In evolutionary biology, it is often natural to assume that the geographic locations of the individuals observed bring some information about their ancestry. This is the rational of various models for geo-referenced genetic data. [ 14 ] [ 21 ] Variations on LDA have been used to automatically put natural images into categories, such as \"bedroom\" or \"forest\", by treating an image as a document, and small patches of the image as words; [ 22 ]  one of the variations is called  spatial latent Dirichlet allocation . [ 23 ]"
  },
  {
    "id": 95,
    "title": "Learning to rank",
    "content": "Learning to rank [ 1 ]  or  machine-learned ranking  ( MLR ) is the application of  machine learning , typically  supervised ,  semi-supervised  or  reinforcement learning , in the construction of  ranking models  for  information retrieval  systems. [ 2 ]   Training data  may, for example, consist of lists of items with some  partial order  specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data. Ranking is a central part of many  information retrieval  problems, such as  document retrieval ,  collaborative filtering ,  sentiment analysis , and  online advertising . A possible architecture of a machine-learned search engine is shown in the accompanying figure. Training data consists of queries and documents matching them together with the relevance degree of each match. It may be prepared manually by human  assessors  (or  raters , as  Google  calls them), who check results for some queries and determine  relevance  of each result. It is not feasible to check the relevance of all documents, and so typically a technique called pooling is used — only the top few documents, retrieved by some existing ranking models are checked. This technique may introduce selection bias. Alternatively, training data may be derived automatically by analyzing  clickthrough logs  (i.e. search results which got clicks from users), [ 3 ]   query chains , [ 4 ]  or such search engines' features as Google's (since-replaced)  SearchWiki . Clickthrough logs can be biased by the tendency of users to click on the top search results on the assumption that they are already well-ranked. Training data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries. Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used. [ 5 ]  First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the  vector space model ,  Boolean model , weighted AND, [ 6 ]  or  BM25 . This phase is called  top- \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  document retrieval  and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes. [ 7 ]  In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents. Learning to rank algorithms have been applied in areas other than information retrieval: For the convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called  feature vectors . Such an approach is sometimes called  bag of features  and is analogous to the  bag of words  model and  vector space model  used in information retrieval for representation of documents. Components of such vectors are called  features ,  factors  or  ranking signals . They may be divided into three groups (features from  document retrieval  are shown as examples): Some examples of features, which were used in the well-known  LETOR  dataset: Selecting and designing good features is an important area in machine learning, which is called  feature engineering . There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare the performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics. Examples of ranking quality measures: DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used. [ 11 ]  Other metrics such as MAP, MRR and precision, are defined only for binary judgments. Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric: Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document. Tie-Yan Liu  of Microsoft Research Asia has analyzed existing algorithms for learning to rank problems in his book  Learning to Rank for Information Retrieval . [ 1 ]  He categorized them into three groups by their input spaces, output spaces, hypothesis spaces (the core function of the model) and  loss functions : the pointwise, pairwise, and listwise approach. In practice, listwise approaches often outperform pairwise approaches and pointwise approaches. This statement was further supported by a large scale experiment on the performance of different learning-to-rank methods on a large collection of benchmark data sets. [ 14 ] In this section, without further notice,  \n \n \n \n x \n \n \n {\\displaystyle x} \n \n  denotes an object to be evaluated, for example, a document or an image,  \n \n \n \n f \n ( \n x \n ) \n \n \n {\\displaystyle f(x)} \n \n  denotes a single-value hypothesis,  \n \n \n \n h \n ( \n ⋅ \n ) \n \n \n {\\displaystyle h(\\cdot )} \n \n  denotes a bi-variate or multi-variate function and  \n \n \n \n L \n ( \n ⋅ \n ) \n \n \n {\\displaystyle L(\\cdot )} \n \n  denotes the loss function. In this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score. Formally speaking, the pointwise approach aims at learning a function  \n \n \n \n f \n ( \n x \n ) \n \n \n {\\displaystyle f(x)} \n \n  predicting the real-value or ordinal score of a document  \n \n \n \n x \n \n \n {\\displaystyle x} \n \n  using the loss function  \n \n \n \n L \n ( \n f \n ; \n \n x \n \n j \n \n \n , \n \n y \n \n j \n \n \n ) \n \n \n {\\displaystyle L(f;x_{j},y_{j})} \n \n . A number of existing  supervised  machine learning algorithms can be readily used for this purpose.  Ordinal regression  and  classification  algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values. In this case, the learning-to-rank problem is approximated by a classification problem — learning a  binary classifier   \n \n \n \n h \n ( \n \n x \n \n u \n \n \n , \n \n x \n \n v \n \n \n ) \n \n \n {\\displaystyle h(x_{u},x_{v})} \n \n  that can tell which document is better in a given pair of documents. The classifier shall take two documents as its input and the goal is to minimize a loss function  \n \n \n \n L \n ( \n h \n ; \n \n x \n \n u \n \n \n , \n \n x \n \n v \n \n \n , \n \n y \n \n u \n , \n v \n \n \n ) \n \n \n {\\displaystyle L(h;x_{u},x_{v},y_{u,v})} \n \n . The loss function typically reflects the number and magnitude of  inversions  in the induced ranking. In many cases, the binary classifier  \n \n \n \n h \n ( \n \n x \n \n u \n \n \n , \n \n x \n \n v \n \n \n ) \n \n \n {\\displaystyle h(x_{u},x_{v})} \n \n  is implemented with a scoring function  \n \n \n \n f \n ( \n x \n ) \n \n \n {\\displaystyle f(x)} \n \n . As an example, RankNet  [ 15 ]  adapts a probability model and defines  \n \n \n \n h \n ( \n \n x \n \n u \n \n \n , \n \n x \n \n v \n \n \n ) \n \n \n {\\displaystyle h(x_{u},x_{v})} \n \n  as the estimated probability of the document  \n \n \n \n \n x \n \n u \n \n \n \n \n {\\displaystyle x_{u}} \n \n  has higher quality than  \n \n \n \n \n x \n \n v \n \n \n \n \n {\\displaystyle x_{v}} \n \n : where  \n \n \n \n \n CDF \n \n ( \n ⋅ \n ) \n \n \n {\\displaystyle {\\text{CDF}}(\\cdot )} \n \n  is a  cumulative distribution function , for example, the  standard logistic CDF , i.e. These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is often difficult in practice because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used. For example the SoftRank algorithm. [ 16 ]  LambdaMART is a pairwise algorithm which has been empirically shown to approximate listwise objective functions. [ 17 ] A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method: Regularized least-squares based ranking. The work is extended in\n [ 26 ]  to learning to rank from general preference graphs. Note: as most supervised learning-to-rank algorithms can be applied to pointwise, pairwise and listwise case, only those methods which are specifically designed with ranking in mind are shown above. Norbert Fuhr  introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation; [ 49 ]  a specific variant of this approach (using  polynomial regression ) had been published by him three years earlier. [ 18 ]  Bill Cooper proposed  logistic regression  for the same purpose in 1992  [ 19 ]  and used it with his  Berkeley  research group to train a successful ranking function for  TREC . Manning et al. [ 50 ]  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques. Several conferences, such as  NeurIPS ,  SIGIR  and  ICML  have had workshops devoted to the learning-to-rank problem since the mid-2000s (decade). Commercial  web search engines  began using machine-learned ranking systems since the 2000s (decade). One of the first search engines to start using it was  AltaVista  (later its technology was acquired by  Overture , and then  Yahoo ), which launched a  gradient boosting -trained ranking function in April 2003. [ 51 ] [ 52 ] Bing 's search is said to be powered by  RankNet  algorithm, [ 53 ] [ when? ]  which was invented at  Microsoft Research  in 2005. In November 2009 a Russian search engine  Yandex  announced [ 54 ]  that it had significantly increased its search quality due to deployment of a new proprietary  MatrixNet  algorithm, a variant of  gradient boosting  method which uses oblivious decision trees. [ 55 ]  Recently they have also sponsored a machine-learned ranking competition \"Internet Mathematics 2009\" [ 56 ]  based on their own search engine's production data. Yahoo has announced a similar competition in 2010. [ 57 ] As of 2008,  Google 's  Peter Norvig  denied that their search engine exclusively relies on machine-learned ranking. [ 58 ]   Cuil 's CEO, Tom Costello, suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models \"learn what people say they like, not what people actually like\". [ 59 ] In January 2017, the technology was included in the  open source  search engine  Apache Solr . [ 60 ]  It is also available in the open source  OpenSearch  and the  source-available   Elasticsearch . [ 61 ] [ 62 ]  These implementations make learning to rank widely accessible for enterprise search. Similar to recognition applications in  computer vision , recent neural network based ranking algorithms are also found to be susceptible to covert  adversarial attacks , both on the candidates and the queries. [ 63 ]  With small perturbations imperceptible to human beings, ranking order could be arbitrarily altered. In addition, model-agnostic transferable adversarial examples are found to be possible, which enables black-box adversarial attacks on deep ranking systems without requiring access to their underlying implementations. [ 63 ] [ 64 ] Conversely, the robustness of such ranking systems can be improved via adversarial defenses such as the Madry defense. [ 65 ]"
  },
  {
    "id": 96,
    "title": "Orthogonality",
    "content": "In  mathematics ,  orthogonality  is the generalization of the geometric notion of  perpendicularity . Whereas  perpendicular  is typically followed by  to  when relating two lines to one another (e.g., \"line A is perpendicular to line B\"), [ 1 ]   orthogonal  is commonly used without  to  (e.g., \"orthogonal lines A and B\"). [ 2 ] Orthogonality  is also used with various meanings that are often weakly related or not related at all with the mathematical meanings. The word comes from the  Ancient Greek   ὀρθός  ( orthós ), meaning \"upright\", [ 3 ]  and  γωνία  ( gōnía ), meaning \"angle\". [ 4 ] The Ancient Greek  ὀρθογώνιον  ( orthogṓnion ) and  Classical Latin   orthogonium  originally denoted a  rectangle . [ 5 ]  Later, they came to mean a  right triangle . In the 12th century, the post-classical Latin word  orthogonalis  came to mean a right angle or something related to a right angle. [ 6 ] In  mathematics ,  orthogonality  is the generalization of the geometric notion of  perpendicularity  to the  linear algebra  of  bilinear forms . Two elements  u  and  v  of a  vector space  with bilinear form  \n \n \n \n B \n \n \n {\\displaystyle B} \n \n  are orthogonal when  \n \n \n \n B \n ( \n \n u \n \n , \n \n v \n \n ) \n = \n 0 \n \n \n {\\displaystyle B(\\mathbf {u} ,\\mathbf {v} )=0} \n \n .  Depending on the bilinear form, the vector space may contain  null vectors , non-zero self-orthogonal vectors, in which case perpendicularity is replaced with  hyperbolic orthogonality . In the case of  function spaces , families of functions are used to form an orthogonal  basis , such as in the contexts of  orthogonal polynomials ,  orthogonal functions , and  combinatorics . In  optics ,  polarization  states are said to be orthogonal when they propagate independently of each other, as in vertical and horizontal  linear polarization  or right- and left-handed  circular polarization . In  special relativity , a time axis determined by a  rapidity  of motion is  hyperbolic-orthogonal  to a space axis of simultaneous events, also determined by the rapidity. The theory features  relativity of simultaneity . In  quantum mechanics , a sufficient (but not necessary) condition that two  eigenstates  of a  Hermitian operator ,  \n \n \n \n \n ψ \n \n m \n \n \n \n \n {\\displaystyle \\psi _{m}} \n \n  and  \n \n \n \n \n ψ \n \n n \n \n \n \n \n {\\displaystyle \\psi _{n}} \n \n , are orthogonal is that they correspond to different eigenvalues. This means, in  Dirac notation , that  \n \n \n \n ⟨ \n \n ψ \n \n m \n \n \n \n | \n \n \n ψ \n \n n \n \n \n ⟩ \n = \n 0 \n \n \n {\\displaystyle \\langle \\psi _{m}|\\psi _{n}\\rangle =0} \n \n  if  \n \n \n \n \n ψ \n \n m \n \n \n \n \n {\\displaystyle \\psi _{m}} \n \n  and  \n \n \n \n \n ψ \n \n n \n \n \n \n \n {\\displaystyle \\psi _{n}} \n \n  correspond to different eigenvalues. This follows from the fact that  Schrödinger's equation  is a  Sturm–Liouville  equation (in Schrödinger's formulation) or that observables are given by Hermitian operators (in Heisenberg's formulation). [ citation needed ] In art, the  perspective  (imaginary) lines pointing to the  vanishing point  are referred to as \"orthogonal lines\". The term \"orthogonal line\" often has a quite different meaning in the literature of modern art criticism. Many works by painters such as  Piet Mondrian  and  Burgoyne Diller  are noted for their exclusive use of \"orthogonal lines\" — not, however, with reference to perspective, but rather referring to lines that are straight and exclusively horizontal or vertical, forming right angles where they intersect. For example, an essay at the  web site  of the  Thyssen-Bornemisza Museum  states that \"Mondrian ... dedicated his entire oeuvre to the investigation of the balance between orthogonal lines and primary colours.\"  Archived  2009-01-31 at the  Wayback Machine Orthogonality in programming language design is the ability to use various language features in arbitrary combinations with consistent results. [ 8 ]  This usage was introduced by  Van Wijngaarden  in the design of  Algol 68 : The number of independent primitive concepts has been minimized in order that the language be easy to describe, to learn, and to implement. On the other hand, these concepts have been applied “orthogonally” in order to maximize the expressive power of the language while trying to avoid deleterious superfluities. [ 9 ] Orthogonality is a system design property which guarantees that modifying the technical effect produced by a component of a system neither creates nor propagates side effects to other components of the system. Typically this is achieved through the  separation of concerns  and  encapsulation , and it is essential for feasible and compact designs of complex systems. The emergent behavior of a system consisting of components should be controlled strictly by formal definitions of its logic and not by side effects resulting from poor integration, i.e., non-orthogonal design of modules and interfaces. Orthogonality reduces testing and development time because it is easier to verify designs that neither cause side effects nor depend on them. An  instruction set  is said to be orthogonal if it lacks redundancy (i.e., there is only a single instruction that can be used to accomplish a given task) [ 10 ]  and is designed such that instructions can use any  register  in any  addressing mode . This terminology results from considering an instruction as a vector whose components are the instruction fields.  One field identifies the registers to be operated upon and another specifies the addressing mode. An  orthogonal instruction set  uniquely encodes all combinations of registers and addressing modes. [ 11 ] In  telecommunications ,  multiple access  schemes are orthogonal when an ideal receiver can completely reject arbitrarily strong unwanted signals from the desired signal using different  basis functions . One such scheme is  time-division multiple access  (TDMA), where the orthogonal basis functions are nonoverlapping rectangular pulses (\"time slots\"). Another scheme is  orthogonal frequency-division multiplexing  (OFDM), which refers to the use, by a single transmitter, of a set of frequency multiplexed signals with the exact minimum frequency spacing needed to make them orthogonal so that they do not interfere with each other.  Well known examples include ( a ,  g , and  n ) versions of  802.11   Wi-Fi ;  WiMAX ;  ITU-T   G.hn ,  DVB-T , the terrestrial digital TV broadcast system used in most of the world outside North America; and DMT (Discrete Multi Tone), the standard form of  ADSL . In OFDM, the  subcarrier  frequencies are chosen [ how? ]  so that the subcarriers are orthogonal to each other, meaning that crosstalk between the subchannels is eliminated and intercarrier guard bands are not required. This greatly simplifies the design of both the transmitter and the receiver. In conventional FDM, a separate filter for each subchannel is required. When performing statistical analysis,  independent variables  that affect a particular  dependent variable  are said to be orthogonal if they are uncorrelated, [ 12 ]  since the covariance forms an inner product. In this case the same results are obtained for the effect of any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables  individually with  simple regression  or simultaneously with  multiple regression . If  correlation  is present, the factors are not orthogonal and different results are obtained by the two methods. This usage arises from the fact that if centered by subtracting the  expected value  (the mean), uncorrelated variables are orthogonal in the geometric sense discussed above, both as observed data (i.e., vectors) and as random variables (i.e., density functions).\nOne  econometric  formalism that is alternative to the  maximum likelihood  framework, the  Generalized Method of Moments , relies on orthogonality conditions. In particular, the  Ordinary Least Squares  estimator may be easily derived from an orthogonality condition between the explanatory variables and model residuals. In  taxonomy , an orthogonal classification is one in which no item is a member of more than one group, that is, the classifications are mutually exclusive. In chemistry and biochemistry, an orthogonal interaction occurs when there are two pairs of substances and each substance can interact with their respective partner, but does not interact with either substance of the other pair. For example,  DNA  has two orthogonal pairs: cytosine and guanine form a base-pair, and adenine and thymine form another base-pair, but other base-pair combinations are strongly disfavored. As a chemical example, tetrazine reacts with transcyclooctene and azide reacts with cyclooctyne without any cross-reaction, so these are mutually orthogonal reactions, and so, can be performed simultaneously and selectively. [ 13 ] In  organic synthesis ,  orthogonal protection  is a strategy allowing the deprotection of  functional groups  independently of each other. In  supramolecular chemistry  the notion of orthogonality refers to the possibility of two or more supramolecular, often  non-covalent , interactions being compatible; reversibly forming without interference from the other. In  analytical chemistry , analyses are \"orthogonal\" if they make a measurement or identification in completely different ways, thus increasing the reliability of the measurement. Orthogonal testing thus can be viewed as \"cross-checking\" of results, and the \"cross\" notion corresponds to the  etymologic origin of  orthogonality . Orthogonal testing is often required as a part of a  new drug application . In the field of system reliability orthogonal redundancy is that form of redundancy where the form of backup device or method is completely different from the prone to error device or method. The failure mode of an orthogonally redundant back-up device or method does not intersect with and is completely different from the failure mode of the device or method in need of redundancy to safeguard the total system against catastrophic failure. In  neuroscience , a sensory map in the brain which has overlapping stimulus coding (e.g. location and quality) is called an orthogonal map. In  philosophy , two topics, authors, or pieces of writing are said to be \"orthogonal\" to each other when they do not substantively cover what could be considered potentially overlapping or competing claims.  Thus, texts in philosophy can either support and complement one another, they can offer competing explanations or systems, or they can be orthogonal to each other in cases where the scope, content, and purpose of the pieces of writing are entirely unrelated. In board games such as  chess  which feature a grid of squares, 'orthogonal' is used to mean \"in the same row/'rank' or column/'file'\".  This is the counterpart to squares which are \"diagonally adjacent\". [ 27 ]  In the ancient Chinese board game  Go  a player can capture the stones of an opponent by occupying all orthogonally adjacent points. Stereo vinyl records encode both the left and right stereo channels in a single groove.  The V-shaped groove in the vinyl has walls that are 90 degrees to each other, with variations in each wall separately encoding one of the two analogue channels that make up the stereo signal.  The cartridge senses the motion of the stylus following the groove in two orthogonal directions: 45 degrees from vertical to either side. [ 28 ]   A pure horizontal motion corresponds to a mono signal, equivalent to a stereo signal in which both channels carry identical (in-phase) signals."
  },
  {
    "id": 97,
    "title": "Independence (mathematical logic)",
    "content": "In  mathematical logic ,  independence  is the unprovability of some specific  sentence  from some specific set of other sentences. The sentences in this set are referred to as \"axioms\". A  sentence  σ is  independent  of a given  first-order theory   T  if  T  neither proves nor refutes σ; that is, it is impossible to prove σ from  T , and it is also impossible to prove from  T  that σ is false. Sometimes, σ is said (synonymously) to be  undecidable  from  T . (This concept is unrelated to the idea of \" decidability \" as in a  decision problem .) A theory  T  is  independent  if no axiom in  T  is provable from the remaining axioms in  T . A theory for which there is an independent set of axioms is  independently axiomatizable . Some authors say that σ is independent of  T  when  T  simply cannot prove σ, and do not necessarily assert by this that  T  cannot refute σ. These authors will sometimes say \"σ is independent of and consistent with  T \" to indicate that  T  can neither prove nor refute σ. Many interesting statements in set theory are independent of  Zermelo–Fraenkel set theory  (ZF). The following statements in set theory are known to be independent of ZF, under the assumption that ZF is consistent: The following statements (none of which have been proved false) cannot be proved in ZFC (the Zermelo–Fraenkel set theory plus the axiom of choice) to be independent of ZFC, under the added hypothesis that ZFC is consistent. The following statements are inconsistent with the axiom of choice, and therefore with ZFC. However they are probably independent of ZF, in a corresponding sense to the above: They cannot be proved in ZF, and few working set theorists expect to find a refutation in ZF. However ZF cannot prove that they are independent of ZF, even with the added hypothesis that ZF is consistent. Since 2000, logical independence has become understood as having crucial significance in the foundations of physics. [ 1 ] [ 2 ]"
  },
  {
    "id": 98,
    "title": "Dimensionality reduction",
    "content": "Dimensionality reduction , or  dimension reduction , is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its  intrinsic dimension . Working in high-dimensional spaces can be undesirable for many reasons; raw data are often  sparse  as a consequence of the  curse of dimensionality , and analyzing the data is usually  computationally intractable . Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as  signal processing ,  speech recognition ,  neuroinformatics , and  bioinformatics . [ 1 ] Methods are commonly divided into linear and nonlinear approaches. [ 1 ]  Approaches can also be divided into  feature selection  and  feature extraction . [ 2 ]  Dimensionality reduction can be used for  noise reduction ,  data visualization ,  cluster analysis , or as an intermediate step to facilitate other analyses. The process of  feature selection  aims to find a suitable subset of the input variables ( features , or  attributes ) for the task at hand. The three strategies are: the  filter  strategy (e.g.,  information gain ), the  wrapper  strategy (e.g., accuracy-guided search), and the  embedded  strategy (features are added or removed while building the model based on prediction errors). Data analysis  such as  regression  or  classification  can be done in the reduced space more accurately than in the original space. [ 3 ] Feature projection (also called feature extraction) transforms the data from the  high-dimensional space  to a space of fewer dimensions. The data transformation may be linear, as in  principal component analysis  (PCA), but many  nonlinear dimensionality reduction  techniques also exist. [ 4 ] [ 5 ]  For multidimensional data,  tensor representation  can be used in dimensionality reduction through  multilinear subspace learning . [ 6 ] The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the  covariance  (and sometimes the  correlation )  matrix  of the data is constructed and the  eigenvectors  on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proved on a case-by-case basis as not all systems exhibit this behavior. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.  [ citation needed ] NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist, [ 7 ] [ 8 ]  such as astronomy. [ 9 ] [ 10 ]  NMF is well known since the multiplicative update rule by Lee & Seung, [ 7 ]  which has been continuously developed: the inclusion of uncertainties, [ 9 ]  the consideration of missing data and parallel computation, [ 11 ]  sequential construction [ 11 ]  which leads to the stability and linearity of NMF, [ 10 ]  as well as other  updates  including handling missing data in  digital image processing . [ 12 ] With a stable component basis during construction, and a linear modeling process,  sequential NMF [ 11 ]  is able to preserve the flux in direct imaging of circumstellar structures in astronomy, [ 10 ]  as one of the  methods of detecting exoplanets , especially for the direct imaging of  circumstellar discs . In comparison with PCA, NMF does not remove the mean of the matrices, which leads to physical non-negative fluxes; therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al. [ 10 ] Principal component analysis can be employed in a nonlinear way by means of the  kernel trick . The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is called  kernel PCA . Other prominent nonlinear techniques include  manifold learning  techniques such as  Isomap ,  locally linear embedding  (LLE), [ 13 ]  Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis. [ 14 ]  These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA. More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using  semidefinite programming . The most prominent example of such a technique is  maximum variance unfolding  (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space) while maximizing the distances between points that are not nearest neighbors. An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical  multidimensional scaling , which is identical to PCA;  Isomap , which uses geodesic distances in the data space;  diffusion maps , which use diffusion distances in the data space;  t-distributed stochastic neighbor embedding  (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis. A different approach to nonlinear dimensionality reduction is through the use of  autoencoders , a special kind of  feedforward neural networks  with a bottleneck hidden layer. [ 15 ]  The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of  restricted Boltzmann machines ) that is followed by a finetuning stage based on  backpropagation . Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the  support-vector machines  (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space. [ 16 ] [ 17 ]  Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter. Autoencoders can be used to learn nonlinear dimension reduction functions and codings together with an inverse function from the coding to the original representation. T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique useful for the visualization of high-dimensional datasets. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well. [ 18 ] Uniform manifold approximation and projection  (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a  locally connected   Riemannian manifold  and that the  Riemannian metric  is locally constant or approximately locally constant. For high-dimensional datasets, dimension reduction is usually performed prior to applying a  k -nearest neighbors  ( k -NN) algorithm in order to mitigate the  curse of dimensionality . [ 19 ] Feature extraction  and dimension reduction can be combined in one step, using  principal component analysis  (PCA),  linear discriminant analysis  (LDA),  canonical correlation analysis  (CCA), or  non-negative matrix factorization  (NMF) techniques to pre-process the data, followed by clustering via  k -NN on  feature vectors  in a reduced-dimension space. In  machine learning , this process is also called low-dimensional  embedding . [ 20 ] For high-dimensional datasets (e.g., when performing similarity search on live video streams, DNA data, or high-dimensional  time series ), running a fast  approximate   k -NN search using  locality-sensitive hashing ,  random projection , [ 21 ]  \"sketches\", [ 22 ]  or other high-dimensional similarity search techniques from the  VLDB conference  toolbox may be the only feasible option. A dimensionality reduction technique that is sometimes used in  neuroscience  is  maximally informative dimensions , [ citation needed ]  which finds a lower-dimensional representation of a dataset such that as much  information  as possible about the original data is preserved."
  },
  {
    "id": 99,
    "title": "Co-occurrence",
    "content": "In linguistics,  co-occurrence  or  cooccurrence  is an above-chance frequency of ordered  occurrence  of two adjacent  terms  in a  text corpus . Co-occurrence in this  linguistic  sense can be interpreted as an indicator of  semantic proximity  or an  idiomatic  expression. Corpus linguistics and its statistic analyses reveal patterns of co-occurrences within a language and enable to work out typical  collocations  for its lexical items. A  co-occurrence restriction  is identified when linguistic elements never occur together. Analysis of these restrictions can lead to discoveries about the  structure  and development of a language. [ 1 ] Co-occurrence can be seen an extension of  word counting  in higher dimensions. Co-occurrence can be quantitatively described using measures like  correlation  or  mutual information . This  linguistics  article is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 100,
    "title": "Evaluation measures (information retrieval)",
    "content": "Evaluation measures  for an  information retrieval  (IR) system assess how well an index, search engine, or database returns results from a collection of resources that satisfy a user's query. They are therefore fundamental to the success of information systems and digital platforms. The most important factor in determining a system's effectiveness for users is the overall relevance of results retrieved in response to a query. [ 1 ]  The success of an IR system may be judged by a range of criteria including relevance, speed, user satisfaction, usability, efficiency and reliability. [ 2 ]  Evaluation measures may be categorised in various ways including offline or online, user-based or system-based and include methods such as observed user behaviour, test collections, precision and recall, and scores from prepared benchmark test sets. Evaluation for an information retrieval system should also include a validation of the measures used, i.e. an assessment of how well they measure what they are intended to measure and how well the system fits its intended use case. [ 3 ]  Measures are generally used in two settings: online experimentation, which assesses users' interactions with the search system, and offline evaluation, which measures the effectiveness of an information retrieval system on a static offline collection. Indexing and classification methods to assist with information retrieval have a long history dating back to the earliest libraries and collections however  systematic evaluation of their effectiveness began in earnest in the 1950s with the rapid expansion in research production across military, government and education and the introduction of computerised catalogues. At this time there were a number of different indexing, classification and cataloguing systems in operation which were expensive to produce and it was unclear which was the most effective. [ 4 ] Cyril Cleverdon , Librarian of the College of Aeronautics, Cranfield, England, began a  series of experiments  of print indexing and retrieval methods in what is known as the Cranfield paradigm, or Cranfield tests, which set the standard for IR evaluation measures for many years. [ 4 ]  Cleverdon developed a test called ‘known-item searching’ - to check whether an IR system returned the documents that were known to be relevant or correct for a given search. Cleverdon’s experiments established a number of key aspects required for IR evaluation: a test collection, a set of queries and a set of pre-determined relevant items which combined would determine precision and recall. Cleverdon's approach formed a blueprint for the successful  Text Retrieval Conference  series that began in 1992. Evaluation of IR systems is central to the success of any search engine including internet search, website search, databases and library catalogues. Evaluations measures are used in studies of  information behaviour ,  usability testing , business costs and efficiency assessments. Measuring the effectiveness of IR systems has been the main focus of IR research, based on test collections combined with evaluation measures. [ 5 ]  A number of academic conferences have been established that focus specifically on evaluation measures including the Text Retrieval Conference (TREC), Conference and Labs of the Evaluation Forum (CLEF) and NTCIR. Online metrics are generally created from search logs. The metrics are often used to determine the success of an  A/B test . Session abandonment rate is a ratio of search sessions which do not result in a click. Click-through rate  (CTR) is the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an  online advertising  campaign for a particular website as well as the effectiveness of email campaigns. [ 6 ] Session success rate measures the ratio of user sessions that lead to a success. Defining \"success\" is often dependent on context, but for search a successful result is often measured using  dwell time  as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet. Zero result rate  ( ZRR ) is the ratio of Search Engine Results Pages (SERPs) which returned with zero results. The metric either indicates a  recall  issue, or that the information being searched for is not in the index. Offline metrics are generally created from relevance judgment sessions where the judges score the quality of the search results. Both binary (relevant/non-relevant) and multi-level (e.g., relevance from 0 to 5) scales can be used to score each document returned in response to a query. In practice, queries may be  ill-posed , and there may be different shades of relevance. For instance, there is ambiguity in the query \"mars\": the judge does not know if the user is searching for the planet  Mars , the  Mars  chocolate bar, the singer  Bruno Mars , or   the Roman deity Mars . Precision is the fraction of the documents retrieved that are  relevant  to the user's information need. In  binary classification , precision is analogous to  positive predictive value . Precision takes all retrieved documents into account. It can also be evaluated considering only the topmost results returned by the system using  Precision@k . Note that the meaning and usage of \"precision\" in the field of information retrieval differs from the definition of  accuracy and precision  within other branches of science and  statistics . Recall is the fraction of the documents that are relevant to the query that are successfully retrieved. In binary classification, recall is often called  sensitivity . So it can be looked at as  the probability that a relevant document is retrieved by the query . It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision. The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available: In binary classification, fall-out is the opposite of  specificity  and is equal to  \n \n \n \n ( \n 1 \n − \n \n \n specificity \n \n \n ) \n \n \n {\\displaystyle (1-{\\mbox{specificity}})} \n \n . It can be looked at as  the probability that a non-relevant document is retrieved by the query . It is trivial to achieve fall-out of 0% by returning zero documents in response to any query. The weighted  harmonic mean  of precision and recall, the traditional F-measure or balanced F-score is: This is also known as the  \n \n \n \n \n F \n \n 1 \n \n \n \n \n {\\displaystyle F_{1}} \n \n  measure, because recall and precision are evenly weighted. The general formula for non-negative real  \n \n \n \n β \n \n \n {\\displaystyle \\beta } \n \n  is: Two other commonly used F measures are the  \n \n \n \n \n F \n \n 2 \n \n \n \n \n {\\displaystyle F_{2}} \n \n  measure, which weights recall twice as much as precision, and the  \n \n \n \n \n F \n \n 0.5 \n \n \n \n \n {\\displaystyle F_{0.5}} \n \n  measure, which weights precision twice as much as recall. The F-measure was derived by  van Rijsbergen  (1979) so that  \n \n \n \n \n F \n \n β \n \n \n \n \n {\\displaystyle F_{\\beta }} \n \n  \"measures the effectiveness of retrieval with respect to a user who attaches  \n \n \n \n β \n \n \n {\\displaystyle \\beta } \n \n  times as much importance to recall as precision\".  It is based on van Rijsbergen's effectiveness measure  \n \n \n \n E \n = \n 1 \n − \n \n \n 1 \n \n \n \n α \n P \n \n \n + \n \n \n \n 1 \n − \n α \n \n R \n \n \n \n \n \n \n \n {\\displaystyle E=1-{\\frac {1}{{\\frac {\\alpha }{P}}+{\\frac {1-\\alpha }{R}}}}} \n \n .  Their relationship is: Since F-measure combines information from both precision and recall it is a way to represent overall performance without presenting two numbers. Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision  \n \n \n \n p \n ( \n r \n ) \n \n \n {\\displaystyle p(r)} \n \n  as a function of recall  \n \n \n \n r \n \n \n {\\displaystyle r} \n \n . Average precision computes the average value of  \n \n \n \n p \n ( \n r \n ) \n \n \n {\\displaystyle p(r)} \n \n  over the interval from  \n \n \n \n r \n = \n 0 \n \n \n {\\displaystyle r=0} \n \n  to  \n \n \n \n r \n = \n 1 \n \n \n {\\displaystyle r=1} \n \n : [ 7 ] That is the area under the precision-recall curve.\nThis integral is in practice replaced with a finite sum over every position in the ranked sequence of documents: where  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  is the rank in the sequence of retrieved documents,  \n \n \n \n n \n \n \n {\\displaystyle n} \n \n  is the number of retrieved documents,  \n \n \n \n P \n ( \n k \n ) \n \n \n {\\displaystyle P(k)} \n \n  is the precision at cut-off  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  in the list, and  \n \n \n \n Δ \n r \n ( \n k \n ) \n \n \n {\\displaystyle \\Delta r(k)} \n \n  is the change in recall from items  \n \n \n \n k \n − \n 1 \n \n \n {\\displaystyle k-1} \n \n  to  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n . [ 7 ] This finite sum is equivalent to: where  \n \n \n \n rel \n ⁡ \n ( \n k \n ) \n \n \n {\\displaystyle \\operatorname {rel} (k)} \n \n  is an indicator function equaling 1 if the item at rank  \n \n \n \n k \n \n \n {\\displaystyle k} \n \n  is a relevant document, zero otherwise. [ 8 ]  Note that the average is over relevant documents in top-k retrieved documents and the relevant documents not retrieved get a precision score of zero. Some authors choose to interpolate the  \n \n \n \n p \n ( \n r \n ) \n \n \n {\\displaystyle p(r)} \n \n  function to reduce the impact of \"wiggles\" in the curve. [ 9 ] [ 10 ]  For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) until 2010 [ 11 ]  computed the average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}: [ 9 ] [ 10 ] where  \n \n \n \n \n p \n \n interp \n \n \n ( \n r \n ) \n \n \n {\\displaystyle p_{\\operatorname {interp} }(r)} \n \n  is an interpolated precision that takes the maximum precision over all recalls greater than  \n \n \n \n r \n \n \n {\\displaystyle r} \n \n : An alternative is to derive an analytical  \n \n \n \n p \n ( \n r \n ) \n \n \n {\\displaystyle p(r)} \n \n  function by assuming a particular parametric distribution for the underlying decision values. For example, a  binormal precision-recall curve  can be obtained by assuming decision values in both classes to follow a Gaussian distribution. [ 12 ] The minimum achievable AveP for a given classification task is given by: 1 \n \n n \n \n p \n o \n s \n \n \n \n \n \n ∑ \n \n k \n = \n 1 \n \n \n \n n \n \n p \n o \n s \n \n \n \n \n \n \n k \n \n k \n + \n \n n \n \n n \n e \n g \n \n \n \n \n \n \n \n {\\displaystyle {\\frac {1}{n_{pos}}}\\sum _{k=1}^{n_{pos}}{\\frac {k}{k+n_{neg}}}} \n \n [ 13 ] For modern (web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them.  Precision  at k documents (P@k) is still a useful metric (e.g., P@10 or \"Precision at 10\" corresponds to the number of relevant results among the top 10 retrieved documents), but fails to take into account the positions of the relevant documents among the top k. [ 14 ]   Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1. [ 15 ]   It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not. R-precision requires knowing all documents that are relevant to a query.  The number of relevant documents,  \n \n \n \n R \n \n \n {\\displaystyle R} \n \n , is used as the cutoff for calculation, and this varies from query to query.  For example, if there are 15 documents relevant to \"red\" in a corpus (R=15), R-precision for \"red\" looks at the top 15 documents returned, counts the number that are relevant  \n \n \n \n r \n \n \n {\\displaystyle r} \n \n  turns that into a relevancy fraction:  \n \n \n \n r \n \n / \n \n R \n = \n r \n \n / \n \n 15 \n \n \n {\\displaystyle r/R=r/15} \n \n . [ 16 ] Note that the R-Precision is equivalent to both the precision at the  \n \n \n \n R \n \n \n {\\displaystyle R} \n \n -th position (P@ \n \n \n \n R \n \n \n {\\displaystyle R} \n \n ) and the recall at the  \n \n \n \n R \n \n \n {\\displaystyle R} \n \n -th position. [ 15 ] Empirically, this measure is often highly correlated to mean average precision. [ 15 ] Mean average precision (MAP) for a set of queries is the mean of the  average precision  scores for each query. where  Q  is the number of queries. DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The DCG accumulated at a particular rank position  \n \n \n \n p \n \n \n {\\displaystyle p} \n \n  is defined as: Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p ( \n \n \n \n I \n D \n C \n \n G \n \n p \n \n \n \n \n {\\displaystyle IDCG_{p}} \n \n ), which normalizes the score: The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the  \n \n \n \n D \n C \n \n G \n \n p \n \n \n \n \n {\\displaystyle DCG_{p}} \n \n  will be the same as the  \n \n \n \n I \n D \n C \n \n G \n \n p \n \n \n \n \n {\\displaystyle IDCG_{p}} \n \n  producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable. Visualizations of information retrieval performance include: Measuring how many queries are performed on the search system per (month/day/hour/minute/sec) tracks the utilization of the search system. It can be used for diagnostics to indicate an unexpected spike in queries, or simply as a baseline when comparing with other metrics, like query latency. For example, a spike in query traffic, may be used to explain a spike in query latency."
  },
  {
    "id": 101,
    "title": "Precision and recall",
    "content": "In pattern recognition, information  retrieval ,  object detection  and  classification (machine learning) ,  precision  and  recall  are performance metrics that apply to data retrieved from a  collection ,  corpus  or  sample space . Precision  (also called  positive predictive value ) is the fraction of relevant instances among the retrieved instances. Written as a formula: Precision \n \n = \n \n \n Relevant retrieved instances \n \n \n All  \n \n \n \n retrieved \n \n \n \n  instances \n \n \n \n \n \n \n {\\displaystyle {\\text{Precision}}={\\frac {\\text{Relevant retrieved instances}}{{\\text{All }}{\\textbf {retrieved}}{\\text{ instances}}}}} Recall  (also known as  sensitivity ) is the fraction of relevant instances that were retrieved. Written as a formula: Recall \n \n = \n \n \n Relevant retrieved instances \n \n \n All  \n \n \n \n relevant \n \n \n \n  instances \n \n \n \n \n \n \n {\\displaystyle {\\text{Recall}}={\\frac {\\text{Relevant retrieved instances}}{{\\text{All }}{\\textbf {relevant}}{\\text{ instances}}}}} Both precision and recall are therefore based on  relevance . Consider a computer program for recognizing dogs (the  relevant  element) in a digital photograph. Upon processing a picture which contains ten cats and twelve dogs, the program identifies eight dogs. Of the eight elements identified as dogs, only five actually are dogs ( true positives ), while the other three are cats ( false positives ). Seven dogs were missed ( false negatives ), and seven cats were correctly excluded ( true negatives ). The program's precision is then 5/8 (true positives / selected elements) while its recall is 5/12 (true positives / relevant elements). Adopting a  hypothesis-testing  approach, where in this case, the  null hypothesis  is that a given item is  irrelevant  (not a dog), absence of  type I and type II errors  (perfect  specificity and sensitivity ) corresponds respectively to perfect precision (no false positives) and perfect recall (no false negatives). More generally, recall is simply the complement of the type II error rate (i.e., one minus the type II error rate). Precision is related to the type I error rate, but in a slightly more complicated way, as it also depends upon the  prior distribution  of seeing a relevant vs. an irrelevant item. The above cat and dog example contained 8 − 5 = 3 type I errors (false positives) out of 10 total cats (true negatives), for a type I error rate of 3/10, and 12 − 5 = 7 type II errors (false negatives), for a type II error rate of 7/12.  Precision can be seen as a measure of quality, and recall as a measure of quantity. \nHigher precision means that an algorithm returns more relevant results than irrelevant ones, and high recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned). In a  classification  task, the precision for a class is the  number of true positives  (i.e. the number of items correctly labelled as belonging to the positive class)  divided by the total number of elements labelled as belonging to the positive class  (i.e. the sum of true positives and  false positives , which are items incorrectly labelled as belonging to the class). Recall in this context is defined as the  number of true positives divided by the total number of elements that actually belong to the positive class  (i.e. the sum of true positives and  false negatives , which are items which were not labelled as belonging to the positive class but should have been). Precision and recall are not particularly useful metrics when used in isolation. For instance, it is possible to have perfect recall by simply retrieving every single item. Likewise, it is possible to achieve perfect precision by selecting only a very small number of extremely likely items. In a classification task, a precision score of 1.0 for a class C means that every item labelled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labelled correctly) whereas a recall of 1.0 means that every item from class C was labelled as belonging to class C (but says nothing about how many items from other classes were incorrectly also labelled as belonging to class C). Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other, but context may dictate if one is more valued in a given situation: A smoke detector is generally designed to commit many Type I errors (to alert in many situations when there is no danger), because the cost of a Type II error (failing to sound an alarm during a major fire) is prohibitively high.  As such, smoke detectors are designed with recall in mind (to catch all real danger), even while giving little weight to the losses in precision (and making many false alarms).  In the other direction,  Blackstone's ratio , \"It is better that ten guilty persons escape than that one innocent suffer,\" emphasizes the costs of a Type I error (convicting an innocent person). As such, the criminal justice system is geared toward precision (not convicting innocents), even at the cost of losses in recall (letting more guilty people go free). A brain surgeon removing a cancerous tumor from a patient's brain illustrates the tradeoffs as well: The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain they remove to ensure they have extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain cells they remove to ensure they extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome). Usually, precision and recall scores are not discussed in isolation.  A  precision-recall curve  plots precision as a function of recall; usually precision will decrease as the recall increases. Alternatively, values for one measure can be compared for a fixed level at the other measure (e.g.  precision at a recall level of 0.75 ) or both are combined into a single measure. Examples of measures that are a combination of precision and recall are the  F-measure  (the weighted  harmonic mean  of precision and recall), or the  Matthews correlation coefficient , which is a  geometric mean  of the chance-corrected variants: the  regression coefficients   Informedness  (DeltaP') and  Markedness  (DeltaP). [ 1 ] [ 2 ]   Accuracy  is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence). [ 1 ]  Inverse Precision and Inverse Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  True Positive Rate  and  False Positive Rate , or equivalently Recall and 1 - Inverse Recall, are frequently plotted against each other as  ROC  curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the  contingency table , and they are easily manipulated by biasing the predictions. [ 1 ]   The first problem is 'solved' by using  Accuracy  and the second problem is 'solved' by discounting the chance component and renormalizing to  Cohen's kappa , but this no longer affords the opportunity to explore tradeoffs graphically. However,  Informedness  and  Markedness  are Kappa-like renormalizations of Recall and Precision, [ 3 ]  and their geometric mean  Matthews correlation coefficient  thus acts like a debiased F-measure. For classification tasks, the terms  true positives ,  true negatives ,  false positives , and  false negatives  compare the results of the classifier under test with trusted external judgments.  The terms  positive  and  negative  refer to the classifier's prediction (sometimes known as the  expectation ), and the terms  true  and  false  refer to whether that prediction corresponds to the external judgment (sometimes known as the  observation ). Let us define an experiment from  P  positive instances and  N  negative instances for some condition. The four outcomes can be formulated in a 2×2  contingency table  or  confusion matrix , as follows: Precision and recall are then defined as: [ 12 ] Precision \n \n \n \n \n = \n \n \n \n t \n p \n \n \n t \n p \n + \n f \n p \n \n \n \n \n \n \n \n \n Recall \n \n \n \n \n = \n \n \n \n t \n p \n \n \n t \n p \n + \n f \n n \n \n \n \n \n \n \n \n \n \n \n {\\displaystyle {\\begin{aligned}{\\text{Precision}}&={\\frac {tp}{tp+fp}}\\\\{\\text{Recall}}&={\\frac {tp}{tp+fn}}\\,\\end{aligned}}} Recall in this context is also referred to as the true positive rate or  sensitivity , and precision is also referred to as  positive predictive value  (PPV); other related measures used in classification include true negative rate and  accuracy . [ 12 ]  True negative rate is also called  specificity . True negative rate \n \n = \n \n \n \n t \n n \n \n \n t \n n \n + \n f \n p \n \n \n \n \n \n \n {\\displaystyle {\\text{True negative rate}}={\\frac {tn}{tn+fp}}\\,} Both precision and recall may be useful in cases where there is imbalanced data. However, it may be valuable to prioritize one over the other in cases where the outcome of a false positive or false negative is costly. For example, in medical diagnosis, a false positive test can lead to unnecessary treatment and expenses. In this situation, it is useful to value precision over recall. In other cases, the cost of a false negative is high. For instance, the cost of a false negative in fraud detection is high, as failing to detect a fraudulent transaction can result in significant financial loss.  [ 13 ] Precision and recall can be interpreted as (estimated)  conditional probabilities : [ 14 ] \nPrecision is given by  \n \n \n \n \n P \n \n ( \n C \n = \n P \n \n | \n \n \n \n \n C \n ^ \n \n \n \n = \n P \n ) \n \n \n {\\displaystyle \\mathbb {P} (C=P|{\\hat {C}}=P)} \n \n  while recall is given by  \n \n \n \n \n P \n \n ( \n \n \n \n C \n ^ \n \n \n \n = \n P \n \n | \n \n C \n = \n P \n ) \n \n \n {\\displaystyle \\mathbb {P} ({\\hat {C}}=P|C=P)} \n \n , [ 15 ]  where  \n \n \n \n \n \n \n C \n ^ \n \n \n \n \n \n {\\displaystyle {\\hat {C}}} \n \n  is the predicted class and  \n \n \n \n C \n \n \n {\\displaystyle C} \n \n  is the actual class (i.e.  \n \n \n \n C \n = \n P \n \n \n {\\displaystyle C=P} \n \n  means the actual class is positive). Both quantities are, therefore, connected by  Bayes' theorem . The probabilistic interpretation allows to easily derive how a no-skill classifier would perform. A no-skill classifiers is defined by the property that the joint probability  \n \n \n \n \n P \n \n ( \n C \n = \n P \n , \n \n \n \n C \n ^ \n \n \n \n = \n P \n ) \n = \n \n P \n \n ( \n C \n = \n P \n ) \n \n P \n \n ( \n \n \n \n C \n ^ \n \n \n \n = \n P \n ) \n \n \n {\\displaystyle \\mathbb {P} (C=P,{\\hat {C}}=P)=\\mathbb {P} (C=P)\\mathbb {P} ({\\hat {C}}=P)} \n \n  is just the product of the unconditional probabilites since the classification and the presence of the class are  independent . For example the precision of a no-skill classifier is simply a constant  \n \n \n \n \n P \n \n ( \n C \n = \n P \n \n | \n \n \n \n \n C \n ^ \n \n \n \n = \n P \n ) \n = \n \n \n \n \n P \n \n ( \n C \n = \n P \n , \n \n \n \n C \n ^ \n \n \n \n = \n P \n ) \n \n \n \n P \n \n ( \n \n \n \n C \n ^ \n \n \n \n = \n P \n ) \n \n \n \n = \n \n P \n \n ( \n C \n = \n P \n ) \n , \n \n \n {\\displaystyle \\mathbb {P} (C=P|{\\hat {C}}=P)={\\frac {\\mathbb {P} (C=P,{\\hat {C}}=P)}{\\mathbb {P} ({\\hat {C}}=P)}}=\\mathbb {P} (C=P),} \n \n  i.e. determined by the probability/frequency with which the class P occurs. A similar argument can be made for the recall:\n \n \n \n \n \n P \n \n ( \n \n \n \n C \n ^ \n \n \n \n = \n P \n \n | \n \n C \n = \n P \n ) \n = \n \n \n \n \n P \n \n ( \n C \n = \n P \n , \n \n \n \n C \n ^ \n \n \n \n = \n P \n ) \n \n \n \n P \n \n ( \n C \n = \n P \n ) \n \n \n \n = \n \n P \n \n ( \n \n \n \n C \n ^ \n \n \n \n = \n P \n ) \n \n \n {\\displaystyle \\mathbb {P} ({\\hat {C}}=P|C=P)={\\frac {\\mathbb {P} (C=P,{\\hat {C}}=P)}{\\mathbb {P} (C=P)}}=\\mathbb {P} ({\\hat {C}}=P)} \n \n  which is the probability for a positive classification. Accuracy \n \n = \n \n \n \n T \n P \n + \n T \n N \n \n \n T \n P \n + \n T \n N \n + \n F \n P \n + \n F \n N \n \n \n \n \n \n \n {\\displaystyle {\\text{Accuracy}}={\\frac {TP+TN}{TP+TN+FP+FN}}\\,} Accuracy can be a misleading metric for imbalanced data sets. Consider a sample with 95 negative and 5 positive values. Classifying all values as negative in this case gives 0.95 accuracy score. There are many metrics that don't suffer from this problem. For example, balanced accuracy [ 16 ]  (bACC) normalizes true positive and true negative predictions by the number of positive and negative samples, respectively, and divides their sum by two: Balanced accuracy \n \n = \n \n \n \n T \n P \n R \n + \n T \n N \n R \n \n 2 \n \n \n \n \n \n {\\displaystyle {\\text{Balanced accuracy}}={\\frac {TPR+TNR}{2}}\\,} For the previous example (95 negative and 5 positive samples), classifying all as negative gives 0.5 balanced accuracy score (the maximum bACC score is one), which is equivalent to the expected value of a random guess in a balanced data set. Balanced accuracy can serve as an overall performance metric for a model, whether or not the true labels are imbalanced in the data, assuming the cost of FN is the same as FP. The TPR and FPR are a property of a given classifier operating at a specific threshold. However, the overall number of TPs, FPs  etc  depend on the class imbalance in the data via the class ratio  \n \n \n \n r \n = \n P \n \n / \n \n N \n \n \n {\\textstyle r=P/N} \n \n . As the recall (or TPR) depends only on positive cases, it is not affected by  \n \n \n \n r \n \n \n {\\textstyle r} \n \n , but the precision is. We have that Precision \n \n = \n \n \n \n T \n P \n \n \n T \n P \n + \n F \n P \n \n \n \n = \n \n \n \n P \n ⋅ \n T \n P \n R \n \n \n P \n ⋅ \n T \n P \n R \n + \n N \n ⋅ \n F \n P \n R \n \n \n \n = \n \n \n \n T \n P \n R \n \n \n T \n P \n R \n + \n \n \n 1 \n r \n \n \n F \n P \n R \n \n \n \n . \n \n \n {\\displaystyle {\\text{Precision}}={\\frac {TP}{TP+FP}}={\\frac {P\\cdot TPR}{P\\cdot TPR+N\\cdot FPR}}={\\frac {TPR}{TPR+{\\frac {1}{r}}FPR}}.} Thus the precision has an explicit dependence on  \n \n \n \n r \n \n \n {\\textstyle r} \n \n . [ 17 ]  Starting with balanced classes at  \n \n \n \n r \n = \n 1 \n \n \n {\\textstyle r=1} \n \n  and gradually decreasing  \n \n \n \n r \n \n \n {\\textstyle r} \n \n , the corresponding precision will decrease, because the denominator increases. Another metric is the predicted positive condition rate (PPCR), which identifies the percentage of the total population that is flagged. For example, for a search engine that returns 30 results (retrieved documents) out of 1,000,000 documents, the PPCR is 0.003%. Predicted positive condition rate \n \n = \n \n \n \n T \n P \n + \n F \n P \n \n \n T \n P \n + \n F \n P \n + \n T \n N \n + \n F \n N \n \n \n \n \n \n \n {\\displaystyle {\\text{Predicted positive condition rate}}={\\frac {TP+FP}{TP+FP+TN+FN}}\\,} According to Saito and Rehmsmeier, precision-recall plots are more informative than ROC plots when evaluating binary classifiers on imbalanced data. In such scenarios, ROC plots may be visually deceptive with respect to conclusions about the reliability of classification performance. [ 18 ] Different from the above approaches, if an imbalance scaling is applied directly by weighting the confusion matrix elements, the standard metrics definitions still apply even in the case of imbalanced datasets. [ 19 ]  The weighting procedure relates the confusion matrix elements to the support set of each considered class. A measure that combines precision and recall is the  harmonic mean  of precision and recall, the traditional F-measure or balanced F-score: F \n = \n 2 \n ⋅ \n \n \n \n \n p \n r \n e \n c \n i \n s \n i \n o \n n \n \n ⋅ \n \n r \n e \n c \n a \n l \n l \n \n \n \n \n p \n r \n e \n c \n i \n s \n i \n o \n n \n \n + \n \n r \n e \n c \n a \n l \n l \n \n \n \n \n \n \n {\\displaystyle F=2\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}} This measure is approximately the average of the two when they are close, and is more generally the  harmonic mean , which, for the case of two numbers, coincides with the square of the  geometric mean  divided by the  arithmetic mean . There are several reasons that the F-score can be criticized, in particular circumstances, due to its bias as an evaluation metric. [ 1 ]  This is also known as the  \n \n \n \n \n F \n \n 1 \n \n \n \n \n {\\displaystyle F_{1}} \n \n  measure, because recall and precision are evenly weighted. It is a special case of the general  \n \n \n \n \n F \n \n β \n \n \n \n \n {\\displaystyle F_{\\beta }} \n \n  measure (for non-negative real values of  \n \n \n \n β \n \n \n {\\displaystyle \\beta } \n \n ): F \n \n β \n \n \n = \n ( \n 1 \n + \n \n β \n \n 2 \n \n \n ) \n ⋅ \n \n \n \n \n p \n r \n e \n c \n i \n s \n i \n o \n n \n \n ⋅ \n \n r \n e \n c \n a \n l \n l \n \n \n \n \n β \n \n 2 \n \n \n ⋅ \n \n p \n r \n e \n c \n i \n s \n i \n o \n n \n \n + \n \n r \n e \n c \n a \n l \n l \n \n \n \n \n \n \n {\\displaystyle F_{\\beta }=(1+\\beta ^{2})\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\beta ^{2}\\cdot \\mathrm {precision} +\\mathrm {recall} }}} Two other commonly used  \n \n \n \n F \n \n \n {\\displaystyle F} \n \n  measures are the  \n \n \n \n \n F \n \n 2 \n \n \n \n \n {\\displaystyle F_{2}} \n \n  measure, which weights recall higher than precision, and the  \n \n \n \n \n F \n \n 0.5 \n \n \n \n \n {\\displaystyle F_{0.5}} \n \n  measure, which puts more emphasis on precision than recall. The F-measure was derived by van Rijsbergen (1979) so that  \n \n \n \n \n F \n \n β \n \n \n \n \n {\\displaystyle F_{\\beta }} \n \n  \"measures the effectiveness of retrieval with respect to a user who attaches  \n \n \n \n β \n \n \n {\\displaystyle \\beta } \n \n  times as much importance to recall as precision\".  It is based on van Rijsbergen's effectiveness measure  \n \n \n \n \n E \n \n α \n \n \n = \n 1 \n − \n \n \n 1 \n \n \n \n α \n P \n \n \n + \n \n \n \n 1 \n − \n α \n \n R \n \n \n \n \n \n \n \n {\\displaystyle E_{\\alpha }=1-{\\frac {1}{{\\frac {\\alpha }{P}}+{\\frac {1-\\alpha }{R}}}}} \n \n , the second term being the weighted harmonic mean of precision and recall with weights  \n \n \n \n ( \n α \n , \n 1 \n − \n α \n ) \n \n \n {\\displaystyle (\\alpha ,1-\\alpha )} \n \n .  Their relationship is  \n \n \n \n \n F \n \n β \n \n \n = \n 1 \n − \n \n E \n \n α \n \n \n \n \n {\\displaystyle F_{\\beta }=1-E_{\\alpha }} \n \n  where  \n \n \n \n α \n = \n \n \n 1 \n \n 1 \n + \n \n β \n \n 2 \n \n \n \n \n \n \n \n {\\displaystyle \\alpha ={\\frac {1}{1+\\beta ^{2}}}} \n \n . There are other parameters and strategies for performance metric of information retrieval system, such as the area under the  ROC curve  (AUC) [ 20 ]  or  pseudo-R-squared . Precision and recall values can also be calculated for classification problems with more than two classes. [ 21 ]  To obtain the precision for a given class, we divide the number of true positives by the classifier bias towards this class (number of times that the classifier has predicted the class). To calculate the recall for a given class, we divide the number of true positives by the prevalence of this class (number of times that the class occurs in the data sample). The class-wise precision and recall values can then be combined into an overall multi-class evaluation score, e.g., using the  macro F1 metric . [ 21 ]"
  },
  {
    "id": 102,
    "title": "Ground truth",
    "content": "Ground truth  is information that is known to be real or true, provided by direct observation and measurement (i.e.  empirical evidence ) as opposed to information provided by  inference . The  Oxford English Dictionary  (s.v.  ground truth ) records the use of the word  Groundtruth  in the sense of 'fundamental truth' from Henry Ellison's poem \"The Siberian Exile's Tale\", published in 1833. [ 1 ] \"Ground truth\" may be seen as a conceptual term relative to the knowledge of the truth concerning a specific question. It is the ideal expected result. [ 2 ]  This is used in  statistical models  to prove or disprove  research   hypotheses .  The term \"ground truthing\" refers to the process of gathering the proper  objective  (provable) data for this test.  Compare with  gold standard .\nFor example, suppose we are testing a  stereo vision  system to see how well it can estimate 3D positions. The \"ground truth\" might be the positions given by a laser rangefinder which is known to be much more accurate than the camera system. Bayesian spam filtering  is a common example of supervised learning.  In this system, the algorithm is manually taught the differences between spam and non-spam.  This depends on the  ground truth  of the messages used to train the algorithm – inaccuracies in the ground truth will correlate to inaccuracies in the resulting spam/non-spam verdicts. In  remote sensing , \"ground truth\" refers to information collected at the imaged location. Ground truth allows image data to be related to real features and materials on the ground. The collection of ground truth data enables calibration of remote-sensing data, and aids in the interpretation and analysis of what is being sensed.  Examples include  cartography ,  meteorology , analysis of  aerial photographs ,  satellite imagery  and other techniques in which data are gathered at a distance. More specifically, ground truth may refer to a process in which \" pixels \" [ 3 ]  on a  satellite  image are compared to what is imaged (at the time of capture) in order to verify the contents of the \"pixels\" in the image (noting that the concept of \"pixel\" is imaging-system-dependent). In the case of a classified image, supervised classification can help to determine the accuracy of the classification by the remote sensing system which can minimize error in the classification. Ground truth is usually done on site, correlating what is known with surface observations and measurements of various properties of the features of the ground resolution cells under study in the remotely sensed digital image. The process also involves taking geographic coordinates of the ground resolution cell with GPS technology and comparing those with the coordinates of the \"pixel\" being studied provided by the remote sensing software to understand and analyze the location errors and how it may affect a particular study. Ground truth is important in the initial supervised classification of an image. When the identity and location of land cover types are known through a combination of field work, maps, and personal experience these areas are known as training sites. The spectral characteristics of these areas are used to train the remote sensing software using decision rules for classifying the rest of the image. These decision rules such as Maximum Likelihood Classification, Parallelopiped Classification, and Minimum Distance Classification offer different techniques to classify an image. Additional ground truth sites allow the remote sensor to establish an error matrix that validates the accuracy of the classification method used. Different classification methods may have different percentages of error for a given classification project. It is important that the remote sensor chooses a classification method that works best with the number of classifications used while providing the least amount of error. Ground truth also helps with  atmospheric correction . Since images from satellites have to pass through the atmosphere, they can get distorted because of absorption in the atmosphere. So ground truth can help fully identify objects in satellite photos. An example of an error of commission is when a pixel reports the presence of a feature (such a tree) that, in reality, is absent (no tree is actually present). Ground truthing ensures that the error matrices have a higher accuracy percentage than would be the case if no pixels were ground-truthed. This value is the inverse of the user's accuracy, i.e. Commission Error = 1 - user's accuracy. An example of an error of omission is when pixels of a certain type, for example, maple trees, are not classified as maple trees.  The process of ground-truthing helps to ensure that the pixel is classified correctly and the error matrices are more accurate. This value is the inverse of the producer's accuracy, i.e. Omission Error = 1 - producer's accuracy In GIS the spatial data is modeled as  field  (like in  remote sensing raster images ) or as  object  (like in  vectorial map  representation). [ 4 ]   They are modeled from the real world (also named  geographical reality ),  typically by a cartographic process (illustrated). Geographic information systems  such as GIS, GPS, and GNSS, have become so widespread that the term \"ground truth\" has taken on special meaning in that context. If the location coordinates returned by a location method such as GPS are an estimate of a location, then the \"ground truth\" is the actual location on Earth. A smart phone might return a set of estimated location coordinates such as 43.87870,-103.45901. The ground truth being estimated by those coordinates is the tip of George Washington's nose on  Mount Rushmore . The accuracy of the estimate is the maximum distance between the location coordinates and the ground truth. We could say in this case that the estimate accuracy is 10 meters, meaning that the point on earth represented by the location coordinates is thought to be within 10 meters of George's nose—the ground truth. In slang, the coordinates indicate where we think George Washington's nose is located, and the ground truth is where it really is. In practice a smart phone or hand-held GPS unit is routinely able to estimate the ground truth within 6–10 meters. Specialized instruments can reduce GPS measurement error to under a centimeter. [ 5 ] US  military slang  uses \"ground truth\" to refer to the facts comprising a tactical situation—as opposed to intelligence reports, mission plans, and other descriptions reflecting the conative or policy-based projections of the industrial·military complex. The term appears in the title of the  Iraq War  documentary film  The Ground Truth  (2006), and also in military publications, for example  Stars and Stripes  saying: \"Stripes decided to figure out what the ground truth was in Iraq.\" [ citation needed ]"
  },
  {
    "id": 103,
    "title": "Well-posed problem",
    "content": "In  mathematics , a  well-posed problem  is one for which the following properties hold: [ a ] Examples of  archetypal  well-posed problems include the  Dirichlet problem for Laplace's equation , and the  heat equation  with specified initial conditions. These might be regarded as 'natural' problems in that there are physical processes modelled by these problems. Problems that are not well-posed in the sense above are termed  ill-posed .  Inverse problems  are often ill-posed; for example, the inverse heat equation, deducing a previous distribution of temperature from final data, is not well-posed in that the solution is highly sensitive to changes in the final data. Continuum models must often be  discretized  in order to obtain a numerical solution. While solutions may be continuous with respect to the initial conditions, they may suffer from  numerical instability  when solved with finite  precision , or with  errors  in the data. Even if a problem is well-posed, it may still be  ill-conditioned , meaning that a small error in the initial data can result in much larger errors in the answers. Problems in nonlinear  complex systems  (so-called  chaotic  systems) provide well-known examples of instability. An ill-conditioned problem is indicated by a large  condition number . If the problem is well-posed, then it stands a good chance of solution on a computer using a  stable algorithm . If it is not well-posed, it needs to be re-formulated for numerical treatment. Typically this involves including additional assumptions, such as smoothness of solution. This process is known as  regularization . [ 1 ]   Tikhonov regularization  is one of the most commonly used for regularization of linear ill-posed problems. The energy method is useful for establishing both uniqueness and continuity with respect to initial conditions (i.e. it does not establish existence).\nThe method is based upon deriving an upper bound of an energy-like functional for a given problem. Example :\nConsider the diffusion equation on the unit interval with homogeneous  Dirichlet boundary conditions  and suitable initial data  \n \n \n \n f \n ( \n x \n ) \n \n \n {\\displaystyle f(x)} \n \n  (e.g. for which  \n \n \n \n f \n ( \n 0 \n ) \n = \n f \n ( \n 1 \n ) \n = \n 0 \n \n \n {\\displaystyle f(0)=f(1)=0} \n \n ). u \n \n t \n \n \n \n \n \n = \n D \n \n u \n \n x \n x \n \n \n , \n \n \n \n 0 \n < \n x \n < \n 1 \n , \n \n t \n > \n 0 \n , \n \n D \n > \n 0 \n , \n \n \n \n \n u \n ( \n x \n , \n 0 \n ) \n \n \n \n = \n f \n ( \n x \n ) \n , \n \n \n \n \n u \n ( \n 0 \n , \n t \n ) \n \n \n \n = \n 0 \n , \n \n \n \n \n u \n ( \n 1 \n , \n t \n ) \n \n \n \n = \n 0 \n , \n \n \n \n \n \n \n {\\displaystyle {\\begin{aligned}u_{t}&=Du_{xx},&&0<x<1,\\,t>0,\\,D>0,\\\\u(x,0)&=f(x),\\\\u(0,t)&=0,\\\\u(1,t)&=0,\\\\\\end{aligned}}} Multiply the equation  \n \n \n \n \n u \n \n t \n \n \n = \n D \n \n u \n \n x \n x \n \n \n \n \n {\\displaystyle u_{t}=Du_{xx}} \n \n  by  \n \n \n \n u \n \n \n {\\displaystyle u} \n \n  and integrate in space over the unit interval to obtain ∫ \n \n 0 \n \n \n 1 \n \n \n u \n \n u \n \n t \n \n \n d \n x \n \n \n \n = \n D \n \n ∫ \n \n 0 \n \n \n 1 \n \n \n u \n \n u \n \n x \n x \n \n \n d \n x \n \n \n \n \n ⟹ \n \n \n \n \n ∫ \n \n 0 \n \n \n 1 \n \n \n \n \n 1 \n 2 \n \n \n \n ∂ \n \n t \n \n \n \n u \n \n 2 \n \n \n d \n x \n \n \n \n = \n D \n u \n \n u \n \n x \n \n \n \n \n \n | \n \n \n \n 0 \n \n \n 1 \n \n \n − \n D \n \n ∫ \n \n 0 \n \n \n 1 \n \n \n ( \n \n u \n \n x \n \n \n \n ) \n \n 2 \n \n \n d \n x \n \n \n \n \n ⟹ \n \n \n \n \n \n 1 \n 2 \n \n \n \n ∂ \n \n t \n \n \n ‖ \n u \n \n ‖ \n \n 2 \n \n \n 2 \n \n \n \n \n \n = \n 0 \n − \n D \n \n ∫ \n \n 0 \n \n \n 1 \n \n \n ( \n \n u \n \n x \n \n \n \n ) \n \n 2 \n \n \n d \n x \n ≤ \n 0 \n \n \n \n \n \n \n {\\displaystyle {\\begin{aligned}&&\\int _{0}^{1}uu_{t}dx&=D\\int _{0}^{1}uu_{xx}dx\\\\\\Longrightarrow &&\\int _{0}^{1}{\\frac {1}{2}}\\partial _{t}u^{2}dx&=Duu_{x}{\\Big |}_{0}^{1}-D\\int _{0}^{1}(u_{x})^{2}dx\\\\\\Longrightarrow &&{\\frac {1}{2}}\\partial _{t}\\|u\\|_{2}^{2}&=0-D\\int _{0}^{1}(u_{x})^{2}dx\\leq 0\\end{aligned}}} This tells us that  \n \n \n \n ‖ \n u \n \n ‖ \n \n 2 \n \n \n \n \n {\\displaystyle \\|u\\|_{2}} \n \n  ( p-norm ) cannot grow in time.\nBy multiplying by two and integrating in time, from  \n \n \n \n 0 \n \n \n {\\displaystyle 0} \n \n  up to  \n \n \n \n t \n \n \n {\\displaystyle t} \n \n , one finds ‖ \n u \n ( \n ⋅ \n , \n t \n ) \n \n ‖ \n \n 2 \n \n \n 2 \n \n \n ≤ \n ‖ \n f \n ( \n ⋅ \n ) \n \n ‖ \n \n 2 \n \n \n 2 \n \n \n \n \n {\\displaystyle \\|u(\\cdot ,t)\\|_{2}^{2}\\leq \\|f(\\cdot )\\|_{2}^{2}} This result is the  energy estimate  for this problem. To show uniqueness of solutions, assume there are two distinct solutions to the problem, call them  \n \n \n \n u \n \n \n {\\displaystyle u} \n \n  and  \n \n \n \n v \n \n \n {\\displaystyle v} \n \n , each satisfying the same initial data. \nUpon defining  \n \n \n \n w \n = \n u \n − \n v \n \n \n {\\displaystyle w=u-v} \n \n  then, via the linearity of the equations, one finds that  \n \n \n \n w \n \n \n {\\displaystyle w} \n \n  satisfies w \n \n t \n \n \n \n \n \n = \n D \n \n w \n \n x \n x \n \n \n , \n \n \n \n 0 \n < \n x \n < \n 1 \n , \n \n t \n > \n 0 \n , \n \n D \n > \n 0 \n , \n \n \n \n \n w \n ( \n x \n , \n 0 \n ) \n \n \n \n = \n 0 \n , \n \n \n \n \n w \n ( \n 0 \n , \n t \n ) \n \n \n \n = \n 0 \n , \n \n \n \n \n w \n ( \n 1 \n , \n t \n ) \n \n \n \n = \n 0 \n , \n \n \n \n \n \n \n {\\displaystyle {\\begin{aligned}w_{t}&=Dw_{xx},&&0<x<1,\\,t>0,\\,D>0,\\\\w(x,0)&=0,\\\\w(0,t)&=0,\\\\w(1,t)&=0,\\\\\\end{aligned}}} Applying the energy estimate tells us  \n \n \n \n ‖ \n w \n ( \n ⋅ \n , \n t \n ) \n \n ‖ \n \n 2 \n \n \n 2 \n \n \n ≤ \n 0 \n \n \n {\\displaystyle \\|w(\\cdot ,t)\\|_{2}^{2}\\leq 0} \n \n  which implies  \n \n \n \n u \n = \n v \n \n \n {\\displaystyle u=v} \n \n  ( almost everywhere ). Similarly, to show continuity with respect to initial conditions, assume that  \n \n \n \n u \n \n \n {\\displaystyle u} \n \n  and  \n \n \n \n v \n \n \n {\\displaystyle v} \n \n  are solutions corresponding to different initial data  \n \n \n \n u \n ( \n x \n , \n 0 \n ) \n = \n f \n ( \n x \n ) \n \n \n {\\displaystyle u(x,0)=f(x)} \n \n  and  \n \n \n \n v \n ( \n x \n , \n 0 \n ) \n = \n g \n ( \n x \n ) \n \n \n {\\displaystyle v(x,0)=g(x)} \n \n .\nConsidering  \n \n \n \n w \n = \n u \n − \n v \n \n \n {\\displaystyle w=u-v} \n \n  once more, one finds that  \n \n \n \n w \n \n \n {\\displaystyle w} \n \n  satisfies the same equations as above but with  \n \n \n \n w \n ( \n x \n , \n 0 \n ) \n = \n f \n ( \n x \n ) \n − \n g \n ( \n x \n ) \n \n \n {\\displaystyle w(x,0)=f(x)-g(x)} \n \n . \nThis leads to the energy estimate  \n \n \n \n ‖ \n w \n ( \n ⋅ \n , \n t \n ) \n \n ‖ \n \n 2 \n \n \n 2 \n \n \n ≤ \n D \n ‖ \n f \n ( \n ⋅ \n ) \n − \n g \n ( \n ⋅ \n ) \n \n ‖ \n \n 2 \n \n \n 2 \n \n \n \n \n {\\displaystyle \\|w(\\cdot ,t)\\|_{2}^{2}\\leq D\\|f(\\cdot )-g(\\cdot )\\|_{2}^{2}} \n \n  which establishes continuity (i.e. as  \n \n \n \n f \n \n \n {\\displaystyle f} \n \n  and  \n \n \n \n g \n \n \n {\\displaystyle g} \n \n  become closer, as measured by the  \n \n \n \n \n L \n \n 2 \n \n \n \n \n {\\displaystyle L^{2}} \n \n  norm of their difference, then  \n \n \n \n ‖ \n w \n ( \n ⋅ \n , \n t \n ) \n \n ‖ \n \n 2 \n \n \n → \n 0 \n \n \n {\\displaystyle \\|w(\\cdot ,t)\\|_{2}\\to 0} \n \n ). The  maximum principle  is an alternative approach to establish uniqueness and continuity of solutions with respect to initial conditions for this example.\nThe existence of solutions to this problem can be established using  Fourier series ."
  },
  {
    "id": 104,
    "title": "Lemur Project",
    "content": "The  Lemur Project  is a collaboration between the Center for Intelligent Information Retrieval at the  University of Massachusetts Amherst  and the  Language Technologies Institute  at  Carnegie Mellon University . The Lemur Project develops search engines, browser toolbars, text analysis tools, and data resources that support research and development of information retrieval and text mining software. The project is best known for its Indri and Galago search engines, the ClueWeb09 and ClueWeb12 datasets, and the RankLib learning-to-rank library. The software and datasets are used widely in scientific and research applications, as well as in some commercial applications. The Lemur Project's software development philosophy emphasizes state-of-the-art accuracy, flexibility, and efficiency. For example, the Indri search engine provides accurate search for large text collections 'out of the box', and data is stored in an accessible manner to support development of new retrieval strategies. Software from the Lemur Project is distributed under open-source licenses that provide flexibility to scientists and software developers. The programming languages used to create Lemur are  C ,  C++ , and  Java , and it comes along with the source files and build instructions. The provided source code can be modified for the purpose of developing new libraries. It is compatible with various operating systems which include Linux and Windows. Lemur supports the following features: Lemur Project has the following components: Updates to the Lemur Project components are made twice a year, in June and December.\nThe latest version of the Indri search engine is 5.17.\nThe latest version of the Galago search engine is version 3.18.\nThe latest version of the RankLib learning-to-rank library is 2.14.\nThe latest version of the Sifaka data mining application is 1.8. The Indri search engine is one of the components developed by the Lemur Project. It is open source. The query language that is used in Indri allows researchers to index data or structure documents using simple command line instructions. Indri offers flexibility in terms of adaptation to various current applications. It also can be distributed across a cluster of nodes for high performance. The Indri search engine can handle large collections of data and can understand various data formats like  HTML  and  XML . The Indri API supports various programming and scripting languages like C++,  Java ,  C# , and  PHP . This  free and open-source software  article is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 105,
    "title": "Apache Lucene",
    "content": "Apache Lucene  is a  free and open-source   search engine   software library , originally written in  Java  by  Doug Cutting . It is supported by the  Apache Software Foundation  and is released under the  Apache Software License .  Lucene is widely used as a standard foundation for production search applications. [ 2 ] [ 3 ] [ 4 ] Lucene has been ported to other programming languages including  Object Pascal ,  Perl ,  C# ,  C++ ,  Python ,  Ruby  and  PHP . [ 5 ] Doug Cutting  originally wrote Lucene in 1999. [ 6 ]  Lucene was his fifth search engine. He had previously written two while at  Xerox PARC , one at  Apple , and a fourth at  Excite . [ 7 ]  It was initially available for download from its home at the  SourceForge  web site. It joined the Apache Software Foundation's  Jakarta  family of open-source Java products in September 2001 and became its own top-level Apache project in February 2005. The name Lucene is Doug Cutting's wife's middle name and her maternal grandmother's first name. [ 8 ] Lucene formerly included a number of sub-projects, such as Lucene.NET,  Mahout ,  Tika  and  Nutch . These three are now independent top-level projects. In March 2010, the  Apache Solr  search server joined as a Lucene sub-project, merging the developer communities. Version 4.0 was released on October 12, 2012. [ 9 ] In March 2021, Lucene changed its logo, and  Apache Solr  became a top level Apache project again, independent from Lucene. While suitable for any application that requires full text  indexing  and searching capability, Lucene is recognized for its utility in the implementation of  Internet search engines  and local, single-site searching. [ 10 ] [ 11 ] Lucene includes a feature to perform a fuzzy search based on  edit distance . [ 12 ] Lucene has also been used to implement recommendation systems. [ 13 ]  For example, Lucene's 'MoreLikeThis' Class can generate recommendations for similar documents. In a comparison of the term vector-based similarity approach of 'MoreLikeThis' with citation-based document similarity measures, such as  co-citation  and co-citation proximity analysis, Lucene's approach excelled at recommending documents with very similar structural characteristics and more narrow relatedness. [ 14 ]  In contrast, citation-based document similarity measures tended to be more suitable for recommending more broadly related documents, [ 14 ]  meaning citation-based approaches may be more suitable for generating  serendipitous  recommendations, as long as documents to be recommended contain in-text citations. Lucene itself is just an indexing and search library and does not contain  crawling  and HTML  parsing  functionality. However, several projects extend Lucene's capability:"
  },
  {
    "id": 106,
    "title": "Apache Solr",
    "content": "Solr  (pronounced \"solar\") is an  open-source   enterprise-search  platform, written in  Java . Its major features include  full-text search , hit highlighting,  faceted search , real-time indexing, dynamic clustering, database integration,  NoSQL  features [ 2 ]  and rich document (e.g., Word, PDF) handling. Providing distributed search and index replication, Solr is designed for scalability and  fault tolerance . [ 3 ]  Solr is widely used for enterprise search and analytics use cases and has an active development community and regular releases. Solr runs as a standalone full-text search server. It uses the  Lucene  Java search library at its core for full-text indexing and search, and has  REST -like  HTTP / XML  and  JSON  APIs that make it usable from most popular programming languages. Solr's external configuration allows it to be tailored to many types of applications without Java coding, and it has a plugin architecture to support more advanced customization. Apache Solr is developed in an open, collaborative manner by the Apache Solr project at the  Apache Software Foundation . In 2004, Solr was created by Yonik Seeley at  CNET Networks  as an in-house project to add search capability for the company website. [ 4 ] In January 2006, CNET Networks decided to openly publish the source code by donating it to the  Apache Software Foundation . [ 5 ]  Like any new Apache project, it entered an incubation period that helped solve organizational, legal, and financial issues. In January 2007, Solr graduated from incubation status into a standalone top-level project (TLP) and grew steadily with accumulated features, thereby attracting users, contributors, and committers. Although quite new as a public project, it powered several high-traffic websites. [ 6 ] In September 2008, Solr 1.3 was released including distributed search capabilities and performance enhancements among many others. [ 7 ] In January 2009, Yonik Seeley along with Grant Ingersoll and Erik Hatcher joined  Lucidworks  (formerly Lucid Imagination), the first company providing commercial support and training for Apache Solr search technologies. [ citation needed ]  Since then, support offerings around Solr have been abundant. [ 8 ] In November 2009, saw the release of Solr 1.4. This version introduced enhancements in indexing, searching and faceting along with many other improvements such as rich document processing ( PDF ,  Word ,  HTML ), Search Results clustering based on  Carrot2  and also improved database integration. The release also features many additional plug-ins. [ 9 ] In March 2010, the  Lucene  and Solr projects merged. [ 10 ]  Separate downloads continued, but the products were now jointly developed by a single set of committers. In 2011, the Solr version number scheme was changed in order to match that of Lucene. After Solr 1.4, the next release of Solr was labeled 3.1, in order to keep Solr and Lucene on the same version number. [ 11 ] In October 2012, Solr version 4.0 was released, including the new SolrCloud feature. [ 12 ]  2013 and 2014 saw a number of Solr releases in the 4.x line, steadily growing the feature set and improving reliability. In February 2015, Solr 5.0 was released, [ 13 ]  the first release where Solr is packaged as a standalone application, [ 14 ]  ending official support for deploying Solr as a  war . Solr 5.3 featured a built-in pluggable Authentication and Authorization framework. [ 15 ] In April 2016, Solr 6.0 was released. [ 16 ]  Added support for executing Parallel SQL queries across SolrCloud collections. Includes StreamExpression support and a new JDBC Driver for the SQL Interface. In September 2017, Solr 7.0 was released. [ 17 ]  This release among other things, added support multiple replica types, auto-scaling, and a Math engine. In March 2019, Solr 8.0 was released including many bugfixes and component updates. [ 18 ]  Solr nodes can now listen and serve HTTP/2 requests. Be aware that by default, internal requests are also sent by using HTTP/2. Furthermore, an admin UI login was added with support for BasicAuth and Kerberos. And plotting math expressions in  Apache Zeppelin  is now possible. In November 2020, Bloomberg donated the  Solr Operator  to the Lucene/Solr project. The Solr Operator helps deploy and run Solr in  Kubernetes . In February 2021, Solr was established as a separate Apache project (TLP), independent from Lucene. In May 2022, Solr 9.0 was released, [ 19 ]  as the first release independent from Lucene, requiring Java 11, and with highlights such as KNN \"Neural\" search, better modularization, more security plugins and more. In order to search a document, Apache Solr performs the following operations in sequence: Solr has both individuals and companies who contribute new features and bug fixes. [ 20 ] \n [ 21 ] [ 22 ] [ 23 ] [ 24 ] Solr is bundled as the built-in search in many applications such as  content management systems  and  enterprise content management  systems.  Hadoop  distributions from  Cloudera , [ 25 ]   Hortonworks [ 26 ]  and  MapR  all bundle Solr as the search engine for their products marketed for  big data .  DataStax  DSE integrates Solr as a search engine with  Cassandra . [ 27 ]  Solr is supported as an end point in various data processing frameworks and  Enterprise integration  frameworks. [ citation needed ] Solr exposes industry standard  HTTP   REST-like   APIs  with both  XML  and  JSON  support, and will integrate with any system or programming language supporting these standards. For ease of use there are also client libraries available for  Java ,  C# ,  PHP ,  Python ,  Ruby  and most other popular programming languages. [ 28 ]"
  },
  {
    "id": 107,
    "title": "Elasticsearch",
    "content": "Elasticsearch  is a  search engine  based on  Apache Lucene . It provides a distributed,  multitenant -capable  full-text search  engine with an  HTTP  web interface and schema-free  JSON  documents. Official clients are available in  Java , [ 2 ]   .NET [ 3 ]  ( C# ),  PHP , [ 4 ]   Python , [ 5 ]   Ruby [ 6 ]  and many other languages. [ 7 ]  According to the  DB-Engines ranking , Elasticsearch is the most popular enterprise search engine. [ 8 ] Shay Banon created the precursor to Elasticsearch, called Compass, in 2004. [ 9 ]  While thinking about the third version of Compass he realized that it would be necessary to rewrite big parts of Compass to \"create a scalable search solution\". [ 9 ]  So he created \"a solution built from the ground up to be distributed\" and used a common interface,  JSON  over  HTTP , suitable for programming languages other than Java as well. [ 9 ]  Shay Banon released the first version of Elasticsearch in February 2010. [ 10 ] Elastic NV  was founded in 2012 to provide commercial services and products around Elasticsearch and related software. [ 11 ]  In June 2014, the company announced raising $70 million in a Series C funding round, just 18 months after forming the company. The round was led by  New Enterprise Associates  (NEA). Additional funders include  Benchmark Capital  and  Index Ventures . This round brought total funding to $104M. [ 12 ] In March 2015, the company  Elasticsearch  changed its name to  Elastic . [ 13 ] In June 2018, Elastic filed for an  initial public offering  with an estimated valuation of between 1.5 and 3 billion dollars. [ 14 ]  On 5 October 2018, Elastic was listed on the  New York Stock Exchange . [ 15 ] Developed from the Found acquisition by Elastic in 2015, [ 16 ]  Elastic Cloud is a family of Elasticsearch-powered  SaaS  offerings which include the Elasticsearch Service, as well as Elastic App Search Service, and Elastic Site Search Service which were developed from Elastic's acquisition of  Swiftype . [ 17 ]  In late 2017, Elastic formed partnerships with  Google  to offer Elastic Cloud in  Google Cloud Platform (GCP) , and  Alibaba  to offer Elasticsearch and  Kibana  in  Alibaba Cloud . Elasticsearch Service users can create secure deployments with partners, Google Cloud Platform (GCP) and Alibaba Cloud. [ 18 ] In January 2021, Elastic announced that starting with version 7.11, they would be relicensing their Apache 2.0 licensed code in Elasticsearch and  Kibana  to be dual licensed under  Server Side Public License  and the Elastic License, neither of which is recognized as an  open-source license . [ 19 ] [ 20 ]  Elastic blamed  Amazon Web Services  (AWS) for this change, objecting to AWS offering Elasticsearch and Kibana  as a service  directly to consumers and claiming that AWS was not appropriately collaborating with Elastic. [ 20 ] [ 21 ]  Critics of the re-licensing decision predicted that it would harm Elastic's ecosystem and noted that Elastic had previously promised to \"never....change the license of the Apache 2.0 code of Elasticsearch, Kibana, Beats, and Logstash\". Amazon responded with plans to  fork  the projects and continue development under Apache License 2.0. [ 22 ] [ 23 ]  Other users of the Elasticsearch ecosystem, including  Logz.io ,  CrateDB  and  Aiven , also committed to the need for a fork, leading to a discussion of how to coordinate the open source efforts. [ 24 ] [ 25 ] [ 26 ]  Due to potential trademark issues with using the name \"Elasticsearch\", AWS rebranded their fork as  OpenSearch  in April 2021. [ 27 ] [ 28 ] In August 2024 the GNU Affero General Public Licence was added as an option, making it free and open-source once again. [ 22 ] Elasticsearch can be used to search any kind of document. It provides scalable search, has near  real-time search , and supports  multitenancy . [ 29 ]  \"Elasticsearch is distributed, which means that indices can be divided into  shards  and each shard can have zero or more replicas. Each node hosts one or more shards and acts as a coordinator to delegate operations to the correct shard(s). Rebalancing and  routing  are done automatically\". [ 29 ]  Related data is often stored in the same index, which consists of one or more primary shards, and zero or more replica shards. Once an index has been created, the number of primary shards cannot be changed. [ 30 ] Elasticsearch is developed alongside the  data collection  and  log -parsing engine Logstash, the analytics and visualization platform  Kibana , and the collection of lightweight data shippers called Beats. The four products are designed for use as an integrated solution, referred to as the \"Elastic Stack\". [ 31 ]  (Formerly the \"ELK stack\", short for \"Elasticsearch, Logstash, Kibana\".) Elasticsearch uses  Lucene  and tries to make all its features available through the  JSON  and  Java API . It supports  facetting  and percolating (a form of  prospective search ), [ 32 ]   [ 33 ]  which can be useful for notifying if new documents match for registered queries. Another feature, \"gateway\", handles the long-term persistence of the index; [ 34 ]  for example, an index can be recovered from the gateway in  the event  of a server crash. Elasticsearch supports real-time  GET requests , which makes it suitable as a  NoSQL  datastore, [ 35 ]  but it lacks  distributed transactions . [ 36 ] On 20 May 2019, Elastic made the core security features of the Elastic Stack available free of charge, including  TLS  for  encrypted  communications, file and native realm for creating and managing users, and role-based access control for controlling user access to cluster  APIs  and indexes. [ 37 ]  The corresponding source code is available under the “Elastic License”, a  source-available  license. [ 38 ]  In addition, Elasticsearch now offers  SIEM [ 39 ]  and  Machine Learning   [ 40 ]  as part of its offered services."
  },
  {
    "id": 108,
    "title": "Sketch Engine",
    "content": "Sketch Engine  is a  corpus manager  and  text analysis   software  developed by Lexical Computing since 2003. Its purpose is to enable people studying  language  behaviour ( lexicographers , researchers in  corpus linguistics ,  translators  or  language learners ) to search large text collections according to complex and linguistically motivated queries. Sketch Engine gained its name after one of the key features,  word sketches : one-page, automatic, corpus-derived summaries of a word's grammatical and collocational behaviour. [ 2 ]  Currently, it supports and provides corpora in over 90 languages. [ 3 ] Sketch Engine is a product of Lexical Computing, a company founded in 2003 by the lexicographer and research scientist  Adam Kilgarriff . [ 4 ]  He started a collaboration with Pavel Rychlý, a computer scientist working at the Natural Language Processing Centre,  Masaryk University , [ 5 ]  and the developer of Manatee and Bonito (two major parts of the software suite). Kilgarriff also introduced the concept of  word sketches . Since then, Sketch Engine has been commercial software, however, all the core features of Manatee and Bonito that were developed by 2003 (and extended since then) are freely available under the  GPL  license within the NoSketch Engine suite. [ 6 ] A list of tools available in Sketch Engine: Sketch Engine can perform automatic  term extraction  by identifying words typical of a particular corpus, document, or text. Single words and multi-word units can be extracted from monolingual or bilingual texts. The terminology extraction feature provides a list of relevant terms based on comparison with a large corpus of general language. This functionality is also available as a separate service called  OneClick Terms  with a dedicated interface. [ 8 ] A free web service based on Sketch Engine and aimed at  language learners  and  teachers  is SKELL (formerly  SkELL ). It exploits Sketch Engine's proprietary GDEX (Good Dictionary Examples) scoring function to provide authentic example sentences for specific target words. Results are drawn from a special corpus of high-quality texts covering everyday, standard, formal, and professional language and displayed as a  concordance . SKELL also includes simplified versions of Sketch Engine's  word sketch  and  thesaurus  functions. [ 9 ] It has been suggested that SKELL can be used, for instance, to help students understand the meaning and/or usage of a word or phrase; to help teachers wanting to use example sentences in a class; to discover and explore  collocates ; to create  gap-fill exercises ; to teach various kinds of  homonyms  and  polysemous words . [ 10 ] [ 11 ]  SKELL was first presented in 2014, when only  English  was supported. [ 9 ]  Later, support was added for  Russian , [ 12 ]   Czech , [ 13 ]   German , [ 14 ]   Italian [ 15 ]  and  Estonian . [ 16 ] Sketch Engine provides access to more than 700 text corpora. There are monolingual as well as multilingual corpora of different sizes (from thousand of words up to 60 billions of words) and various sources (e.g. web, books, subtitles, legal documents). The list of corpora includes  British National Corpus ,  Brown Corpus ,  Cambridge Academic English Corpus  and Cambridge Learner Corpus,  CHILDES  corpora of child language, OpenSubtitles (a set of 60 parallel corpora), 24 multilingual corpora of  EUR-Lex  documents, the  TenTen Corpus Family  (multi-billion web corpora), and Trends corpora (monitor corpora with daily updates). Sketch Engine consists of three main components: an underlying  database management system  called Manatee, a web interface search front-end called Bonito, and a web interface for corpus building and management called Corpus Architect.\n [ 17 ] Manatee is a  database management system  specifically devised for effective indexing of large text corpora. It is based on the idea of  inverted indexing  (keeping an index of all positions of a given word in the text). It has been used to index text corpora comprising tens of billions of words. [ 18 ] Searching corpora indexed by Manatee is performed by formulating queries in the Corpus Query Language (CQL). [ 19 ] Manatee is written in  C++  and offers an  API  for a number of other programming languages including  Python ,  Java ,  Perl  and  Ruby . Recently, it was rewritten into  Go  for faster processing of corpus queries. [ 20 ] Bonito is a web interface for Manatee providing access to corpus search. In the  client–server model , Manatee is the server and Bonito plays the client part. It is written in  Python . [ 17 ] Corpus Architect is a web interface providing corpus building and management features. It is also written in  Python . Sketch Engine has been used by major  British  and other publishing houses for producing dictionaries such as  Macmillan English Dictionary ,  Dictionnaires Le Robert ,  Oxford University Press  or  Shogakukan . Four of United Kingdom's five biggest dictionary publishers use Sketch Engine. [ 21 ]"
  },
  {
    "id": 109,
    "title": "Sphinx (search engine)",
    "content": "Sphinx  is a  fulltext   search engine  that provides text search functionality to client applications. Sphinx can be used either as a stand-alone server or as a  storage engine  (\"SphinxSE\") for the MySQL family of databases.\nWhen run as a standalone server Sphinx operates similar to a  DBMS  and can communicate with  MySQL ,  MariaDB  and  PostgreSQL  through their native protocols or with any ODBC-compliant DBMS via  ODBC .\n MariaDB , a fork of MySQL, is distributed with SphinxSE. [ 2 ] If Sphinx is run as a stand-alone server, it is possible to use SphinxAPI to connect an application to it. Official implementations of the API are available for  PHP ,  Java ,  Perl ,  Ruby  and  Python  languages. Unofficial implementations for other languages, as well as various third party [ 3 ]  plugins and modules are also available. Other data sources can be indexed via pipe in a custom  XML  format. [ 4 ] The Sphinx search daemon supports the MySQL binary network protocol and can be accessed with the regular MySQL API and/or clients. Sphinx supports a subset of  SQL  known as SphinxQL. It supports standard querying of all index types with SELECT, modifying RealTime indexes with INSERT, REPLACE, and DELETE, and more. Sphinx can also provide a special storage engine for MariaDB and MySQL databases. This allows those MySQL, MariaDB to communicate with Sphinx's  searchd  to run queries and obtain results. Sphinx indices are treated like regular SQL tables. The SphinxSE storage engine is shipped with MariaDB. Sphinx is configured to examine a data set via its Indexer. The Indexer process creates a full-text index (a special  data structure  that enables quick keyword searches) from the given data/text.   Full-text  fields are the resulting content that is indexed by Sphinx; they can be (quickly) searched for keywords. Fields are named, and you can limit your searches to a single field (e.g. search through \"title\" only) or a subset of fields (e.g. to \"title\" and \"abstract\" only). Sphinx's index format generally supports up to 256 fields. Note that the original data is not stored in the Sphinx index, but are discarded during the Indexing process; Sphinx assumes that you store those contents elsewhere. Attributes are additional values associated with each document that can be used to perform additional filtering and sorting during search. Attributes are named. Attribute names are case insensitive. Attributes are not full-text indexed; they are stored in the index as is. Currently supported attribute types are: (since 1.10-beta); (since 2.1.1-beta); [ 5 ] [ 6 ] Sphinx, like classic SQL  databases , works with a so-called fixed  schema , that is, a set of predefined attribute columns. These work well when most of the data stored actually has values: mapping sparse data to static columns can be cumbersome. Assume for example that you're running a price comparison or an auction site with many different products categories. Some of the attributes like the price or the vendor are identical across all goods. But from there, for laptops, you also need to store the weight, screen size, HDD type, RAM size, etc. And, say, for shovels, you probably want to store the color, the handle length, and so on. So it's manageable across a single category, but all the distinct fields that you need for all the goods across all the categories are legion. The JSON field can be used to overcome this. Inside the JSON attribute you don't need a fixed structure. You can have various keys which may or may not be present in all documents. When you try to filter on one of these keys, Sphinx will ignore documents that don't have the key in the JSON attribute and will work only with those documents that have it. Up until version 3, Sphinx is  dual licensed ; either: Since version 3, Sphinx has become proprietary, with a promise to release its source code in the future [ 7 ] In 2017, key members of the original Sphinx team forked the project under the name Manticore, [ 18 ]  with the intention of fixing bugs and developing new features. [ 19 ]  Unlike Sphinx, Manticore continues to be released as open source under version 3 of the  GPL . [ 20 ]"
  },
  {
    "id": 110,
    "title": "Sphinx (search engine)",
    "content": "Sphinx  is a  fulltext   search engine  that provides text search functionality to client applications. Sphinx can be used either as a stand-alone server or as a  storage engine  (\"SphinxSE\") for the MySQL family of databases.\nWhen run as a standalone server Sphinx operates similar to a  DBMS  and can communicate with  MySQL ,  MariaDB  and  PostgreSQL  through their native protocols or with any ODBC-compliant DBMS via  ODBC .\n MariaDB , a fork of MySQL, is distributed with SphinxSE. [ 2 ] If Sphinx is run as a stand-alone server, it is possible to use SphinxAPI to connect an application to it. Official implementations of the API are available for  PHP ,  Java ,  Perl ,  Ruby  and  Python  languages. Unofficial implementations for other languages, as well as various third party [ 3 ]  plugins and modules are also available. Other data sources can be indexed via pipe in a custom  XML  format. [ 4 ] The Sphinx search daemon supports the MySQL binary network protocol and can be accessed with the regular MySQL API and/or clients. Sphinx supports a subset of  SQL  known as SphinxQL. It supports standard querying of all index types with SELECT, modifying RealTime indexes with INSERT, REPLACE, and DELETE, and more. Sphinx can also provide a special storage engine for MariaDB and MySQL databases. This allows those MySQL, MariaDB to communicate with Sphinx's  searchd  to run queries and obtain results. Sphinx indices are treated like regular SQL tables. The SphinxSE storage engine is shipped with MariaDB. Sphinx is configured to examine a data set via its Indexer. The Indexer process creates a full-text index (a special  data structure  that enables quick keyword searches) from the given data/text.   Full-text  fields are the resulting content that is indexed by Sphinx; they can be (quickly) searched for keywords. Fields are named, and you can limit your searches to a single field (e.g. search through \"title\" only) or a subset of fields (e.g. to \"title\" and \"abstract\" only). Sphinx's index format generally supports up to 256 fields. Note that the original data is not stored in the Sphinx index, but are discarded during the Indexing process; Sphinx assumes that you store those contents elsewhere. Attributes are additional values associated with each document that can be used to perform additional filtering and sorting during search. Attributes are named. Attribute names are case insensitive. Attributes are not full-text indexed; they are stored in the index as is. Currently supported attribute types are: (since 1.10-beta); (since 2.1.1-beta); [ 5 ] [ 6 ] Sphinx, like classic SQL  databases , works with a so-called fixed  schema , that is, a set of predefined attribute columns. These work well when most of the data stored actually has values: mapping sparse data to static columns can be cumbersome. Assume for example that you're running a price comparison or an auction site with many different products categories. Some of the attributes like the price or the vendor are identical across all goods. But from there, for laptops, you also need to store the weight, screen size, HDD type, RAM size, etc. And, say, for shovels, you probably want to store the color, the handle length, and so on. So it's manageable across a single category, but all the distinct fields that you need for all the goods across all the categories are legion. The JSON field can be used to overcome this. Inside the JSON attribute you don't need a fixed structure. You can have various keys which may or may not be present in all documents. When you try to filter on one of these keys, Sphinx will ignore documents that don't have the key in the JSON attribute and will work only with those documents that have it. Up until version 3, Sphinx is  dual licensed ; either: Since version 3, Sphinx has become proprietary, with a promise to release its source code in the future [ 7 ] In 2017, key members of the original Sphinx team forked the project under the name Manticore, [ 18 ]  with the intention of fixing bugs and developing new features. [ 19 ]  Unlike Sphinx, Manticore continues to be released as open source under version 3 of the  GPL . [ 20 ]"
  },
  {
    "id": 111,
    "title": "Terrier (search engine)",
    "content": "The  Terrier IR Platform  is a modular  open source  software for the rapid development of large-scale  information retrieval  applications. Terrier was developed by members of the  Information Retrieval Research Group , Department of Computing Science, at the  University of Glasgow . A core version of Terrier is available as open source software under the  Mozilla Public License  (MPL), with the aim to facilitate experimentation and research in the wider information retrieval community. Terrier is written in  Java . [ 1 ] This  free and open-source software  article is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 112,
    "title": "Xapian",
    "content": "Xapian  is a  free and open-source  probabilistic  information retrieval  library, released under the  GNU General Public License  (GPL). [ 2 ]   It is a full-text  search engine  library for programmers. It is written in  C++ , with bindings to allow use from  Perl ,  Python  (2 and 3),  PHP  (5 and 7),  Java ,  Tcl ,  C# ,  Ruby ,  Lua ,  Erlang ,  Node.js  and  R . [ 1 ] [ 3 ]   Xapian is highly portable and runs on  Linux ,  OS X ,  FreeBSD ,  NetBSD ,  OpenBSD ,  Solaris ,  HP-UX ,  AIX ,  Windows ,  OS/2 [ 4 ] [ 2 ]  and  Hurd , [ 5 ] [ 6 ]  as well as  Tru64 . [ citation needed ]  Xapian grew out of the Muscat search engine, written by  Dr. Martin F. Porter  at the University of Cambridge. [ 7 ]  The first official release of Xapian was version 0.5.0 on September 20, 2002. [ 8 ] Xapian allows developers to add advanced indexing and search facilities to their own applications.\nOrganisations and projects using Xapian include the Library of the University of Cologne,  Debian ,  Die Zeit ,  MoinMoin , and  One Laptop per Child . [ 9 ]"
  },
  {
    "id": 113,
    "title": "Whoosh",
    "content": "Whoosh  may refer to:"
  },
  {
    "id": 114,
    "title": "Joseph Marie Jacquard",
    "content": "Joseph Marie Charles   dit  (called or nicknamed)  Jacquard  ( French:   [ʒakaʁ] ; 7 July 1752 – 7 August 1834) was a  French  weaver and merchant. He played an important role in the development of the earliest programmable loom (the \" Jacquard loom \"), which in turn played an important role in the development of other programmable machines, such as an early version of digital compiler used by  IBM  to develop the modern day  computer . In his grandfather's generation, several branches of the Charles family lived in Lyon's Couzon-Au-Mont d’Or suburb (on Lyon's north side, along the  Saône  River).  To distinguish the various branches, the community gave them nicknames; Joseph's branch was called \"Jacquard\" Charles.  Thus, Joseph's grandfather was Bartholomew Charles  dit  [called] Jacquard. [ 1 ] [ 2 ] Joseph Marie Charles  dit  Jacquard was born into a conservative Catholic family in  Lyon , France, on 7 July 1752.  He was one of nine children of Jean Charles  dit  Jacquard, a master weaver of Lyon, and his wife, Antoinette Rive.  However, only Joseph and his sister Clémence (born 7 November 1747) survived to adulthood.  Although his father was a man of property, Joseph received no formal schooling and remained illiterate until he was 13.  He was finally taught by his brother-in-law, Jean-Marie Barret, who ran a printing and book selling business.  Barret also introduced Joseph to learned societies and scholars. [ 3 ]  Joseph initially helped his father operate his loom, but the work proved too arduous, so Jacquard was placed first with a bookbinder and then with a maker of printers' type. [ 4 ] His mother died in 1762, and when his father died in 1772, Joseph inherited his father's house, looms and workshop as well as a vineyard and quarry in Couzon-au-Mont d’Or.  Joseph then dabbled in real estate.  In 1778, he listed his occupations as master weaver and silk merchant. [ 3 ]   Jacquard's occupation at this time is problematic because by 1780 most silk weavers did not work independently; instead, they worked for wages from silk merchants, and Jacquard was not registered as a silk merchant in Lyon. [ 5 ] There is some confusion about Jacquard's early work history.  British economist Sir  John Bowring  met Jacquard, who told Bowring that at one time he had been a maker of straw hats. [ 6 ]  Eymard claimed that before becoming involved in the weaving of silk, Jacquard was a type-founder (a maker of printers' type), a soldier, a bleacher ( blanchisseur ) of straw hats, and a lime burner (a maker of  lime  for mortar). [ 7 ]  Barlow claims that before marrying, Jacquard had worked for a bookbinder, a type-founder, and a maker of cutlery. After marrying, Jacquard tried cutlery making, type-founding, and weaving. [ 8 ]  However, Barlow does not cite any sources for that information. On 26 July 1778, Joseph married Claudine Boichon.  She was a middle-class widow from Lyon who owned property and had a substantial dowry.  However, Joseph soon fell deeply into debt and was brought to court. Barlow claims that after Jacquard's father died, Jacquard started a figure-weaving business but failed and lost all his wealth. However, Barlow cites no sources to support his claim. [ 8 ]   To settle his debts, he was obliged to sell his inheritance and to appropriate his wife's dowry.  His wife retained a house in Oullins (on Lyon's south side, along the Rhone River), where the couple resided.  On 19 April 1779, the couple had their only child, a son, Jean Marie. [ 3 ]   Charles Ballot stated that after the  rebellion of Lyon in 1793  was suppressed, Joseph and his son escaped from the city by joining the revolutionary army.  They fought together in the  Rhine campaign of 1795 , serving in the Rhone-and-Loire battalion under General  Jean Charles Pichegru .  Joseph's son was killed outside of  Heidelberg .  However, Ballot repeated rumors and was a sloppy historian. For example, he stated that Jacquard's wife Claudette Boichon was the daughter of Antoine-Hélon Boichon, a master swordsmith, whereas Claudette was a widow who had been married to a Mr. Boichon before she married Jacquard. [ 9 ] By 1800, Joseph began inventing various devices.  He invented a treadle loom in 1800, a loom to weave fishing nets in 1803, and starting in 1804, the \"Jacquard\" loom, which would weave patterned silk automatically.  However, these early inventions did not operate well and thus were unsuccessful. [ 5 ] In 1801, Jacquard exhibited his invention at the  Exposition des produits de l'industrie française  in Paris, where he was awarded a bronze medal. [ 10 ] \nIn 1803 he was summoned to  Paris  and attached to the  Conservatoire des Arts et Metiers . A loom by  Jacques de Vaucanson  on display there suggested various improvements in his own, which he gradually perfected to its final state. The loom was declared public property in 1805, and Jacquard was rewarded with a pension and a royalty on each machine. Although his invention was fiercely opposed by the silk-weavers, who feared that its introduction, owing to the saving of labour, would deprive them of their livelihood, its advantages secured its general adoption, and by 1812 there were 11,000 Jacquard looms in use in France. This claim has been challenged:  Initially few Jacquard looms were sold because of problems with the  punched card  mechanism.  Only after 1815 — once Jean Antoine Breton had solved the problems with the punched card mechanism — did sales of looms increase. [ 11 ] [ 12 ] [ 13 ] Jacquard died at  Oullins  (Rhône), 7 August 1834. [ 14 ]  Six years later, a statue was erected to him in Lyon, on the site where his 1801 exhibit loom was destroyed. The Jacquard Loom is a mechanical  loom  that uses pasteboard cards with punched holes, each card corresponding to one row of the design.  Multiple rows of holes are punched in the cards and the many cards that compose the design of the textile are strung together in order. It is based on earlier inventions by the Frenchmen  Basile Bouchon  (1725), Jean-Baptiste Falcon (1728) and  Jacques Vaucanson  (1740). [ 15 ] To understand the Jacquard loom, some basic knowledge of  weaving  is necessary.  Parallel threads (the \"warp\") are stretched across a rectangular frame (the \"loom\").  For plain cloth, every other warp thread is raised.  Another thread (the \"weft thread\") is then passed (at a right angle to the warp) through the space (the \"shed\") between the lower and the upper warp threads.  Then the raised warp threads are lowered, the alternate warp threads are raised, and the weft thread is passed through the shed in the opposite direction.  With hundreds of such cycles, the cloth is gradually created. By raising different (not just alternate) warp threads and using colored threads in the weft, the texture, color, design, and pattern can be varied to create varied and highly desirable fabrics.  Weaving elaborate patterns or designs manually is a slow, complicated procedure subject to error.  Jacquard's loom was intended to automate this process. Jacquard was not the first to try to automate the process of weaving.  In 1725  Basile Bouchon  invented an attachment for draw looms that used a broad strip of punched paper to select the warp threads that would be raised during weaving. [ 19 ]   Specifically, Bouchon's innovation involved a row of hooks.  The curved portion of each hook snagged a string that could raise one of the warp threads, whereas the straight portion of each hook pressed against the punched paper, which was draped around a perforated cylinder.  Whenever the hook pressed against the solid paper, pushing the cylinder forward would raise the corresponding warp thread; whereas whenever the hook met a hole in the paper, pushing the cylinder forward would allow the hook to slip inside the cylinder and the corresponding warp thread would not be raised.  Bouchon's loom was unsuccessful because it could handle only a modest number of warp threads. [ 20 ] [ 21 ] By 1737, a master silk weaver of  Lyon , Jean Falcon, had increased the number of warp threads that the loom could handle automatically.  He developed an attachment for looms in which Bouchon's paper strip was replaced by a chain of punched cards, which could deflect multiple rows of hooks simultaneously.  Like Bouchon, Falcon used a \"cylinder\" (actually, a four-sided perforated tube) to hold each card in place while it was pressed against the rows of hooks. [ 22 ]   His loom was modestly successful; about 40 such looms had been sold by 1762. [ 23 ] In 1741,  Jacques de Vaucanson , a French inventor who designed and built automated mechanical toys, was appointed inspector of silk factories. [ 24 ]   Between 1747 and 1750, [ 25 ]  he tried to automate Bouchon's mechanism.  In Vaucanson's mechanism, the hooks that were to lift the warp threads were selected by long pins or \"needles\", which were pressed against a sheet of punched paper that was draped around a perforated cylinder.  Specifically, each hook passed at a right angle through an eyelet of a needle.  When the cylinder was pressed against the array of needles, some of the needles, pressing against solid paper, would move forward, which in turn would tilt the corresponding hooks.  The hooks that were tilted would  not  be raised, so the warp threads that were snagged by those hooks would remain in place; however, the hooks that were  not  tilted, would be raised, and the warp threads that were snagged by those hooks would also be raised.  By placing his mechanism above the loom, Vaucanson eliminated the complicated system of weights and cords (tail cords, simple, pulley box, etc.) that had been used to select which warp threads were to be raised during weaving.  Vaucanson also added a ratchet mechanism to advance the punched paper each time that the cylinder was pushed against the row of hooks. [ 26 ] [ 27 ] [ 28 ] [ 29 ]   However, Vaucanson's loom was not successful, probably because, like Bouchon's mechanism, it could not control enough warp threads to make sufficiently elaborate patterns to justify the cost of the mechanism. [ 25 ] To stimulate the French textile industry, which was competing with Britain's industrialized industry,  Napoleon Bonaparte  placed large orders for Lyon's silk, starting in 1802. [ 13 ]   In 1804, [ 30 ]  at the urging of Lyon fabric maker and inventor Gabriel Dutillieu, Jacquard studied Vaucanson's loom, which was stored at the  Conservatoire des Arts et Métiers  in Paris. [ 5 ]   By 1805 Jacquard had eliminated the paper strip from Vaucanson's mechanism and returned to using Falcon's chain of punched cards. [ 31 ] The potential of Jacquard's loom was immediately recognized.  On 12 April 1805, Emperor Napoleon and  Empress Josephine  visited Lyon and viewed Jacquard's new loom.  On 15 April 1805, the emperor granted the patent for Jacquard's loom to the city of Lyon.  In return, Jacquard received a lifelong pension of 3,000 francs; furthermore, he received a royalty of 50 francs for each loom that was bought and used during the period from 1805 to 1811. [ 13 ]"
  },
  {
    "id": 115,
    "title": "Jacquard machine",
    "content": "The  Jacquard machine  ( French:   [ʒakaʁ] ) is a device fitted to a  loom  that simplifies the process of manufacturing  textiles  with such complex patterns as  brocade ,  damask  and  matelassé . [ 3 ]  The resulting ensemble of the loom and Jacquard machine is then called a  Jacquard loom . The machine was patented by  Joseph Marie Jacquard  in 1804, [ 4 ] [ 5 ] [ 6 ] [ 7 ]  based on earlier inventions by the Frenchmen  Basile Bouchon  (1725), Jean Baptiste Falcon (1728), and  Jacques Vaucanson  (1740). [ 8 ]  The machine was controlled by a \"chain of cards\"; a number of  punched cards  laced together into a continuous sequence. [ 9 ]  Multiple rows of holes were punched on each card, with one complete card corresponding to one row of the design. Both the Jacquard process and the necessary loom attachment are named after their inventor. This mechanism is probably one of the most important  weaving  innovations as Jacquard  shedding  made possible the automatic production of unlimited varieties of complex pattern weaving. The term \"Jacquard\" is not specific or limited to any particular loom, but rather refers to the added control mechanism that automates the patterning. The process can also be used for patterned knitwear and machine-knitted textiles such as  jerseys . [ 10 ] This use of replaceable  punched cards  to control a sequence of operations is considered an important step in the  history of computing hardware , having inspired  Charles Babbage 's  Analytical Engine . Traditionally, figured designs were made on a  drawloom . The  heddles  with warp ends to be pulled up were manually selected by a second operator, the draw boy, not the weaver. The work was slow and labour-intensive, and the complexity of the pattern was limited by practical factor. The first prototype of a Jacquard-type loom was made in the second half of the 15th century by an  Italian  weaver from  Calabria , Jean le Calabrais, who was invited to  Lyon  by  Louis XI . [ 11 ] [ 12 ]  He introduced a new kind of machine which was able to work the yarns faster and more precisely. Over the years, improvements to the loom were ongoing. [ 13 ] An improvement of the draw loom took place in 1725, when  Basile Bouchon  introduced the principle of applying a perforated band of paper. A continuous roll of paper was punched by hand, in sections, each of which represented one lash or tread, and the length of the roll was determined by the number of shots in each repeat of pattern. The Jacquard machine then evolved from this approach. Joseph Marie Jacquard saw that a  mechanism  could be developed for the production of sophisticated patterns. He possibly combined mechanical elements of other inventors, but certainly innovated. His machine was generally similar to  Vaucanson 's arrangement, but he made use of Jean-Baptiste Falcon's individual  pasteboard  cards and his square prism (or card \"cylinder\"): he is credited with having fully perforated each of its four sides, replacing Vaucanson's perforated \"barrel\". Jacquard's machine contained eight rows of needles and uprights, where Vaucanson had a double row. This modification enabled him to increase the figuring capacity of the machine. In his first machine, he supported the harness by knotted cords, which he elevated by a single trap board. One of the chief advantages claimed for the Jacquard machine was that unlike previous damask-weaving machines, in which the figuring shed was usually drawn once for every four shots, with the new apparatus, it could be drawn on every shot, thus producing a fabric with greater definition of outline. [ 14 ] Jacquard's invention had a deep influence on  Charles Babbage . In that respect, he is viewed by some authors as a precursor of modern  computing  technology. [ 15 ] As shown in the diagram, the cards are fastened into a continuous chain (1) which passes over a square box. At each quarter rotation, a new card is presented to the Jacquard head which represents one row (one \"pick\" of the  shuttle  carrying the  weft ). The box swings from the right to the position shown and presses against the control rods (2). For each hole in the card, a rod passes through and is unmoved; where there is no hole, a rod is pushed to the left. Each rod acts upon a hook (3). When the rod is pushed in, the hook moves out of position to the left; a rod that is not pushed in leaves its hook in place.  A beam (4) then rises under the hooks, and the hooks in the rest position are raised. The hooks that have been displaced are not moved by the beam. Each hook can have multiple cords (5). Each cord passes through a guide (6) and is attached to a corresponding  heddle  (7) and return weight (8). The heddles raise the  warp  to create the  shed  through which the shuttle carrying the weft will pass. [ 16 ]  A loom with a 400-hook head might have four threads connected to each hook, resulting in a fabric that is 1600  warp  ends wide with four repeats of the weave going across. The term \"Jacquard loom\" is somewhat inaccurate.  It is the \"Jacquard head\" that adapts to a great many  dobby looms  that allow the weaving machine to then create the intricate patterns often seen in Jacquard weaving. Jacquard-driven looms, although relatively common in the textile industry, are not as ubiquitous as dobby looms which are usually faster and much cheaper to operate. However, dobby looms are not capable of producing many different weaves from one  warp . Modern jacquard machines are controlled by computers in place of the original punched cards and can have thousands of hooks. The threading of a Jacquard machine is so labor-intensive that many looms are threaded only once. Subsequent  warps  are then tied into the existing warp with the help of a knotting robot which ties on each new thread individually. Even for a small loom with only a few thousand  warp ends , the process of re-threading can take days. Originally, Jacquard machines were  mechanical , and the fabric design was stored on a series of  punched cards  which were joined to form a continuous chain. The Jacquards were often small and controlled relatively few warp ends. This required a number of repeats across the loom width. Larger capacity machines, or the use of multiple machines, allowed greater control with fewer repeats; hence, larger designs could be woven across the loom width. A  factory  must choose looms and shedding mechanisms to suit its commercial requirements. As a rule, greater warp control means greater expense. So it is not economical to purchase Jacquard machines if one can make do with a  dobby mechanism . Beyond the capital expense, Jacquard machines cost more to maintain as they are complex, require highly-skilled operators, and use expensive systems to prepare designs for the loom. Thus, they are more likely to produce faults than dobby or cam shedding. Also, the looms will not run as quickly and down-time will increase because it takes time to change the continuous chain of cards when a design changes. It is best to weave larger batches with mechanical Jacquards. In 1855, a Frenchman [ 17 ]  adapted the Jacquard mechanism to a system by which it could be worked by electro-magnets. There was significant interest, but trials were not successful, and the development was soon forgotten. Bonas Textile Machinery NV launched the first successful electronic Jacquard at ITMA  Milan  in 1983. [ 18 ] [ a ]  Although the machines were initially small, modern technology has allowed Jacquard machine capacity to increase significantly, and single end warp control can extend to more than 10,000 warp ends. [ 20 ]  This eliminates the need for repeats and symmetrical designs and invites almost infinite versatility. The computer-controlled machines significantly reduce the down time associated with changing punchcards, thereby allowing smaller batch sizes. However, electronic Jacquards are costly and may not be necessary in a factory weaving large batch sizes and smaller designs. Larger machines accommodating single-end warp control are very expensive and can only be justified when great versatility or very specialized designs are required. For example, they are an ideal tool to increase the ability and versatility of niche linen Jacquard weavers who remain active in Europe and the West, while most large batch commodity weaving has moved to low-cost production. [ citation needed ] Linen  products associated with Jacquard weaving are linen damask napery, Jacquard apparel fabrics and damask bed linen. Jacquard weaving uses all sorts of fibers and blends of fibers, and it is used in the production of fabrics for many end uses. Jacquard weaving can also be used to create fabrics that have a  Matelassé  or a  brocade  pattern. [ 21 ] A pinnacle of production using a Jacquard machine is a prayer book, woven in silk, entitled  Livre de Prières. Tissé d'après les enluminures des manuscrits du XIVe au XVIe siècle . [ 22 ]  All 58 pages of the prayer book were woven silk, made with a Jacquard machine using black and gray thread, at 160 threads per cm (400 threads per inch). The pages have elaborate borders with text and pictures of saints. An estimated 200,000 to 500,000 punchcards were necessary to encode the pages. The book was issued in 1886 and 1887 in Lyon, France, and was publicly displayed at the 1889  Exposition Universelle  (World's Fair). It was designed by R. P. J. Hervier, woven by J. A. Henry, and published by A. Roux. [ 23 ]   It took two years and almost 50 trials to get correct. An estimated 50 or 60 copies were produced. The Jacquard head used replaceable  punched cards  to control a sequence of operations. It is considered an important step in the  history of computing hardware . [ 24 ]   The ability to change the pattern of the loom's weave by simply changing cards was an important conceptual precursor to the development of  computer programming  and data entry.  Charles Babbage  knew of Jacquard machines and planned to use cards to store programs in his  Analytical Engine . In the late 19th century,  Herman Hollerith  took the idea of using punched cards to store information a step further when he created a punched card tabulating machine which he used to input data for the  1890 U.S. Census . A large data processing industry using punched-card technology was developed in the first half of the twentieth century—dominated initially by the  International Business Machine  corporation (IBM) with its line of  unit record equipment . The cards were used for data, however, with programming done by  plugboards . Some early computers, such as the 1944  IBM Automatic Sequence Controlled Calculator  (Harvard Mark I) received program instructions from a paper tape punched with holes, similar to Jacquard's string of cards. Later computers executed programs from higher-speed memory, though cards were commonly used to load the programs into memory. Punched cards remained in use in computing up until the mid-1980s."
  },
  {
    "id": 116,
    "title": "Herman Hollerith",
    "content": "Herman Hollerith  (February 29, 1860 – November 17, 1929) was a German-American statistician, inventor, and businessman who developed an electromechanical  tabulating machine  for  punched cards  to assist in summarizing information and, later, in accounting. His invention of the punched card tabulating machine, patented in 1884, marks the beginning of the era of mechanized binary code and semiautomatic  data processing  systems, and his concept dominated that landscape for nearly a century. [ 1 ] [ 2 ] [ 3 ] Hollerith founded a company that was amalgamated in 1911 with several other companies to form the  Computing-Tabulating-Recording Company . In 1924, the company was renamed \"International Business Machines\" ( IBM ) and became one of the largest and most successful companies of the 20th century. Hollerith is regarded as one of the seminal figures in the development of data processing. [ 4 ] Herman Hollerith was born in  Buffalo, New York , in 1860, where he also spent his early childhood. [ 5 ]  His parents were  German  immigrants; his father, Georg Hollerith, was a school teacher from  Großfischlingen ,  Rhineland-Palatinate . [ 6 ] [ 7 ]  He entered the  City College of New York  in 1875, graduated from the  Columbia School of Mines  with an  Engineer of Mines  degree in 1879 at age 19, and, in 1890, earned a  Doctor of Philosophy  based on his development of the tabulating system. [ 1 ] [ 8 ]  In 1882, Hollerith joined the  Massachusetts Institute of Technology  where he taught mechanical engineering and conducted his first experiments with punched cards. [ 9 ]  He eventually moved to Washington, D.C., living in  Georgetown  with a home on 29th Street and a business building at 31st Street and the  Chesapeake and Ohio Canal , where today there is a commemorative plaque installed by  IBM . He died of a heart attack in Washington, D.C., at age 69. [ 9 ] At the suggestion of  John Shaw Billings , Hollerith developed a mechanism using electrical connections to increment a counter, recording information. [ 10 ]  A key idea was that a datum could be recorded by the presence or absence of a hole at a specific location on a card. For example, if a specific hole location indicates  marital status , then a hole there can indicate  married  while not having a hole indicates  single . Hollerith determined that data in specified locations on a card, arranged in rows and columns, could be counted or sorted electromechanically. A description of this system,  An Electric Tabulating System (1889) , was submitted by Hollerith to  Columbia University  as his doctoral thesis, [ 11 ]  and is reprinted in  Brian Randell 's 1982  The Origins of Digital Computers, Selected Papers . [ 12 ]  On January 8, 1889, Hollerith was issued U.S. Patent 395,782, [ 13 ]  claim 2 of which reads: The herein-described method of compiling statistics, which consists in recording separate statistical items pertaining to the individual by holes or combinations of holes punched in sheets of electrically non-conducting material, and bearing a specific relation to each other and to a standard, and then counting or tallying such statistical items separately or in combination by means of mechanical counters operated by electro-magnets the circuits through which are controlled by the perforated sheets, substantially as and for the purpose set forth. Hollerith had left teaching and began working for the  United States Census Bureau  in the year he filed his first patent application. Titled \"Art of Compiling Statistics\", it was filed on September 23, 1884; U.S. Patent 395,782 was granted on January 8, 1889. [ 13 ] Hollerith initially did business under his own name, as  The Hollerith Electric Tabulating System , specializing in  punched card data processing equipment . [ 16 ]  He provided  tabulators  and other machines under contract for the Census Office, which used them for the  1890 census . The net effect of the many changes from the 1880 census: the larger population, the data items to be collected, the Census Bureau headcount, the scheduled publications, and the use of Hollerith's electromechanical tabulators, reduced the time required to process the census from eight years for the  1880 census  to six years for the 1890 census. [ 17 ] In 1896, Hollerith founded the Tabulating Machine Company (in 1905 renamed The Tabulating Machine Company). [ 18 ]  Many major census bureaus around the world leased his equipment and purchased his cards, as did major insurance companies. Hollerith's machines were used for censuses in  England & Wales ,  Italy ,  Germany ,  Russia ,  Austria ,  Canada ,  France ,  Norway ,  Puerto Rico ,  Cuba , and the  Philippines , and again in the  1900 U.S. census . [ 1 ] He invented the first automatic card-feed mechanism and the first  keypunch . The 1890 Tabulator was  hardwired  to operate on 1890 Census cards. A  control panel  in his 1906 Type I Tabulator simplified rewiring for different jobs. The 1920s  removable control panel  supported prewiring and near instant job changing. These inventions were among the foundations of the data processing industry, and Hollerith's punched cards (later used for  computer input/output ) continued in use for almost a century. [ 19 ] In 1911, four corporations, including Hollerith's firm, were amalgamated to form a fifth company, the  Computing-Tabulating-Recording Company  (CTR). [ 20 ]  Under the presidency of  Thomas J. Watson , CTR was renamed  International Business Machines Corporation  (IBM) in 1924. By 1933 The Tabulating Machine Company name had disappeared as subsidiary companies were subsumed by IBM. [ 21 ] Herman Hollerith died November 17, 1929. Hollerith is buried at  Oak Hill Cemetery  in the  Georgetown neighborhood  of Washington, D.C. [ 15 ] [ 22 ] Hollerith cards  were named after Herman Hollerith,\nas were  Hollerith strings and Hollerith constants .\n [ 23 ] His great-grandson, the Rt. Rev.  Herman Hollerith IV , was the  Episcopal  bishop of the  Diocese of Southern Virginia , and another great-grandson,  Randolph Marshall Hollerith , is an Episcopal priest and the dean of  Washington National Cathedral  in Washington, D.C. [ 24 ] [ 25 ]"
  },
  {
    "id": 117,
    "title": "Punched card",
    "content": "A  punched card  (also  punch card [ 1 ]  or  punched-card [ 2 ] ) is a piece of  card stock  that stores  digital data  using punched holes. Punched cards were once common in  data processing  and the control of  automated machines . Punched cards were widely used in the 20th century, where  unit record machines , organized into  data processing systems , used punched cards for data input, output, and storage. [ 3 ] [ 4 ]  The  IBM  12-row/80-column punched card format came to dominate the industry. Many early  digital computers  used punched cards as the primary medium for input of both  computer programs  and  data . Data can be entered onto a punched card using a  keypunch . While punched cards are now obsolete as a  storage medium , as of 2012, some  voting machines  still used punched cards to record votes. [ 5 ]  Punched cards also had a significant cultural impact in the 20th century. The idea of control and data storage via punched holes was developed independently on several occasions in the modern period. In most cases there is no evidence that each of the inventors was aware of the earlier work. Basile Bouchon  developed the control of a  loom  by punched holes in paper tape in 1725. The design was improved by his assistant Jean-Baptiste Falcon and by  Jacques Vaucanson . [ 6 ]  Although these improvements controlled the patterns woven, they still required an assistant to operate the mechanism. In 1804  Joseph Marie Jacquard  demonstrated a mechanism to automate loom operation. A number of punched cards were linked into a chain of any length. Each card held the instructions for  shedding  (raising and lowering the  warp ) and selecting the shuttle for a single pass. [ 7 ] Semyon Korsakov  was reputedly the first to propose punched cards in informatics for information store and search. Korsakov announced his new method and machines in September 1832. [ 8 ] Charles Babbage  proposed the use of \"Number Cards\", \"pierced with certain holes and stand[ing] opposite levers connected with a set of figure wheels ... advanced they push in those levers opposite to which there are no holes on the cards and thus transfer that number together with its sign\" in his description of the Calculating Engine's Store. [ 9 ]  There is no evidence that he built a practical example. In 1881,  Jules Carpentier  developed a method of recording and playing back performances on a  harmonium  using punched cards. The system was called the  Mélographe Répétiteur  and \"writes down ordinary music played on the keyboard dans le langage de Jacquard\", [ 10 ]  that is as holes punched in a series of cards. By 1887 Carpentier had separated the mechanism into the  Melograph  which recorded the player's key presses and the  Melotrope  which played the music. [ 11 ] [ 12 ] At the end of the 1800s  Herman Hollerith  created a method for recording data on a medium that could then be read by a machine, [ 13 ] [ 14 ] [ 15 ] [ 16 ]  developing punched card data processing technology for the  1890 U.S. census . [ 17 ]  His  tabulating machines  read and summarized data stored on punched cards and they began use for government and commercial data processing. Initially, these  electromechanical  machines only counted holes, but by the 1920s they had units for carrying out basic arithmetic operations. [ 18 ] : 124  \nHollerith founded the  Tabulating Machine Company  (1896) which was one of four companies that were  amalgamated via stock acquisition  to form a fifth company,  Computing-Tabulating-Recording Company  (CTR) in 1911, later renamed  International Business Machines Corporation (IBM)  in 1924. Other companies entering the punched card business included  The Tabulator Limited  (Britain, 1902),  Deutsche Hollerith-Maschinen Gesellschaft mbH (Dehomag)  (Germany, 1911),  Powers Accounting Machine Company  (US, 1911),  Remington Rand  (US, 1927), and  H.W. Egli Bull  (France, 1931). [ 19 ]  These companies, and others, manufactured and marketed a variety of punched cards and  unit record machines  for creating, sorting, and tabulating punched cards, even after the development of electronic computers in the 1950s. Both IBM and Remington Rand tied punched card purchases to machine leases, a violation of the US 1914  Clayton Antitrust Act . In 1932, the US government took both to court on this issue. Remington Rand settled quickly. IBM viewed its business as providing a service and that the cards were part of the machine. IBM fought all the way to the Supreme Court and lost in 1936; the court ruled that IBM could only set card specifications. [ 20 ] [ 21 ] : 300–301 \"By 1937... IBM had 32 presses at work in Endicott, N.Y., printing, cutting and stacking five to 10 million punched cards every day.\" [ 22 ]  Punched cards were even used as legal documents, such as  U.S. Government  checks [ 23 ]  and savings bonds. [ 24 ] During  World War II  punched card equipment was used by the Allies in some of their efforts to decrypt Axis communications. See, for example,  Central Bureau  in Australia. At  Bletchley Park  in England, \"some 2 million punched cards a week were being produced, indicating the sheer scale of this part of the operation\". [ 25 ]  In Nazi Germany, punched cards were used for the censuses of various regions and other purposes [ 26 ] [ 27 ]  (see  IBM and the Holocaust ). Punched card technology developed into a powerful tool for business data-processing. By 1950 punched cards had become ubiquitous in industry and government. \"Do not fold,  spindle  or mutilate,\" a warning that appeared on some punched cards distributed as documents such as checks and utility bills to be returned for processing, became a motto for the post- World War II  era. [ 28 ] [ 29 ] In 1956 [ 30 ]  IBM signed a  consent decree  requiring, amongst other things, that IBM would by 1962 have no more than one-half of the punched card manufacturing capacity in the United States.  Tom Watson Jr.'s  decision to sign this decree, where IBM saw the punched card provisions as the most significant point, completed the transfer of power to him from  Thomas Watson, Sr . [ 21 ] The Univac  UNITYPER  introduced magnetic tape for data entry in the 1950s. During the 1960s, the punched card was gradually replaced as the primary means for  data storage  by  magnetic tape , as better, more capable computers became available.  Mohawk Data Sciences  introduced a magnetic tape encoder in 1965, a system marketed as a keypunch replacement which was somewhat successful. Punched cards were still commonly used for entering both data and computer programs until the mid-1980s when the combination of lower cost  magnetic disk storage , and affordable  interactive terminals  on less expensive  minicomputers  made punched cards obsolete for these roles as well. [ 31 ] : 151   However, their influence lives on through many standard conventions and file formats. The terminals that replaced the punched cards, the  IBM 3270  for example, displayed 80  columns of text  in  text mode , for compatibility with existing software. Some programs still operate on the convention of 80 text columns, although fewer and fewer do as newer systems employ  graphical user interfaces  with variable-width type fonts. The terms  punched card ,  punch card , and  punchcard  were all commonly used, as were  IBM card  and  Hollerith card  (after  Herman Hollerith ). [ 1 ]  IBM used \"IBM card\" or, later, \"punched card\" at first mention in its documentation and thereafter simply \"card\" or \"cards\". [ 33 ] [ 34 ]  Specific formats were often indicated by the number of character positions available, e.g.  80-column card . A sequence of cards that is input to or output from some step in an application's processing is called a  card deck  or simply  deck . The rectangular, round, or oval bits of paper punched out were called  chad  ( chads ) or  chips  (in IBM usage). Sequential card columns allocated for a specific use, such as names, addresses, multi-digit numbers, etc., are known as a  field . The first card of a group of cards, containing fixed or indicative information for that group, is known as a  master card . Cards that are not master cards are  detail cards . The Hollerith punched cards used for the 1890 U.S. census were blank. [ 35 ]  Following that, cards commonly had printing such that the row and column position of a hole could be easily seen. Printing could include having fields named and marked by vertical lines, logos, and more. [ 36 ]  \"General purpose\" layouts (see, for example, the IBM 5081 below) were also available. For applications requiring master cards to be separated from following detail cards, the respective cards had different upper corner diagonal cuts and thus could be separated by a sorter. [ 37 ]  Other cards typically had one upper corner diagonal cut so that cards not oriented correctly, or cards with different corner cuts, could be identified. Herman Hollerith  was awarded three patents [ 39 ]  in 1889 for electromechanical  tabulating machines . These patents described both  paper tape  and rectangular cards as possible recording media. The card shown in  U.S. patent 395,781  of January 8 was printed with a template and had hole positions arranged close to the edges so they could be reached by a  railroad conductor 's  ticket punch , with the center reserved for written descriptions. Hollerith was originally inspired by railroad tickets that let the conductor encode a rough description of the passenger: I was traveling in the West and I had a ticket with what I think was called a punch photograph...the conductor...punched out a description of the individual, as light hair, dark eyes, large nose, etc. So you see, I only made a punch photograph of each person. [ 18 ] : 15 When use of the ticket punch proved tiring and error-prone, Hollerith developed the  pantograph  \"keyboard punch\". It featured an enlarged diagram of the card, indicating the positions of the holes to be punched. A printed reading board could be placed under a card that was to be read manually. [ 35 ] : 43 Hollerith envisioned a number of card sizes. In an article he wrote describing his proposed system for tabulating the  1890 U.S. census , Hollerith suggested a card  3 by  5 + 1 ⁄ 2  inches (7.6 by 14.0 cm) of  Manila stock  \"would be sufficient to answer all ordinary purposes.\" [ 40 ]  The cards used in the 1890 census had round holes, 12 rows and 24 columns. A reading board for these cards can be seen at the Columbia University Computing History site. [ 41 ]  At some point,  3 + 1 ⁄ 4  by  7 + 3 ⁄ 8  inches (83 by 187 mm) became the standard card size. These are the dimensions of the  then-current paper currency  of 1862–1923. [ 42 ]  This size was needed in order to use available banking-type storage for the 60,000,000 punched cards to come nationwide. [ 41 ] Hollerith's original system used an ad hoc coding system for each application, with groups of holes assigned specific meanings, e.g. sex or marital status. His tabulating machine had up to 40 counters, each with a dial divided into 100 divisions, with two indicator hands; one which stepped one unit with each counting pulse, the other which advanced one unit every time the other dial made a complete revolution. This arrangement allowed a count up to 9,999. During a given tabulating run counters were assigned specific holes or, using  relay logic , combination of holes. [ 40 ] Later designs led to a card with ten rows, each row assigned a digit value, 0 through 9, and 45 columns. [ 43 ]  \nThis card provided for fields to record multi-digit numbers that tabulators could sum, instead of their simply counting cards. Hollerith's 45 column punched cards are illustrated in  Comrie 's  The application of the Hollerith Tabulating Machine to Brown's Tables of the Moon . [ 44 ] By the late 1920s, customers wanted to store more data on each punched card.  Thomas J. Watson Sr. , IBM's head, asked two of his top inventors,  Clair D. Lake  and  J. Royden Pierce , to independently develop ways to increase data capacity without increasing the size of the punched card. Pierce wanted to keep round holes and 45 columns but to allow each column to store more data; Lake suggested rectangular holes, which could be spaced more tightly, allowing 80 columns per punched card, thereby nearly doubling the capacity of the older format. [ 45 ]  Watson picked the latter solution, introduced as  The IBM Card , in part because it was compatible with existing tabulator designs and in part because it could be protected by patents and give the company a distinctive advantage. [ 46 ] This IBM card format, introduced in 1928, [ 47 ]  has rectangular holes, 80 columns, and 10 rows. [ 48 ]  Card size is  7 + 3 ⁄ 8  by  3 + 1 ⁄ 4  inches (187 by 83 mm). The cards are made of smooth stock, 0.007 inches (180 μm) thick. There are about 143 cards to the inch (56/cm). In 1964, IBM changed from square to round corners. [ 49 ]  They come typically in boxes of 2,000 cards [ 50 ]  or as  continuous form  cards. Continuous form cards could be both pre-numbered and pre-punched for document control (checks, for example). [ 51 ] Initially designed to record responses to  yes–no questions , support for numeric,  alphabetic and special characters  was added through the use of columns and zones. The top three positions of a column are called  zone punching positions , 12 (top), 11, and 0 (0 may be either a zone punch or a digit punch). [ 52 ]  For decimal data the lower ten positions are called  digit punching positions , 0 (top) through 9. [ 52 ]  An arithmetic sign can be specified for a decimal field by  overpunching  the field's rightmost column with a zone punch: 12 for plus, 11 for minus (CR). For  Pound sterling   pre-decimalization currency  a  penny  column represents the values zero through eleven; 10 (top), 11, then 0 through 9 as above. An arithmetic sign can be punched in the adjacent  shilling  column. [ 53 ] : 9   Zone punches had other uses in processing, such as indicating a master card. [ 54 ] Diagram: [ 55 ]  Note: The 11 and 12 zones were also called the X and Y zones, respectively. In 1931, IBM began introducing upper-case letters and special characters (Powers-Samas had developed the first commercial alphabetic punched card representation in 1921). [ 56 ] [ 57 ] [ nb 1 ]  The 26 letters have two punches (zone [12,11,0] + digit [1–9]). The languages of Germany, Sweden, Denmark, Norway, Spain, Portugal and Finland require up to three additional letters; their punching is not shown here. [ 58 ] : 88–90   Most special characters have two or three punches (zone [12,11,0, or none] + digit [2–7] + 8); a few special characters were exceptions: \"&\" is 12 only, \"-\" is 11 only, and \"/\" is 0 + 1). The Space character has no punches. [ 58 ] : 38   The information represented in a column by a combination of zones [12, 11, 0] and digits [0–9] is dependent on the use of that column. For example, the combination \"12-1\" is the letter \"A\" in an alphabetic column, a plus signed digit \"1\" in a signed numeric column, or an unsigned digit \"1\" in a column where the \"12\" has some other use. The introduction of  EBCDIC  in 1964 defined columns with as many as six punches (zones [12,11,0,8,9] + digit [1–7]). IBM and other manufacturers used many different 80-column card  character encodings . [ 59 ] [ 60 ]  A 1969 American National Standard defined the punches for 128 characters and was named the  Hollerith Punched Card Code  (often referred to simply as  Hollerith Card Code ), honoring Hollerith. [ 58 ] : 7 For some computer applications,  binary  formats were used, where each hole represented a single binary digit (or \" bit \"), every column (or row) is treated as a simple  bit field , and every combination of holes is permitted. For example, on the  IBM 701 [ 61 ]  and  IBM 704 , [ 62 ]  card data was read, using an  IBM 711 , into memory in row binary format. For each of the twelve rows of the card, 72 of the 80 columns, skipping the other eight, would be read into two  36-bit  words, requiring 864 bits to store the whole card; a control panel was used to select the 72 columns to be read. Software would translate this data into the desired form. One convention was to use columns 1 through 72 for data, and columns 73 through 80 to sequentially number the cards, as shown in the picture above of a punched card for FORTRAN. Such numbered cards could be sorted by machine so that if a deck was dropped the sorting machine could be used to arrange it back in order. This convention continued to be used in FORTRAN, even in later systems where the data in all 80 columns could be read. The IBM card readers 3504,  3505  and the multifunction unit 3525 used a different encoding scheme for column binary data, also known as  card image , where each column, split into two rows of 6 (12–3 and 4–9) was encoded into two 8-bit bytes, holes in each group represented by bits 2 to 7 (MSb  numbering , bit 0 and 1 unused ) in successive bytes. This required 160 8-bit bytes, or 1280 bits, to store the whole card. [ 63 ] As an aid to humans who had to deal with the punched cards, the IBM 026 and later 029 and 129 key punch machines could print human-readable text above each of the 80 columns. As a prank, punched cards could be made where every possible punch position had a hole. Such \" lace cards \" lacked structural strength, and would frequently buckle and jam inside the machine. [ 64 ] The IBM 80-column punched card format dominated the industry, becoming known as just  IBM cards , even though other companies made cards and equipment to process them. [ 65 ] One of the most common punched card formats is the IBM 5081 card format, a general purpose layout with no field divisions. This format has digits printed on it corresponding to the punch positions of the digits in each of the 80 columns. Other punched card vendors manufactured cards with this same layout and number. Long cards were available with a scored stub on either end which, when torn off, left an 80 column card. The torn off card is called a  stub card . 80-column cards were available scored, on either end, creating both a  short card  and a  stub card  when torn apart. Short cards can be processed by other IBM machines. [ 51 ] [ 66 ]  A common length for stub cards was 51 columns. Stub cards were used in applications requiring tags, labels, or carbon copies. [ 51 ] According to the IBM Archive:  IBM's Supplies Division introduced the Port-A-Punch in 1958 as a fast, accurate means of manually punching holes in specially scored IBM punched cards. Designed to fit in the pocket, Port-A-Punch made it possible to create punched card documents anywhere. The product was intended for \"on-the-spot\" recording operations—such as physical inventories, job tickets and statistical surveys—because it eliminated the need for preliminary writing or typing of source documents. [ 67 ] In 1969 IBM introduced a new, smaller, round-hole, 96-column card format along with the  IBM System/3  low-end business computer. These cards have tiny, 1 mm diameter circular holes, smaller than those in  paper tape . Data is stored in 6-bit  BCD , with three rows of 32 characters each, or 8-bit  EBCDIC . In this format, each column of the top tiers are combined with two punch rows from the bottom tier to form an 8-bit byte, and the middle tier is combined with two more punch rows, so that each card contains 64 bytes of 8-bit-per-byte binary coded data. [ 68 ]  As in the 80 column card, readable text was printed in the top section of the card. There was also a 4th row of 32 characters that could be printed. This format was never widely used; it was IBM-only, but they did not support it on any equipment beyond the System/3, where it was quickly superseded by the 1973  IBM 3740 Data Entry System  using  8-inch floppy disks . The Powers/Remington Rand card format was initially the same as Hollerith's; 45 columns and round holes. In 1930,  Remington Rand  leap-frogged IBM's 80 column format from 1928 by coding two characters in each of the 45 columns – producing what is now commonly called the 90-column card. [ 31 ] : 142   There are two sets of six rows across each card. The rows in each set are labeled 0, 1/2, 3/4, 5/6, 7/8 and 9. The even numbers in a pair are formed by combining that punch with a 9 punch. Alphabetic and special characters use 3 or more punches. [ 69 ] [ 70 ] The British  Powers-Samas  company used a variety of card formats for their  unit record equipment . They began with 45 columns and round holes. Later 36, 40 and 65 column cards were provided. A 130 column card was also available – formed by dividing the card into two rows, each row with 65 columns and each character space with 5 punch positions. A 21 column card was comparable to the IBM Stub card. [ 53 ] : 47–51 Mark sense  ( electrographic ) cards, developed by  Reynold B. Johnson  at IBM, [ 71 ]  have printed ovals that could be marked with a special electrographic pencil. Cards would typically be punched with some initial information, such as the name and location of an inventory item. Information to be added, such as quantity of the item on hand, would be marked in the ovals. Card punches with an option to detect mark sense cards could then punch the corresponding information into the card. Aperture cards  have a cut-out hole on the right side of the punched card. A piece of 35 mm microfilm containing a  microform  image is mounted in the hole. Aperture cards are used for  engineering drawings  from all engineering disciplines. Information about the drawing, for example the drawing number, is typically punched and printed on the remainder of the card. IBM's Fred M. Carroll [ 72 ]  developed a series of rotary presses that were used to produce punched cards, including a 1921 model that operated at 460 cards per minute (cpm). In 1936 he introduced a completely different press that operated at 850 cpm. [ 22 ] [ 73 ]  Carroll's high-speed press, containing a printing cylinder, revolutionized the company's manufacturing of punched cards. [ 74 ]  It is estimated that between 1930 and 1950, the Carroll press accounted for as much as 25 percent of the company's profits. [ 21 ] Discarded printing plates from these card presses, each printing plate the size of an IBM card and formed into a cylinder, often found use as desk pen/pencil holders, and even today are collectible IBM artifacts (every card layout [ 75 ]  had its own printing plate). In the mid-1930s a box of 1,000 cards cost $1.05 (equivalent to $23 in 2023). [ 76 ] While punched cards have not been widely used for generations, the impact was so great for most of the 20th century that they still appear from time to time in popular culture. For example: metaphor... symbol of the \"system\"—first the registration system and then bureaucratic systems more generally ... a symbol of alienation ... Punched cards were the symbol of information machines, and so they became the symbolic point of attack. Punched cards, used for class registration, were first and foremost a symbol of uniformity. .... A student might feel \"he is one of out of 27,500 IBM cards\" ... The president of the Undergraduate Association criticized the University as \"a machine ... IBM pattern of education.\"... Robert Blaumer explicated the symbolism: he referred to the \"sense of impersonality... symbolized by the IBM technology.\"... A common example of the requests often printed on punched cards which were to be individually handled, especially those intended for the public to  use and return  is \"Do Not Fold,  Spindle  or Mutilate\" (in the UK \"Do not bend, spike, fold or mutilate\"). [ 28 ] : 43–55   Coined by Charles A. Phillips, [ 86 ]  it became a motto [ 87 ]  for the post– World War II  era (even though many people had no idea what spindle meant), and was widely mocked and satirized. Some 1960s students at Berkeley wore buttons saying: \"Do not fold, spindle or mutilate. I am a student\". [ 88 ]  The motto was also used for a 1970 book by  Doris Miles Disney [ 89 ]  with a plot based around an early  computer dating  service and a 1971  made-for-TV   movie  based on that book, and a similarly titled 1967 Canadian short film,  Do Not Fold, Staple, Spindle or Mutilate . Processing of punched cards was handled by a variety of machines, including:"
  },
  {
    "id": 118,
    "title": "Keypunch",
    "content": "A  keypunch  is a device for precisely punching holes into stiff paper cards at specific locations as determined by keys struck by a human operator.  Other devices included here for that same function include the gang punch, the pantograph punch, and the stamp. The term was also used for similar machines used by humans to transcribe data onto  punched tape  media. For  Jacquard looms , the resulting punched cards were joined together to form a paper tape, called a \"chain\", containing a  program  that, when read by a loom, directed its operation. [ 1 ] For  Hollerith machines  and other  unit record machines  the resulting  punched cards  contained  data  to be processed by those machines. For computers equipped with a  punched card input/output  device the resulting punched cards were either data or programs directing the computer's operation. Early Hollerith keypunches were manual devices.  Later keypunches were electromechanical devices which combined several functions in one unit. These often resembled small desks with  keyboards  similar to those on typewriters and were equipped with hoppers for blank cards and stackers for punched cards. Some keypunch models could print, at the top of a column, the character represented by the hole(s) punched in that column. The small pieces punched out by a keypunch fell into a  chad   box , [ 2 ] [ 3 ]  or (at  IBM )  chip box , or  bit bucket . In many  data processing  applications, the punched cards were verified by keying exactly the same data a second time, checking to see if the second keying and the punched data were the same (known as  two pass verification ).  There was a great demand for  keypunch operators , usually women, [ 4 ]  who worked full-time on keypunch and verifier machines, often in large  keypunch departments  with dozens or hundreds of other operators, all performing  data input . In the 1950s,  Remington Rand  introduced the  UNITYPER , which enabled  data entry  directly to  magnetic tape  for  UNIVAC  systems.  Mohawk Data Sciences  subsequently produced an improved magnetic tape encoder in 1965, which was somewhat successfully marketed as a keypunch replacement. The rise of microprocessors and inexpensive computer terminals led to the development of additional  key-to-tape  and  key-to-disk  systems from smaller companies such as  Inforex  and  Pertec . [ 5 ] Keypunches and punched cards were still commonly used for both data and program entry through the 1970s but were rapidly made obsolete by changes in the entry paradigm and by the availability of inexpensive CRT  computer terminals . Eliminating the step of transferring punched cards to tape or disk (with the added benefit of saving the cost of the cards themselves) allowed for improved checking and correction during the entry process. The development of  video display terminals , interactive  timeshared systems  and, later,  personal computers  allowed those who originated the data or program to enter it directly instead of writing it on forms to be entered by keypunch operators. Jacquard  cards were said to be stamped or cut, rather than punched. The first Jacquard cards were stamped by hand, sometimes using a guide plate. An improvement involved placing the card between two perforated metal plates, hinged together, inserting punches according to the desired pattern, and then passing the assembly through a press to cut the card. These essentially manual processes were later replaced by machines. 'Piano machines,' so named for their keys, operated by keyboards and comparable in function to unit record keypunches, became the most common. [ 1 ] Herman Hollerith 's first device for punching cards from the 1890s was  ...any ordinary ticket punch, cutting a round hole 3/16 of an inch in diameter . [ 8 ]  Use of such a punch was facilitated by placing the holes to be used near the edges of the card.  Hollerith soon developed a more accurate and simpler to use Keyboard Punch, using a  pantograph  to link a punch mechanism to a guide pointer that an operator would place over the appropriate mark in a 12 by 20 matrix to line up a manual punch over the correct hole in one of 20 columns. [ 9 ] In 1901 Hollerith patented [ 10 ]  a mechanism where an operator pressed one of 12 keys to punch a hole, with the card automatically advancing to the next column.  This first-generation Type 001 keypunch [ 11 ]  used  45 columns and round holes . In 1923 The Tabulating Machine Company introduced the first electric keypunch, the Type 011 Electric Keypunch, [ 12 ]  a similar looking device where each key closed an electrical contact that activated a  solenoid  which punched the hole. The  80 column punched card format  was introduced in 1928. [ 13 ]   Later Hollerith keypunches included the Type 016 Motor-Driven Electric Duplicating Keypunch [ 14 ] [ 15 ]  (1929), the Type 31 Alphabetical Duplicating Punch [ 16 ]  (1933), and the Type 32 Alphabetical Printing Punch [ 17 ]  (1933). \"Alphabetical duplicating keypunches recorded alphabetic information in tabulating cards so that complete words and names, together with numerical data, could be later printed by an alphabetical accounting machine. The Type 31 Alphabetical Duplicating Punch [ 16 ]  was introduced by IBM in 1933, and it automatically ejected one card and fed another in 0.65 second. These machines were equipped with separate alphabetical and numerical keyboards. The alphabetical keyboard was similar to a conventional manual typewriter [ 17 ]  except that the shift, tab, backspace and character keys were eliminated, and a skip, release, stacker and '1' key were provided.\" – IBM Archives [ 18 ] (manufactured by British  ICT ) (1960s) Most  IBM  keypunch and verifiers used a common electrical/mechanical design in their keyboards to encode the mechanical keystrokes.  As a key was depressed, a link on the keystem tripped a corresponding set of bails at the top of the keyboard assembly.  The bails in turn made (closed) contacts to encode the characters electrically.  As each key stroke was detected by the machine, a feed-back circuit energized a pair of magnets with a bail which restored the keystem mechanically, reset the bails performing the electrical encoding, and gave the \"feel\" and sound to the operator of a completed action.  Each machine had a tendency to develop a \"feel\" of its own based on several variables such as the amount of wear, dirt, and clearance of the bail contacts within the keyboard, as well as factors in the base machine.  The keyboards, however, had no provision for adjusting the \"feel\" other than the correct adjustment of the contacts on the restore bail contacts and the encoding bail contacts.  Special function keys such as shift, release, duplication and others, had only electrical contacts under their stems, with no mechanical linkage to the bail assembly for encoding. IBM  keypunches such as the 024, 026, and 029 provided for the mounting of a program card that controlled various functions, such as tabbing and automatic duplication of fields from the previous card.  The later 129 used electronic circuit cards to store simple programs written by the keypunch operator. The IBM 024 Card Punch and IBM 026 Printing Card Punch  [ 19 ]   were announced in 1949. They were almost identical, with the exception of the printing mechanism.  The heart of the 024 and 026 keypunches was a set of twelve precision punches, one per card row, each with an actuator of relatively high power.  Punch cards were stepped across the punch one column at a time, and the appropriate punches were activated to create the holes, resulting in a distinctive \"chunk, chunk\" sound as columns were punched.  Both machines could process 51-, 60-, 66-, and 80-column cards. [ 20 ] The 026 could print the punched character above each column. By 1964 there were ten versions with slightly different character sets. The scientific versions printed parentheses, equal sign and plus sign in place of four less frequently used characters in the commercial character sets. [ 21 ] Logic consisted of  diodes ,  25L6   vacuum tubes  and  relays . The tube circuits used 150VDC, but this voltage was only used to operate the punch-clutch magnet [ clarification needed ] . Most other circuits used 48VDC. Characters were printed using a 5 × 7  dot matrix  array of wires; the device from which it derived the shape of the character was a metal plate, called the \"code plate,\" with space for 1960 pins (35 pins times 56 printable characters). If the dot was not to be printed in a given character, the pin was machined off. By correctly positioning the plate and pressing it against one end of the array of printing wires, only the correct wires were pressed against the  ribbon  and then the punched card. (This printer mechanism was generally considered by IBM Customer Engineers to be difficult to repair. One of the most common problems was wires breaking in the tightly curved narrow tube between the code plate and the ribbon—extracting the fragments and replacing the bundle of 35 wires was very tedious). The printing mechanism was prone to be damaged if a user attempted to duplicate \"binary\" cards with non-standard punch patterns. These could cause the code-plate positioning mechanism to try to shift the plate beyond its intended range of motion, sometimes causing damage. Turning off printing did not actually prevent the damage, as many people assumed, because the code-plate mechanism remained engaged with the punch unit and shifted the code plate. Turning off printing only suppressed pressing the printing pins into the ribbon and card. Raymond Loewy , industrial designer of \"streamlined\" motifs who also designed railway passenger cars of the 1930s and 1940s, did the award-winning external design of the 026/024 Card Punches for IBM. Their heavy steel construction and rounded corners [ 22 ]  indeed echo the industrial  Art Deco  style. The IBM 056 was the verifier companion to the 024 Card Punch and 026 Printing Card Punch.  The verifier was similar to the 026 keypunch except for a red error lens in the machine cover lower center.  The verifier operator entered exactly the same data as the keypunch operator and the verifier machine then  checked  to see if the punched data matched.  Successfully verified cards had a small notch punched on the right hand edge. The IBM 056 verifier used most of the same mechanical and electrical components as the 024/026 keypunches with the exception of the punch unit and print head.  The punch unit had sensing pins in place of the punches. The holes sensed or not sensed would trip a contact bail when the configuration was other than that entered by the verifier operator.  This stopped the forward motion of the card, and presented a red error light on the machine cover.  The notching mechanism was located in the area occupied by the print mechanism on a 026 printing keypunch.  It had a solenoid which drove the notching mechanism, and another that selected the top notch punch or end of card punch. When an operator keying data to be verified encountered an error, the operator was given a second and third try to re-enter the data that was supposed to be in the field.  If the third try was incorrect an error notch was put on the top of the card over the column with the error and the \"OK\" punch at the end of the card was not enabled. The data on the card could actually be correct, since the verifier operator was just as likely to make an error as the keypunch operator.  However, with three tries, the operator was less likely to repeatedly make the same error. Some verifier operators were able to guess the error on the card created by the previous keypunch operator, defeating the purpose of the verify procedure, and thus some machines were altered to allow only one entry and error notched on the second try. [ clarification needed ] Cards with error notches were re-punched (using an 024 or 026) usually by \"duplicating\" up to the column in error, then entering the correct data.  The duplicating function was accomplished by feeding the card through the punch station without punching it. At the next station sensing pins read the holes present in the original card and transferred the data to the punching station and onto a blank card.  Columns with errors were corrected instead of being duplicated.  The corrected card was then verified to check the data again and be \"OK notched\". The first combination of card punch and typewriter, permitting selected text to be typed and punched, was developed by the Powers company in 1925. [ 23 ]   The IBM 824 Typewriter Card Punch was an IBM 024 where the 024 keyboard was replaced by an IBM electric typewriter. [ 24 ]   Similarly, the IBM 826 used an IBM 026 Keypunch. [ 25 ] Introduced with  System/360  in 1964, the 029 had new character codes for  parentheses, equal  and plus as well as other new symbols used in the  EBCDIC  code. The IBM 029 was mechanically similar to the IBM 026 and printed the punched character on the top of the card using the same kind of mechanism as the 026, although it used a larger code plate with 2240 printing-pin sites due to the larger set of characters in EBCDIC. The 029's logic consisted of wire contact relays on later models and reed relays and  diodes  on  SMS cards  for early ones. The more \"advanced\" reed relays used at first proved to be less reliable than expected, causing IBM to revert to the older-style wire-contact relay-based design. All ran on 48 volts DC, and did not require the vacuum tubes that were used in the 024/026. A common additional feature made available (at additional cost) was the leading zeros feature (termed \"Left-Zero\"). This was delivered by an additional set of four SMS cards. The field was programmed for leading zeros using the program card. If it was (say) a six digit field, the operator only had to key in the actual value (for example 73). The feature would then fill the field by punching the leading four zeros, followed by the 73, in effect right justifying the field, thus: 000073. The IBM 5924 Key Punch was the 029 model T01 attached with a special keyboard in IBM's 1971 announcement of the  IBM Kanji System , the keypunch operator's left hand selecting one of 15 shift keys and the right hand selecting one of 240 Kanji characters for that shift. It introduced the computer processing of  Chinese ,  Japanese  and  Korean languages  that typically used large  character sets  over 10,000 characters. The IBM 059 was the Verifier companion to the IBM 029 Card Punch. In design, it differed radically from the earlier 056 verifier, in that it used optical sensing of card holes instead of mechanical sensing pins.  This made the 059 much quieter than the 056 (which was often louder than the 024 keypunch).  The optical sensors used a single light source, which was distributed to various sites within the machine via fiber-optic lightpipes. Despite the technology, the basic mode of operation remained essentially the same as with the 056. Ironically, not all verifier operators appreciated the noise reduction.  When used in a room also containing 029 keypunch machines, the verifier operators sometimes missed the auditory feedback provided by the loud \"thunk\" noise emitted by the older 056. Some were known to compensate by hitting the keys harder, sometimes actually wearing out keyboard parts. Introduced with the  System/370  in 1971, the IBM 129 was capable of punching, verifying, and use as an auxiliary, on line, 80 column card reader/punch for some computers. A switch on the keyboard console provided the ability to toggle between the punch and verify modes. The transistorized IBM 129 Card Data Recorder's primary advantage over other IBM keypunches was that it featured an electronic 80-column buffer to hold the card image. When using earlier IBM keypunches, a keystroke error required the card to be ejected by pressing the Release and Register keys, the error corrected by pressing the Duplicate key until the error column was reached, typing the correct data for the rest of that card, then pressing the Release key and manually removing the bad card from the output card stacker before it was placed in the deck (this required some practice, but quickly became an automatic action that you no longer had to think about). With the 129, a keystroke error could be erased by pressing the  Backspace  key and re-keyed. The entire 80-column card was punched automatically, as fast as the mechanism could go, when the Release key was pressed. Logic was in  SLT  modules on a swing out, wire-wrapped backplane. A secondary advantage of the 129 was that the speed of the keying operation was not limited by punching each column at the time of the keystroke. The 129 could store six programs in its memory, selectable by a rotary switch. Unlike earlier keypunch machines, the program cards were read into memory via the regular card-feed path, and were not wrapped around a \"program drum\". Thanks to its use of electronic memory, the 129 did not have a separate \"read station\" with a pin-sense unit to enable duplication of data from one card to the next. Instead, duplication was based on the stored image of the previous card. Cards could also be \"read-in\" through an optical read unit integrated into the punch station. IBM 024, 026, and 029 keypunches and their companion verifiers, the 056 and 059, could be programmed to a limited extent using a  Program Card , [ 26 ]  also known as a drum card. The keypunch or verifier could be programmed to automatically advance to the beginning of each field, default to certain character types within the field, duplicate a field from the previous card, and so on.  Program cards were an improvement over the  Skip Bar  used in some earlier keypunches. [ 27 ] The program was encoded on a punched card and could be prepared on any keypunch (a keypunch would operate even if no program card was in place). The program card was wrapped around the program drum, and clamped in place. The drum rotated as the card being punched moved through the punching mechanism. The holes in the program card were sensed by an array of starwheels that would cause levers to rise and fall as the holes in the program card passed beneath the starwheels, activating electrical contacts.  The program was encoded in the top six rows [12,11,0,1,2,3]. If the optional  Second Program  feature was installed, another program could be encoded in the bottom six rows [4,5,6,7,8,9].  A switch let the operator select which program to use.  The central cover on the keypunch could be tilted open toward the operator and a locking lever released, allowing the program drum to be removed and replaced. The program card was punched with characters that controlled its function as follows: Many programming languages, such as  FORTRAN ,  RPG , and the IBM  Assembler , coded operations in specific card columns, such as 1, 10, 16, 36, and 72.  The program card for such a setup might be coded as: In this example, if the keypunch operator typed a few characters at the beginning of the card and then pressed the skip key, the keypunch would tab to column 10. When a program code of blank is followed by \"Field Definition\" (12) (or (4) for program 2), it defines a \"Numeric Shift\" field. In the example above, columns 72-80 are defined in the program as a Numeric Shift field. In practice, this definition would likely be used for punching a special symbol as a \"continuation character\" in column 72, and then columns 73-80 could either be punched with a card sequence number or the card could be released at that point, if no further typing was required. Note: \"Field Definition\" (12) and \"Alphabetic Shift\" (1) prints as an  A . If program 2 codes were punched, invalid characters could be generated that the printer did not know how to print, some of which could even damage the printer. Program cards could automate certain tasks, such as \"gang punching\", the insertion of a constant field into each card of a deck of cards. For amusement, program cards could even be set up to play music by gang-punching \"noisy\" characters (characters represented by many holes, usually special characters) and \"quiet\" numbers and letters in rhythmic patterns. In 1969, IBM introduced the  System/3  family of low-end business computers which featured a new, smaller-sized,  96 column punched card . [ 28 ]   The IBM 5496 Data Recorder, a keypunch with print and verify functions, and IBM 5486 Card Sorter were made for these 96-column cards. Beginning around 1906, an employee of the  United States Census Bureau , James Powers, developed the  Powers Keypunch , which was specific to the census application and had 240 keys. [ 29 ] [ 30 ]   In 1911, Powers formed  Powers Accounting Machine Company . That company was taken over by  Remington Rand  in 1927. [ 31 ]   Remington Rand's  UNIVAC  division made keypunches for their 90-column cards and similar machines for the IBM 80-column card. Their 90-column keypunches used a mechanical system developed by Remington Rand to avoid IBM patent issues (long before the acquisition of  Eckert–Mauchly Computer Corporation ).  UNIVAC keypunches stored the sequence of characters for an entire card, then punched all its holes in a single pass, which allowed for corrections instead of wasting a card in case of error. Remington Rand keypunches included: UNIVAC Card Code Punch Type 306-5, 90 Column Alphabetical (Types 306-2, 306-3), 90 Column Numerical (Types 204-2, 204-3), Portable Electric Punch Type 202,  Spot Punch Type 301, and the Automatic Verifying Machine Type 313. [ 32 ] The Type 306-2 provided for verification; the cards were passed through the keypunch a second time  and keyed again.  The verify-punching of the same cards in the same  sequence ... results in the elongation of perforations for correct information.  Round perforations indicate incorrect information. Complete and rapid detection of errors is performed mechanically by the Automatic Verifying Machine [ 33 ] The UNIVAC 1710 Verifying Interpreting Punch was introduced in 1969. [ 34 ] Saying that something would be  keypunched (to  keypunch  as a verb) , [ 35 ]  now that the actual device called a keypunch has become obsolete, [ 36 ]  refers to  data entry . [ 37 ] This use of the verb has replaced the former process, described  [ 38 ] \nas \"When a key is struck on a keypunch, it prints the character on the top of the card \nbut also punches a series of holes that the computer\" [ 39 ]  \ncan interpret.\" In the 1950s,  Remington Rand  introduced the  UNITYPER , [ 40 ] [ 41 ]  which enabled  data entry  directly to  magnetic tape  for  UNIVAC  systems.  Mohawk Data Sciences  subsequently produced an improved magnetic tape encoder in 1965, which was somewhat successfully marketed as a keypunch replacement. In the mid-1970s, the rise of microprocessors and inexpensive computer terminals led to the development of additional key-to-tape and key-to-disk systems from smaller companies such as  Inforex  and  Pertec . Punched cards were still commonly used for data entry and programming until the mid-1980s. However, eliminating the step of transferring punched cards to tape or disk (with the added benefit of saving the cost of the cards themselves) allowed for improved checking and correction during the data entry process. The development of  video display terminals , interactive  timeshared systems  and, later,  personal computers  allowed workers who originated the data to enter it directly instead of writing it on forms to be entered by  data entry clerks ."
  },
  {
    "id": 119,
    "title": "Tabulating machine",
    "content": "The  tabulating machine  was an  electromechanical  machine designed to assist in summarizing information stored on  punched cards . Invented by  Herman Hollerith , the machine was developed to help process data for the  1890 U.S. Census . Later models were widely used for business applications such as  accounting  and  inventory control . It spawned a class of machines, known as  unit record equipment , and the data processing industry. The term \" Super Computing \" was used by the  New York World  newspaper in 1931 to refer to a large custom-built tabulator that  IBM  made for  Columbia University . [ 1 ] The  1880 census  had taken eight years to process. [ 2 ]   Since the  U.S. Constitution  mandates a census every ten years to apportion both  congressional representatives  and  direct taxes  among the  states , a combination of larger staff and faster-recording systems was required. In the late 1880s  Herman Hollerith , inspired by  conductors  using holes punched in different positions on a  railway ticket  to record traveler details such as gender and approximate age, invented the recording of data on a machine-readable medium. Prior uses of machine-readable media had been for lists of instructions (not data) to drive  programmed machines  such as  Jacquard looms .  \"After some initial trials with paper tape, he settled on  punched cards ...\" [ 3 ]   Hollerith used punched cards with round holes, 12 rows, and 24 columns. The cards measured  3 + 1 ⁄ 4  by  6 + 5 ⁄ 8  inches (83 by 168 mm). [ 4 ]  His tabulator used electromechanical  solenoids  to increment mechanical counters. A set of spring-loaded wires were suspended over the card reader. The card sat over pools of  mercury , pools corresponding to the possible hole positions in the card.  When the wires were pressed onto the card, punched holes allowed wires to dip into the mercury pools, making an electrical contact [ 5 ] [ 6 ]  that could be used for counting, sorting, and setting off a bell to let the operator know the card had been read. The tabulator had 40 counters, each with a dial divided into 100 divisions, with two indicator hands; one which stepped one unit with each counting pulse, the other which advanced one unit every time the other dial made a complete revolution. This arrangement allowed a count of up to 9,999.  During a given tabulating run, counters could be assigned to a specific hole or, by using  relay logic , to a combination of holes, e.g. to count married couples. [ 7 ]  If the card was to be sorted, a compartment lid of the sorting box would open for storage of the card, the choice of compartment depending on the data in the card. [ 8 ] Hollerith's method was used for the 1890 census. Clerks used  keypunches  to punch holes in the cards entering age, state of residence, gender, and other information from the returns. Some 100 million cards were generated and \"the cards were only passed through the machines four times during the whole of the operations.\" [ 4 ]  According to the U.S. Census Bureau, the census results were \"... finished months ahead of schedule and far under budget.\" [ 9 ] The advantages of the technology were immediately apparent for  accounting  and tracking  inventory .  Hollerith started his own business as  The Hollerith Electric Tabulating System , specializing in  punched card data processing equipment . [ 10 ]  In 1896 he incorporated the Tabulating Machine Company. In that year he introduced the Hollerith Integrating Tabulator, which could add numbers coded on punched cards, not just count the number of holes. Punched cards were still read manually using the pins and mercury pool reader. 1900 saw the Hollerith Automatic Feed Tabulator used in that year's U.S. census. A  control panel  was incorporated in the 1906 Type 1. [ 11 ] In 1911, four corporations, including Hollerith's firm, were  amalgamated (via stock acquisition)  to form a fifth company, the  Computing-Tabulating-Recording Company  (CTR). The  Powers Accounting Machine Company  was formed that same year and, like Hollerith, with machines first developed at the Census Bureau. In 1919 the first  Bull  tabulator prototype was developed. Tabulators that could print, and with removable control panels, appeared in the 1920s. In 1924 CTR was renamed  International Business Machines  (IBM).  In 1927 Remington Rand acquired the Powers Accounting Machine Company.  In 1933 The Tabulating Machine Company was subsumed into IBM. These companies continued to develop faster and more sophisticated tabulators, culminating in tabulators such as 1949  IBM 407  and 1952  Remington Rand 409 . Tabulating machines continued to be used well after the introduction of commercial electronic  computers  in the 1950s. Many applications using unit record tabulators were migrated to computers such as the  IBM 1401 .  Two programming languages,  FARGO  and  RPG , were created to aid this migration.  Since tabulator control panels were based on the machine cycle, both FARGO and RPG emulated the notion of the machine cycle and training material showed the control panel vs. programming language coding sheet relationships. In its basic form, a tabulating machine would read one card at a time, print portions (fields) of the card on  fan-fold paper , possibly rearranged, and add one or more numbers punched on the card to one or more counters, called  accumulators .  On early models, the accumulator register dials would be read manually after a card run to get totals. Later models could print totals directly. Cards with a particular punch could be treated as master cards causing different behavior. For example, customer master cards could be merged with sorted cards recording individual items purchased. When read by the tabulating machine to create invoices, the billing address and customer number would be printed from the master card, and then individual items purchased and their price would be printed. When the next master card was detected, the total price would be printed from the accumulator and the page ejected to the top of the next page, typically using a  carriage control tape . With successive stages or cycles of punched-card processing, fairly complex calculations could be made if one had a sufficient set of equipment. (In modern data processing terms, one can think of each stage as an  SQL  clause: SELECT (filter columns), then WHERE (filter cards, or \"rows\"), then maybe a GROUP BY for totals and counts, then a SORT BY; and then perhaps feed those back to another set of SELECT and WHERE cycles again if needed.) A human operator had to retrieve, load, and store the various card decks at each stage. Hollerith's first tabulators were used to compile mortality statistics for Baltimore, Jersey City and New York City in 1886. [ 13 ] The first Tabulating Machine Company (TMC) automatic feed tabulator, operating at 150 cards/minute, was developed in 1906. [ 14 ] The first TMC printing tabulator was developed in 1920. [ 15 ] TMC Type IV Accounting Machine (later renamed the IBM 301), from the  IBM Archives : The 301 (better known as the  Type IV ) Accounting Machine was the first card-controlled machine to incorporate class selection, automatic subtraction, and printing of a net positive or negative balance. Dating to 1928, this machine exemplifies the transition from tabulating to accounting machines. The Type IV could list 100 cards per minute. H.W.Egli - BULL Tabulator model T30 , 1931 IBM 401: The 401, introduced in 1933, was an early entry in a long series of IBM alphabetic tabulators and accounting machines. It was developed by a team headed by  J. R. Peirce  and incorporated significant functions and features invented by  A. W. Mills ,  F. J. Furman  and  E. J. Rabenda . The 401 added at a speed of 150 cards per minute and listed alphanumerical data at 80 cards per minute. [ 16 ] IBM 405: Introduced in 1934, the 405 Alphabetical Accounting Machine was the basic bookkeeping and accounting machine marketed by IBM for many years. Important features were expanded adding capacity, greater flexibility of counter grouping, [ b ]  direct printing of the entire alphabet, direct subtraction [ c ]  and printing of either debit or credit balance from any counter. Commonly called the 405 \"tabulator,\" this machine remained the flagship of IBM's product line until after World War II. [ 17 ] [ 18 ]  The British at  Hut 8  used Hollerith machinery to gain some knowledge of  Known-plaintext attack  cribs used by encrypted German messages. [ 19 ] IBM 402  and 403, from 1948, were modernized successors to the 405. The 1952 Bull Gamma 3 could be attached to this tabulator or to a card read/punch. [ 20 ] [ 21 ] IBM 407 Introduced in 1949, the 407 was the mainstay of the IBM unit record product line for almost three decades. It was later adapted to serve as an input/output peripheral for several early electronic calculators and computers. Its printing mechanism was used in the  IBM 716  line printer for the  IBM 700/7000 series  and later with the  IBM 1130  through the mid-1970s. The IBM 407 Accounting Machine was withdrawn from marketing in 1976, signaling the end of the unit record era. [ 22 ] IBM 421 For early use of tabulators for scientific computations see"
  },
  {
    "id": 120,
    "title": "1890 United States census",
    "content": "The  1890 United States census  was taken beginning June 2, 1890. The census determined the resident population of the United States to be 62,979,766, an increase of 25.5 percent over the 50,189,209 persons enumerated during the  1880 census . The data reported that the distribution of the population had resulted in the disappearance of the  American frontier . This was the first  census  in which a majority of states recorded populations of over one million and the first in which three cities,  New York City ,  Chicago , and  Philadelphia , recorded populations of over one million. The census also saw Chicago rise in rank to the nation's second-most populous city, a position it would hold until  Los Angeles , the 57th-most populous city as of 1890, supplanted it in 1990. This was the first U.S. census to use machines to tabulate the collected data. Most of the 1890 census materials were destroyed on January 10, 1921, when the Commerce Department building caught fire, and in the subsequent disposal of the remaining damaged records. The 1890 census collected the following information: [ 1 ] The 1890 census was the first to be compiled using methods invented by  Herman Hollerith  and was overseen by Superintendents  Robert P. Porter  (1889–1893) and  Carroll D. Wright (1893–1897). Data was entered on a machine readable medium ( punched cards ) and  tabulated by machine . [ 2 ]  Changes from the 1880 census included the larger population, the number of data items to be collected from individuals, the  Census Bureau  headcount, the volume of scheduled publications, and the use of Hollerith's electromechanical tabulators. The net effect of these changes was to reduce the time required to process the census from eight years for the  1880 census  to six years for the 1890 census. [ 3 ]  The total population of 62,947,714, the family, or  rough , count, was announced after only six weeks of processing (punched cards were not used for this tabulation). [ 4 ] [ 2 ] : 61   The public reaction to this tabulation was disbelief, as it was widely believed that the \"right answer\" was at least 75,000,000. [ 5 ] The United States census of 1890 showed a total of 248,253  Native Americans living in the United States , down from  400,764 Native Americans identified in the  census of 1850 . [ 6 ] The 1890 census announced that the  frontier  region of the  United States  no longer existed, [ 7 ]  and that the Census Bureau would no longer track the westward migration of the U.S. population. By 1890, settlement in the American West had reached sufficient population density that the frontier line had disappeared. For the 1890 census, the Census Bureau released a bulletin declaring the closing of the frontier, stating: \"Up to and including 1880 the country had a frontier of settlement, but at present the unsettled area has been so broken into by isolated bodies of settlement that there can hardly be said to be a frontier line. In the discussion of its extent, its westward movement, etc., it can not, therefore, any longer have a place in the census reports.\" [ 8 ] Statisticians at the time argued that the results of the 1890 census was most likely an undercount of the total population in the United States from anywhere between one million to several millions, and that African Americans, working class people, and young children were most likely to be undercounted. [ 9 ] The original data for the 1890 census is mostly unavailable. The population schedules were damaged in a fire in the basement of the  U.S. Department of Commerce  building in  Washington, D.C.  in 1921. Some 25% of the materials were presumed destroyed and another 50% damaged by smoke and water, although the actual damage may have been closer to 15–25%. The damage to the records led to an outcry for a permanent  National Archives . [ 10 ] [ 11 ]  In December 1932, following standard federal record-keeping procedures, the Chief Clerk of the Bureau of the Census sent the  Librarian of Congress  a list of papers to be destroyed, including the original 1890 census schedules. The Librarian was asked by the Bureau to identify any records which should be retained for historical purposes, but the Librarian did not accept the census records. Congress authorized destruction of that list of records on February 21, 1933, and the surviving original 1890 census records were destroyed by government order by 1934 or 1935. Few sets of  microdata  from the 1890 census survive. [ 12 ]   Aggregate data  for small areas, together with compatible cartographic boundary files, can be downloaded from the  National Historical Geographic Information System ."
  },
  {
    "id": 121,
    "title": "The Atlantic",
    "content": "The Atlantic  is an American magazine and multi-platform publisher based in  Washington, D.C.  It features articles on politics, foreign affairs, business and the economy, culture and the arts, technology, and science. It was founded in 1857 in  Boston  as  The Atlantic Monthly , a literary and cultural magazine that published leading writers' commentary on education, the  abolition of slavery , and other major political issues of that time. Its founders included  Francis H. Underwood [ 3 ] [ 4 ]  and prominent writers  Ralph Waldo Emerson ,  Oliver Wendell Holmes Sr. ,  Henry Wadsworth Longfellow ,  Harriet Beecher Stowe , and  John Greenleaf Whittier . [ 5 ] [ 6 ]   James Russell Lowell  was its first editor. [ 7 ]  During the 19th and 20th centuries, the magazine also published the annual  The Atlantic Monthly  Almanac . [ 8 ]  The magazine was purchased in 1999 by businessman  David G. Bradley , who fashioned it into a general editorial magazine primarily aimed at serious national readers and \" thought leaders \"; in 2017, he sold a majority interest in the publication to  Laurene Powell Jobs 's  Emerson Collective . [ 9 ] [ 10 ] [ 11 ] The magazine was published monthly until 2001, when 11 issues were produced; since 2003, it has published 10 per year. It dropped \"Monthly\" from the cover with the January/February 2004 issue, and officially changed the name in 2007. [ 12 ]  In 2024, it announced that it will resume publishing monthly issues in 2025. [ 13 ] [ 14 ] In 2016, the periodical was named Magazine of the Year by the  American Society of Magazine Editors . [ 15 ]  In 2022, its writers won  Pulitzer Prizes for feature writing  and, in 2022, 2023, and 2024  The Atlantic  won the award for general excellence by the American Society of Magazine Editors. In 2024, it was reported that the magazine had crossed one million subscribers [ 13 ]  and become profitable, three years after losing $20 million in a single year and laying off 17% of its staff. As of 2024, the website's executive editor is  Adrienne LaFrance , the editor-in-chief is  Jeffrey Goldberg , and the CEO is  Nicholas Thompson . In the autumn of 1857,  Moses Dresser Phillips , a publisher from  Boston , created  The Atlantic Monthly . The plan for the magazine was launched at a dinner party, which was described in a letter by Phillips: I must tell you about a little dinner-party I gave about two weeks ago. It would be proper, perhaps, to state the origin of it was a desire to confer with my literary friends on a somewhat extensive literary project, the particulars of which I shall reserve till you come. But to the Party: My invitations included only  R. W. Emerson ,  H. W. Longfellow ,  J. R. Lowell ,  Mr. Motley  (the 'Dutch Republic' man),  O. W. Holmes ,  Mr. Cabot , and  Mr. Underwood , our literary man. Imagine your uncle as the head of such a table, with such guests. The above named were the only ones invited, and they were all present. We sat down at three P.M., and rose at eight. The time occupied was longer by about four hours and thirty minutes than I am in the habit of consuming in that kind of occupation, but it was the richest time intellectually by all odds that I have ever had. Leaving myself and 'literary man' out of the group, I think you will agree with me that it would be difficult to duplicate that number of such conceded scholarship in the whole country besides... Each one is known alike on both sides of the Atlantic, and is read beyond the limits of the English language. [ 16 ] At that dinner he announced his idea for the magazine: Mr. Cabot is much wiser than I am. Dr. Holmes can write funnier verses than I can. Mr. Motley can write history better than I. Mr. Emerson is a philosopher and I am not. Mr. Lowell knows more of the old poets than I. But none of you knows the American people as well as I do. [ 16 ] The Atlantic ' s first issue was published in November 1857, and quickly gained notability as one of the finest magazines in the English-speaking world. In 1878, the magazine absorbed  The Galaxy , a competitor monthly magazine founded a dozen years previously by  William Conant Church  and his brother  Francis P. Church ; it had published works by  Mark Twain ,  Walt Whitman ,  Ion Hanford Perdicaris  and  Henry James . [ 17 ] In 1879,  The Atlantic  had offices in  Winthrop Square  in Boston and at 21  Astor Place  in  New York City . [ 18 ] A leading literary magazine,  The Atlantic  has published many significant works and authors. It was the first to publish pieces by the abolitionists  Julia Ward Howe  (\" Battle Hymn of the Republic \" on February 1, 1862), and  William Parker , whose  slave narrative , \"The Freedman's Story\" was published in February and March 1866. It also published  Charles W. Eliot 's \"The New Education\", a call for practical reform that led to his appointment to the presidency of  Harvard University  in 1869, works by  Charles Chesnutt  before he collected them in  The Conjure Woman  (1899), and poetry and short stories, and helped launch many national literary careers. [ citation needed ]  In 2005, the magazine won a National Magazine Award for fiction. [ 20 ] Editors have recognized major cultural changes and movements. For example, of the emerging writers of the 1920s,  Ernest Hemingway  had his short story \" Fifty Grand \" published in the July 1927 edition. Harking back to its abolitionist roots, in its August 1963 edition, at the height of the  civil rights movement , the magazine published  Martin Luther King Jr. 's defense of  civil disobedience , \" Letter from Birmingham Jail \", [ 21 ]  under the headline \"The Negro Is Your Brother\". [ 22 ] The magazine has published speculative articles that inspired the development of new technologies. The classic example is  Vannevar Bush 's essay \" As We May Think \" (July 1945), which inspired  Douglas Engelbart  and later  Ted Nelson  to develop the modern  workstation  and  hypertext  technology. [ 23 ] [ 24 ] The Atlantic Monthly  founded the Atlantic Monthly Press in 1917; for many years, it was operated in partnership with  Little, Brown and Company . Its published books included  Drums Along the Mohawk  (1936) and  Blue Highways  (1982). The press was sold in 1986; today it is an imprint of  Grove Atlantic . [ 25 ] In addition to publishing notable fiction and poetry,  The Atlantic  has emerged in the 21st century as an influential platform for  longform  storytelling and newsmaker interviews. Influential cover stories have included  Anne Marie Slaughter 's \"Why Women Still Can't Have It All\" (2012) and  Ta-Nehisi Coates 's \"A Case for Reparations\" (2014). [ 26 ]  In 2015,  Jeffrey Goldberg 's \"Obama Doctrine\" was widely discussed by American media and prompted response by many world leaders. [ 27 ] As of 2022, writers and frequent contributors to the print magazine included  James Fallows , Jeffrey Goldberg, Ta-Nehisi Coates,  Caitlin Flanagan ,  Jonathan Rauch ,  McKay Coppins , Gillian White,  Adrienne LaFrance ,  Vann R. Newkirk II ,  Derek Thompson ,  David Frum , Jennifer Senior,  George Packer ,  Ed Yong , and James Parker. On August 2, 2023, it was announced that  Jeffrey Goldberg , who had served as editor-in-chief of The Atlantic since 2016, had been named as  Washington Week 's tenth moderator, and that the politics and culture publication would also enter into an editorial partnership with the television program – which was retitled accordingly as Washington Week with The Atlantic – similar to the earlier collaboration with the  National Journal . [ 28 ] [ 29 ]   The first episode under the longer title, and with Goldberg as moderator, was the one broadcast on August 11, 2023. [ 30 ] In 1860, three years into publication,  The Atlantic ' s then-editor  James Russell Lowell  endorsed  Republican   Abraham Lincoln  for his first run for president and also endorsed the  abolition of slavery . [ 31 ] In 1964, Edward Weeks wrote on behalf of the editorial board in endorsing  Democratic  President  Lyndon B. Johnson  and rebuking Republican  Barry Goldwater 's candidacy. [ 32 ] In 2016, during the  2016 presidential campaign , the editorial board endorsed a candidate for the third time in the magazine's history, urging readers to support Democratic nominee  Hillary Clinton  in a rebuke of Republican  Donald Trump 's candidacy. [ 33 ] After Trump prevailed in the November 2016 election, the magazine became a strong critic of him. In March 2019, a cover article by editor  Yoni Appelbaum  called for the  impeachment of Donald Trump : \"It's time for Congress to judge the president's fitness to serve.\" [ 34 ] [ 35 ] [ 36 ] In September 2020, it published a story, citing several anonymous sources, reporting that Trump referred to dead American soldiers as \"losers\". [ 37 ]  Trump called it a \"fake story\", and suggested the magazine would soon be out of business. [ 38 ] [ 39 ] In 2020,  The Atlantic  endorsed the Democratic presidential nominee  Joe Biden  in the  2020 presidential election , and urged its readers to oppose Trump's re-election bid. [ 40 ]  In early 2024,  The Atlantic  published a special 24-article issue titled \"If Trump Wins,\" warning about a potential second term for Trump being worse than his first. [ 41 ] [ 42 ]  In October, the publication endorsed Democratic nominee  Kamala Harris  in her presidential bid against Trump in the  2024 election . [ 43 ] In 2005,  The Atlantic  and the  Aspen Institute  launched the  Aspen Ideas Festival , a ten-day event in and around the city of  Aspen, Colorado . [ 44 ]  The annual conference features 350 presenters, 200 sessions, and 3,000 attendees. The event has been called a \"political  who's who \" as it often features policymakers, journalists, lobbyists, and  think tank  leaders. [ 45 ] On January 22, 2008, TheAtlantic.com dropped its  subscriber wall  and allowed users to freely browse its site, including all past archives. [ 46 ]  By 2011  The Atlantic ' s web properties included TheAtlanticWire.com, a news- and opinion-tracking site launched in 2009, [ 47 ]  and TheAtlanticCities.com, a stand-alone website started in 2011 that was devoted to global cities and trends. [ 48 ]  According to a  Mashable  profile in December 2011, \"traffic to the three web properties recently surpassed 11 million uniques per month, up a staggering 2500% since  The Atlantic  brought down its paywall in early 2008.\" [ 49 ] In 2009, the magazine launched  The Atlantic Wire  as a stand-alone  news aggregator  site. It was intended as a curated selection of news and opinions from online, print, radio, and television outlets. [ 50 ] [ 51 ] [ 52 ]  At its launch, it published  op-eds  from across the media spectrum and summarized significant positions in each debate. [ 52 ]  It later expanded to feature news and original reporting. Regular features in the magazine included \"What I Read\", describing the  media diets  of people from entertainment, journalism, and politics; and \"Trimming the Times\", the feature editor's summary of the best content in  The New York Times . [ 53 ]   The Atlantic Wire  rebranded itself as  The Wire  in November 2013, [ 54 ]  and was folded back into  The Atlantic  the following year. [ 55 ] In August 2011, it created its video channel. [ 56 ]  Initially created as an aggregator,  The Atlantic ' s video component, Atlantic Studios, has since evolved in an in-house production studio that creates custom video series and original documentaries. [ 57 ] In September 2011,  The Atlantic  launched  CityLab , a separate website. Its co-founders included  Richard Florida , urban theorist and professor. The stand-alone site has been described as exploring and explaining \"the most innovative ideas and pressing issues facing today's global cities and neighborhoods.\" [ 58 ]  In 2014, it was rebranded as  CityLab.com , and covers transportation, environment, equity, life, and design. Among its offerings are Navigator, \"a guide to urban life\"; and Solutions, which covers solutions to problems in a dozen topics. [ 59 ] In December 2011, a new Health Channel launched on TheAtlantic.com, incorporating coverage of food, as well as topics related to the mind, body, sex, family, and public health. Its launch was overseen by Nicholas Jackson, who had previously been overseeing the Life channel and initially joined the website to cover technology. [ 60 ]  TheAtlantic.com has also expanded to  visual storytelling , with the addition of the \"In Focus\" photo blog, curated by Alan Taylor. [ 61 ] In 2015, TheAtlantic.com launched a dedicated Science section [ 62 ]  and in January 2016 it redesigned and expanded its politics section in conjunction with the 2016 U.S. presidential race. [ 63 ] In 2015,  CityLab  and  Univision  launched  CityLab Latino , which features original journalism in Spanish as well as translated reporting from the English language edition of  CityLab.com . [ 64 ]  The site has not been updated since 2018. In early December 2019, Atlantic Media sold  CityLab  to  Bloomberg Media , [ 65 ] [ 66 ]  which promptly laid off half the staff. [ 67 ]  The site was relaunched on June 18, 2020, with few major changes other than new branding and linking the site with other Bloomberg verticals and its data terminal. [ 68 ] In September 2019, TheAtlantic.com introduced a digital subscription model, restricting unsubscribed readers' access to five free articles per month. [ 69 ] [ 70 ] In June 2020,  The Atlantic  released its first full-length documentary,  White Noise , a film about three  alt-right  activists. [ 71 ] By its third year, it was published by  Boston  publishing house  Ticknor and Fields , which later became part of  Houghton Mifflin , [ citation needed ]  based in the city known for literary culture. The magazine was purchased in 1908 by editor at the time,  Ellery Sedgwick , and remained in Boston. In 1980, the magazine was acquired by  Mortimer Zuckerman , property magnate and founder of  Boston Properties , who became its chairman. On September 27, 1999, Zuckerman transferred ownership of the magazine to  David G. Bradley , owner of the  National Journal Group , which focused on  Washington, D.C.  and  federal government  news. Bradley had promised that the magazine would stay in Boston for the foreseeable future, as it did for the next five-and-a-half years. In April 2005, however, the publishers announced that the editorial offices would be moved from their longtime home at 77 North Washington Street in Boston to join the company's advertising and circulation divisions in Washington, D.C. [ 86 ]  Later in August, Bradley told  The New York Observer  that the move was not made to save money—near-term savings would be $200,000–$300,000, a relatively small amount that would be swallowed by severance-related spending—but instead would serve to create a hub in Washington, D.C., where the top minds from all of Bradley's publications could collaborate under the  Atlantic Media Company  umbrella. Few of the Boston staff agreed to move, and Bradley then commenced an open search for a new editorial staff. [ 87 ] In 2006, Bradley hired  James Bennet , the  Jerusalem  bureau chief for  The New York Times , as editor-in-chief. Bradley also hired Jeffrey Goldberg and  Andrew Sullivan  as writers for the magazine. [ 88 ] In 2008, Jay Lauf joined the organization as publisher and vice-president; as of 2017, he was publisher and president of  Quartz . [ 89 ] In early 2014, Bennet and Bob Cohn became co-presidents of  The Atlantic , and Cohn became the publication's sole president in March 2016 when Bennet was tapped to lead  The New York Times ' s editorial page. [ 90 ] [ 91 ]  Jeffrey Goldberg was named editor-in-chief in October 2016. [ 92 ] On July 28, 2017,  The Atlantic  announced that billionaire investor and philanthropist  Laurene Powell Jobs  (the widow of former  Apple Inc.  chairman and CEO  Steve Jobs ) had acquired majority ownership through her  Emerson Collective  organization, with a staff member of Emerson Collective, Peter Lattman, being immediately named as vice chairman of  The Atlantic . David G. Bradley and Atlantic Media retained a minority share position in this sale. [ 93 ] In May 2019, technology journalist Adrienne LaFrance became executive editor. [ 94 ] In December 2020, former  Wired  editor-in-chief  Nicholas Thompson  was named CEO of  The Atlantic . [ 95 ]"
  },
  {
    "id": 122,
    "title": "Hans Peter Luhn",
    "content": "Hans Peter Luhn  (July 1, 1896 – August 19, 1964) was a German-American [ 2 ]  researcher in the field of computer science and Library & Information Science for  IBM , and creator of the  Luhn algorithm ,  KWIC  ( K ey  W ords  I n  C ontext) indexing, and   selective dissemination of information (\"SDI\") . His inventions have found applications in diverse areas like computer science, the  textile industry ,  linguistics , and  information science . He was awarded over 80  patents . He created one of the earliest practical hash functions in the 1950s. Luhn was born in  Barmen ,  Germany  (now part of  Wuppertal ) on July 1, 1896. After he completed secondary school, Luhn moved to Switzerland to learn the printing trade so he could join the family business. His career in printing was halted by his service as a communications officer in the German Army during  World War I . After the war, Luhn entered the textile field, which eventually led him to the United States, where he invented a thread-counting gauge (the Lunometer) still on the market. [ 3 ]  From the late 1920s to the early 1940s, during which time he obtained patents for a broad range of inventions, Luhn worked in textiles and as an independent engineering consultant. He joined IBM as a senior research engineer in 1941, and soon became manager of the  information retrieval  research division. His introduction to the field of documentation/information science came in 1947 when he was asked to work on a problem brought to IBM by James Perry and Malcolm Dyson that involved searching for chemical compounds recorded in coded form. [ 4 ]  He came up with a solution for that and other problems using  punched cards , but often had to overcome the limitations of the available machines by coming up with new ways of using them. By the dawn of the  computer age  in the 1950s, software became the means to surmount the limitations inherent in the punched card machines of the past. Luhn spent greater and greater amounts of time on the problems of information retrieval and storage faced by libraries and documentation centers, and pioneered the use of data processing equipment in resolving these problems. \"Luhn was the first, or among the first, to work out many of the basic techniques now commonplace in information science.\" These techniques included  full-text processing ;  hash codes ;  Key Word in Context  indexing (see also  Herbert Marvin Ohlman );  auto-indexing ;  automatic abstracting  and the concept of  selective dissemination of information  (SDI). Luhn was a pioneer in  hash coding . In 1953, he suggested putting information into  buckets  in order to speed up search. He did not only consider handling numbers more efficiently. He was applying his concepts to text as well. Luhn’s methods were improved by computer scientists decades after his inventions. Today,  hashing algorithms  are essential for many applications such as textual tools,  cloud services , data-intensive research and  cryptography  among numerous other uses. It is surprising that his name and contributions to information handling are largely forgotten. [ 5 ] Two of Luhn's greatest achievements are the idea for an SDI system and the KWIC [ 6 ]  method of indexing. Today's SDI systems owe a great deal to a 1958 paper by Luhn, \"A Business Intelligence System\", [ 7 ]  which described an \"automatic method to provide current awareness services to scientists and engineers\" who needed help to cope with the rapid post-war growth of scientific and technical literature.  Luhn apparently coined the term  business intelligence  in that paper. [ 8 ] Luhn received the  Award of Merit  from the  Association for Information Science and Technology  in 1964. [ 9 ]"
  },
  {
    "id": 123,
    "title": "Allen Kent",
    "content": "Allen Kent  (October 24, 1921 – May 1, 2014) was an American  information scientist . He was born in New York City. [ 1 ]  At  City College of New York , he earned a degree in  chemistry . [ 2 ]  During  World War II , he served in the  United States Army Air Forces . [ 2 ]  After the war, he worked on a classified project at  MIT  in mechanized document encoding and search. [ 1 ] In 1955, he helped found the Center for Documentation Communication Research at  Western Reserve University . [ 3 ]  This was \"the first academic program in the field of mechanized information retrieval, first using  cards , then utilizing new  reel-to-reel  tape technology.\" [ 1 ]  In the same year, he introduced the measures of  precision and recall  in  Perry, Kent & Berry (1955) . In 1959, he wrote an article for Harper's magazine entitled, \"A Machine That Does Research\" which provided one of the first insights in mainstream media about how Americans' lives can change due to electronic information technology. [ 4 ]   He joined the faculty of the  University of Pittsburgh  in 1963, where in 1970 he began the Department of Information Science. [ 5 ]  He retired from the university in 1992. [ 5 ]  At the time of his death, he was Distinguished Service Professor in the School of Information Sciences at the  University of Pittsburgh [ 5 ]  The school named a scholarship after him. [ 6 ]"
  },
  {
    "id": 124,
    "title": "Citation index",
    "content": "A  citation index  is a kind of  bibliographic index , an index of  citations  between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by  citators  such as  Shepard's Citations  (1873). In 1961,  Eugene Garfield 's  Institute for Scientific Information  (ISI) introduced the first citation index for papers published in  academic journals , first the  Science Citation Index  (SCI), and later the  Social Sciences Citation Index  (SSCI) and the  Arts and Humanities Citation Index  (AHCI).  American Chemical Society  converted its printed  Chemical Abstract Service  (established in 1907) into internet-accessible  SciFinder  in 2008. The first automated citation indexing  [ 1 ]  was done by  CiteSeer  in 1997 and was patented. [ 2 ]  Other sources for such data include  Google Scholar ,  Microsoft Academic , Elsevier's  Scopus , and the  National Institutes of Health 's  iCite . [ 3 ] The earliest known citation index is an index of biblical citations in  rabbinic literature , the  Mafteah ha-Derashot , attributed to  Maimonides  and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study. The Talmudic citation index  En Mishpat  (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century  Shepard's Citations . [ 4 ] [ 5 ]  Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed. In English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with  Raymond's Reports  (1743) and followed by  Douglas's Reports  (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision. [ 6 ]  These early tables of legal citations (\"citators\") were followed by a more complete, book length index, Labatt's  Table of Cases...California...  (1860) and in 1872 by Wait's  Table of Cases...New York... . The most important and best-known citation index for legal cases was released in 1873 with the publication of  Shepard's Citations . [ 6 ] William Adair, a former president of  Shepard's Citations , suggested in 1920 that citation indexes could serve as a tool for tracking science and engineering literature. [ 7 ]  After learning that  Eugene Garfield  held a similar opinion, Adair corresponded with Garfield in 1953. [ 8 ]  The correspondence prompted Garfield to examine  Shepard's Citations  index as a model that could be extended to the sciences. Two years later Garfield published \"Citation indexes for science\" in the journal  Science . [ 9 ]  In 1959, Garfield started a consulting business, the  Institute for Scientific Information  (ISI), in  Philadelphia  and began a correspondence with  Joshua Lederberg  about the idea. [ 7 ]  In 1961 Garfield received a grant from the  U.S. National Institutes of Health  to compile a citation index for Genetics. To do so, Garfield's team gathered 1.4 million citations from 613 journals. [ 8 ]  From this work, Garfield and the ISI produced the first version of the  Science Citation Index , published as a book in 1963. [ 10 ] General-purpose, subscription-based academic citation indexes include: Each of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They are not open-access and differ widely in cost: Web of Science and Scopus are available by subscription (generally to libraries). In addition,  CiteSeer  and  Google Scholar  are freely available online. Several open-access, subject-specific citation indexing services also exist, such as: Clarivate Analytics '  Web of Science  (WoS) and Elsevier's  Scopus  databases are synonymous with data on international research, and considered as the two most trusted or authoritative sources of bibliometric data for peer-reviewed global research knowledge across disciplines. [ 12 ] [ 13 ] [ 14 ] [ 15 ] [ 16 ] [ 17 ]  They are both also used widely for the purposes of researcher evaluation and promotion, institutional impact (for example the role of WoS in the UK Research Excellence Framework 2021 [ note 1 ] ), and international league tables (Bibliographic data from Scopus represents more than 36% of assessment criteria in the THE rankings [ note 2 ] ). But while these databases are generally agreed to contain rigorously-assessed, high quality research, they do not represent the sum of current global research knowledge. [ 18 ] It is often mentioned in popular science articles that the research output of countries in South America, Asia, and Africa are disappointingly low. Sub-Saharan Africa is cited as an example for having \"13.5% of the global population but less than 1% of global research output\". [ note 3 ]  This fact is based on data from a World Bank/Elsevier report from 2012 which relies on data from Scopus. [ note 4 ]  Research outputs in this context refers to papers specifically published in peer-reviewed journals that are indexed in Scopus. Similarly, many others have analysed putatively global or international collaborations and mobility using the even more selective WoS database. [ 19 ] [ 20 ] [ 21 ]  Research outputs in this context refers to papers specifically published in peer-reviewed journals that are indexed either in Scopus or WoS. Both WoS and Scopus are considered highly selective. Both are commercial enterprises, whose standards and assessment criteria are mostly controlled by panels in North America and Western Europe. The same is true for more comprehensive databases such as Ulrich's Web which lists as many as 70,000 journals, [ 22 ]  while Scopus has fewer than 50% of these, and WoS has fewer than 25%. [ 12 ]  While Scopus is larger and geographically broader than WoS, it still only covers a fraction of journal publishing outside North America and Europe. For example, it reports a coverage of over 2,000 journals in Asia (\"230% more than the nearest competitor\"), [ note 5 ]  which may seem impressive until you consider that in Indonesia alone there are more than 7,000 journals listed on the government's Garuda portal [ note 6 ]  (of which more than 1,300 are currently listed on DOAJ); [ note 7 ]  whilst at least 2,500 Japanese journals listed on the J-Stage platform. [ note 8 ]  Similarly, Scopus claims to have about 700 journals listed from Latin America, in comparison with SciELO's 1,285 active journal count; [ note 9 ]  but that is just the tip of the iceberg judging by the 1,300+ DOAJ-listed journals in Brazil alone. [ note 10 ]  Furthermore, the editorial boards of the journals contained in Wos and Scopus databases are integrated by researchers from western Europe and North America. For example, in the journal  Human Geography , 41% of editorial board members are from the United States, and 37.8% from the UK. [ 23 ]  Similarly, [ 24 ] ) studied ten leading marketing journals in WoS and Scopus databases, and concluded that 85.3% of their editorial board members are based in the United States. It comes as no surprise that the research that gets published in these journals is the one that fits the editorial boards' world view. [ 24 ] Comparison with subject-specific indexes has further revealed the geographical and topic bias – for example Ciarli [ 25 ]  found that by comparing the coverage of rice research in CAB Abstracts (an agriculture and global health database) with WoS and Scopus, the latter \"may strongly under-represent the scientific production by developing countries, and over-represent that by industrialised countries\", and this is likely to apply to other fields of agriculture. This under-representation of applied research in Africa, Asia, and South America may have an additional negative effect on framing research strategies and policy development in these countries. [ 26 ]  The overpromotion of these databases diminishes the important role of \"local\" and \"regional\" journals for researchers who want to publish and read locally-relevant content. Some researchers deliberately bypass \"high impact\" journals when they want to publish locally useful or important research in favour of outlets that will reach their key audience quicker, and in other cases to be able to publish in their native language. [ 27 ] [ 28 ] [ 29 ] Furthermore, the odds are stacked against researchers for whom English is a foreign language. 95% of WoS journals are English [ 30 ] [ 31 ]  consider the use of English language a hegemonic and unreflective linguistic practice. The consequences include that non-native speakers spend part of their budget on translation and correction and invest a significant amount of time and effort on subsequent corrections, making publishing in English a burden. [ 32 ] [ 33 ]  A far-reaching consequence of the use of English as the  lingua franca  of science is in knowledge production, because its use benefits \"worldviews, social, cultural, and political interests of the English-speaking center\" ( [ 31 ]  p. 123). The small proportion of research from South East Asia, Africa, and Latin America which makes it into WoS and Scopus journals is not attributable to a lack of effort or quality of research; but due to hidden and invisible epistemic and structural barriers (Chan 2019 [ note 11 ] ). These are a reflection of \"deeper historical and structural power that had positioned former colonial masters as the centers of knowledge production, while relegating former colonies to peripheral roles\" (Chan 2018 [ note 12 ] ). Many North American and European journals demonstrate conscious and unconscious bias against researchers from other parts of the world. [ note 13 ]  Many of these journals call themselves \"international\" but represent interests, authors, and even references only in their own languages. [ note 14 ] [ 34 ]  Therefore, researchers in non-European or North American countries commonly get rejected because their research is said to be \"not internationally significant\" or only of \"local interest\" (the wrong \"local\"). This reflects the current concept of \"international\" as limited to a Euro/Anglophone-centric way of knowledge production. [ 35 ] [ 30 ]  In other words, \"the ongoing internationalisation has not meant academic interaction and exchange of knowledge, but the dominance of the leading Anglophone journals in which international debates occurs and gains recognition\". [ 36 ] Clarivate Analytics have made some positive steps to broaden the scope of WoS, integrating the SciELO citation index – a move not without criticism [ note 15 ]  – and through the creation of the Emerging Sources Index (ESI), which has allowed database access to many more international titles. However, there is still a lot of work to be done to recognise and amplify the growing body of research literature generated by those outside North America and Europe. The Royal Society have previously identified that \"traditional metrics do not fully capture the dynamics of the emerging global science landscape\", and that academia needs to develop more sophisticated data and impact measures to provide a richer understanding of the global scientific knowledge that is available to us. [ 37 ] Academia has not yet built digital infrastructures which are equal, comprehensive, multi-lingual and allows fair participation in knowledge creation. [ 38 ]  One way to bridge this gap is with discipline- and region-specific preprint repositories such as  AfricArXiv  and  InarXiv . Open access advocates recommend to remain critical of those \"global\" research databases that have been built in Europe or Northern America and be wary of those who celebrate these products act as a representation of the global sum of human scholarly knowledge. Finally, let us also be aware of the geopolitical impact that such systematic discrimination has on knowledge production, and the inclusion and representation of marginalised research demographics within the global research landscape. [ 18 ]"
  },
  {
    "id": 125,
    "title": "Eugene Garfield",
    "content": "Eugene Eli Garfield  (September 16, 1925 – February 26, 2017) [ 2 ] [ 3 ]  was an American  linguist  and businessman, one of the founders of  bibliometrics  and  scientometrics . [ 4 ]  He helped to create  Current Contents ,  Science Citation Index  (SCI),  Journal Citation Reports , and  Index Chemicus , among others, and founded the magazine  The Scientist . [ 5 ] [ 6 ] [ 7 ] [ 8 ] Garfield was born in 1925 in New York City as Eugene Eli Garfinkle, [ 2 ]  his mother being of  Lithuanian Jewish  ancestry. [ 9 ] [ 10 ]  His parents were second generation immigrants living in  East Bronx  in New York City. [ 11 ]  He studied at the  University of Colorado  and  University of California, Berkeley  before getting a Bachelor of Science degree in  chemistry  from  Columbia University  in 1949. [ 12 ] [ 13 ]  Garfield also received a degree in Library Science from Columbia University in 1953. [ 11 ] [ 14 ]  He went on to do his PhD in the Department of  Linguistics  at the  University of Pennsylvania , which he completed in 1961 for developing an  algorithm  for translating  chemical nomenclature  into  chemical formulas . [ 1 ] [ 15 ] Working as a laboratory assistant at Columbia University after his graduation, Garfield indexed all previously synthesized compounds so that not to remake them, which helped him understand that his inclination to information towards science was bigger than towards chemistry. In 1951, he got a position at the Welch Medical Library at  Johns Hopkins University  in Baltimore, Maryland, where most of the  National Library of Medicine  information systems were developed. There he built search and cataloging system methods using  punch-cards . In 1953, at the First Symposium on Machine Methods in Scientific Documentation, Garfield got introduced to  Shepard's Citations . [ 11 ] In 1960, Garfield founded the  Institute for Scientific Information (ISI) , which was located in Philadelphia, Pennsylvania. [ 16 ]  In the 1990's, ISI was faced with bankruptcy and was acquired by JPT Holdings who later sold it to Thomson (Thomas Business Information) where it formed a major part of the science division of  Thomson Reuters . In October 2016 Thomson Reuters completed the sale of its intellectual property and science division; it is now known as  Clarivate Analytics . [ 17 ] Garfield was responsible for many innovative bibliographic products, including  Current Contents , the  Science Citation Index  (SCI), and other citation databases, the  Journal Citation Reports , and  Index Chemicus . He was the founding editor and publisher of  The Scientist , a news magazine for life scientists. [ 11 ]  In 2003, the  University of South Florida  School of Information was honored to have him as lecturer for the  Alice G. Smith Lecture . In 2007, he launched  Histcite , a bibliometric analysis and visualization software package. Following ideas inspired by  Vannevar Bush 's highly cited 1945 article  As We May Think , Garfield undertook the development of a comprehensive  citation index  showing the propagation of scientific thinking; he started the  Institute for Scientific Information  in 1956 (it was sold to the  Thomson Corporation  in 1992 [ 18 ] ). According to Garfield, \"the citation index ... may help a historian to measure the influence of an article — that is, its 'impact factor'\". [ 19 ]  The creation of the  Science Citation Index  made it possible to calculate  impact factor , [ 20 ]  which ostensibly measures the importance of scientific journals. It led to the unexpected discovery that a few journals like  Nature  and  Science  were core for all of  hard science . The same pattern does not happen with the humanities or the social sciences. [ 21 ] [ 22 ] His entrepreneurial flair in having turned what was, at least at the time, an obscure and specialist metric into a highly profitable business has been noted. [ 23 ]  A scientometric analysis of his top fifty cited papers has been conducted. [ 24 ] Garfield's work led to the development of several  information retrieval  algorithms, like the  HITS algorithm  and  PageRank . Both use the structured citation between websites through hyperlinks. Google co-founders  Larry Page  and  Sergey Brin  acknowledged Gene in their development of PageRank, the algorithm that powers their company's search engine. [ 23 ]  Garfield published over 1,000 essays. Garfield was honored with the  Award of Merit  from the  Association for Information Science and Technology  in 1975. He was awarded the  John Price Wetherill Medal  in 1984, [ 16 ]  the  Derek de Solla Price Memorial Medal  in 1984, [ 25 ]  and the Miles Conrad Award in 1985. [ 26 ]  He was also awarded the  Richard J. Bolte Sr. Award  in 2007. [ 27 ]  He was elected to the  American Philosophical Society  that same year. [ 28 ] The  Association for Library and Information Science Education  has a fund for doctoral research through an award named after Garfield. Writing in  Physiology News , No. 69, Winter 2007,  David Colquhoun  of the Department of Pharmacology,  University College London , described the \" impact factor ,\" a method for comparing scholarly journals, as \"the invention of Eugene Garfield, a man who has done enormous harm to true science.\" Colquhoun ridiculed C. Hoeffel's assertion that Garfield's impact factor \"has the advantage of already being in existence and is, therefore, a good technique for scientific evaluation\" by saying, \"you can't get much dumber than that. It is a 'good technique' because it is already in existence? There is something better. Read the papers.\" Garfield is survived by a wife, three sons, a daughter, two granddaughters, and two great-grandchildren. [ 2 ] [ 16 ] [ 29 ]"
  },
  {
    "id": 126,
    "title": "Calvin Mooers",
    "content": "Calvin Northrup Mooers  (October 24, 1919 – December 1, 1994), was an American  computer scientist  known for his work in  information retrieval  and for the programming language  TRAC . Mooers was a native of  Minneapolis, Minnesota , attended the  University of Minnesota , and received a bachelor's degree in mathematics in 1941.  He worked at the  Naval Ordnance Laboratory  from 1941 to 1946, and then attended the  Massachusetts Institute of Technology , where he earned a master's degree in mathematics and physics.  At M.I.T. he developed a mechanical system using  superimposed codes  of  descriptors  for information retrieval called  Zatocoding .  He founded the Zator Company in 1947 to market this idea, and pursued work in  information theory ,  information retrieval , and  artificial intelligence . He coined the term \" information retrieval \", using it first in a conference paper presented in March 1950. [ 1 ]  See also a short paper published later that year from Mooers. [ 2 ] He coined \" Mooers's law \" (not to be confused with  Moore's law ) and its corollary in 1959: He founded the Rockford Research Institute in 1961, where he developed the  TRAC programming language , and attempted to control its distribution and development using trademark law and a unique invocation of\ncopyright. [ 3 ]  (At the time  patent law  would not allow him to control what he saw as his  intellectual property  and profit from it.)  The trademark strategy was later used by  Ada . Mooers received the  American Society for Information Science 's  Award of Merit   in 1978.  The citation reads in part: Mooers died in 1994 in  Cambridge, Massachusetts . Mooers's article critical of  John Vincent Atanasoff  and his brief tenure as chief of a failed computer construction project at the  Naval Ordnance Laboratory  during  World War II , was published posthumously in the May–June 2001 issue of  IEEE Annals of the History of Computing ."
  },
  {
    "id": 127,
    "title": "Massachusetts Institute of Technology",
    "content": "The  Massachusetts Institute of Technology    ( MIT ) is a  private   research university  in  Cambridge, Massachusetts , United States. Established in 1861, MIT has played a significant role in the development of many areas of modern  technology  and  science . Founded in response to the increasing  industrialization of the United States , MIT adopted a European  polytechnic university model  and stressed laboratory instruction in  applied science  and  engineering . MIT is one of three private land-grant universities in the United States, the others being  Cornell University  and  Tuskegee University . The institute has an urban campus that extends more than a mile (1.6 km) alongside the  Charles River , and operates off-campus facilities including the  MIT Lincoln Laboratory , the Bates Center, and the  Haystack Observatory , as well as affiliated laboratories such as the  Broad  and  Whitehead Institutes . As of October 2024 [update] ,  105 Nobel laureates , [ 10 ]  26  Turing Award  winners, and 8  Fields Medalists  have been affiliated with MIT as alumni, faculty members, or researchers. [ 11 ]  In addition, 58  National Medal of Science  recipients, 29  National Medals of Technology and Innovation  recipients, 50  MacArthur Fellows , [ 12 ]  83  Marshall Scholars , [ 13 ]  41  astronauts , [ 14 ]  16  Chief Scientists of the US Air Force , and  1 foreign head of state  have been affiliated with MIT. The institute also has a strong  entrepreneurial culture  and MIT alumni have founded or co-founded many notable companies. [ 15 ] [ 16 ]  MIT is a member of the  Association of American Universities . [ 17 ] [...] a school of industrial science aiding the advancement, development and practical application of science in connection with arts, agriculture, manufactures, and commerce [...] In 1859, a proposal was submitted to the  Massachusetts General Court  to use newly filled lands in  Back Bay , Boston for a \" Conservatory of Art and Science \", but the proposal failed. [ 19 ] [ 20 ]  A charter for the  incorporation  of the Massachusetts Institute of Technology, proposed by  William Barton Rogers , was signed by  John Albion Andrew , the  governor of Massachusetts , on April 10, 1861. [ 21 ] Rogers, who was educated at the  College of William & Mary  and later held professorships at both  William & Mary  and the  University of Virginia , [ 22 ]  wanted to establish an institution to address rapid scientific and technological advances. [ 23 ] [ 24 ]  He did not wish to found a  professional school , but a combination with elements of both professional and  liberal education , [ 25 ]  proposing that: The true and only practicable object of a polytechnic school is, as I conceive, the teaching, not of the minute details and manipulations of the arts, which can be done only in the workshop, but the inculcation of those scientific principles which form the basis and explanation of them, and along with this, a full and methodical review of all their leading processes and operations in connection with physical laws. [ 26 ] The Rogers Plan reflected the  German research university model , emphasizing an independent faculty engaged in research, as well as instruction oriented around seminars and laboratories. [ 27 ] [ 28 ] Two days after MIT was chartered, the  first battle  of the  Civil War  broke out. After a long delay through the war years, MIT's first classes were held in the Mercantile Building in Boston in 1865. [ 29 ]  The new institute was founded as part of the  Morrill Land-Grant Colleges Act  to fund institutions \"to promote the liberal and practical education of the industrial classes\" and was a land-grant school. [ 30 ] [ 31 ]  In 1863 under the same act, the Commonwealth of Massachusetts founded the  Massachusetts Agricultural College , which developed as the  University of Massachusetts Amherst . In 1866, the proceeds from land sales went toward new buildings in the Back Bay. [ 32 ] MIT was informally called \"Boston Tech\". [ 32 ]  The institute adopted the  European polytechnic university model  and emphasized laboratory instruction from an early date. [ 27 ]  Despite chronic financial problems, the institute saw growth in the last two decades of the 19th century under President  Francis Amasa Walker . [ 33 ]  Programs in electrical, chemical, marine, and sanitary engineering were introduced, [ 34 ] [ 35 ]  new buildings were built, and the size of the student body increased to more than one thousand. [ 33 ] The curriculum drifted to a vocational emphasis, with less focus on theoretical science. [ 36 ]  The fledgling school still suffered from chronic financial shortages which diverted the attention of the MIT leadership. During these \"Boston Tech\" years, MIT faculty and alumni rebuffed  Harvard University  president (and former MIT faculty)  Charles W. Eliot 's repeated attempts to merge MIT with Harvard College's  Lawrence Scientific School . [ 37 ]  There would be at least six attempts to absorb MIT into Harvard. [ 38 ]  In its cramped Back Bay location, MIT could not afford to expand its overcrowded facilities, driving a desperate search for a new campus and funding. Eventually, the MIT Corporation approved a formal agreement to merge with Harvard and move to Allston, over the vehement objections of MIT faculty, students, and alumni. [ 38 ]  The merger plan collapsed in 1905 when the Massachusetts Supreme Judicial Court rules that MIT could not sell its Back Bay land. [ 39 ] In 1912, MIT acquired its current campus by purchasing a one-mile (1.6 km) tract of  filled lands  along the Cambridge side of the Charles River. [ 40 ] [ 41 ]  The  neoclassical  \"New Technology\" campus was designed by  William W. Bosworth [ 42 ]  and had been funded largely by anonymous donations from a mysterious \"Mr. Smith\", starting in 1912. In January 1920, the donor was revealed to be the industrialist  George Eastman , an inventor of film production methods and founder of  Eastman Kodak . Between 1912 and 1920, Eastman donated $20 million ($304.2 million in 2024 dollars) in cash and Kodak stock to MIT. [ 43 ]  In 1916, with the first academic buildings complete, the MIT administration and the MIT charter crossed the Charles River on the ceremonial barge  Bucentaur  built for the occasion. [ 44 ] [ 45 ] Institute faculty continued to debate whether the Institute education should continue to emphasize \"hands on\" industrial training or scientific research. [ 46 ]  Needing funds to match Eastman's gift and cover retreating state support, President  Richard MacLaurin  launched an industry funding model known as the \"Technology Plan\" in 1920. [ 46 ] [ 47 ] [ 48 ]  As MIT grew under the Tech Plan, it built new postgraduate programs that stressed laboratory work on industry problems, including a new program in electrical engineering. [ 46 ]   Gerard Swope , MIT's chairman and head of  General Electric , believed talented engineers needed scientific research training. [ 46 ]  In 1930, he recruited  Karl Taylor Compton  to helm MIT's transformation as a \"technological\" research university and to build more autonomy from private industry. [ 46 ] [ 49 ] In the 1930s, President  Karl Taylor Compton  and Vice-President (effectively  Provost )  Vannevar Bush  emphasized the importance of pure sciences like physics and chemistry and reduced the vocational practice required in shops and drafting studios. [ 50 ]  The Compton reforms \"renewed confidence in the ability of the Institute to develop leadership in science as well as in engineering\". [ 51 ]  Unlike  Ivy League  schools, MIT catered more to middle-class families, and depended more on  tuition  than on  endowments  or  grants  for its funding. [ 47 ]  The school was elected to the  Association of American Universities  in 1934. [ 52 ] Still, as late as 1949, the Lewis Committee lamented in its report on the state of education at MIT that \"the Institute is widely conceived as basically a vocational school\", a \"partly unjustified\" perception the committee sought to change. The report comprehensively reviewed the undergraduate curriculum, recommended offering a broader education, and warned against letting engineering and government-sponsored research detract from the sciences and humanities. [ 53 ] [ 54 ]  The  School of Humanities, Arts, and Social Sciences  and the  MIT Sloan School of Management  were formed in 1950 to compete with the powerful Schools of  Science  and  Engineering . Previously marginalized faculties in the areas of economics, management, political science, and linguistics emerged into cohesive and assertive departments by attracting respected professors and launching competitive graduate programs. [ 55 ] [ 56 ]  The School of Humanities, Arts, and Social Sciences continued to develop under the successive terms of the more  humanistically oriented  presidents  Howard W. Johnson  and  Jerome Wiesner  between 1966 and 1980. [ 57 ] MIT's involvement in  military science  surged during  World War II . In 1941,  Vannevar Bush  was appointed head of the federal  Office of Scientific Research and Development  and directed funding to only a select group of universities, including MIT. [ 58 ]  Engineers and scientists from across the country gathered at MIT's  Radiation Laboratory , established in 1940 to assist the  British military  in developing  microwave   radar . The work done there significantly affected both the war and subsequent research in the area. [ 59 ]  Other defense projects included  gyroscope -based and other complex  control systems  for  gunsight ,  bombsight , and  inertial navigation  under  Charles Stark Draper 's  Instrumentation Laboratory ; [ 60 ] [ 61 ]  the development of a  digital computer  for flight simulations under  Project Whirlwind ; [ 62 ]  and  high-speed  and  high-altitude  photography under  Harold Edgerton . [ 63 ] [ 64 ]  By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush), [ 58 ]  employing nearly 4000 in the Radiation Laboratory alone [ 59 ]  and receiving in excess of $100 million ($1.2 billion in 2015 dollars) before 1946. [ 51 ]  Work on defense projects continued even after then. Post-war  government-sponsored research  at MIT included  SAGE  and guidance systems for  ballistic missiles  and  Project Apollo . [ 65 ] ... a special type of educational institution which can be defined as a university polarized around science, engineering, and the arts. We might call it a university limited in its objectives but unlimited in the breadth and the thoroughness with which it pursues these objectives. These activities affected MIT profoundly. A 1949 report noted the lack of \"any great slackening in the pace of life at the Institute\" to match the return to peacetime, remembering the \"academic tranquility of the prewar years\", though acknowledging the significant contributions of military research to the increased emphasis on graduate education and rapid growth of personnel and facilities. [ 67 ]  The faculty doubled and the graduate student body quintupled during the presidential terms of  Karl Taylor Compton  (1930–1948),  James Rhyne Killian  (1948–1957), and chancellor  Julius Adams Stratton  (1952–1957), whose institution-building strategies shaped the expanding university. By the 1950s, MIT no longer simply benefited the industries with which it had worked for three decades, and it had developed closer working relationships with new patrons, philanthropic foundations and the federal government. [ 68 ] In late 1960s and early 1970s, student and faculty activists protested against the  Vietnam War  and MIT's defense research. [ 69 ] [ 70 ]  In this period MIT's various departments were researching helicopters, smart bombs and counterinsurgency techniques for the war in Vietnam as well as guidance systems for nuclear missiles. [ 71 ]  The  Union of Concerned Scientists  was founded on March 4, 1969 during a meeting of faculty members and students seeking to shift the emphasis on military research toward environmental and social problems. [ 72 ]  MIT ultimately divested itself from the Instrumentation Laboratory and moved all classified research off-campus to the  MIT Lincoln Laboratory  facility in 1973 in response to the protests. [ 73 ] [ 74 ]  The student body, faculty, and administration remained comparatively unpolarized during what was a tumultuous time for many other universities. [ 69 ]  Johnson was seen to be highly successful in leading his institution to \"greater strength and unity\" after these times of turmoil. [ 75 ]  However six MIT students were sentenced to prison terms at this time and some former student leaders, such as  Michael Albert  and  George Katsiaficas , are still indignant about MIT's role in military research and its suppression of these protests. [ 76 ]  ( Richard Leacock 's film,  November Actions , records some of these tumultuous events. [ 77 ] ) In the 1980s, there was more controversy at MIT over its involvement in SDI (space weaponry) and CBW (chemical and biological warfare) research. [ 78 ]  More recently, MIT's research for the military has included work on robots, drones and 'battle suits'. [ 79 ] MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and  networking  technologies, [ 80 ] [ 81 ]  students, staff, and faculty members at  Project MAC , the  Artificial Intelligence Laboratory , and the  Tech Model Railroad Club  wrote some of the earliest interactive  computer video games  like  Spacewar!  and created much of modern  hacker   slang  and culture. [ 82 ]  Several major computer-related organizations have originated at MIT since the 1980s:  Richard Stallman 's  GNU Project  and the subsequent  Free Software Foundation  were founded in the mid-1980s at the AI Lab; the  MIT Media Lab  was founded in 1985 by  Nicholas Negroponte  and Jerome Wiesner to promote research into novel uses of computer technology; [ 83 ]  the  World Wide Web Consortium   standards organization  was founded at the  Laboratory for Computer Science  in 1994 by  Tim Berners-Lee ; [ 84 ]  the  OpenCourseWare  project has made course materials for over 2,000 MIT classes available online free of charge since 2002; [ 85 ]  and the  One Laptop per Child  initiative to expand computer education and connectivity to children worldwide was launched in 2005. [ 86 ] MIT was named a  sea-grant college  in 1976 to support its programs in oceanography and marine sciences and was named a  space-grant college  in 1989 to support its aeronautics and astronautics programs. [ 87 ] [ 88 ]  Despite diminishing government financial support over the past quarter century, MIT launched several successful  development campaigns  to significantly expand the campus: new dormitories and athletics buildings on west campus; the  Tang Center for Management Education ; several buildings in the northeast corner of campus supporting research into  biology ,  brain and cognitive sciences ,  genomics ,  biotechnology , and  cancer research ; and a number of new \"backlot\" buildings on Vassar Street including the  Stata Center . [ 89 ]  Construction on campus in the 2000s included expansions of the Media Lab, the Sloan School's eastern campus, and graduate residences in the northwest. [ 90 ] [ 91 ]  In 2006, President Hockfield launched the MIT Energy Research Council to investigate the interdisciplinary challenges posed by increasing  global energy consumption . [ 92 ] In 2001, inspired by the  open source  and  open access movements , [ 93 ]  MIT launched  OpenCourseWare  to make the lecture notes,  problem sets , syllabi, exams, and lectures from the great majority of its courses available online for no charge, though without any formal accreditation for coursework completed. [ 94 ]  While the cost of supporting and hosting the project is high, [ 95 ]  OCW expanded in 2005 to include other universities as a part of the OpenCourseWare Consortium, which currently includes more than 250 academic institutions with content available in at least six languages. [ 96 ]  In 2011, MIT announced it would offer formal certification (but not credits or degrees) to online participants completing coursework in its \"MITx\" program, for a modest fee. [ 97 ]  The \" edX \" online platform supporting MITx was initially developed in partnership with  Harvard  and its analogous \"Harvardx\" initiative. The courseware platform is open source, and other universities have already joined and added their own course content. [ 98 ]  In March 2009 the MIT faculty adopted an  open-access policy  to make its scholarship  publicly accessible  online. [ 99 ] MIT has its own police force. Three days after the  Boston Marathon bombing  of April 2013,  MIT Police  patrol officer  Sean Collier  was fatally shot by the suspects  Dzhokhar  and  Tamerlan Tsarnaev , setting off a violent manhunt that shut down the campus and much of the Boston metropolitan area for a day. [ 100 ]  One week later, Collier's memorial service was attended by more than 10,000 people, in a ceremony hosted by the MIT community with thousands of police officers from the New England region and Canada. [ 101 ] [ 102 ] [ 103 ]  On November 25, 2013, MIT announced the creation of the Collier Medal, to be awarded annually to \"an individual or group that embodies the character and qualities that Officer Collier exhibited as a member of the MIT community and in all aspects of his life\". The announcement further stated that \"Future recipients of the award will include those whose contributions exceed the boundaries of their profession, those who have contributed to building bridges across the community, and those who consistently and selflessly perform acts of kindness\". [ 104 ] [ 105 ] [ 106 ] In September 2017, the school announced the creation of an  artificial intelligence  research lab called the MIT-IBM Watson AI Lab.  IBM  will spend $240 million over the next decade, and the lab will be staffed by MIT and IBM scientists. [ 107 ]  In October 2018 MIT announced that it would open a new  Schwarzman College of Computing  dedicated to the study of artificial intelligence, named after lead donor and  The Blackstone Group  CEO  Stephen Schwarzman . The focus of the new college is to study not just AI, but interdisciplinary AI education, and how AI can be used in fields as diverse as history and biology. The cost of buildings and new faculty for the new college is expected to be $1 billion upon completion. [ 108 ] The  Laser Interferometer Gravitational-Wave Observatory  (LIGO) was designed and constructed by a team of scientists from  California Institute of Technology , MIT, and industrial contractors, and funded by the  National Science Foundation . It was designed to open the field of  gravitational-wave astronomy  through the detection of  gravitational waves  predicted by  general relativity . [ 109 ]  Gravitational waves were  detected for the first time  by the LIGO detector in 2015. For contributions to the LIGO detector and the observation of gravitational waves, two Caltech physicists,  Kip Thorne  and  Barry Barish , and MIT physicist  Rainer Weiss  won the  Nobel Prize in physics  in 2017. [ 110 ]  Weiss, who is also an MIT graduate, designed the laser interferometric technique, which served as the essential blueprint for the LIGO. [ 111 ] In April of 2024, MIT students joined other  campuses across the United States in protests  and setting up encampments against the  Israel–Hamas war . [ 112 ] [ 113 ] [ 114 ] [ 115 ]   Student likened their actions to the  historic protests  against the American invasion of  Vietnam  and MIT investment’s in South African  apartheid ; [ 116 ]  they called for ending ties to the  Israeli Ministry of Defense . [ 117 ] MIT's 166-acre (67.2 ha) campus in the city of  Cambridge  spans approximately a mile along the north side of the  Charles River  basin. [ 6 ]  The campus is divided roughly in half by  Massachusetts Avenue , with most dormitories and student life facilities to the west and most academic buildings to the east. The bridge closest to MIT is the  Harvard Bridge , which is known for being marked off in a  non-standard unit of length  – the  smoot . [ 118 ] [ 119 ] The  Kendall/MIT   MBTA Red Line  station is located on the northeastern edge of the campus, in  Kendall Square . The Cambridge neighborhoods surrounding MIT are a mixture of high tech companies occupying both modern office and rehabilitated industrial buildings, as well as socio-economically diverse residential neighborhoods. [ 120 ] [ 121 ]  In early 2016, MIT presented its updated Kendall Square Initiative to the City of Cambridge, with plans for mixed-use educational, retail, residential, startup incubator, and office space in a dense high-rise  transit-oriented development  plan. The  MIT Museum  has moved immediately adjacent to a Kendall Square subway entrance, joining the  List Visual Arts Center  on the eastern end of the campus. [ 122 ] Each building at MIT  has a number  (possibly preceded by a  W ,  N ,  E , or  NW ) designation, and most have a name as well. Typically, academic and office buildings are referred to primarily by number while residence halls are referred to by name. The organization of building numbers roughly corresponds to the order in which the buildings were built and their location relative (north, west, and east) to the original center cluster of Maclaurin buildings. [ 123 ]  Many of the buildings are connected above ground as well as through an extensive network of tunnels, providing protection from the Cambridge weather as well as a venue for  roof and tunnel hacking . [ 124 ] [ 125 ] MIT's on-campus nuclear reactor [ 126 ]  is one of the most powerful university-based  nuclear reactors  in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial, [ 127 ]  but MIT maintains that it is well-secured. [ 128 ] MIT Nano, also known as Building 12, is an interdisciplinary facility for nanoscale research. Its 100,000 sq ft (9,300 m 2 )  cleanroom  and research space, visible through expansive glass facades, is the largest research facility of its kind in the nation. [ 129 ]  With a cost of US$400 million, it is also one of the costliest buildings on campus. The facility also provides state-of-the-art nanoimaging capabilities with vibration damped imaging and metrology suites sitting atop a 5 × 10 ^ 6  lb (2,300,000 kg) slab of concrete underground. [ 130 ] Other notable campus facilities include a pressurized  wind tunnel  for testing  aerodynamic  research, a  towing tank  for testing ship and ocean structure designs, and previously  Alcator C-Mod , which was the largest fusion device operated by any university. [ 131 ] [ 132 ]  MIT's campus-wide wireless network was completed in the fall of 2005 and consists of nearly 3,000 access points covering 9.4 × 10 ^ 6  sq ft (870,000 m 2 ) of campus. [ 133 ] The campus' primary energy source is natural gas. In connection with capital campaigns to expand the campus, the Institute has also extensively renovated existing buildings to improve their energy efficiency. MIT has also taken steps to reduce its environmental impact by running  alternative fuel  campus shuttles, subsidizing  public transportation passes , constructing  solar power offsets , and building a  cogeneration  plant to power campus electricity, heating, and cooling requirements. [ 134 ] [ 135 ] MIT has substantial  commercial real estate  holdings in Cambridge on which it pays  property taxes , plus an additional voluntary  payment in lieu of taxes  (PILOT) on academic buildings which are legally tax-exempt. As of 2017 [update] , it is the largest taxpayer in the city, contributing approximately 14% of the city's annual revenues. [ 136 ]  Holdings include  Technology Square , parts of  Kendall Square ,  University Park , and many properties in  Cambridgeport  and  Area 4  neighboring the main campus. [ 137 ]  The land is held for investment purposes and potential long-term expansion. [ 138 ] MIT's School of Architecture , founded in 1865 [ 139 ]  and now called the School of Architecture and Planning, was the first formal architecture program in the United States, [ 140 ]  and it has a history of commissioning progressive buildings. [ 141 ] [ 142 ]  The first buildings constructed on the Cambridge campus, completed in 1916, are sometimes called the \"Maclaurin buildings\" after Institute president  Richard Maclaurin  who oversaw their construction. Designed by  William Welles Bosworth , these imposing buildings were built of  reinforced concrete , a first for a non-industrial – much less university – building in the US. [ 143 ]  Bosworth's design was influenced by the  City Beautiful Movement  of the early 1900s [ 143 ]  and features the  Pantheon -esque Great Dome housing the Barker Engineering Library. The Great Dome overlooks Killian Court, where  graduation  ceremonies are held each year. The friezes of the limestone-clad buildings around Killian Court are engraved with the names of important scientists and philosophers. [ a ]  The spacious Building 7 atrium at  77 Massachusetts Avenue  is regarded as the entrance to the  Infinite Corridor  and the rest of the campus. [ 121 ] Alvar Aalto 's Baker House (1947),  Eero Saarinen 's  MIT Chapel  and  Kresge Auditorium  (1955), and  I.M. Pei 's  Green , Dreyfus, Landau, and  Wiesner  buildings represent high forms of post-war  modernist architecture . [ 146 ] [ 147 ] [ 148 ]  More recent buildings like  Frank Gehry 's  Stata Center  (2004),  Steven Holl 's  Simmons Hall  (2002),  Charles Correa 's Building 46 (2005), and  Fumihiko Maki 's Media Lab Extension (2009) stand out among the Boston area's classical architecture and serve as examples of contemporary campus \" starchitecture \". [ 141 ] [ 149 ]  These buildings have not always been well received; [ 150 ] [ 151 ]  in 2010,  The Princeton Review  included MIT in a list of twenty schools whose campuses are \"tiny, unsightly, or both\". [ 152 ] Undergraduates are guaranteed four-year housing in one of MIT's 11 undergraduate dormitories. [ 153 ]  Those living on campus can receive support and mentoring from live-in graduate student tutors, resident advisors, and faculty housemasters. [ 154 ]  Because housing assignments are made based on the preferences of the students themselves, diverse social atmospheres can be sustained in different living groups; for example, according to the  Yale Daily News  staff's  The Insider's Guide to the Colleges, 2010 , \"The split between East Campus and West Campus is a significant characteristic of MIT. East Campus has gained a reputation as a thriving counterculture.\" [ 155 ]  MIT also has 5 dormitories for single graduate students and 2 apartment buildings on campus for married student families. [ 156 ] MIT has an active Greek and  co-op housing  system, including thirty-six  fraternities ,  sororities , and independent living groups ( FSILGs ). [ 157 ]  As of 2015 [update] , 98% of all undergraduates lived in MIT-affiliated housing; 54% of the men participated in fraternities and 20% of the women were involved in sororities. [ 158 ]  Most FSILGs are located across the river in  Back Bay  near where MIT was founded, and there is also a cluster of fraternities on MIT's West Campus that face the Charles River Basin. [ 159 ]  After the 1997 alcohol-related death of Scott Krueger, a new pledge at the  Phi Gamma Delta  fraternity, MIT required all freshmen to live in the dormitory system starting in 2002. [ 160 ]  Because FSILGs had previously housed as many as 300 freshmen off-campus, the new policy could not be implemented until  Simmons Hall  opened in that year. [ 161 ] In 2013–2014, MIT abruptly closed and then demolished undergrad dorm Bexley Hall, citing extensive water damage that made repairs infeasible. In 2017, MIT shut down Senior House after a century of service as an undergrad dorm. That year, MIT administrators released data showing just 60% of Senior House residents had graduated in four years. Campus-wide, the four-year graduation rate is 84% (the cumulative graduation rate is significantly higher). [ 162 ] MIT is a publicly-chartered  nonprofit corporation  governed by a privately appointed  board  known as the  MIT Corporation . [ 163 ]  A large board since MIT's founding, [ 163 ]  the Corporation has 60–80 members at any time, some with fixed terms, some with life appointments, and eight who serve  ex officio . [ 164 ] [ 165 ] [ 166 ]  The Corporation approves the budget, new programs, degrees and faculty appointments, and elects a president to manage the university and preside over the Institute's faculty. [ 121 ] [ 163 ]  The current president is  Sally Kornbluth , a cell biologist and former provost at  Duke University , who became MIT's eighteenth president in January 2023. [ 167 ] MIT has five schools ( Science ,  Engineering ,  Architecture and Planning ,  Management , and  Humanities, Arts, and Social Sciences ) and one college ( Schwarzman College of Computing ), but no schools of law or medicine. [ 168 ] [ b ] [ 170 ]  While faculty committees assert substantial control over many areas of MIT's curriculum, research, student life, and administrative affairs, [ 171 ]  the chair of each of MIT's 32 academic departments reports to the dean of that department's school, who in turn reports to the Provost under the President. [ 172 ]  Academic departments are also evaluated by \"Visiting Committees,\" specialized bodies of Corporation members and outside experts who review the performance, activities, and needs of each department. MIT's  endowment , real estate, and other financial assets are managed through by the MIT Investment Management Company (MITIMCo), a subsidiary of the MIT Corporation created in 2004. [ 173 ]  A minor revenue source for much of the Institute's history, the endowment's role in MIT operations has grown due to strong investment returns since the 1990s, making it  one the largest endowments held by American universities . [ 174 ]  Among its holdings are a majority of shares in the audio equipment manufacturer  Bose Corporation . [ 175 ] MIT is a large, highly residential, research university with a majority of enrollments in graduate and professional programs. [ 178 ]  The university has been  accredited  by the  New England Association of Schools and Colleges  since 1929. [ 179 ]  MIT operates on a  4–1–4 academic calendar  with the fall semester beginning after  Labor Day  and ending in mid-December, a 4-week \"Independent Activities Period\" in the month of January, and the spring semester commencing in early February and ceasing in late May. [ 180 ] MIT students refer to both their majors and classes using numbers or acronyms alone. [ 181 ]  Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is  Course 1 , while Linguistics and Philosophy is  Course 24 . [ 182 ]  Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as \"Course 6\". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; for instance, the introductory calculus-based  classical mechanics  course is simply \"8.01\" (pronounced  eight-oh-one ) at MIT. [ 183 ] [ c ] The four-year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences. In 2010, it was dubbed \"most selective\" by  U.S. News , [ 190 ]  admitting few transfer students [ 178 ]  and 4.1% of its applicants in the 2020–2021 admissions cycle. [ 191 ]  It is  need-blind  for both domestic and international applicants. [ 192 ]  MIT offers 44 undergraduate degrees across its five schools. [ 193 ]  In the 2017–2018 academic year, 1,045 Bachelor of Science degrees (abbreviated \" SB \") were granted, the only type of undergraduate degree MIT now awards. [ needs update ] [ 194 ] [ 195 ]  In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 63% of students in its 19 degree programs, followed by the School of Science (29%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (2%). [ needs update ]  The largest undergraduate degree programs were in Electrical Engineering and Computer Science ( Course 6–2 ), Computer Science and Engineering ( Course 6–3 ), Mechanical Engineering ( Course 2 ), Physics ( Course 8 ), and Mathematics ( Course 18 ). [ 184 ] All undergraduates are required to complete a core curriculum called the General Institute Requirements (GIRs). [ 196 ]  The Science Requirement, generally completed during freshman year as prerequisites for classes in science and engineering majors, comprises two semesters of physics, two semesters of calculus, one semester of chemistry, and one semester of biology. There is a Laboratory Requirement, usually satisfied by an appropriate class in a course major. The Humanities, Arts, and Social Sciences (HASS) Requirement consists of eight semesters of classes in the humanities, arts, and social sciences, including at least one semester from each division as well as the courses required for a designated concentration in a HASS division. Under the Communication Requirement, two of the HASS classes, plus two of the classes taken in the designated major must be \"communication-intensive\", [ 197 ]  including \"substantial instruction and practice in oral presentation\". [ 198 ]  Finally, all students are required to complete a  swimming  test; [ 199 ]  non-varsity athletes must also take four quarters of  physical education  classes. [ 196 ] Most classes rely on a combination of lectures, recitations led by associate professors or graduate students, weekly problem sets (\"p-sets\"), and periodic quizzes or tests. While the pace and difficulty of MIT coursework has been compared to \"drinking from a fire hose\", [ 200 ] [ 201 ] [ 202 ]  the freshmen retention rate at MIT is similar to other research universities. [ 190 ]  The \"pass/no-record\" grading system relieves some pressure for first-year undergraduates. For each class taken in the fall term, freshmen transcripts will either report only that the class was passed, or otherwise not have any record of it. In the spring term, passing grades (A, B, C) appear on the transcript while non-passing grades are again not recorded. [ 203 ]  (Grading had previously been \"pass/no record\" all freshman year, but was amended for the Class of 2006 to prevent students from  gaming the system  by completing required major classes in their freshman year. [ 204 ] ) Also, freshmen may choose to join alternative learning communities, such as  Experimental Study Group ,  Concourse , or Terrascope. [ 203 ] In 1969,  Margaret MacVicar  founded the  Undergraduate Research Opportunities Program  (UROP) to enable undergraduates to collaborate directly with faculty members and researchers. Students join or initiate research projects (\"UROPs\") for academic credit, pay, or on a volunteer basis through postings on the UROP website or by contacting faculty members directly. [ 205 ]  A substantial majority of undergraduates participate. [ 206 ] [ 207 ]  Students often become  published , file  patent applications , and/or launch  start-up companies  based upon their experience in UROPs. [ 208 ] [ 209 ] In 1970, the then-Dean of Institute Relations, Benson R. Snyder, published  The Hidden Curriculum ,  arguing that education at MIT was often slighted in favor of following a set of unwritten expectations and that graduating with good grades was more often the product of figuring out the system rather than a solid education. The successful student, according to Snyder, was the one who was able to discern which of the formal requirements were to be ignored in favor of which unstated norms. For example, organized student groups had compiled \" course bibles \"—collections of problem-set and examination questions and answers for later students to use as references. This sort of gamesmanship, Snyder argued, hindered development of a creative intellect and contributed to student discontent and unrest. [ 210 ] [ 211 ] MIT's graduate program has high coexistence with the undergraduate program, and many courses are taken by qualified students at both levels. MIT offers a comprehensive doctoral program with degrees in the humanities, social sciences, and  STEM fields  as well as professional degrees, including the  Master of Business Administration  (MBA). [ 178 ]  The Institute offers graduate programs leading to academic degrees such as the Master of Science (which is abbreviated as MS at MIT), various Engineer's Degrees, Doctor of Philosophy ( PhD ), and  Doctor of Science  (DSc) and interdisciplinary graduate programs such as the  MD-PhD  (with  Harvard Medical School ) and a joint program in  oceanography  with  Woods Hole Oceanographic Institution . [ 212 ] [ 213 ] [ 214 ] [ 215 ] Admission to graduate programs is decentralized; applicants apply directly to the department or degree program. More than 90% of doctoral students are supported by fellowships, research assistantships (RAs), or teaching assistantships (TAs). [ 216 ] MIT Bootcamps are \"intense week-long innovation and leadership programs\". [ 217 ]  MIT Bootcamp instructors include  Eric von Hippel ,  Sanjay Sarma ,  Erdin Beshimov , and  Bill Aulet . [ 218 ]  MIT Bootcamps were founded by  Erdin Beshimov . [ 219 ] [ 220 ] [ 221 ] MIT places among the top five in many overall rankings of universities (see table right) and rankings based on students'  revealed preferences . [ 229 ] [ 230 ] [ 231 ]  For several years,  U.S. News & World Report , the  QS World University Rankings , and the  Academic Ranking of World Universities  have ranked MIT's School of Engineering first, as did the 1995  National Research Council  report. [ 232 ]  In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, architecture, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy. [ 233 ] Times Higher Education  has recognized MIT as one of the world's \"six super brands\" on its  World Reputation Rankings , along with  Berkeley ,  Cambridge ,  Harvard ,  Oxford , and  Stanford . [ 234 ]  In 2019, it was ranked #3 among the universities around the world by  SCImago Institutions Rankings . [ 235 ]  In 2017, the  Times Higher Education World University Rankings  also rated MIT the #2 university for arts and humanities. [ 236 ] [ 237 ]  MIT was ranked #7 in 2015 and #6 in 2017 of the Nature Index Annual Tables, which measure the largest contributors to papers published in 82 leading journals. [ 238 ] [ 239 ] [ 240 ]  Georgetown University researchers ranked MIT #3 in the US for 20-year  return on investment . [ 241 ] The university historically pioneered research and training collaborations between academia, industry and government. [ 242 ] [ 243 ]  In 1946, President Compton, Harvard Business School professor  Georges Doriot , and Massachusetts Investor Trust chairman Merrill Grisswold founded  American Research and Development Corporation , the first American  venture-capital  firm. [ 244 ] [ 245 ]  In 1948, Compton established the MIT Industrial Liaison Program. [ 246 ]  Throughout the late 1980s and early 1990s, American politicians and business leaders accused MIT and other universities of contributing to a  declining economy  by  transferring  taxpayer-funded research and technology to international – especially  Japanese  – firms that were competing with struggling American businesses. [ 247 ] [ 248 ]  On the other hand, MIT's extensive collaboration with the federal government on research projects has led to several MIT leaders serving as  presidential scientific advisers  since 1940. [ d ]  MIT established a Washington Office in 1991 to continue effective  lobbying  for research funding and national  science policy . [ 250 ] [ 251 ] The  US Justice Department  began an investigation in 1989, and in 1991 filed an  antitrust suit  against MIT, the eight  Ivy League  colleges, and eleven other institutions for allegedly engaging in  price-fixing  during their annual \"Overlap Meetings\", which were held to prevent bidding wars over promising prospective students from consuming funds for need-based scholarships. [ 252 ] [ 253 ]  While the Ivy League institutions  settled , [ 254 ]  MIT contested the charges, arguing that the practice was not anti-competitive because it ensured the availability of aid for the greatest number of students. [ 255 ] [ 256 ]  MIT ultimately prevailed when the Justice Department dropped the case in 1994. [ 257 ] [ 258 ] MIT's proximity [ e ]  to  Harvard University  (\"the other school up the  river \") has led to a substantial number of research collaborations such as the  Harvard-MIT Division of Health Sciences and Technology  and the  Broad Institute . [ 259 ]  In addition, students at the two schools can  cross-register  for credits toward their own school's degrees without any additional fees. [ 259 ]  A cross-registration program between MIT and  Wellesley College  has also existed since 1969, and in 2002 the  Cambridge–MIT Institute  launched an undergraduate exchange program between MIT and the  University of Cambridge . [ 259 ]  MIT also has a long-term partnership with  Imperial College London , for both student exchanges and research collaboration. [ 260 ] [ 261 ]  More modest cross-registration programs have been established with  Boston University ,  Brandeis University ,  Tufts University ,  Massachusetts College of Art , and the  School of the Museum of Fine Arts, Boston . [ 259 ] MIT maintains substantial research and faculty ties with independent research organizations in the Boston area, such as the  Charles Stark Draper Laboratory , the  Whitehead Institute for Biomedical Research , and the  Woods Hole Oceanographic Institution . [ 215 ]  Ongoing international research and educational collaborations include the  Amsterdam Institute for Advanced Metropolitan Solutions  (AMS Institute), [ 262 ]  Singapore-MIT Alliance, MIT- Politecnico di Milano , [ 259 ] [ 263 ]  MIT- Zaragoza  International Logistics Program, and projects in other countries through the MIT International Science and Technology Initiatives (MISTI) program. [ 259 ] [ 264 ] The mass-market magazine  Technology Review  is published by MIT through a subsidiary company, as is a special edition that also serves as an  alumni magazine . [ 265 ] [ 266 ]  The  MIT Press  is a major  university press , publishing over 200 books and 30 journals annually, emphasizing science and technology as well as arts, architecture, new media, current events, and social issues. [ 267 ] MIT Microphotonics Center and  PhotonDelta  founded the global roadmap for integrated photonics: Integrated Photonics Systems Roadmap – International (IPSR-I). The first edition has been published in 2020. The roadmap is an amalgamation of two previously independent roadmaps: the IPSR roadmap of MIT Microphotonics Center and AIM Photonics in the United States, and the WTMF (World Technology Mapping Forum) of PhotonDelta in Europe. [ 268 ]  In 2022, Open Philanthropy donated $13,277,348 to MIT to study potential risks from AI. [ 269 ] The MIT library system consists of five subject libraries: Barker (Engineering), Dewey (Economics), Hayden (Humanities and Science), Lewis (Music), and Rotch (Arts and Architecture). There are also various specialized libraries and archives. The libraries contain more than 2.9 million printed volumes, 2.4 million microforms, 49,000 print or electronic journal subscriptions, and 670 reference databases. The past decade has seen a trend of increased focus on digital over print resources in the libraries. [ 270 ]  Notable collections include the Lewis Music Library with an emphasis on 20th and 21st-century music and electronic music, [ 271 ]  the  List Visual Arts Center 's rotating exhibitions of contemporary art, [ 272 ]  and the Compton Gallery's cross-disciplinary exhibitions. [ 273 ]  MIT allocates a percentage of the budget for all new construction and renovation to commission and support its extensive public art and outdoor sculpture collection. [ 274 ] [ 275 ] The  MIT Museum  was founded in 1971 and collects, preserves, and exhibits artifacts significant to the culture and  history of MIT . The museum now engages in significant educational outreach programs for the general public, including the annual  Cambridge Science Festival , the first celebration of this kind in the United States. Since 2005, its official mission has been, \"to engage the wider community with MIT's science, technology and other areas of scholarship in ways that will best serve the nation and the world in the 21st century\". [ 276 ] MIT was elected to the  Association of American Universities  in 1934 and is  classified  among \"R1: Doctoral Universities – Very high research activity\"; [ 52 ] [ 178 ]  research expenditures totaled $952 million in 2017. [ 277 ]  The federal government was the largest source of sponsored research, with the  Department of Health and Human Services  granting $255.9 million,  Department of Defense  $97.5 million,  Department of Energy  $65.8 million,  National Science Foundation  $61.4 million, and  NASA  $27.4 million. [ 278 ]  MIT employs approximately 1300 researchers in addition to faculty. [ 279 ]  In 2011, MIT faculty and researchers disclosed 632 inventions, were issued 153 patents, earned $85.4 million in cash income, and received $69.6 million in royalties. [ 280 ]  Through programs like the Deshpande Center, MIT faculty leverage their research and discoveries into multi-million-dollar commercial ventures. [ 281 ] In electronics,  magnetic-core memory ,  radar ,  single-electron transistors , and  inertial guidance  controls were invented or substantially developed by MIT researchers. [ 282 ] [ 283 ]   Harold Eugene Edgerton  was a pioneer in  high-speed photography  and  sonar . [ 284 ] [ 285 ]   Claude E. Shannon  developed much of modern  information theory  and discovered the application of  Boolean logic  to  digital circuit  design theory. [ 286 ]  In the domain of computer science, MIT faculty and researchers made fundamental contributions to  cybernetics ,  artificial intelligence ,  computer languages ,  machine learning ,  robotics , and  cryptography . [ 283 ] [ 287 ]  At least nine  Turing Award  laureates and seven recipients of the  Draper Prize  in engineering have been or are currently associated with MIT. [ 288 ] [ 289 ] Current and previous physics faculty have won eight  Nobel Prizes , [ 290 ]  four  ICTP Dirac Medals , [ 291 ]  and three  Wolf Prizes  predominantly for their contributions to subatomic and  quantum  theory. [ 292 ]  Members of the chemistry department have been awarded three  Nobel Prizes  and one Wolf Prize for the discovery of novel syntheses and methods. [ 290 ]  MIT biologists have been awarded six  Nobel Prizes  for their contributions to genetics, immunology, oncology, and molecular biology. [ 290 ]  Professor  Eric Lander  was one of the principal leaders of the  Human Genome Project . [ 293 ] [ 294 ]   Positronium  atoms, [ 295 ]  synthetic  penicillin , [ 296 ]   synthetic self-replicating molecules , [ 297 ]  and the genetic bases for  Amyotrophic lateral sclerosis  (also known as ALS or Lou Gehrig's disease) and  Huntington's disease  were first discovered at MIT. [ 298 ]   Jerome Lettvin  transformed the study of cognitive science with his paper \"What the frog's eye tells the frog's brain\". [ 299 ]  Researchers developed a system to convert MRI scans into 3D printed physical models. [ 300 ] In the domain of humanities, arts, and social sciences, as of October 2019 MIT economists have been awarded seven  Nobel Prizes  and nine  John Bates Clark Medals . [ 290 ] [ 301 ]  Linguists  Noam Chomsky  and  Morris Halle  authored seminal texts on  generative grammar  and  phonology . [ 302 ] [ 303 ]  The  MIT Media Lab , founded in 1985 within the  School of Architecture and Planning  and known for its unconventional research, [ 304 ] [ 305 ]  has been home to influential researchers such as  constructivist  educator and  Logo  creator  Seymour Papert . [ 306 ] Spanning many of the above fields,  MacArthur Fellowships  (the so-called \"Genius Grants\") have been awarded to 50 people associated with MIT. [ 307 ]  Five  Pulitzer Prize –winning writers currently work at or have retired from MIT. [ 308 ]  Four current or former faculty are members of the  American Academy of Arts and Letters . [ 309 ] Allegations of  research misconduct  or improprieties have received substantial press coverage. Professor  David Baltimore , a  Nobel Laureate , became embroiled in a misconduct investigation starting in 1986 that led to Congressional hearings in 1991. [ 310 ] [ 311 ]  Professor  Ted Postol  has accused the MIT administration since 2000 of attempting to  whitewash  potential research misconduct at the Lincoln Lab facility involving a  ballistic missile defense  test, though a final investigation into the matter has not been completed. [ 312 ] [ 313 ]  Associate Professor  Luk Van Parijs  was dismissed in 2005 following allegations of scientific misconduct and found guilty of the same by the  United States Office of Research Integrity  in 2009. [ 314 ] [ 315 ] In 2019,  Clarivate Analytics  named 54 members of MIT's faculty to its list of \"Highly Cited Researchers\". That number places MIT eighth among the world's universities. [ 316 ] MIT alumni and faculty have founded numerous companies, some of which are shown below: [ 347 ] [ 348 ] The faculty and student body place a high value on  meritocracy  and on technical proficiency. [ 350 ] [ 351 ]  MIT has never awarded an  honorary degree , [ 352 ]  nor does it award  athletic scholarships , [ 353 ]   ad eundem   degrees , [ citation needed ]  or  Latin honors [ 354 ]  upon graduation. However, MIT has twice awarded honorary professorships: to  Winston Churchill  in 1949 and  Salman Rushdie  in 1993. [ 355 ] Many  upperclass  students and alumni wear a large, heavy, distinctive  class ring  known as the \" Brass Rat \". [ 356 ] [ 357 ]  Originally created in 1929, the ring's official name is the \"Standard Technology Ring\". [ 358 ]  The undergraduate ring design (a separate graduate student version exists as well) varies slightly from year to year to reflect the unique character of the MIT experience for that class, but always features a three-piece design, with the MIT seal and the class year each appearing on a separate face, flanking a large rectangular bezel bearing an image of a  beaver . [ 356 ]  The  initialism   IHTFP , representing the informal school motto \"I Hate This Fucking Place\" and jocularly  euphemized  as \"I Have Truly Found Paradise\", \"Institute Has The Finest Professors\", \"Institute of Hacks, TomFoolery and Pranks\", \"It's Hard to Fondle Penguins\", and other variations, has occasionally been featured on the ring given its historical prominence in student culture. [ 359 ] MIT also shares a well-known  rivalry  with the  California Institute of Technology  (Caltech), stemming from both institutions' reputations as two of the highest ranked and most highly recognized science and engineering schools in the world. [ 360 ]  The rivalry is an unusual college rivalry given its focus on academics and pranks instead of sports, and due to the geographic distance between the two (their campuses are separated by about 2580 miles and are on  opposite   coasts  of the United States). In 2005, Caltech students pranked MIT's Campus Preview Weekend by distributing t-shirts that read \"MIT\" on the front, and \"...because not everyone can go to Caltech\" on the back. [ 361 ] [ 362 ] [ 363 ]  Additionally, the word Massachusetts in the \"Massachusetts Institute of Technology\" engraving on the exterior of the Lobby 7 dome was covered with a banner so that it read \"That Other Institute of Technology\". In 2006, MIT retaliated by posing as contractors and stealing the 1.7-ton, 130-year-old  Fleming cannon , a Caltech landmark. The cannon was relocated to Cambridge, where it was displayed in front of the  Green Building  during the 2006 Campus Preview Weekend. [ 364 ] [ 365 ]  In September 2010, MIT students unsuccessfully tried to place a life-sized model of the  TARDIS  time machine from the  Doctor Who  (1963–present) television series on top of Baxter Hall at Caltech. A few months later, Caltech students collaborated to help MIT students place the TARDIS on top of their originally planned destination. [ 366 ]  The rivalry has continued, most recently in 2014, when a group of Caltech students gave out mugs sporting the MIT logo on the front and the words \"The Institute of Technology\" on the back. When heated, the mugs turned orange and read, \"Caltech, The Hotter Institute of Technology\". [ 367 ] MIT has over 500 recognized student activity groups, [ 368 ]  including a  campus radio station ,  The Tech  student newspaper, an annual  entrepreneurship competition , a  crime club , and weekly screenings of popular films by the  Lecture Series Committee . Less traditional activities include the \"world's largest open-shelf  collection of science fiction \" in English, a  model railroad club , and a vibrant  folk dance  scene. Students, faculty, and staff are involved in over 50 educational outreach and public service programs through the  MIT Museum , Edgerton Center, and MIT Public Service Center. [ 369 ] Fraternities and sororities provide a base of activities in addition to housing. Approximately 1,000 undergrads, 48% of men and 30% of women, participate in one of several dozen Greek Life men's, women's and co-ed chapters on the campus. [ 370 ] The  Independent Activities Period  is a four-week-long \"term\" offering hundreds of optional classes, lectures, demonstrations, and other activities throughout the month of January between the Fall and Spring semesters. Some of the most popular recurring IAP activities are Autonomous Robot Design (course 6.270), Robocraft Programming (6.370), and MasLab  competitions , [ 371 ]  the annual  \"mystery hunt\" , [ 372 ]  and  Charm School . [ 373 ] [ 374 ]  More than 250 students pursue  externships  annually at companies in the US and abroad. [ 375 ] [ 376 ] Many MIT students also engage in \"hacking\", which encompasses both the  physical exploration of areas  that are generally off-limits (such as rooftops and steam tunnels), as well as  elaborate practical jokes . [ 377 ] [ 378 ]  Examples of high-profile hacks have included the  abduction of Caltech's cannon , [ 379 ]  reconstructing a  Wright Flyer  atop the Great Dome, [ 380 ]  and adorning the  John Harvard  statue with the  Master Chief's Mjölnir Helmet . [ 381 ] MIT sponsors 31 varsity sports and has one of the three broadest NCAA Division III athletic programs. [ 382 ] [ 383 ]  MIT participates in the  NCAA's   Division III , the  New England Women's and Men's Athletic Conference , the  New England Football Conference ,  NCAA's  Division I  Patriot League  for women's crew, and the  Collegiate Water Polo Association (CWPA)  for Men's Water Polo. Men's crew competes outside the NCAA in the  Eastern Association of Rowing Colleges (EARC) . The intercollegiate sports teams, called the MIT Engineers won 22 Team National Championships, 42 Individual National Championships. MIT is the all-time Division III leader in producing  Academic All-Americas  (302) and rank second across all NCAA Divisions only behind the University of Nebraska. [ 384 ]  MIT Athletes won 13  Elite 90  awards and ranks first among NCAA Division III programs, and third among all divisions. [ 385 ]  In April 2009, budget cuts led to MIT eliminating eight of its 41 sports, including the mixed men's and women's teams in alpine skiing and pistol; separate teams for men and women in ice hockey and gymnastics; and men's programs in golf and wrestling. [ 386 ] [ 387 ] MIT enrolled 4,602 undergraduates and 6,972 graduate students in 2018–2019. [ 389 ]  Undergraduate and graduate students came from all 50 US states as well as from 115 foreign countries. [ 390 ] MIT received 33,240 applications for admission to the undergraduate Class of 2025: it admitted 1,365 (4.1 percent). [ 391 ]  In 2019, 29,114 applications were received for graduate and advanced degree programs across all departments; 3,670 were admitted (12.6 percent) and 2,312 enrolled (63 percent). [ 392 ]  In August 2024, after the  U.S. Supreme Court  overruled race-based  affirmative action  in  Students for Fair Admissions v. Harvard  (2023), the university reported that for the class of 2028, Black and Latino student enrollment decreased from previous averages to 5 and 11 percent, respectively, while  Asian American  enrollment increased to 47 percent. [ 393 ] [ 394 ] Undergraduate tuition and fees for 2019–2020 was $53,790 for nine months. 59% of students were awarded a need-based MIT scholarship. Graduate tuition and fees for 2019–2020 was also $53,790 for nine months, and summer tuition was $17,800. Financial support for graduate students are provided in large part by individual departments. They include fellowships, traineeships, teaching and research assistantships, and loans. [ 395 ]  The annual increase in expenses had led to a student tradition (dating back to the 1960s) of tongue-in-cheek \"tuition riots\". [ 396 ] MIT has been nominally  co-educational  since admitting  Ellen Swallow Richards  in 1870. Richards also became the first female member of MIT's faculty, specializing in  sanitary chemistry . [ 397 ] [ 398 ]  Female students remained a small minority prior to the completion of the first wing of a women's dormitory,  McCormick Hall , in 1963. [ 399 ] [ 400 ] [ 401 ]  Between 1993 and 2009 the proportion of women rose from 34 percent to 45 percent of undergraduates and from 20 percent to 31 percent of graduate students. [ 184 ] [ 402 ]  As of 2009 [update] , women outnumbered men in Biology, Brain & Cognitive Sciences, Architecture, Urban Planning, and Biological Engineering. [ 184 ] [ 403 ] As of 2021 [update] , MIT had 1,069  faculty  members. [ 4 ]  Faculty are responsible for lecturing classes, for advising both graduate and undergraduate students, and for sitting on academic committees, as well as for conducting original research. Between 1964 and 2009 a total of seventeen faculty and staff members affiliated with MIT won  Nobel Prizes  (thirteen of them in the latter 25 years). [ 404 ]  As of October 2020, 37 MIT faculty members, past or present, have won Nobel Prizes, the majority in  Economics  or  Physics . [ 405 ] As of October 2013 [update] , current faculty and teaching staff included 67  Guggenheim Fellows , 6  Fulbright Scholars , and 22  MacArthur Fellows . [ 4 ]  Faculty members who have made extraordinary contributions to their research field as well as the MIT community are granted appointments as  Institute Professors  for the remainder of their tenures.  Susan Hockfield , a molecular  neurobiologist , served as MIT's president from 2004 to 2012. She was the first woman to hold the post. [ 406 ] MIT faculty members have often been recruited to lead other colleges and universities. Founding faculty-member  Charles W. Eliot  became president of Harvard University in 1869, a post he would hold for 40 years, during which he wielded considerable influence both on American higher education and on secondary education. MIT alumnus and faculty member  George Ellery Hale  played a central role in the development of the  California Institute of Technology  (Caltech), and other faculty members have been key founders of  Franklin W. Olin College of Engineering  in nearby  Needham, Massachusetts . As of 2014 [update]  former provost  Robert A. Brown  served as president of  Boston University ; former provost  Mark Wrighton  is chancellor of  Washington University in St. Louis ; former associate provost  Alice Gast  is president of  Lehigh University ; and former professor  Suh Nam-pyo  is president of  KAIST . Former dean of the School of Science  Robert J. Birgeneau  was the chancellor of the  University of California, Berkeley  (2004–2013); former professor  John Maeda  was president of  Rhode Island School of Design  (RISD, 2008–2013); former professor  David Baltimore  was president of  Caltech  (1997–2006); and MIT alumnus and former assistant professor  Hans Mark  served as chancellor of the  University of Texas  system (1984–1992). In addition, faculty members have been recruited to lead governmental agencies; for example, former professor  Marcia McNutt  is president of the  National Academy of Sciences , [ 407 ]  urban studies professor  Xavier de Souza Briggs  served as the associate director of the  White House Office of Management and Budget , [ 408 ]  and biology professor  Eric Lander  was a co-chair of the  President's Council of Advisors on Science and Technology . [ 409 ]  In 2013, faculty member  Ernest Moniz  was nominated by President Obama and later confirmed as  United States Secretary of Energy . [ 410 ] [ 411 ]  Former professor Hans Mark served as Secretary of the Air Force from 1979 to 1981. Alumna and Institute Professor Sheila Widnall served as Secretary of the Air Force between 1993 and 1997, making her the first female Secretary of the Air Force and first woman to lead an entire branch of the US military in the Department of Defense. A 1999 report, met by promises of change by President Charles Vest, found that senior female faculty in the School of Science were often marginalized, and in return for equal professional accomplishments received reduced \"salary, space, awards, resources, and response to outside offers\". [ 412 ] As of 2017 [update] , MIT was the second-largest employer in the city of Cambridge. [ 136 ]  Based on feedback from employees, MIT was ranked No. 7 as a place to work, among US colleges and universities as of March 2013 [update] . [ 413 ]  Surveys cited a \"smart\", \"creative\", \"friendly\" environment, noting that the  work-life balance  tilts towards a \"strong work ethic\" but complaining about \"low pay\" compared to an industry position. [ 414 ] Many of MIT's over 120,000 alumni have achieved considerable success in scientific research, public service, education, and  business . As of October 2020 [update] , 41 MIT alumni have won Nobel Prizes, 48 have been selected as  Rhodes Scholars , [ 415 ]  61 have been selected as  Marshall Scholars , [ 416 ]  and 3 have been selected as  Mitchell Scholars . [ 417 ] Alumni in United States politics and public service include former  Chairman of the Federal Reserve   Ben Bernanke , former  MA-1  Representative  John Olver , former  CA-13  Representative  Pete Stark ,  KY-4  Representative  Thomas Massie , California Senator  Alex Padilla , former  National Economic Council  chairman  Lawrence H. Summers , [ 418 ]  and former  Council of Economic Advisers  chairman  Christina Romer . MIT alumni in international politics include  Foreign Affairs Minister of Iran   Ali Akbar Salehi ,  Education Minister of Nepal Sumana Shrestha ,  President of Colombia   Virgilio Barco Vargas , former  President of the European Central Bank   Mario Draghi , former Governor of the Reserve Bank of India  Raghuram Rajan , former  British Foreign Minister   David Miliband , former  Greek Prime Minister   Lucas Papademos , former  UN Secretary General   Kofi Annan , former  Iraqi Deputy Prime Minister   Ahmed Chalabi , former Minister of Education and Culture of The Republic of Indonesia  Yahya Muhaimin , former Jordanian Minister of Education, Higher Education and Scientific Research and former Jordanian Minister of Energy and Mineral Resources  Khaled Toukan . Alumni in sports have included Olympic fencing champion  Johan Harmenberg . MIT alumni founded or co-founded many notable companies, such as  Intel ,  McDonnell   Douglas ,  Texas Instruments ,  3Com ,  Qualcomm ,  Bose ,  Raytheon ,  Apotex ,  Koch Industries ,  Rockwell International ,  Genentech ,  Dropbox , and  Campbell Soup . According to the British newspaper  The Guardian , \"a survey of living MIT alumni found that they have formed 25,800 companies, employing more than three million people including about a quarter of the workforce of Silicon Valley. Those firms collectively generate global revenues of about $1.9 trillion (£1.2 trillion) a year\". If the companies founded by MIT alumni were a country, they would have the 11th-highest  GDP  of any country in the world. [ 419 ] [ 420 ] [ 421 ] MIT alumni have led prominent institutions of higher education, including the  University of California  system,  Harvard University , the  New York Institute of Technology ,  Johns Hopkins University ,  Carnegie Mellon University ,  Tufts University ,  Rochester Institute of Technology ,  Rhode Island School of Design (RISD) ,  UC Berkeley College of Environmental Design , the  New Jersey Institute of Technology ,  Northeastern University ,  Tel Aviv University ,  Lahore University of Management Sciences ,  Rensselaer Polytechnic Institute ,  Tecnológico de Monterrey ,  Purdue University ,  Virginia Polytechnic Institute ,  Korea Advanced Institute of Science and Technology , and  Quaid-e-Azam University .  Berklee College of Music , the largest independent college of contemporary music in the world, was founded and led by MIT alumnus  Lawrence Berk  for more than three decades. More than one third of the  United States' crewed spaceflights  have included  MIT-educated astronauts , a contribution exceeding that of any university excluding the  United States service academies . [ 422 ]  Of the  12 people who have set foot on the Moon  as of 2019 [update] , four graduated from MIT (among them  Apollo 11   Lunar Module  Pilot  Buzz Aldrin ). Alumnus and former faculty member  Qian Xuesen  led the  Chinese nuclear-weapons program  and became instrumental in Chinese rocket-program. [ 423 ] MIT alumni played a significant role in the creation of the  Atomic Energy Commission  and  Department of Energy .  Carroll Wilson  (a student and professor at MIT) served as the first General Manager of the Atomic Energy Commission.   John Deutch  served as Under Secretary of Energy for  President Carter ;  William F. Martin  served as Deputy Secretary of Energy for  Ronald Reagan  and  Ernest Moniz  served as Secretary of Energy for  President Obama . Indeed, modern post World War II history has been influenced by MIT and its alumni in the fields of nuclear energy and high energy physics. Noted alumni in non-scientific fields include author  Hugh Lofting , [ 424 ]  sculptor  Daniel Chester French , guitarist  Tom Scholz  of the band  Boston , the British  BBC  and  ITN  correspondent and political advisor  David Walter ,  The New York Times  columnist and Nobel Prize-winning economist  Paul Krugman ,  The Bell Curve  author  Charles Murray ,  United States Supreme Court building  architect  Cass Gilbert , [ 425 ] \n Pritzker Prize -winning architects  I.M. Pei  and  Gordon Bunshaft ."
  },
  {
    "id": 128,
    "title": "Case Western Reserve University",
    "content": "Case Western Reserve University  ( CWRU ) is a  private   research university  in  Cleveland, Ohio , United States. It was established in 1967 by a merger between Western Reserve University [ a ]  and the Case Institute of Technology. [ b ] Case Western Reserve University comprises eight schools that offer more than 100 undergraduate programs and about 160 graduate and professional options across fields in STEM, medicine, arts, and the humanities. [ 9 ]  In 2024, the university enrolled 12,475 students (6,528 undergraduate plus 5,947 graduate and professional) from all 50 states and 106 countries and employed more than 1,182 full-time faculty members. The university's athletic teams, Case Western Reserve Spartans, play in  NCAA Division III  as a founding member of the  University Athletic Association . Case Western Reserve University is a member of the  Association of American Universities  and is  classified  among \"R1: Doctoral Universities – Very high research activity\". [ 10 ]  According to the  National Science Foundation , in 2019 the university had  research and development  (R&D) expenditures of $439 million, ranking it 20th among private institutions and 58th in the nation. [ 11 ] Case alumni, scientists, and scholars have played significant roles in many scientific breakthroughs and discoveries including the discovery of  gravitational waves ; the  invention of the MRI ; the  first external defibrillator ;  isolation of the poliovirus ; the collaboration between the Case Institute of Technology and the Harshaw Chemical Company [ 12 ] [ 13 ] [ 14 ]  which enriched a majority of the uranium used in the  Manhattan Project ; the  Michelson-Morley experiment , which disproved the existence of \"luminiferous aether\" and confirmed that light did not need a medium of travel, was conducted in the basement of a Western Reserve University dormitory in 1887, and  Albert A. Michelson  became the first American to win the Nobel Prize in science. As of April 2024, 2  Surgeons General of the United States , 1  Justice of the United States Supreme Court , 18  heads of state , 5  Olympic medallists , 1  NASA astronaut , 3 billionaires, 69 appointees to the  National Academies , and 17  Nobel laureates  are numbered among Case Western Reserve University faculty or alumni, or one of its predecessors prior to federation. [ 15 ] [ 16 ] [ 17 ] Western Reserve College, the college of the  Connecticut Western Reserve , was founded in 1826 in  Hudson, Ohio , as the  Western Reserve College and Preparatory School . Western Reserve College, or \"Reserve\" as it was popularly called, was the first college in northern Ohio. [ 18 ]  The school was called \"Yale of the West\"; its campus, now that of the  Western Reserve Academy , imitated that of Yale. It had the same motto, \"Lux et Veritas\" (Light and Truth), the same entrance standards, and nearly the same curriculum. It was different from Yale in that it was a  manual labor college , in which students were required to perform manual labor, seen as psychologically beneficial. [ 19 ] Western Reserve College's founders sought to instill in students an \"evangelical ethos\" and train ministers for Ohio, where there was an acute shortage of them. The college was located in Hudson because the town made the largest financial offer to help in its construction. [ 20 ] : 422   That town, about 30 miles southeast of Cleveland, had been an antislavery center from the beginning: its founder,  David Hudson , was against slavery, and founding trustee  Owen Brown  was a noted  abolitionist  who secured the location for the college. The abolitionist  John Brown , who would lead the  1859 raid on Harpers Ferry , grew up in Hudson and was the son of co-founder Owen Brown. Hudson was a major stop on the  Underground Railroad . Along with  Presbyterian  influences of its founding, the school's origins were strongly though briefly associated with the pre- Civil War   abolitionist  movement; [ 21 ]  the abolition of slavery was the dominant topic on campus in 1831. The trustees were unhappy with the situation. The college's chaplain and Bible professor,  Beriah Green , gave four sermons on the topic [ 22 ]  and then resigned, expecting that he would be fired. President  Charles Backus Storrs  took a leave of absence for health, and soon died. One of the two remaining professors,  Elizur Wright , soon left to head the  American Anti-Slavery Society . [ 23 ] Western Reserve was the first college west of the  Appalachian Mountains  to enroll (1832) and graduate (1836) an  African-American  student,  John Sykes Fayette . [ 24 ]   Frederick Douglass  gave the  commencement speech  in 1854. [ 25 ] In 1838, the  Loomis Observatory  was built by astronomer  Elias Loomis , and today remains the second oldest  observatory  in the United States, and the oldest still in its original location. [ 26 ] In 1852, the  Medical School  became the second  medical?  school in the United States to graduate a woman,  Nancy Talbot Clark . Five more women graduated over the next four years, including  Emily Blackwell  and  Marie Zakrzewska , giving Western Reserve the distinction of graduating six of the first eight female physicians in the United States. [ 27 ] By 1875, Cleveland had emerged as the dominant population and business center of the region, and the city wanted a prominent higher education institution. In 1882, with funding from  Amasa Stone , Western Reserve College moved to Cleveland and changed its name to Adelbert College of Western Reserve University. Adelbert was the name of Stone's son. [ 28 ] In 1877,  Leonard Case Jr.  began laying the groundwork for the Case School of Applied Science by secretly donating valuable pieces of Cleveland real estate to a trust. He asked his confidential advisor, Henry Gilbert Abbey, to administer the trust and to keep it secret until after his death in 1880. On March 29, 1880, articles of incorporation were filed for the founding of the  Case School of Applied Science . Classes began on September 15, 1881. [ 29 ]  The school received its charter by the state of  Ohio  in 1882. For the first four years of the school's existence, it was located in the Case family's home on Rockwell Street in  downtown Cleveland . Classes were held in the family house, while the chemistry and physics laboratories were on the second floor of the barn.  Amasa Stone 's gift to relocate Western Reserve College to Cleveland also included a provision for the purchase of land in the  University Circle  area, adjacent to Western Reserve University, for the Case School of Applied Science. The school relocated to  University Circle  in 1885. In 1921  Albert Einstein  came to the Case campus during his first visit to the United States, out of respect for the  physics  work performed there. Besides noting the research done in the  Michelson–Morley experiment , Einstein also met with physics professor  Dayton Miller  to discuss his own research. [ 30 ] During  World War II , Case School of Applied Science was one of 131 colleges and universities nationally that took part in the  V-12 Navy College Training Program  which offered students a path to a Navy commission. [ 31 ] Over time, the Case School of Applied Science expanded to encompass broader subjects, adopting the name Case Institute of Technology in 1947 to reflect the institution's growth. [ 28 ] Led by polymer expert  Eric Baer  in 1963, the nation's first stand-alone Polymer Science and Engineering program was founded, to eventually become the Department of Macromolecular Science and Engineering. [ 32 ] Although the trustees of Case Institute of Technology and Western Reserve University did not formally federate their institutions until 1967, the institutions already shared buildings and staff when necessary and worked together often. One such example was seen in 1887, when Case physicist  Albert Michelson  and Reserve chemist  Edward Morley  collaborated on the famous  Michelson–Morley experiment . There had been some discussion of a merger of the two institutions as early as 1890, but those talks dissolved quickly. In the 1920s, the Survey Commission on Higher Education in Cleveland took a strong stand in favor of federation and the community was behind the idea as well, but in the end all that came of the study was a decision by the two institutions to cooperate in founding Cleveland College, a special unit for part-time and adult students in  downtown Cleveland . By the 1960s, Reserve President  John Schoff Millis  and Case President  T. Keith Glennan  shared the idea that federation would create a complete university, one better able to attain national distinction. Financed by the  Carnegie Corporation ,  Cleveland Foundation ,  Greater Cleveland Associated Foundation , and several local donors, a study commission of national leaders in higher education and public policy was charged with exploring the idea of federation. The Heald Commission, so known for its chair, former  Ford Foundation  President  Henry T. Heald , predicted in its final report that a federation could create one of the largest private universities in the nation. In 1967, Case Institute of Technology, a school with its emphasis on engineering and science, and Western Reserve University, a school with professional programs and liberal arts, came together to form Case Western Reserve University. [ 33 ] In 1968, the  Department of Biomedical Engineering  launched as a newly unified collaboration between the  School of Engineering  and  School of Medicine  as the first in the nation and as one of the first  Biomedical Engineering  programs in the world. [ 34 ]  The following year in 1969, the first Biomedical Engineering MD/PhD program in the world began at Case Western Reserve. [ 35 ] The first  computer engineering  degree program in the United States was established in 1971 at Case Western Reserve. In 2003, the university unveiled a new logo and branding campaign that emphasized the \"Case\" portion of its name. In 2006, interim  university president  Gregory Eastwood convened a task group to study reactions to the campaign. The panel's report indicated that it had gone so poorly that, \"There appear to be serious concerns now about the university's ability to recruit and maintain high-quality faculty, fund-raising and leadership.\" Also, the logo was derided among the university's community and alumni and throughout northeastern Ohio; critics said it looked like \"...a fat man with a surfboard.\" [ 36 ] On May 9, 2003, the  2003 Case Western Reserve University shooting  occurred when Biswanath Halder entered the Peter B. Lewis Building of the  Weatherhead School of Management  where he then killed graduate student Norman Wallace and wounded two professors. Halder took people in the building hostage, and they ran and barricaded themselves and hid during the seven hours that the gunman roamed the building, shooting indiscriminately. He was finally apprehended by a SWAT team. Halder was convicted on multiple felony counts and sentenced to life in prison; he lost a 2008 appeal. [ citation needed ] In 2007, the university's board of trustees approved a shift back to giving equal weight to \"Case\" and \"Western Reserve\". A new logo was chosen and implementation began July 1. [ 37 ]  In an open letter to the university community, interim president Eastwood admitted that \"the university had misplaced its own history and traditions.\" [ 38 ]  The \"Forward Thinking\" campaign was launched in 2011 by President  Barbara Snyder  and raised $1 billion in 30 months. The board of trustees unanimously agreed to expand the campaign to $1.5 billion, which reached its mark in 2017. [ 39 ]  The campaign ultimately raised $1.82 billion. [ 40 ] A  2020 United States presidential debate , the first of two, was held at the Samson Pavilion of the  Health Education Campus  (HEC), shared by the  Cleveland Clinic . [ 41 ] In February 2020, president Barbara Snyder was appointed the president of  Association of American Universities  (AAU). Later that year, former  Tulane University  president  Scott Cowen  was appointed interim president. On October 29, 2020,  Eric W. Kaler , former  University of Minnesota  president, was appointed as the new Case Western Reserve University president, effective July 1, 2021. [ 42 ] Case Western Reserve University's main campus is approximately 5 miles (8 km) east of  Downtown Cleveland  in the neighborhood known as  University Circle , an area containing many educational, medical, and cultural institutions. [ 43 ]  Case Western Reserve has a number of programs taught in conjunction with other University Circle institutions, including  University Hospitals , the  Cleveland Clinic , the Louis Stokes Cleveland Department of Veteran's Affairs Medical Center,  Cleveland Institute of Music , the  Cleveland Hearing & Speech Center , the  Cleveland Museum of Art , the  Cleveland Institute of Art , the  Cleveland Museum of Natural History .  Severance Hall , home of the  Cleveland Orchestra , is on the Case Western Reserve campus. The Case Quadrangle, known also to students as the Engineering Quad, contains most engineering and science buildings, notably the  John D. Rockefeller  Physics Building. [ 44 ]  The Case Quad also houses administration buildings, including  Adelbert Hall . The  Michelson–Morley experiment  occurred here, commemorated by a marker and the  Michelson-Morley Memorial Fountain . The southernmost edge consists of athletic areas— Adelbert Gymnasium , Van Horn Field and the Veale Convocation, Recreation and Athletic Center (commonly referred to as the Veale Center). The Veale Center houses the  Horsburgh Gymnasium  and the Veale Natatorium. The Flora Stone Mather Quadrangle is located north of Euclid Avenue between East Blvd., East 115th Street, and Juniper Road. The  Flora Stone Mather College Historic District  is more strictly defined by the area between East Blvd, Bellflower Road, and Ford Road north of Euclid Avenue. Named for the philanthropist wife of prominent industrialist  Samuel Mather  and sister-in-law of the famous statesman  John Hay , the Mather Quad is home to  Weatherhead School of Management ,  School of Law ,  Mandel School of Applied Social Sciences , and many departments of the  College of Arts and Sciences . The Kelvin Smith Library, Thwing Center, and Tinkham Veale Student Center (known also as \"The Tink\") sit on the western edge of the Mather Quad. On and near campus, CircleLink is a free public shuttle service in University Circle and Little Italy. Colloquially, the shuttle buses are known as  Greenies . [ 45 ]  To supplement evening and nighttime hours, the Safe Ride Program provides personal pickup of students and staff upon request. [ 46 ] For city  public transit , rail and bus access are managed by the  Greater Cleveland Regional Transit Authority  (RTA). Unlimited use RTA passes are provided to undergraduate and full-time graduate students. The two  Red Line rapid train  stations are  Little Italy–University Circle  and  Cedar–University . Notably, the Red Line connects campus to  Cleveland Hopkins Airport  and  Downtown Cleveland . The  bus rapid transit  (BRT)  HealthLine  runs down the center of campus along  Euclid Ave . Numerous RTA bus routes run through campus. [ 47 ] The university in its present form consists of eight schools that offer more than 100 undergraduate programs and about 160 graduate and professional options. [ 48 ] CWRU also supports over 100 interdisciplinary academic and research centers in various fields. [ 49 ] The undergraduate student body hails from all 50 states and over 90 countries. [ 51 ] The six most popular majors are  biomedical engineering ,  biology / biological sciences ,  nursing ,  mechanical engineering , and  psychology . Since 2016, the top fields for graduating CWRU undergraduate students have been engineering, nursing, research and science, accounting and financial services, and information technology. [ 52 ] In 2023, the university received 39,039 applications. It extended offers of admission to 11,193 applicants, or 28.7%. 73% of admitted students were from outside Ohio and 13% from outside the United States. 1,544 accepted students chose to enroll, a yield rate of 13.8%. [ 50 ] Of the 43% of incoming students in 2023 who submitted  SAT  scores, the total  interquartile range  was 1440–1530; of the 23% of incoming students in 2023 who submitted  ACT  scores, the interquartile range of composite scores was 32–35. Of all  matriculating  students, the average high school GPA was 3.8. 71% of admitted students graduated in the top 10% of their high school class. [ 50 ] In  U.S. News & World Report ' s 2025  rankings , Case Western Reserve was ranked as tied for 51st among national universities and 160th among global universities. [ 60 ] [ 61 ]  The 2020 edition of  The Wall Street Journal / Times Higher Education  (WSJ/THE)  rankings ranked Case Western Reserve as 52nd among US colleges and universities. [ 62 ] In 2018, Case Western Reserve was ranked 37th in the category American \"national universities\" and 146th in the category \"global universities\" by  U.S. News & World Report . In 2019  U.S. News  ranked it tied for 42nd and 152nd, respectively. Case Western Reserve was also ranked 32nd among U.S. universities—and 29th among private institutions—in the inaugural 2016 edition of  The Wall Street Journal/Times Higher Education (WSJ/THE)  rankings, but ranked tied for 39th among U.S. universities in 2019. [ 62 ] Case Western Reserve University's  biochemistry  program is jointly administered with the  CWRU School of Medicine , and was ranked 14th nationally in the latest rankings by  Blue Ridge Institute for Medical Research . [ 63 ] Case Western Reserve is noted (among other fields) for research in  electrochemistry  and  electrochemical engineering . The  Michelson–Morley interferometer experiment  was conducted in 1887 in the basement of a campus dormitory by  Albert A. Michelson  of Case School of Applied Science and  Edward W. Morley  of Western Reserve University. Michelson became the first American to win a  Nobel Prize  in science. [ 64 ] Also in 2018,  The Hollywood Reporter  ranked CWRU's Department of Theater Master of Fine Arts program with the  Cleveland Play House  as 18th in the English-speaking world. In 2019, this ranking improved to 12th. [ 65 ] In 2014,  Washington Monthly  ranked Case Western Reserve University as the 9th best National University, [ 66 ] [ 67 ]  but in the 2018 rankings, Case Western Reserve was ranked the 118th best National University. [ 68 ] In 2013,  Washington Monthly  ranked Case Western Reserve as the nation's 4th best National University for contributing to the public good. The publication's ranking was based upon a combination of factors including social mobility, research, and service. [ 69 ]  In 2009, the school had ranked 15th. [ 70 ]  Although  Washington Monthly  no longer ranks contributions to the public good as such, in its 2018 rankings of National Universities Case Western Reserve was ranked 180th in Social mobility and 118th in Service. [ 68 ] In 2013, Case Western Reserve was among the Top 25 LGBT-Friendly Colleges and Universities, according to Campus Pride. The recognition follows Case Western Reserve's first five-star ranking on the Campus Pride Index, a detailed survey of universities' policies, services and institutional support for LGBT individuals. [ 71 ] Case Western Reserve ranks 13th among private institutions (26th among all) in federal expenditures for science and engineering research and development, per the  National Science Foundation . [ 72 ] Case Western Reserve University is a member of the  Association of American Universities  and is  classified  among \"R1: Doctoral Universities – Very high research activity\". [ 10 ]  Following is a partial list of major contributions made by faculty, staff, and students at Case Western Reserve since 1887: [ citation needed ] Today, the university operates several facilities off campus for scientific research. One example of this is the  Warner and Swasey Observatory  at  Kitt Peak National Observatory  in  Arizona . CWRU has contributed to the electrochemical sciences since the 1930s beginning with Frank Hovorka's studies of  quinhydrone  (quinone) and other electrodes. Subsequently, Ernest Yeager carried out pioneering studies on ultrasound electrodeposition and  oxygen reduction reaction  (ORR), which is directly relevant for H2-O2 fuel cells and batteries that use air electrodes such as zinc-air, iron-air, etc. The Yeager Center for Electrochemical Sciences (YCES), formerly the Case Center for Electrochemical Sciences, has provided annual workshops on electrochemical measurements since the late 1970s. The leadership in the  Electrochemical Society  have frequently included CWRU professors, and the university is home to six Fellows of the Electrochemical Society. Some notable achievements involve the work on ultrasound electrochemistry, oxygen reduction fundamentals, boron-doped diamond electrodes, in-situ electrochemical spectroscopy, polybenzimidazole (PBI) membranes for high-temperature fuel cells (HT-PEM), methanol fuel cells, iron-based flow batteries, metal deposition studies, dendrite modeling and electrochemical sensors. Noted laboratories at Case include the Electrochemical Engineering and Energy Laboratory (EEEL), the Electrochemical Materials Fabrication Laboratory (EMFL), the Case Electrochemical Capacitor Fabrication Facility and the ENERGY LAB. Larry Sears and Sally Zlotnick Sears think[box] is a public-access design and innovation center at Case Western Reserve University that allows students and other users to access  prototyping  equipment and other invention resources. The makerspace is located in the Richey Mixon building, a seven-story, 50,000 sq. ft. facility behind the campus athletic center. Over $35 million has been invested in space including in large part from a funding of $10 million from alumni Larry Sears and his wife Sally Zlotnick Sears. [ 86 ] [ 87 ]  Larry Sears is an adjunct faculty member in the Department of Electrical Engineering and Computer Science at CWRU and the founder of Hexagram, Inc. (now ACLARA Wireless Technologies). [ 88 ] \nMany projects and  startup companies  have come out of the  makerspace . [ 89 ] The primary area for restaurants and shopping is the  Uptown  district along Euclid Ave adjacent to campus. Cleveland's  Little Italy  is within walking distance. A campus shuttle runs to  Coventry Village , a shopping district in neighboring  Cleveland Heights . Popular with students,  Downtown Cleveland ,  Ohio City ,  Legacy Village , and  Shaker Square  are all a short driving distance or accessible by  RTA . WRUW-FM  (91.1 FM) is the campus radio station of Case Western Reserve University. WRUW broadcasts at a power of 15,000 watts and covers most of Northeast Ohio. Case Western Reserve is also home to 19 performing ensembles, including  a cappella  groups such as  Dhamakapella , the Case Men's Glee Club, [ 91 ]  Case Women's Glee Club, [ 92 ]  Case in Point, and Solstice. Other ensembles include the Case/University Circle Symphony Orchestra, Camerata Chamber Orchestra, Case/CIM Baroque Orchestra, Concert Choir, Early Music Singers, Jazz Ensemble 1 and 2, Marching Spartans, Percussion Ensemble, Symphonic Winds, University Singers, Collegium Musicum, New Music Ensemble, Wind Ensemble, and Chamber Music. [ 93 ] Case Western Reserve's main music venue is the  Maltz Performing Arts Center . Case Western Reserve also has two main rehearsal spaces for performing arts music majors and school ensembles.  Haydn Hall  contains practice rooms with Steinway pianos, along with the department offices. Denison Hall serves as a rehearsal, practice, and teaching space for the music students and school ensembles, and is attached to Wade Commons. The  Cleveland Youth Wind Symphony  also rehearses in Denison Hall. Music majors may take lessons and courses at the  Cleveland Institute of Music . For performances, all students, ensembles, and a cappella groups use Harkness Chapel. The bands and orchestra also perform at  Severance Hall  (the on-campus home of the  Cleveland Orchestra ) and  CIM 's Kulas Hall. Case Western Reserve had the first  ABET -accredited program in  computer engineering . [ 94 ] In 1968, the university formed a private company, Chi Corporation, to provide computer time to both it and other customers. Initially this was on a  Univac 1108  (replacing the preceding  UNIVAC 1107 ), 36 bit,  ones' complement  machine. [ 95 ]  The company was sold in 1977 to Robert G. Benson in  Beachwood, Ohio  becoming Ecocenters Corporation. Project Logos, under ARPA contract, was begun within the department on a DEC System-10 (later converted to  TENEX  ( BBN ) in conjunction with connection to the  ARPANET ) to develop a computer-aided computer design system. This system consisted in a distributed, networked, graphics environment, a control and data flow designer and logic (both hardware and software) analyzer. An  Imlac PDS-1  with lightpen interrupt was the main design workstation in 1973, communicating with the PDP-10 over a display communications protocol written by Don Huff as a Master Thesis and implemented on the Imlac by Ted Brenneman. Graphics and animation became another departmental focus with the acquisition of an  Evans & Sutherland   LDS-1 (Line Drawing System-1) , which was hosted by the DEC System-10, and later with the acquisition of the stand-alone LDS-2. Case Western Reserve was one of the earliest universities connected to the  ARPANET , predecessor to the  Internet . ARPANET went online in 1969; Case Western Reserve was connected in January 1971. [ 96 ]  Case Western Reserve graduate Ken Biba published the  Biba Integrity Model  in 1977 and served on the ARPA Working Group that developed the  Transmission Control Protocol  (TCP) used on the Internet. Case Western Reserve pioneered the early  Free-net  computer systems, creating the first Free-net, The Cleveland Free-Net, as well as writing the software that drove a majority of those systems, known as FreePort. The Cleveland Free-Net was shut down in late 1999, as it had become obsolete. It was the first university to have an all-fiber-optic network, in 1989. [ 97 ] At the inaugural meeting in October 1996, Case Western Reserve was one of the 34 charter university members of  Internet2 . [ 98 ] The university was ranked No. 1 in  Yahoo  Internet Life's 1999 Most Wired College list. [ 99 ]  There was a perception that this award was obtained through partially false or inaccurate information submitted for the survey, [ 100 ]  and the university did not appear at all on the 2000 Most Wired College list (which included 100 institutions). The numbers reported were much lower than those submitted by Ray Neff in 1999. [ 101 ] [ 102 ]  The university had previously placed No. 13 in the 1997 poll. [ 103 ] In August 2003, Case Western Reserve joined the  Internet Streaming Media Alliance , then one of only two university members. [ 104 ] In September 2003, Case Western Reserve opened 1,230 public wireless access points on the Case Western Reserve campus and University Circle. [ 105 ] Case Western Reserve was one of the founding members of OneCleveland, formed in October 2003. [ 106 ]  OneCleveland is an \"ultra broadband\" (gigabit speed) fiber optic network. This network is for the use of organizations in education, research, government, healthcare, arts, culture, and the nonprofit sector in Greater Cleveland. Case Western Reserve's Virtual Worlds gaming computer lab opened in 2005. The lab has a large network of  Alienware  PCs equipped with game development software such as the  Torque Game Engine  and  Maya  3D modeling software. Additionally, it contains a number of specialized advanced computing rooms including a medical simulation room, a  MIDI  instrument music room, a 3D projection \"immersion room\", a  virtual reality  research room, and console room, which features video game systems such as  Xbox 360 ,  PlayStation 3 , and  Wii . [ 107 ]  This laboratory can be used by any student in the Electrical Engineering and computer science department, and is heavily used for the Game Development (EECS 290) course. Residence halls are divided into two areas: one featuring suite-style rooms for second-year students in the South Residential Village, the other featuring double, single and suite style rooms for first-year students and upperclassmen in the North Residential Village. Suite style housing, known as the Village at 115th, was opened in fall 2005 for upperclassmen and features one- to nine-person, \"apartment-style\"  residence halls . First-year students  are grouped into one of four  residential colleges  that are overseen by first-year coordinators. The Mistletoe, Juniper, and Magnolia residential colleges were established when the \" First Year Experience \" system was introduced, and Cedar was created in the fall of 2005 to accommodate a large influx of new students. In the fall of 2007, Magnolia was integrated into Mistletoe, however, it was later re-separated in the fall of 2012. The areas of focus for each college are – Cedar: visual and performing arts; Mistletoe: service leadership; Juniper: multiculturalism and Magnolia: sustainability. [ 108 ] Nearly one-half of the campus undergraduates are said to be in a  fraternity or sorority . There are dozens of Greek organizations on campus. The Office of Emergency Management prepares for various levels of emergencies on campus, such as chemical spills, severe weather, infectious diseases, and security threats. RAVE, a multi-platform emergency alerting system, is operated by Emergency Management for issuing emergency alerts and instructions for events on campus. The Office of Emergency Management also performs  risk assessment  to identify possible safety issues and aims to mitigate these issues. Additionally,  CERT  is managed through Emergency Management, enabling faculty and staff members to engage in emergency preparedness. The Office of Emergency Management works closely with other campus departments, such as Police and Security Services, University Health Services, and Environmental Health and Safety, as well as community resources including city, state, and federal emergency management agencies. [ 109 ] Case operates a police force of sworn officers as well as a security officers. Starting as security only, the university expanded the role of protective services to include sworn officers who have arrest power and carry firearms. Some officers have additional training, such as  SWAT  training. On top of routine duties such as fingerprinting, traffic control, and bicycle registration, police and security also conduct investigations, undercover operations, and community outreach. Police and Security operate a fleet of vehicles, including police cruisers, scooters, and  Smart  cars. Police and Security are dispatched by a 24/7 campus dispatch center, responsible for emergency call handling, alarm monitoring, and video surveillance. Additionally, the dispatch center can send RAVE notifications and manages CWRU Shield, a mobile application allowing video, image, and text tips, safety checks, and viewing emergency procedures. [ 110 ]  CWRU Police also works closely with RTA transit police, University Circle Police,  Cleveland Police , East Cleveland Police, Cleveland Heights Police, University Hospitals Police Department, and other surrounding emergency services. Police and Security, with conjunction with the Emergency Management Office, conduct tabletop drills and full-scale exercises involving surrounding emergency services. [ 111 ] Case Western Reserve University Emergency Medical Services (CWRU EMS) is a student-run all volunteer ambulance service and a  National Collegiate Emergency Medical Services Foundation  member. Covering University Circle, CWRU EMS is run solely by undergraduates volunteers, who provides free  basic life support  level treatment and transport to local hospitals. [ 112 ]  Crews receive medical direction from University Hospitals. [ 113 ]  CWRU EMS is under operational oversight by the Department of Public Safety. CWRU EMS provides both emergency rescue operations, medical standby services, and free community outreach programs such as Stop the Bleed (STB). CWRU EMS responds to over 300 calls for services annually, and operates 2 Type 3 ambulances, 549 and 660, out of the public safety headquarters. CWRU EMS is primarily operational during the academic year, but have the ability to respond off shift for members within the community. Starting in 1910, the Hudson Relay is an annual relay race event remembering and honoring the university relocation from  Hudson, Ohio  to Cleveland. Conceived by then-student, Monroe Curtis, [ 114 ]  the relay race was run from the old college in Hudson, Ohio to the new university in University Circle. Since the mid-1980s, the race has been run entirely in the University Circle area. The race is a distance of 26 miles (42 km). It is held weekend before spring semester finals. Competing running teams are divided by graduating class. If a class wins the relay all four years, tradition dictates a reward of a champagne and steak dinner with the president of the university be awarded. Only six classes have won all four years—1982, 1990, 1994, 2006, 2011, and 2017. [ 115 ] [ 116 ]  The winning classes of each year is carved on an original boulder located behind  Adelbert Hall . Springfest is a day-long concert and student group festival that occurs later in the same day as Hudson Relays. The Springfest Planning Committee brings in several bands and a beer garden, student groups set up booths to entertain the student body, and various inflatable carnival-style attractions are brought in to add to the festive atmosphere. Occasionally, due to adverse weather conditions, the festival must be moved indoors, usually to Thwing Center or  Adelbert Gym . Since 1976, the Film Society [ 117 ]  of Case Western Reserve University has held a  science fiction marathon . The film festival, the oldest of its type, boasts more than 34 hours of non-stop movies, cartoons, trailers, and shorts spanning many decades and subgenres, using both film and digital projection. The Film Society, which is student-run and open to the public, also shows movies on Friday and Saturday evenings throughout the school year. Case Western Reserve competes in 19 varsity sports—10 men's sports and 9 women's sports. All 19 varsity teams wear a commemorative patch on their uniforms honoring Case alumnus,  M. Frank Rudy , inventor of the  Nike  air-sole. [ 118 ]  The Spartans' primary athletic rival is the  Carnegie Mellon Tartans .  DiSanto Field  is home to the  football , men's soccer, women's soccer, and track and field teams. Case Western Reserve is a founding and current member of the  University Athletic Association  (UAA). The conference participates in the  National Collegiate Athletic Association 's (NCAA)  Division III . Case Institute of Technology and Western Reserve University were also founding members of the  Presidents' Athletic Conference  (PAC) in 1958. The university remained a member of the PAC after the merger of Case Institute of Technology and Western Reserve University and until 1983. In the fall of 1984, the university joined the  North Coast Athletic Conference  (NCAC) as a charter member. The 1998–99 school year marked the final season in which the Spartans were members of the NCAC. As the university had held joint conference membership affiliation with the UAA and the NCAC for over a decade. In 2014, the football team began competing as an associate member of the PAC, as only four out of the eight UAA member institutions sponsored football. [ 119 ] The Case Western Reserve  cross country  team won the conference every year for twelve straight years from 1967 to 1978 led by Coach Bill Sudeck, and also won conference titles in 1985, 1986, 1988, 1992, 1993, and 1994. The Case Western Reserve's women's cross country team finished the 2006 season with a UAA Championship and a bid to the  NCAA  Championship. The Lady Spartans finished 10th in the nation. The women's team went on to finish even higher at nationals in 2007, earning a sixth-place finish at the NCAA DIII national championship. Both the men's and women's Cross Country teams qualified for and competed in the NCAA DIII national championships in 2008, with the women's team coming away with two All-Americans and a 16th-place finish. In 2009, they had two All-Americans and finished 15th. In 2010, the lady Spartans finished 19th, with one all-American. From 2006 to 2010 the women's cross country team earned 8 individual All-American Titles, including current professional marathoner  Esther Erb . The  Case Western Reserve football  team reemerged in the mid-2000s under the direction of Head Coach  Greg Debeljak . The 2007 team finished undefeated earning the school's first playoff appearance and first playoff victory, winning against the  Widener Pride . Notable alumni include  John Charles Cutler , former surgeon general who violated human rights and led to deaths in the  Tuskegee Syphilis Study ,  Terre Haute prison experiments , and the  syphilis experiments in Guatemala ;  Anthony Russo  and  Joe Russo ,  Hollywood  movie directors,  Paul Buchheit , creator and lead developer of  Gmail ;  Craig Newmark , billionaire founder of  Craigslist ;  Peter Tippett , developer of the anti-virus software Vaccine, which  Symantec  purchased and turned into the popular  Norton AntiVirus ; Francis E. Sweeney the main suspect from the Cleveland Torso Murders also was a Case Alumnus. Founders of  Fortune 500  companies include  Herbert Henry Dow , founder of  Dow Chemical ,  Art Parker , founder of  Parker Hannifin , and  Edward Williams , co-founder of  Sherwin-Williams . Other notable alumni include  Larry Hurtado ,  New Testament  scholar;  Harvey Hilbert , a zen master, psychologist and expert on post-Vietnam stress syndrome;  Peter Sterling , neuroscientist and co-founder of the concept of  allostasis ;  Ogiame Atuwatse III , Tsola Emiko the 21st Olu of Warri – a historic monarch of the Itsekiri people in Nigeria's Delta region, and  Donald Knuth , a leading expert on computer algorithms and creator of the  TeX  typesetting system."
  },
  {
    "id": 129,
    "title": "Melvin Earl Maron",
    "content": "Melvin Earl \"Bill\" Maron  (Jan 23, 1924 - September 28, 2016) was an American computer scientist and emeritis professor of  University of California, Berkeley . [ 1 ]  He studied mechanical engineering and physics at the University of Nebraska and received his Ph.D. in philosophy from the University of California in 1951. [ 2 ]  Maron is best known for his work on probabilistic  information retrieval  which he published together with his friend and colleague Lary Kuhns. [ 3 ] [ 4 ] \nQuite remarkably, Maron also pioneered relational databases, proposing a system called the Relational Data File in 1967, on which  Ted Codd  based his  Relational model  of data. [ 5 ] This article about an American scientist in academia is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 130,
    "title": "Cyril Cleverdon",
    "content": "Cyril Cleverdon  (9 September 1914 – 4 December 1997) was a British librarian and computer scientist who is best known for his work on the evaluation of  information retrieval  systems. Cyril Cleverdon was born in  Bristol ,  England . He worked at the Bristol Libraries from 1932 to 1938, and from 1938 to 1946 he was the librarian of the Engine Division of the Bristol Aeroplane Co. Ltd. In 1946 he was appointed librarian of the College of Aeronautics at Cranfield (later the  Cranfield Institute of Technology  and  Cranfield University ), where he served until his retirement in 1979, the last two years as professor of Information Transfer Studies. With the help of  National Science Foundation  funding, Cleverdon started a series of projects in 1957 that lasted for about 10 years in which he and his colleagues set the stage for information retrieval research. In the  Cranfield project , retrieval experiments were conducted on test databases in a controlled, laboratory-like setting. The aim of the research was to improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods. The components of the experiments were: Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures:  precision  and  recall . Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful  Text Retrieval Conference  series that began in 1992. Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the  automatic indexing  as done in today's  search engines . Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said: This conclusion is so controversial and so unexpected that it is bound to throw considerable doubt on the methods which have been used (...) A complete recheck has failed to reveal any discrepancies (...) there is no other course except to attempt to explain the results which seem to offend against every canon on which we were trained as librarians. Cyril Cleverdon also ran, for many years, the Cranfield conferences, which provided a major international forum for discussion of ideas and research in information retrieval. This function was taken over by the  SIGIR  conferences in the 1970s."
  },
  {
    "id": 131,
    "title": "Alvin M. Weinberg",
    "content": "Alvin Martin Weinberg  ( / ˈ w aɪ n b ɜːr ɡ / ; April 20, 1915 – October 18, 2006) was an American  nuclear physicist  who was the administrator of  Oak Ridge National Laboratory  (ORNL) during and after the  Manhattan Project . He came to  Oak Ridge, Tennessee , in 1945 and remained there until his death in 2006. He was the first to use the term \" Faustian bargain \" to describe nuclear energy. A graduate of the  University of Chicago , which awarded him his doctorate in mathematical  biophysics  in 1939, Weinberg joined the Manhattan Project's  Metallurgical Laboratory  in September 1941. The following year he became part of  Eugene Wigner 's Theoretical Group, whose task was to design the  nuclear reactors  that would convert  uranium  into plutonium. Weinberg replaced Wigner as director of research at ORNL in 1948, and became director of the laboratory in 1955. Under his direction it worked on the  Aircraft Nuclear Propulsion  program, and pioneered many innovative reactor designs, including the  pressurized water reactors  (PWRs) and  boiling water reactors  (BWRs) which have since become the dominant reactor types in commercial  nuclear power plants , and  Aqueous Homogeneous Reactor  designs. In 1960, Weinberg was appointed to the  President's Science Advisory Committee  in the  Eisenhower administration  and later served on it in the  Kennedy administration . After leaving the ORNL in 1973, he was named director of the Office of Energy Research and Development in Washington, D.C., in 1974. The following year he founded and became the first director of the Institute for Energy Analysis at  Oak Ridge Associated Universities  (ORAU). Alvin Martin Weinberg was born April 20, 1915, in Chicago, Illinois, [ 1 ]  the son of Jacob Weinberg and Emma Levinson Weinberg, [ 2 ]  two Russian Jewish emigrants who met in 1905 on board the boat carrying them to the United States. [ 1 ]  He had an older sister, Fay Goleman, who was born on November 30, 1910. She later became a  sociology  professor at the  University of the Pacific , [ 3 ] [ 4 ]  and was the mother of  Daniel Goleman . He attended  Theodore Roosevelt High School  in Chicago. [ 5 ] Weinberg entered the  University of Chicago , from which he received his  Bachelor of Science  (B.S.) degree in physics in 1935, and his  Master of Science  (M.S.) in physics the following year. [ 6 ]  He received his Ph.D. from the University of Chicago in mathematical  biophysics  in 1939, writing his thesis on  Mathematical foundations for a theory of biophysical periodicity , [ 7 ]  under the supervision of  Carl Eckart . [ 8 ]  Weinberg later lamented that, in restricting his thesis to  linear systems , he had overlooked interesting  nonlinear systems  that  Ilya Prigogine  later received the  Nobel Prize in Chemistry  for studying. [ 9 ] While at Chicago, Weinberg was hired by the family of Margaret Despres, a student at the University of Chicago, to tutor her in mathematics. [ 8 ]  They were married on June 14, 1940. [ 4 ]  They had two sons, David Robert Weinberg and Richard J. Weinberg. [ 10 ] [ 11 ] Weinberg taught courses at  Wright Junior College . He applied for and received a  National Research Council  fellowship to study under  Kenneth S. Cole  at  Columbia University , but never took it up, as Cole came to Chicago to work on the  Manhattan Project  as a radiation biologist. Weinberg was recruited to work at its  Metallurgical Laboratory  at the University of Chicago in September 1941 by Eckart and  Samuel Allison , who needed someone to work on the latter's  neutron capture  calculations. [ 12 ] In early 1942,  Arthur Compton  concentrated the Manhattan Project's various teams working on  plutonium  at the University of Chicago. This brought in many top scientists including  Herbert Anderson ,  Bernard Feld ,  Enrico Fermi ,  Leó Szilárd  and  Walter Zinn  from Columbia, and  Edward Creutz ,  Gilbert Plass ,  Eugene Wigner  and  John Wheeler  from  Princeton University . Weinberg became a protégé of Wigner. [ 13 ] Wigner led the Theoretical Group at the Metallurgical Laboratory that included Alvin Weinberg,  Katharine Way ,  Gale Young  and Edward Creutz. The group's task was to design the production  nuclear reactors  that would convert  uranium  into plutonium. At the time, reactors existed only on paper, and no reactor had yet gone critical. In July 1942, Wigner chose a conservative 100 MW design, with a  graphite   neutron moderator  and water cooling. [ 14 ]  The choice of water as a coolant was controversial at the time. Water was known to absorb  neutrons , thereby reducing the efficiency of the reactor, but Wigner was confident that his group's calculations were correct and that water would work, while the technical difficulties involved in using  helium  or  liquid metal  as coolants would delay the project. [ 15 ] After the  United States Army Corps of Engineers  took over the Manhattan Project, it gave responsibility for the detailed design and construction of the reactors to  DuPont . There was friction between the company and Wigner and his team. Major differences between Wigner's reactor design and DuPont's included increasing the number of process tubes from 1,500 in a circular array to 2,004 in a square array, and cutting the power from 500 MW to 250 MW. As it turned out, the design decision by DuPont to give the reactor additional tubes came in handy when  neutron poisoning  became a problem for the  B Reactor  at the  Hanford Site . The extra tubes allowed a greater fuel load to overcome the poisoning. Without them the reactor would have had to be run at low power until enough of the  boron  impurities in the graphite had been burned up to allow it to reach full power, which would have delayed full operation by up to a year. [ 16 ] [ 17 ] As the reactors at Hanford came online, the Metallurgical Laboratory turned its attention back to theoretical designs. The discovery of spontaneous fission in reactor-bred plutonium due to contamination by  plutonium-240  led Wigner to propose switching to breeding  uranium-233  from  thorium , but the challenge was met by the  Los Alamos Laboratory  developing an  implosion-type nuclear weapon  design. [ 18 ]  Wigner was also intrigued by the possibility of doing away with much of the complexities of a reactor by having the uranium in solution or a slurry in  heavy water . The Metallurgical Laboratory attempted to find a way of doing this. [ 19 ] Amongst the competing designs, Weinberg proposed the  pressurized water reactor , which ultimately became the most common design. [ 20 ]  This was only one of the many possibilities discussed by Weinberg and his colleagues at Chicago and Oak Ridge. Later, he wrote: In these early days we explored all sorts of power reactors, comparing the advantages and disadvantages of each type. The number of possibilities was enormous, since there are many possibilities for each component of a reactor—fuel, coolant, moderator. The fissile material may be  233 U,  235 U, or  239 Pu; the coolant may be: water, heavy water, gas, or liquid metal; the moderator may be: water, heavy water, beryllium, graphite—or, in a fast- neutron reactor, no moderator. I have calculated that, if one counted all the combinations of fuel, coolant, and moderator, one could identify about a thousand distinct reactors. Thus, at the very beginning of nuclear power, we had to choose which possibilities to pursue, which to ignore. [ 21 ] The ultimate success of the pressurized water reactor, he wrote, was due less to any superior characteristics of water, but rather to the decision to power the prototype of the Mark I submarine thermal reactor with a pressurized version of the Materials Testing Reactor at Oak Ridge. Once pressurized water was established, other possibilities became too expensive to pursue, [ 22 ]  but Weinberg remained interested in other possibilities. According to  Freeman Dyson , he was the only nuclear pioneer who supported the wide universe of reactor designs. [ 23 ] In 1945, Wigner accepted a position as the director of research at the Clinton Laboratories in  Oak Ridge, Tennessee , which then had a staff of about 800. He took with him his protégés  Gale Young ,  Katherine Way  and Weinberg. Weinberg, who was the first to arrive at Oak Ridge in May 1945, [ 24 ]  became head of the Physics Division in 1946. [ 25 ]  But after the  Atomic Energy Commission  took over responsibility for the laboratory's operations from the Manhattan Project at the start of 1947, Wigner, feeling unsuited to a managerial role in the new environment, left Oak Ridge at the end of summer in 1947 and returned to Princeton University. [ 26 ] The administration of the Clinton Laboratories passed from  Monsanto  to the University of Chicago in May 1947, and then to  Union Carbide  in December 1947. [ 27 ]  The Atomic Energy Commission's influential General Advisory Committee, chaired by  J. Robert Oppenheimer , recommended that all work on reactors be concentrated at the  Argonne National Laboratory , the successor to the Metallurgical Laboratory, near Chicago. There was also competition for staff and resources from the newly established  Brookhaven National Laboratory  near New York. Morale was low, and no one could be found to take on the job of director of research at the laboratory, renamed the  Oak Ridge National Laboratory  (ORNL) in January 1948. At least six people turned down the job before Union Carbide's acting Director, Nelson (Bunny) Rucker, asked Weinberg to become Director of Research in March 1948. [ 28 ] [ 29 ] Weinberg was subsequently appointed director in 1955. He often sat in the front row at ORNL division information meetings and he would ask the first, often very penetrating, question after each scientific talk. For young scientists giving their first presentation, the experience could be frightening, but it was also exciting and stimulating. When asked how he found the time to attend every meeting, Weinberg replied jokingly, \"We didn't have a  DOE  in those days.\" [ 25 ] The  Aircraft Nuclear Propulsion  (ANP) project was ORNL's biggest program, using 25% of ORNL's budget. The ANP project's military goal was to produce a nuclear-powered aircraft (a bomber) to overcome the range limitations of jet-fueled aircraft at that time. That the project had little chance of success was not overlooked, but it provided employment and allowed ORNL to stay in the reactor development business. ORNL successfully built and operated a prototype of an aircraft reactor power plant by creating the world's first molten salt fueled and cooled reactor called the  Aircraft Reactor Experiment  (ARE) in 1954, which set a record high temperature of operation of 1,600 °F (870 °C). Due to the radiation hazard posed to aircrew, and people on the ground in the event of a crash, new developments in  ballistic missile  technology,  aerial refueling  and longer range jet bombers, President Kennedy canceled the program in June 1961. [ 30 ] [ 31 ] Weinberg had the  Materials Testing Reactor  converted into a mock-up of a real reactor called the  Low Intensity Test Reactor  (LITR) or \"Poor Man's Pile\". Experiments at the LITR led to the design of both pressurized water reactors (PWRs) and  boiling water reactors  (BWRs), which have since become the dominant reactor types in commercial  nuclear power plants . [ 32 ]  Weinberg was attracted to the simplicity and self-controlling features of nuclear reactors that used fluid fuels, such as  Harold Urey  and Eugene Wigner's proposed  Aqueous Homogeneous Reactor . Therefore, to support the Nuclear Aircraft project in the late 1940s, Weinberg asked ORNL's reactor engineers to design a reactor using liquid instead of solid fuel. [ 33 ] This  Homogeneous Reactor Experiment  (HRE) was affectionately dubbed \"Alvin's 3P reactor\" because it required a pot, a pipe, and a pump. The HRE went into operation in 1950 and, at the  criticality  party, Weinberg brought the appropriate spirits: \"When piles go critical in Chicago, we celebrate with wine. When piles go critical in Tennessee, we celebrate with  Jack Daniel's .\" [ 25 ]  The HRE operated for 105 days before it was closed down. Despite its leaks and corrosion, valuable information was gained from its operation and it proved a simple and safe reactor to control. [ 34 ]  During the time the HRE was online,  Senators   John F. Kennedy  and  Albert Gore, Sr.  visited ORNL and were hosted by Weinberg. [ 25 ] Under Weinberg, ORNL shifted its focus to a civilian version of the meltdown-proof  Molten Salt Reactor  (MSR) away from the military's \"daft\" [ 35 ]  idea of nuclear-powered aircraft. The  Molten-Salt Reactor Experiment  (MSRE) set a record for continuous operation and was the first to use Thorium irradiated to produce uranium-233 as fuel. It also used  plutonium-239  and the standard, naturally occurring  uranium-235 . The MSR was known as the \"chemist's reactor\" because it was proposed mainly by chemists (ORNL's Ray Briant and Ed Bettis (an engineer) and NEPA's Vince Calkins), [ 34 ]  and because it used a chemical solution of melted  salts  containing the  actinides  (uranium, thorium, and/or plutonium) in a carrier salt, most often composed of  beryllium  (BeF 2 ) and  lithium  (LiF) (isotopically depleted in  Lithium-6  to prevent excessive neutron capture or tritium production) –  FLiBe . [ 36 ]  The MSR also afforded the opportunity to change the chemistry of the molten salt while the reactor was operating to remove fission products and add new fuel or change the fuel, all of which is called \"online processing\". [ 37 ] Under Weinberg's tenure as director, ORNL's  Biology  Division grew to five times the size of the next largest division. This division was charged with understanding how  ionizing radiation  interacts with living things and to try to find ways to help them survive radiation damage, such as  bone marrow transplants . In the 1960s Weinberg also pursued new missions for ORNL, such as using nuclear energy to  desalinate  seawater. He recruited  Philip Hammond  from the  Los Alamos National Laboratory  to further this mission and in 1970 started the first big ecology project in the United States: the  National Science Foundation  – Research Applied to National Needs Environmental Program. [ 38 ] In 1958, Weinberg coauthored the first nuclear reactor textbook,  The Physical Theory of Neutron Chain Reactors , with Wigner. The following year, 1959, he was elected president of the  American Nuclear Society  and, in 1960, began service on the  President's Science Advisory Committee  under the  Eisenhower  and  Kennedy administrations . [ 39 ]  Starting in 1945 with Patent #2,736,696, Weinberg, usually with Wigner, filed numerous patents on the  light water reactor  (LWR) technology that has provided the United States' primary nuclear reactors. The main LWR types are Pressurized Water Reactors (PWRs) and  Boiling Water Reactors  (BWRs), that serve in Naval propulsion and commercial nuclear power. [ 40 ]  In 1965 he was appointed vice president of Union Carbide's Nuclear Division. [ 41 ] In a 1971 paper, Weinberg first used the term \" Faustian bargain \" to describe nuclear energy: We nuclear people have made a Faustian bargain with society. On the one hand we offer—in the catalytic nuclear burner (i.e., the breeder)—an inexhaustible source of energy. Even in the short range, when we use ordinary reactors, we offer energy that is cheaper than energy from fossil fuel. Moreover, this source of energy when properly handled is almost nonpolluting. Whereas fossil-fuel burners emit oxides of carbon, nitrogen, and sulfur... there is no intrinsic reason why nuclear systems must emit any pollutant except heat and traces of radioactivity.\nBut the price that we demand of society for this magical source is both a vigilance from and longevity of our social institutions that we are quite unaccustomed to. [ 42 ] Weinberg was fired by the  Nixon administration  from ORNL in 1973 after 18 years as the laboratory's director, because he continued to advocate increased nuclear safety and molten salt reactors (MSRs), instead of the Administration's chosen  Liquid Metal Fast Breeder Reactor  (LMFBR) that the AEC's Director of Reactor Division, Milton Shaw, was appointed to develop. Weinberg's firing effectively halted development of the MSR, as it was virtually unknown by other nuclear laboratories and specialists. [ 43 ]  There was a brief revival of MSR research at ORNL as part of the  Carter administration 's nonproliferation interests, culminating in ORNL-TM-7207, \"Conceptual Design Characteristics of a Denatured Molten-Salt Reactor with Once-Through Fueling\", by Engel,  et al. , which is still considered by many to be the \"reference design\" for commercial molten salt reactors. [ 44 ] [ 45 ] Weinberg was named director of the Office of Energy Research and Development in Washington, D.C., in 1974. The following year he founded and became the first director of Institute for Energy Analysis at  Oak Ridge Associated Universities  (ORAU). This institute focused on evaluating alternatives for meeting future energy requirements. From 1976 to 1984, the Institute for Energy Analysis was a center for study of diverse issues related to  carbon dioxide  and  global warming . [ 46 ]  He worked at ORAU until retiring to become an ORAU distinguished fellow in 1985. [ 25 ] In 1972 Weinberg published a landmark article in Minerva entitled  Science and Trans-science , in which he focused on the interface between science and policy matters, especially governmental policy decisions: Many of the issues which arise in the course of the interaction between science or technology and society—e.g., the deleterious side effects of technology, or the attempts to deal with social problems through the procedures of science—hang on the answers to questions which can be asked of science and yet which cannot be answered by science. I propose the term trans-scientific for these questions since, though they are, epistemologically speaking, questions of fact and can be stated in the language of science, they are unanswerable by science; they transcend science. In so far as public policy involves trans-scientific rather than scientific issues, the role of the scientist in contributing to the promulgation of such policy must be different from his role when the issues can be unambiguously answered by science. [ 47 ] In June, 1977, Weinberg testified at a congressional hearing of the House Subcommittee on the Environment and the Atmosphere concerning the impact of increasing carbon dioxide emissions on global average temperatures. He stated that a doubling of global carbon dioxide emissions by 2025, which some scientists predicted would occur, would lead to a two-degree Celsius increase in global average temperature. [ 48 ] Weinberg remained active in retirement. In 1992 he was named chairman of the  International Friendship Bell Committee , which arranged for the installation of a Japanese bell in Oak Ridge. He also called for strengthening of the  International Atomic Energy Agency  and systems to defend against  nuclear weapons . [ 49 ]  His first wife, Margaret, died in 1969. He later married a stock broker, Genevieve DePersio, who died in 2004. [ 8 ] [ 10 ]  His son David died in 2003. [ 11 ]  Weinberg died at his home in Oak Ridge on October 18, 2006. He was survived by his other son, Richard, and sister Fay Goleman. [ 10 ] The Alvin Weinberg Foundation  is named for him. [ 50 ]"
  },
  {
    "id": 132,
    "title": "Robert M. Hayes (information scientist)",
    "content": "Robert Mayo Hayes  (December 18, 1926 – February 12, 2022) was an American professor and dean of the Graduate School of Library and Information Science (1974–1989), now the  Graduate School of Education and Information Studies , at the  University of California, Los Angeles  (UCLA). An expert on  information systems , Hayes began his academic career in  mathematics  and went on to become a pioneer in the field of  information science . [ 1 ] [ 2 ] Robert Mayo Hayes was born on 3 December 1926, in  New York City . [ 2 ] [ 3 ]  During his childhood his family moved frequently because of his stepfather's acting career; as a result he attended over sixteen different high schools before receiving his diploma. [ 4 ]  By that time the United States had entered the  Second World War . He was drafted into the Navy, and gained acceptance into the  Navy's V-12 program , in the context of which he took courses at the  University of Colorado Boulder . [ 4 ] After the War, Hayes completed his B.A. in mathematics at  UCLA , in 1946; he went on to earn his M.A. in mathematics there in 1949, and his Ph.D. in 1952. [ 5 ]   While completing his Ph.D., he worked in information science at the National Bureau of Standards. [ 4 ] Upon receiving his Ph.D. in 1952, Hayes decided to move into industry, and found a position at Hughes Aircraft, where he programmed a computer to fly an airplane. [ 4 ]  At that time he also taught in UCLA's university-extension program. [ 4 ]   \nIn 1954, he began working at the  National Cash Register Company , and a year later he moved to  Magnavox Research Laboratories . [ 4 ]   His work at Magnavox was related to important developments in information storage and retrieval, such as the Minicard and the Magnacard systems. [ 4 ]  Eager to share his knowledge in the field with students, he subsequently went into teaching; in the 1950s to 1960s he held teaching positions at  American University , the  University of Washington , and the  University of Illinois , as well as  Wright-Patterson Air Force Base . [ 4 ] [ 6 ] In 1958, Hayes was hired as a vice president of  Electrada Corporation , [ 6 ]  where, together with  John A. Postley , he created Advanced Information Systems as a subsidiary of Electrada. [ 4 ] At the  1962 Seattle World's Fair  (\"Century 21 Exposition\") Hayes led the training program in library automation for the professional staff of the  American Library Association  (ALA) exhibit, \"Library 21\", which aimed to introduce online retrieval to the general public. [ 1 ] [ 7 ] He and Joseph Becker co-authored  Information Storage and Retrieval  (1963), the most comprehensive text in the field at the time. [ 1 ]  He also partnered with Becker in 1969 to found Becker and Hayes Incorporated, for the purpose of creating an interlibrary network for the State of Washington, a goal that they eventually accomplished. [ 4 ] A lecturer in mathematics at UCLA since 1952, Hayes became a full-time professor there in 1964. [ 1 ]  Around that time he played a role in the formation of the School of Library Service and the Institute for Library Research. [ 4 ] Hayes was president of the  American Society for Information Science and Technology , formerly known as the  American Documentation Institute , in 1962/1963. [ 8 ]  He received the  Award of Merit  in 1993. He was president  of the Information Science and Automation Division of the American Library Association (later known as the  Library and Information Technology Association , or LITA), in 1969–1970. [ 9 ] [ 6 ] At UCLA he served as dean of the Graduate School of Library and Information Science from 1974 to 1989, and became professor emeritus in 1991. [ 6 ] From 1987 through the 2000s, Hayes was a visiting professor at a variety of institutions internationally, including  Nankai University , Tianjin, China; the University of Library and Information Science,  Tsukuba Science City , Japan;  Keio University , Tokyo;  Khazar University , Baku, Azerbaijan; the  University of New South Wales , Sydney, Australia;  Strossmayer University , Osijek, Croatia;  Loughborough University , England; and the  University of Graz , Austria. [ 6 ] His research as of 2009 focused on the role of libraries in national  information economies , and the philosophical foundations of  information science . [ 10 ] He was inducted into the California Librarian Hall of Fame in 2022.  [ 11 ]"
  },
  {
    "id": 133,
    "title": "Karen Spärck Jones",
    "content": "Karen Ida Boalth Spärck Jones   FBA  (26 August 1935 – 4 April 2007) was a self-taught programmer and a pioneering British computer scientist responsible for the concept of  inverse document frequency  (IDF), a technology that underlies most modern  search engines . [ 2 ] [ 3 ] [ 4 ] [ 5 ] [ 6 ]  She was an advocate for women in computer science, her slogan being, \"Computing is too important to be left to men. [ 7 ] \" In 2019,  The New York Times  published her belated obituary in its series  Overlooked , [ 8 ] [ 9 ]  calling her \"a pioneer of computer science for work combining statistics and linguistics, and an advocate for women in the field.\" [ 9 ]  From 2008, to recognize her achievements in the fields of  information retrieval [ 10 ] [ 11 ]  (IR) and  natural language processing  (NLP), the  Karen Spärck Jones Award  is awarded annually to a recipient for outstanding research in one or both of her fields. [ 12 ] [ 13 ] [ 14 ] [ 15 ] Karen Ida Boalth Spärck Jones was born in  Huddersfield , Yorkshire, England. Her parents were Alfred Owen Jones, a chemistry lecturer, and Ida Spärck, a  Norwegian  who worked for the Norwegian government while in exile in London during World War II. [ 16 ] Spärck Jones was educated at a grammar school in Huddersfield and then from 1953 to 1956 at  Girton College, Cambridge , studying history, with an additional final year in  Moral Sciences (philosophy) . While at Cambridge, Spärck Jones joined the organization known as the Cambridge Language Research Unit (CLRU) and met the head of CLRU  Margaret Masterman , who would inspire her to go into computer science. [ 9 ]  While working at the CLRU, Spärck Jones began pursuing her Ph.D. At the time of submission, her Ph.D thesis was cast aside as uninspired and lacking original thought, but was later published in its entirety as a book. [ 17 ]  She briefly became a school teacher [ 18 ]  before moving into computer science. [ 19 ]  Spärck Jones married fellow Cambridge computer scientist  Roger Needham  in 1958. [ 20 ] [ 9 ] Spärck Jones worked at the Cambridge Language Research Unit from the late 1950s, [ 20 ]  then at  Cambridge University Computer Laboratory  from 1974 until her retirement in 2002. From 1999, she held the post of Professor of Computers and Information. [ 1 ]  Prior to 1999, she was employed on a series of short-term contracts. [ 9 ]  She continued to work in the Computer Laboratory until shortly before her death. Her publications include nine books and numerous papers. A full list of her publications is available from the Cambridge Computer Laboratory. [ 21 ] Spärck Jones' main research interests, since the late 1950s, were  natural language processing  and  information retrieval .  In 1964, Spärck Jones published \"Synonymy and Semantic Classification\", which is now seen as a foundational paper in the field of natural language processing. One of her most important contributions was the concept of  inverse document frequency  (IDF) weighting in information retrieval, which she introduced in a 1972 paper. [ 10 ] [ 22 ]  IDF is used in most  search engines  today, usually as part of the  term frequency–inverse document frequency  (TF–IDF) weighting scheme. [ 23 ]   In the 1980s, Spärck Jones began her work on early  speech recognition  systems. In 1982 she became involved in the  Alvey  Programme [ 9 ]  which was an initiative to motivate more computer science research across the country. These include: Spärck Jones died on April 4, 2007, due to cancer at the age of 71. [ 9 ] In 2008, the  BCS Information Retrieval Specialist Group  (BCS IRSG) in conjunction with the  British Computer Society  established an annual  Karen Spärck Jones Award  in her honour, to encourage and promote research that advances understanding of Natural Language Processing or Information Retrieval. [ 6 ]  The Karen Spärck Jones lecture sponsored by BCS recognises the contribution that women have made to computing. [ 29 ] In August 2017, the  University of Huddersfield  renamed one of its campus buildings in her honour. Formerly known as Canalside West, the Spärck Jones building houses the University's School of Computing and Engineering. [ 30 ]"
  },
  {
    "id": 134,
    "title": "Computational linguistics",
    "content": "Computational linguistics  is an  interdisciplinary  field concerned with the  computational modelling  of  natural language , as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon  linguistics ,  computer science ,  artificial intelligence ,  mathematics ,  logic ,  philosophy ,  cognitive science ,  cognitive psychology ,  psycholinguistics ,  anthropology  and  neuroscience , among others. The field overlapped with  artificial intelligence  since the efforts in the United States in the 1950s to use computers to automatically translate texts from foreign languages, particularly Russian scientific journals, into English. [ 1 ]  Since rule-based approaches were able to make  arithmetic  (systematic) calculations much faster and more accurately than humans, it was expected that  lexicon ,  morphology ,  syntax  and  semantics  can be learned using explicit rules, as well. After the  failure of rule-based approaches ,  David Hays [ 2 ]  coined the term in order to distinguish the field from AI and co-founded both the  Association for Computational Linguistics (ACL)  and the  International Committee on Computational Linguistics  (ICCL) in the 1970s and 1980s. What started as an effort to translate between languages evolved into a much wider field of  natural language processing . [ 3 ] [ 4 ] In order to be able to meticulously study the  English language , an annotated text corpus was much  needed. The Penn  Treebank [ 5 ]  was one of the most used corpora. It consisted of IBM computer manuals, transcribed telephone conversations, and other texts, together containing over 4.5 million words of American English, annotated using both   part-of-speech  tagging and syntactic bracketing. [ 6 ] Japanese sentence corpora were analyzed and a pattern of  log-normality  was found in relation to sentence length. [ 7 ] The fact that during  language acquisition , children are largely only exposed to positive evidence, [ 8 ]  meaning that the only evidence for what is a correct form is provided, and no evidence for what is not correct, [ 9 ]  was a limitation for the models at the time because the now available  deep learning  models were not available in late 1980s. [ 10 ] It has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span, [ 11 ]  which explained the long period of  language acquisition  in human infants and children. [ 11 ] Robots have been used to test linguistic theories. [ 12 ]  Enabled to learn as children might, models were created based on an  affordance  model in which mappings between actions, perceptions, and effects were created and linked to spoken words. Crucially, these robots were able to acquire functioning word-to-meaning mappings without needing grammatical structure. Using the  Price equation  and  Pólya urn  dynamics, researchers have created a system which not only predicts future linguistic evolution but also gives insight into the evolutionary history of modern-day languages. [ 13 ] Chomsky's theories have influenced computational linguistics, particularly in understanding how infants learn complex grammatical structures, such as those described in  Chomsky normal form . [ 14 ]  Attempts have been made to determine how an infant learns a \"non-normal grammar\" as theorized by Chomsky normal form. [ 9 ]  Research in this area combines structural approaches with computational models to analyze large  linguistic corpora  like the Penn  Treebank , helping to uncover patterns in language acquisition. [ 15 ]"
  },
  {
    "id": 135,
    "title": "National Institute of Standards and Technology",
    "content": "The  National Institute of Standards and Technology  ( NIST ) is an agency of the  United States Department of Commerce  whose mission is to promote American innovation and industrial competitiveness. NIST's activities are organized into  physical science  laboratory programs that include  nanoscale science and technology ,  engineering ,  information technology ,  neutron  research, material measurement, and physical measurement.  From 1901 to 1988, the agency was named the  National Bureau of Standards . [ 4 ] The  Articles of Confederation , ratified by the colonies in 1781, provided: The United States in Congress assembled shall also have the sole and exclusive right and power of regulating the alloy and value of coin struck by their own authority, or by that of the respective states—fixing the standards of weights and measures throughout the United States. [ 5 ] Article 1, section 8, of the  Constitution of the United States , ratified in 1789, granted these powers to the new Congress: \"The Congress shall have power ... To coin money, regulate the value thereof, and of foreign coin, and fix the standard of weights and measures\". [ 6 ] In January 1790,  President   George Washington , in his first  annual message to Congress , said, \"Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to.\" [ 7 ] On October 25, 1791, Washington again appealed Congress: A uniformity of the weights and measures of the country is among the important objects submitted to you by the Constitution and if it can be derived from a standard at once invariable and universal, must be no less honorable to the public council than conducive to the public convenience. [ 8 ] In 1821,  President   John Quincy Adams  declared, \"Weights and measures may be ranked among the necessities of life to every individual of human society.\". [ 9 ]  Nevertheless, it was not until 1838 that the United States government adopted a uniform set of standards. [ 6 ] From 1830 until 1901, the role of overseeing weights and measures was carried out by the Office of Standard Weights and Measures, which was part of the Survey of the Coast—renamed the United States Coast Survey in 1836 and the  United States Coast and Geodetic Survey  in 1878—in the  United States Department of the Treasury . [ 10 ] [ 11 ] [ 12 ] In 1901, in response to a bill proposed by Congressman  James H. Southard  (R, Ohio), the  National Bureau of Standards  was founded with the mandate to provide standard weights and measures, and to serve as the national physical laboratory for the United States. Southard had previously sponsored a bill for metric conversion of the United States. [ 13 ] President  Theodore Roosevelt  appointed  Samuel W. Stratton  as the first director. The budget for the first year of operation was $40,000. The Bureau took custody of the copies of the  kilogram  and  meter  bars that were the standards for US measures, and set up a program to provide  metrology  services for United States scientific and commercial users. A laboratory site was constructed in  Washington, DC , and instruments were acquired from the national physical laboratories of Europe. In addition to weights and measures, the Bureau developed instruments for electrical units and for measurement of light. In 1905 a meeting was called that would be the first \"National Conference on Weights and Measures\". Initially conceived as purely a  metrology  agency, the Bureau of Standards was directed by  Herbert Hoover  to set up divisions to develop commercial standards for materials and products. [ 13 ]  Some of these standards were for products intended for government use, but product standards also affected private-sector consumption. Quality standards were developed for products including some types of clothing, automobile brake systems and headlamps,  antifreeze , and electrical safety. During  World War I , the Bureau worked on multiple problems related to war production, even operating its own facility to produce  optical glass  when European supplies were cut off. Between the wars,  Harry Diamond  of the Bureau developed a  blind approach  radio aircraft landing system. During  World War II, military research and development  was carried out, including development of  radio propagation  forecast methods, the  proximity fuze  and the standardized airframe used originally for  Project Pigeon , and shortly afterwards the autonomously radar-guided  Bat  anti-ship guided bomb and the  Kingfisher family  of torpedo-carrying missiles. In 1948, financed by the United States Air Force, the Bureau began design and construction of  SEAC , the Standards Eastern Automatic Computer. The computer went into operation in May 1950 using a combination of  vacuum tubes  and solid-state  diode  logic. About the same time the  Standards Western Automatic Computer , was built at the Los Angeles office of the NBS by  Harry Huskey  and used for research there. A mobile version,  DYSEAC , was built for the Signal Corps in 1954. Due to a changing mission, the \"National Bureau of Standards\" became the \"National Institute of Standards and Technology\" in 1988. [ 10 ]  Following the  September 11, 2001  attacks, under the National Construction Safety Team Act (NCST),  NIST conducted the official investigation into the  collapse of the World Trade Center  buildings. Following the 2021  Surfside condominium building collapse , NIST sent engineers to the site to investigate the cause of the collapse. [ 14 ] In 2019, NIST launched a program named NIST on a Chip to decrease the size of instruments from lab machines to chip size. Applications include aircraft testing, communication with satellites for navigation purposes, and temperature and pressure. [ 15 ] In 2023, the  Biden administration  began plans to create a U.S. AI Safety Institute within NIST to coordinate  AI safety  matters. According to  The Washington Post , NIST is considered \"notoriously underfunded and understaffed\", which could present an obstacle to these efforts. [ 16 ] NIST, known between 1901 and 1988 as the National Bureau of Standards (NBS), is a  measurement standards laboratory , also known as the National Metrological Institute (NMI), which is a non-regulatory agency of the  United States Department of Commerce . The institute's official mission is to: [ 17 ] Promote U.S. innovation and industrial competitiveness by advancing  measurement science ,  standards , and  technology  in ways that enhance economic security and improve our  quality of life . NIST had an operating  budget  for  fiscal year  2007 (October 1, 2006 – September 30, 2007) of about $843.3 million. NIST's 2009 budget was $992 million, and it also received $610 million as part of the  American Recovery and Reinvestment Act . [ 18 ]  NIST employs about 2,900 scientists, engineers, technicians, and support and administrative personnel. About 1,800 NIST associates (guest researchers and engineers from American companies and foreign countries) complement the staff. In addition, NIST partners with 1,400 manufacturing specialists and staff at nearly 350 affiliated centers around the country. NIST publishes the  Handbook 44  that provides the \"Specifications, tolerances, and other technical requirements for weighing and measuring devices\". The  Congress of 1866  made use of the metric system in commerce a legally protected activity through the passage of  Metric Act of 1866 . [ 19 ]  On May 20, 1875, 17 out of 20 countries signed a document known as the  Metric Convention  or the  Treaty of the Meter , which established the  International Bureau of Weights and Measures  under the control of an international committee elected by the  General Conference on Weights and Measures . [ 20 ] NIST is headquartered in  Gaithersburg, Maryland , and operates a facility in  Boulder, Colorado , which was dedicated by  President   Eisenhower  in 1954. [ 21 ] [ 22 ] [ 23 ]  NIST's activities are organized into laboratory programs and extramural programs. Effective October 1, 2010, NIST was realigned by reducing the number of NIST laboratory units from ten to six. [ 24 ]  NIST Laboratories include: [ 25 ] Extramural programs include: NIST's Boulder laboratories are best known for  NIST‑F1 , which houses an  atomic clock . NIST‑F1 serves as the source of the nation's official time. From its measurement of the natural resonance frequency of  cesium —which defines the  second —NIST broadcasts  time signals  via  longwave  radio station  WWVB  near  Fort Collins , Colorado, and  shortwave   radio stations   WWV  and  WWVH , located near Fort Collins and  Kekaha, Hawaii , respectively. [ 33 ] NIST also operates a  neutron  science user facility: the  NIST Center for Neutron Research  (NCNR). The NCNR provides scientists access to a variety of  neutron scattering  instruments, which they use in many research fields (materials science, fuel cells, biotechnology, etc.). The SURF III Synchrotron Ultraviolet Radiation Facility is a source of  synchrotron radiation , in continuous operation since 1961. SURF III now serves as the US national standard for source-based radiometry throughout the generalized optical spectrum. All  NASA -borne, extreme-ultraviolet observation instruments have been calibrated at SURF since the 1970s, and SURF is used for the measurement and characterization of systems for  extreme ultraviolet lithography . The Center for Nanoscale Science and Technology (CNST) performs research in  nanotechnology , both through internal research efforts and by running a user-accessible  cleanroom   nanomanufacturing  facility. This \"NanoFab\" is equipped with tools for  lithographic  patterning and imaging (e.g.,  electron microscopes  and  atomic force microscopes ). NIST has seven standing committees: As part of its mission, NIST supplies industry, academia, government, and other users with over 1,300  Standard Reference Materials  (SRMs). These artifacts are certified as having specific characteristics or component content, used as calibration standards for measuring equipment and procedures, quality control benchmarks for industrial processes, and experimental control samples. NIST publishes the  Handbook 44  each year after the annual meeting of the  National Conference on Weights and Measures  (NCWM). Each edition is developed through cooperation of the  Committee on Specifications and Tolerances  of the NCWM and the  Weights and Measures Division  (WMD) of NIST. The purpose of the book is a partial fulfillment of the statutory responsibility for \"cooperation with the states in securing uniformity of weights and measures laws and methods of inspection\". NIST has been publishing various forms of what is now the  Handbook 44  since 1918 and began publication under the current name in 1949. The 2010 edition conforms to the concept of the primary use of the SI (metric) measurements recommended by the  Omnibus Foreign Trade and Competitiveness Act of 1988 . [ 34 ] [ 35 ] NIST is developing government-wide  identity document  standards for federal employees and contractors to prevent unauthorized persons from gaining access to government buildings and computer systems. [ 36 ] In 2002, the  National Construction Safety Team Act  mandated NIST to conduct an investigation into the  collapse of the World Trade Center  buildings 1 and 2 and the 47-story 7 World Trade Center. The \"World Trade Center Collapse Investigation\", directed by lead investigator Shyam Sunder, [ 37 ]  covered three aspects, including a technical building and  fire safety  investigation to study the factors contributing to the probable cause of the collapses of the WTC Towers (WTC 1 and 2) and WTC 7. NIST also established a research and development program to provide the technical basis for improved building and fire codes, standards, and practices, and a dissemination and technical assistance program to engage leaders of the construction and building community in implementing proposed changes to practices, standards, and codes. NIST also is providing practical guidance and tools to better prepare facility owners, contractors, architects, engineers, emergency responders, and regulatory authorities to respond to future disasters. The investigation portion of the response plan was completed with the release of the final report on 7 World Trade Center on November 20, 2008. The final report on the WTC Towers—including 30 recommendations for improving building and occupant safety—was released on October 26, 2005. [ 38 ] NIST works in conjunction with the  Technical Guidelines Development Committee  of the  Election Assistance Commission  to develop the  Voluntary Voting System Guidelines  for  voting machines  and other election technology. In February 2014 NIST published the  NIST Cybersecurity Framework  that serves as voluntary guidance for organizations to manage and reduce cybersecurity risk. [ 39 ]  It was later amended and Version 1.1 was published in April 2018. [ 40 ]   Executive Order  13800, Strengthening the Cybersecurity of Federal Networks and  Critical Infrastructure , made the Framework mandatory for U.S. federal government agencies. [ 39 ]  An extension to the NIST Cybersecurity Framework is the  Cybersecurity Maturity Model (CMMC)  which was introduced in 2019 (though the origin of CMMC began with Executive Order 13556). [ 41 ] It emphasizes the importance of implementing  Zero-trust architecture (ZTA)  which focuses on protecting resources over the network perimeter. ZTA utilizes zero trust principles which include \"never trust, always verify\", \"assume breach\" and \"least privileged access\" to  safeguard  users, assets, and resources. Since ZTA holds no implicit trust to users within the network perimeter, authentication and authorization are performed at every stage of a digital transaction. This reduces the risk of unauthorized access to resources. [ 42 ] NIST released a draft of the CSF 2.0 for public comment through November 4, 2023. NIST decided to update the framework to make it more applicable to small and medium size enterprises that use the framework, as well as to accommodate the constantly changing nature of cybersecurity. [ 43 ] In August 2024, NIST released a final set of encryption tools designed to withstand the attack of a  quantum computer.  These  post-quantum encryption  standards secure a wide range of electronic information, from confidential email messages to e-commerce transactions that propel the modern economy. [ 44 ] Four scientific researchers at NIST have been awarded  Nobel Prizes  for work in  physics :  William Daniel Phillips  in 1997,  Eric Allin Cornell  in 2001,  John Lewis Hall  in 2005 and  David Jeffrey Wineland  in 2012, which is the largest number for any US government laboratory not accounting for ubiquitous government contracts to state institutions and the private sector. All four were recognized for their work related to  laser cooling  of atoms, which is directly related to the development and advancement of the atomic clock. In 2011,  Dan Shechtman  was awarded the Nobel Prize  in chemistry for his work on  quasicrystals  in the  Metallurgy  Division from 1982 to 1984. In addition,  John Werner Cahn  was awarded the 2011 Kyoto Prize for Materials Science, and the  National Medal of Science  has been awarded to NIST researchers Cahn (1998) and Wineland (2007). Other notable people who have worked at NBS or NIST include: Since 1989, the director of NIST has been a Presidential appointee and is confirmed by the  United States Senate , [ 45 ]  and since that year the average tenure of NIST directors has fallen from 11 years to 2 years in duration. Since the 2011 reorganization of NIST, the director also holds the title of Under Secretary of Commerce for Standards and Technology. Fifteen individuals have officially held the position (in addition to four acting directors who have served on a temporary basis). NIST holds  patents  on behalf of the  Federal government of the United States , [ 46 ]  with at least one of them being custodial to protect public domain use, such as one for a  Chip-scale atomic clock , developed by a NIST team as part of a  DARPA  competition. [ 47 ] In September 2013, both  The Guardian  and  The New York Times  reported that NIST allowed the  National Security Agency  (NSA) to insert a  cryptographically secure pseudorandom number generator  called  Dual EC DRBG  into NIST standard  SP 800-90  that had a  kleptographic   backdoor  that the NSA can use to covertly predict the future outputs of this  pseudorandom number generator  thereby allowing the surreptitious decryption of data. [ 48 ]  Both papers report [ 49 ] [ 50 ]  that the NSA worked covertly to get its own version of SP 800-90 approved for worldwide use in 2006. The whistle-blowing document states that \"eventually, NSA became the sole editor\". The reports confirm suspicions and technical grounds publicly raised by cryptographers in 2007 that the EC-DRBG could contain a  kleptographic  backdoor (perhaps placed in the standard by NSA). [ 51 ] NIST responded to the allegations, stating that \"NIST works to publish the strongest cryptographic standards possible\" and that it uses \"a transparent, public process to rigorously vet our recommended standards\". [ 52 ]  The agency stated that \"there has been some confusion about the standards development process and the role of different organizations in it...The National Security Agency (NSA) participates in the NIST cryptography process because of its recognized expertise. NIST is also required by statute to consult with the NSA.\" [ 53 ]  Recognizing the concerns expressed, the agency reopened the public comment period for the SP800-90 publications, promising that \"if vulnerabilities are found in these or any other NIST standards, we will work with the cryptographic community to address them as quickly as possible\". [ 54 ]  Due to public concern of this  cryptovirology  attack, NIST rescinded the EC-DRBG algorithm from the NIST SP 800-90 standard. [ 55 ] In addition to these journals, NIST (and the National Bureau of Standards before it) has a robust technical reports publishing arm. NIST technical reports are published in several dozen series, which cover a wide range of topics, from computer technology to construction to aspects of standardization including weights, measures and reference data. [ 56 ]  In addition to technical reports, NIST scientists publish many journal and conference papers each year; an database of these, along with more recent technical reports, can be found on the NIST website. [ 57 ]"
  },
  {
    "id": 136,
    "title": "SMART Information Retrieval System",
    "content": "The  SMART (System for the Mechanical Analysis and Retrieval of Text) Information Retrieval System  is an  information retrieval  system developed at  Cornell University  in the 1960s. [ 1 ]  Many important concepts in information retrieval were developed as part of research on the SMART system, including the  vector space model ,  relevance feedback , and  Rocchio classification . Gerard Salton  led the group that developed SMART.  Other contributors included  Mike Lesk . The SMART system also provides a set of corpora, queries and reference rankings, taken from different subjects, notably To the legacy of the SMART system belongs the so-called SMART triple notation, a mnemonic scheme for denoting  tf-idf  weighting variants in the vector space model. The mnemonic for representing a combination of weights takes the form  ddd.qqq , where the first three letters represents the term weighting of the collection document vector and the second three letters represents the term weighting for the query document vector. For example,  ltc.lnn  represents the  ltc  weighting applied to a collection document and the  lnn  weighting applied to a query document. The following tables establish the SMART notation: [ 2 ] The gray letters in the first, fifth, and ninth columns are the scheme used by Salton and Buckley in their 1988 paper. [ 4 ]  The bold letters in the second, sixth, and tenth columns are the scheme used in experiments reported thereafter. This  software-engineering -related article is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 137,
    "title": "MEDLINE",
    "content": "MEDLINE  (Medical Literature Analysis and Retrieval System Online, or MEDLARS Online) is a  bibliographic database  of life sciences and biomedical information. It includes bibliographic information for articles from  academic journals  covering  medicine ,  nursing ,  pharmacy ,  dentistry ,  veterinary medicine , and  health care . MEDLINE also covers much of the literature in  biology  and  biochemistry , as well as fields such as  molecular evolution . Compiled by the  United States National Library of Medicine  (NLM), MEDLINE is freely available on the  Internet  and searchable via  PubMed  and NLM's National Center for Biotechnology Information's  Entrez  system. MEDLARS (Medical Literature Analysis and Retrieval System) is a computerised biomedical  bibliographic  retrieval system. It was launched by the  National Library of Medicine  in 1964 and was the first large-scale, computer-based, retrospective search service available to the general public. [ 1 ] Since 1879, the National Library of Medicine has published  Index Medicus , a monthly guide to medical articles in thousands of journals. The huge volume of bibliographic citations was manually compiled. In 1957 the staff of the NLM started to plan the mechanization of the  Index Medicus , prompted by a desire for a better way to manipulate all this information, not only for  Index Medicus  but also to produce subsidiary products. By 1960 a detailed specification was prepared, and by the spring of 1961,  request for proposals  were sent out to 72 companies to develop the system. As a result, a contract was awarded to the  General Electric Company . A  Minneapolis-Honeywell 800  computer, which was to run MEDLARS, was delivered to the NLM in March 1963, and  Frank Bradway Rogers  (Director of the NLM 1949 to 1963) said at the time, \"..If all goes well, the January 1964 issue of  Index Medicus  will be ready to emerge from the system at the end of this year. It may be that this will mark the beginning of a new era in medical bibliography.\" MEDLARS cost $3 million to develop, and at the time of its completion in 1964, no other publicly available, fully operational electronic storage and retrieval system of its magnitude existed. The original computer configuration operated from 1964 until its replacement by MEDLARS II in January 1975. [ 2 ] [ 3 ] [ 4 ] In late 1971, an online version called MEDLINE (\"MEDLARS Online\") became available as a way to do online searching of MEDLARS from remote medical libraries. [ 5 ]  This early system covered 239 journals and boasted that it could support as many as 25 simultaneous online users (remotely logged in from distant medical libraries) at one time. [ 6 ]  However, this system remained primarily in the hands of libraries, with researchers able to submit pre-programmed search tasks to librarians and obtain results on printouts, but rarely able to interact with the NLM computer output in real-time. This situation continued through the beginning of the 1990s and the rise of the  World Wide Web . In 1996, soon after most home computers began automatically bundling efficient  web browsers , a free public version of MEDLINE was deployed. This system, called  PubMed , was offered to the general online user in June 1997, when MEDLINE searches via the Web were demonstrated. [ 6 ] In May 2022, the database contained more than 34 million records [ 7 ]  from 5,639 [ needs update ]  selected publications [ 8 ]  covering biomedicine and health from 1781 to the present. [ timeframe? ]  Originally, the database covered articles starting from 1965, but this has been enhanced, and records as far back as 1781 are now available within the main index. The database is freely accessible on the Internet via the PubMed interface, and new citations are added Tuesday through Saturday. For citations added during 1995-2003, about 48% are for cited articles published in the U.S., about 88% are published in English (overall about 84% [ 9 ] ), and about 76% have English abstracts written by authors of the articles. Being an aggregated source, the PubMed database suffers from multi-source problems such as inconsistent representations from the upstream data providers. [ 9 ] MEDLINE uses  Medical Subject Headings  (MeSH) for information retrieval. Engines designed to search MEDLINE (such as Entrez and PubMed) generally use a  Boolean expression  combining MeSH terms, words in the abstract and title of the article, author names, date of publication, etc. Entrez and PubMed can also find articles similar to a given one based on a mathematical scoring system that takes into account the similarity of word content of the abstracts and titles of two articles. [ 10 ] MEDLINE added a \"publication type\" term for \"randomized controlled trial\" in 1991 and a MESH subset \"systematic review\" in 2001. [ 11 ] MEDLINE functions as an important resource for biomedical researchers and  journal clubs  from all over the world. Along with the  Cochrane Library  and a number of other databases, MEDLINE facilitates  evidence-based medicine . [ 12 ] [ 13 ] [ 14 ]  Most  systematic review  articles published presently build on extensive searches of MEDLINE to identify articles that might be useful in the review. [ 12 ] [ 13 ]  MEDLINE influences researchers in their choice of journals in which to publish. [ 14 ] More than 5,200 biomedical journals are indexed in MEDLINE. [ 12 ]  New journals are not included automatically or immediately. Several criteria for selection are applied. [ 15 ]  Selection is based on the recommendations of a panel, the Literature Selection Technical Review Committee, based on the scientific scope and quality of a journal. [ 16 ]  The Journals Database (one of the Entrez databases) contains information, such as its name abbreviation and publisher, about all journals included in Entrez, including PubMed. [ 17 ]  Journals that no longer meet the criteria are removed. [ 18 ]  Being indexed in MEDLINE gives a non-predatory identity to a journal. [ 19 ] [ 20 ] [ 21 ] PubMed usage has been on the rise since 2008. In 2011, PubMed/MEDLINE was searched 1.8 billion times, up from 1.6 billion searches in the previous year. [ 22 ] A service such as MEDLINE strives to balance  usability  with power and comprehensiveness. In keeping with the fact that MEDLINE's primary user community is professionals ( medical scientists ,  health care providers ), searching MEDLINE effectively is a learned skill; untrained users are sometimes frustrated with the large numbers of articles returned by simple searches. Counterintuitively, a search that returns thousands of articles is not guaranteed to be comprehensive. Unlike using a typical Internet search engine, PubMed searching MEDLINE requires a little investment of time. Using the MeSH database to define the subject of interest is one of the most useful ways to improve the quality of a search. Using MeSH terms in conjunction with limits (such as publication date or publication type), qualifiers (such as adverse effects or prevention and control), and text-word searching is another. Finding one article on the subject and clicking on the \"Related Articles\" link to get a collection of similarly classified articles can expand a search that otherwise yields few results. For  lay users  who are trying to learn about health and medicine topics, the NIH offers  MedlinePlus ; thus, although such users are still free to search and read the medical literature themselves (via  PubMed ), they also have some help with  curating  it into something comprehensible and practically applicable for patients and family members."
  },
  {
    "id": 138,
    "title": "J. C. R. Licklider",
    "content": "Joseph Carl Robnett Licklider  ( / ˈ l ɪ k l aɪ d ər / ; March 11, 1915 – June 26, 1990), known simply as  J. C. R.  or \" Lick \", was an American  psychologist [ 3 ]  and  computer scientist  who is considered to be among the most prominent figures in  computer science  development and general  computing history . He is particularly remembered for being one of the first to foresee modern-style  interactive computing  and its application to all manner of activities; and also as an  Internet pioneer  with an early vision of a worldwide computer network long before it was built. He did much to initiate this by funding research that led to significant advances in computing technology, including today's canonical  graphical user interface , and the  ARPANET , which is the direct predecessor of the  Internet . Robert Taylor , founder of  Xerox PARC 's Computer Science Laboratory and  Digital Equipment Corporation 's  Systems Research Center , noted that \"most of the significant advances in computer technology—including the work that my group did at Xerox PARC—were simply extrapolations of Lick's vision. They were not really new visions of their own. So he was really the father of it all\". [ 4 ] Licklider was born on March 11, 1915, in  St. Louis, Missouri . [ 5 ]  He was the  only child  of Joseph Parron Licklider, a Baptist minister, and Margaret Robnett Licklider. [ 6 ]  Despite his father's religious background, he was not religious in later life. [ 7 ] He studied at  Washington University in St. Louis , where he received a  B.A.  with a triple major in  physics ,  mathematics , and  psychology  in 1937 [ 8 ] [ 9 ]  and an  M.A.  in psychology in 1938. He received a  Ph.D.  in  psychoacoustics  from the  University of Rochester  in 1942 as well as a Doctorate in Psychology from the University of Rochester, that same year. Thereafter he worked at  Harvard University  as a research fellow and lecturer in the Psycho-Acoustic Laboratory from 1943 to 1950. He became interested in  information technology , and moved to  MIT  in 1950 as an associate professor, where he served on a committee that established the  MIT Lincoln Laboratory  and a psychology program for engineering students. While at MIT, Licklider was involved in the  SAGE project  as head of the team concerned with  human factors . [ 10 ]  In 1957, he received the Franklin V. Taylor Award from the Society of Engineering Psychologists. In 1958, he was elected President of the  Acoustical Society of America , and in 1990 he received the Commonwealth Award for Distinguished Service. [ 11 ] Licklider left MIT to become a vice president at  Bolt Beranek and Newman  in 1957. He learned about  time-sharing  from  Christopher Strachey  at a UNESCO-sponsored conference on Information Processing in Paris in 1959. [ 12 ] [ 13 ]  At BBN he developed the  BBN Time-Sharing System  and conducted the first public demonstration of time-sharing. [ 14 ] In October 1962, Licklider was appointed head of the  Information Processing Techniques Office  (IPTO) at  ARPA , the  United States Department of Defense  Advanced Research Projects Agency, [ 15 ]  an appointment he kept through July 1964. [ 16 ] [ 17 ]  In April 1963, he sent a memo to his colleagues in outlining the early challenges presented in establishing a time-sharing network of computers with the software of that time. [ 18 ]  Ultimately his vision led to  ARPANet , the precursor of today's  Internet . [ 19 ] After serving as manager of information sciences, systems and applications at  IBM 's  Thomas J. Watson Research Center  in  Yorktown Heights, New York  from 1964 to 1967, Licklider rejoined MIT as a professor of electrical engineering in 1968. During this period, he concurrently served as director of  Project MAC  until 1971. [ 20 ]  Project MAC had produced the first computer time-sharing system,  CTSS , and one of the first  online  setups with the development of  Multics  (work on which commenced in 1964). Multics provided inspiration for some elements of the  Unix   operating system  developed at  Bell Labs  by  Ken Thompson  and  Dennis Ritchie  in 1970. [ 21 ] Following a second stint as IPTO director (1974–1975), his MIT faculty line was transferred to the Institute's Laboratory for Computer Science, where he was based for the remainder of his career. He was a founding member of  Infocom  in 1979, known for their  interactive fiction  computer games. [ 22 ]  He retired and became  professor emeritus  in 1985. He died in 1990 in  Arlington, Massachusetts ; [ 11 ]  his cremated remains are interred in  Mount Auburn Cemetery . In the  psychoacoustics  field, Licklider is most remembered for his 1951 \"Duplex Theory of Pitch Perception\", presented in a paper [ 23 ]  which has been cited hundreds of times, [ 24 ]  was reprinted in a 1979 book, [ 25 ]  and formed the basis for modern models of  pitch  perception. [ 26 ]  He was also the first to report  binaural unmasking  of speech. [ 27 ] While at MIT in the 1950s, Licklider worked on  Semi-Automatic Ground Environment  (SAGE), a  Cold War  project to create a computer-aided air defense system. The SAGE system included computers that collected and presented data to a human operator, who then chose the appropriate response. He worked as a human factors expert, which helped convince him of the great potential for human/computer interfaces. [ 28 ] Licklider became interested in  information technology  early in his career. His ideas foretold of graphical computing, point-and-click interfaces, digital libraries, e-commerce, online banking, and software that would exist on a network and migrate wherever it was needed. Much like  Vannevar Bush 's, Licklider's contribution to the development of the  Internet  consists of ideas, not inventions. He foresaw the need for networked  computers  with easy user interfaces. Licklider was instrumental in conceiving, funding and managing the research that led to modern personal computers and the Internet. In 1960 his seminal paper on \" Man-Computer Symbiosis \" [ 29 ]  foreshadowed interactive computing, and he went on to fund early efforts in time-sharing and application development, most notably the work of  Douglas Engelbart , who founded the  Augmentation Research Center  at  Stanford Research Institute  and created the famous  On-Line System  where the  computer mouse  was invented. He also did some seminal early work for the Council on Library Resources, imagining what libraries of the future might look like, [ 30 ]  which he describes as \"thinking centers\" in his 1960 paper. [ 29 ] In \" Man-Computer Symbiosis \", Licklider in 1960 outlined the need for simpler interaction between computers and computer users. [ 31 ]  Licklider has been credited as an early pioneer of  cybernetics  and  artificial intelligence  (AI), [ 32 ]  but unlike other AI practitioners, he never felt sure that men would be replaced by computer-based beings. As he wrote in the article: \"Men will set the goals, formulate the hypotheses, determine the criteria, and perform the evaluations. Computing machines will do the routinizable work that must be done to prepare the way for insights and decisions in technical and scientific thinking\". He goes on to write in the same article: \"In short, it seems worthwhile to avoid argument with (other) enthusiasts for artificial intelligence by conceding dominance in the distant future of cerebration to machines alone\". [ 29 ]  This approach, focusing on effective use of information technology in augmenting human intelligence, is sometimes called  Intelligence amplification  (IA).  Peter Highnam , DARPA director in 2020, focused on  human-machine partnership  as a long-term goal and guiding light ever since Licklider's 1960 publication. [ 33 ] During his time as director of ARPA's  Information Processing Techniques Office  (IPTO) from 1962 to 1964, he funded  Project MAC  at MIT. A large  mainframe computer  was designed to be shared by up to 30 simultaneous users, each sitting at a separate  \"typewriter terminal\" . He also funded similar projects at  Stanford University ,  UCLA ,  UC Berkeley  (called  Project Genie ), and the  AN/FSQ-32  at  System Development Corporation .\nThis time-sharing technology later developed to become what today are known as  servers . Licklider played a similar role in conceiving of and funding early networking research. He formulated the earliest ideas of a global computer network in August 1962 at BBN, in a series of memos discussing the \" Intergalactic Computer Network \" concept. These ideas contained almost everything that the Internet is today, including  cloud computing . [ 34 ] While at IPTO he convinced  Ivan Sutherland ,  Bob Taylor , and  Lawrence G. Roberts  that an all-encompassing computer network was a very important concept. He met with  Donald Davies  in 1965 and inspired his interest in  data communications . [ 35 ] [ 36 ] In 1967 Licklider submitted the paper \"Televistas: Looking ahead through side windows\" to the  Carnegie Commission on Educational Television . [ 37 ]  This paper describes a radical departure from the \"broadcast\" model of television. Instead Licklider advocates for a two-way communications network. The Carnegie Commission led to the creation of the  Corporation for Public Broadcasting . Although the Commission's report explains that \"Dr. Licklider's paper was completed after the Commission had formulated its own conclusions,\" President Johnson said at the signing of the  Public Broadcasting Act of 1967 , \"So I think we must consider new ways to build a great network for knowledge—not just a broadcast system, but one that employs every means of sending and of storing information that the individual can use\". [ 38 ] His 1968 paper  The Computer as a Communication Device  illustrates his vision of network applications and predicts the use of computer networks to support communities of common interest and collaboration without regard to location. [ 39 ] In the same 1968 paper, J. C. R. Licklider and Robert W. Taylor wrote, \"Take any problem worthy of the name, and you find only a few people who can contribute effectively to its solution. Those people must be brought into close intellectual partnership so that their ideas can come into contact with one another. But bring these people together physically in one place to form a team, and you have trouble, for the most creative people are often not the best team players, and there are not enough top positions in a single organization to keep them all happy. Let them go their separate ways, and each creates his own empire, large or small, and devotes more time to the role of emperor than to the role of problem solver.  The principals still get together at meetings. They still visit one another. But the time scale of their communication stretches out, and the correlations among mental models degenerate between meetings so that it may take a year to do a week's communicating. There has to be some way of facilitating communication among people wit bout [sic] [without] bringing them together in one place.\" [ 39 ]  (Evan Herbert edited the article and acted as intermediary during its writing between Licklider in Boston and Taylor in Washington.) The  Licklider Transmission Protocol  is named after him. Licklider wrote numerous articles and lectures, and one book: Articles, a selection:"
  },
  {
    "id": 139,
    "title": "Don R. Swanson",
    "content": "Don R. Swanson  (October 10, 1924 – November 18, 2012) was an American information scientist, most known for his work in  literature-based discovery  in the biomedical domain. His particular method has been used as a model for further work, and is often referred to as  Swanson linking . He was an investigator in the  Arrowsmith System  project, [ 1 ]  which seeks to determine meaningful links between  Medline  articles to identify previously undiscovered public knowledge. He had been professor emeritus of the  University of Chicago  since 1996, and remained active in a post-retirement appointment until his health began to decline in 2009. Swanson was born in  Los Angeles  on October 10, 1924, the son of Harry Windfield and Grace Clara (Sandstrom) Swanson. He served with the  United States Navy Reserve  from 1942 to 1926, [ 2 ]  and received his B.S. in Physics at  Caltech ,  Pasadena, California  in 1945. He gained his M.A from  Rice Institute ,  Houston, Texas , two years later, and then a PhD in Theoretical Physics from the  University of California at Berkeley  in 1952. [ 3 ] From 1952 to 1954 Swanson worked as a computer systems analyst at  Hughes Aircraft Company  Research and Development Laboratories in  Culver City, California . In 1955 had joined  Ramo-Wooldridge Corporation . [ 2 ]  Initially working as a research scientist, [ 4 ]  by 1959 he was manager of the Synthetic Intelligence Dept. at Ramo-Wooldridge. There he led a project contracted to the  Council on Library Resources , with  Noam Chomsky  and Paul L. Garvin as linguistic advisors, to investigate  machine indexing  of 'a small experimental library of scientific text (ca. 300,000 words)'. [ 5 ]  Swanson collaborated further with Garvin on Russian-English  machine translation , considering problems of  polysemy , in work funded by the  Rome Air Development Center . In December 1959 he attended the annual meeting of the  American Anthropological Association , speaking on 'Engineering Aspects' in a symposium on the uses of data processing equipment in anthropology. [ 6 ]  By 1961 he was a member of the  National Science Foundation 's Science Information Council. [ 7 ] In 1963 Swanson joined the  University of Chicago  as a professor in the  Graduate School of Library Science . He also served as dean of the graduate school from 1963 to 1972, from 1977 to 1979 and again from 1987 to 1989. From 1972 to 1976 he was a research fellow at the  Chicago Institute for Psychoanalysis . [ 2 ] In the 1980s Swanson pioneered  literature-based discovery  in the biomedical domain, building the  Arrowsmith System  around a discovery method that has since become known as  Swanson linking . [ 8 ]   He hypothesized that the combination of two separately published results indicating an A-B relationship and a B-C relationship are evidence of an unknown or unexplored A-C relationship. He used this to propose  fish oil  as a treatment for  Raynaud syndrome , due to their shared relationship with  blood viscosity . [ 9 ] From 1992 to 1996 Swanson was professor of the biosciences collection division and the humanities division at Chicago. In 1996 he became professor emeritus. [ 2 ] In 2000, Swanson was awarded the  ASIST  Award of Merit, the highest honor of the society, for his \"lifetime achievements in research and scholarship.\" [ 10 ] [ 11 ]"
  },
  {
    "id": 140,
    "title": "Frederick Wilfrid Lancaster",
    "content": "Frederick Wilfrid  (\" Wilf \") [ 1 ]   Lancaster  (September 4, 1933 – August 25, 2013) [ 2 ]  was a British-American  information scientist . He immigrated to the US in 1959 and worked as information specialist for the  National Library of Medicine  in  Bethesda, Maryland , from 1965 to 1968. He was a professor at the  University of Illinois, Urbana , from 1972 to 1992 and professor  emeritus  from 1992 to 2013. He continued as an honored scholar after retirement speaking on the evolution of librarianship in the 20th and 21st century. [ 3 ] Lancaster made notable achievements with early  online retrieval systems , including evaluation studies of  MEDLARS . He published broadly in  library and information science  over a period of four decades and continuously emerged as a visionary leader in the field, where research, writing, and teaching earned him the highest honors in the profession. Lancaster excelled at many fronts: as scholar, educator, mentor, and writer. Lancaster graduated as an associate of the  British Library Association  from the  University of Northumbria  at Newcastle, England, in 1955 and was named a fellow of the Library Association of Great Britain in 1969. He began his professional career as a senior assistant at the  Newcastle-upon-Tyne  Public Libraries. He immigrated in 1959 to  Akron, Ohio  to become the senior librarian for science and technology at the  Akron Public Library . [ 4 ] Lancaster worked as the technical librarian for  Babcock & Wilcox  from 1960 until he returned to the U.K. in 1962 to become a senior research assistant at  ASLIB  in London. In 1964, Lancaster returned to the U.S. where he was integrally involved in the design and management of MEDLARS, the National Library of Medicine's computerized bibliographic retrieval system for articles in academic journals in medicine and allied health professions. Lancaster was appointed associate professor and director of the biomedical librarianship program at the University of Illinois-Urbana in 1970. In 1972 he was promoted to professor at the university where he did research and taught until 1992. He became professor emeritus in 1992. [ 5 ] Lancaster, was the most cited author, during the 1970s and early 1990s, in the discipline of information science and had broad intellectual influence. [ 6 ]  Over a period of four decades Lancaster was a visionary leader in the field of library and information \nscience. [ 7 ] His evaluation of the MEDLARS Demand Search Service in 1966 and 1967 was an important landmark in the evaluation of a computer-based retrieval system. It was the first application of recall and precision measures in a large, operational database setting. [ 8 ] Information Retrieval Online  (Lancaster & Fayen, 1973) was named ASIS Best Information Science Book in 1974. It was declared a \"major milestone in the literature of online systems\" that \"functioned for years as a textbook, handbook, and encyclopedia on all aspects of online retrieval systems\". [ 9 ] Lancaster was a great analytical thinker and a great synthetic thinker and writer. His work on measurement and evaluation is a sound demonstration of both of these abilities. His books,  The Measurement and Evaluation of Library Services  and  If You Want to Evaluate Your Library , have practical applications for those interested in engaging in evaluation studies. [ 10 ] Lancaster participated in many international conferences and lecture series in Australia; Brazil; Canada; China; Colombia; Costa Rica; Denmark; Egypt; England; Finland; France; Germany; Guatemala; Hong Kong; India; Israel; Italy; Mexico; Namibia; the Netherlands; Norway; Poland; Portugal; Singapore; South Africa; Spain; Sri Lanka; Sweden; Syria; Taiwan; Tunisia; Turkey; and the West Indies. In 1975 he edited the proceedings of the NATO Advanced Study Institute on the evaluation and scientific management of libraries and information centers. [ 11 ] He was a  Fulbright professor  at the  Indian Statistical Institute  (1991); in Denmark at the Royal School of Librarianship, (1985); and in Brazil at the Instituto Brasileiro de Informacao em Ciencia e Technologia, (1975). [ 12 ] In 1980 Lancaster was honored with the first Outstanding Teacher Award by the  American Society for Information Science . [ 13 ]  He \"introduced new approaches to education for information science expressed in a range of courses that were designed to prepare his students for a new kind of professional world that was in the making and to developments in which he was especially attuned\". [ 14 ] Lancaster taught courses in information retrieval, bibliometrics, bibliographic organization, and the evaluation of library and information services. He directed numerous doctoral dissertations, served on many doctoral committees and was especially supportive of international students. A list of dissertation committees on which he served is included in the 2008  Festschrift  published in his honor. [ 15 ] Lancaster edited the journal  Library Trends  from 1986 to 2006. [ 16 ] In a family tribute Cesaria Lancaster (Maria Cesaria Volpe), who married Frederick Wilfrid Lancaster in 1961, and his six children provided warm reflections of their life together. [ 17 ] At the time of his death he had thirteen grandchildren. [ 18 ]"
  },
  {
    "id": 141,
    "title": "Wayback Machine",
    "content": "The  Wayback Machine  is a  digital archive  of the  World Wide Web  founded by the  Internet Archive , an  American nonprofit organization  based in  San Francisco ,  California . Created in 1996 and launched to the public in 2001, it allows users to go \"back in time\" to see how websites looked in the past. Its founders,  Brewster Kahle  and  Bruce Gilliat , developed the Wayback Machine to provide \"universal access to all knowledge\" by preserving archived copies of defunct web pages. [ 2 ] Launched on May 10, 1996, the Wayback Machine had saved more than 38.2 billion web pages by the end of 2009. As of November 2024, the Wayback Machine has archived more than 916 billion web pages and well over 100  petabytes  of data. [ 3 ] [ 4 ] The Internet Archive began archiving  cached  web pages in 1996. One of the earliest known pages was archived on May 10, 1996, at  2:08   p.m.  ( UTC ). [ 5 ] Internet Archive founders  Brewster Kahle  and  Bruce Gilliat  launched the Wayback Machine in  San Francisco ,  California , [ 6 ]  in October 2001, [ 7 ] [ 8 ]  primarily to address the problem of web content vanishing whenever it gets changed or when a website is shut down. [ 9 ]  The service enables users to see archived versions of  web pages  across time, which the archive calls a \"three-dimensional index\". [ 10 ]  Kahle and Gilliat created the machine hoping to archive the entire Internet and provide \"universal access to all knowledge\". [ 11 ]  The name \"Wayback Machine\" is a reference to a fictional time-traveling device in the animated cartoon  The Adventures of Rocky and Bullwinkle and Friends  from the 1960s. [ 12 ] [ 13 ] [ 14 ]  In a segment of the cartoon entitled \"Peabody's Improbable History\", the characters  Mister Peabody  and Sherman use the \" Wayback Machine \" to travel back in time to witness and participate in famous historical events. [ 15 ] From 1996 to 2001, the information was kept on digital tape, with Kahle occasionally allowing researchers and scientists to tap into the \"clunky\"  database . [ 16 ]  When the archive reached its fifth anniversary in 2001, it was unveiled and opened to the public in a ceremony at the  University of California, Berkeley . [ 17 ]  By the time the Wayback Machine launched, it already contained over 10 billion archived pages. [ 18 ]  The data is stored on the Internet Archive's large cluster of  Linux  nodes. [ 11 ]  It revisits and archives new versions of websites on occasion (see technical details below). [ 19 ]  Sites can also be captured manually by entering a website's  URL  into the search box, provided that the website allows the Wayback Machine to \" crawl \" it and save the data. [ 20 ] On October 30, 2020, the Wayback Machine began fact-checking content. [ 21 ]  As of January 2022, domains of  ad servers  are disabled from capturing. [ 22 ] In May 2021, for Internet Archive's 25th anniversary, the Wayback Machine introduced the \"Wayforward Machine\" which allows users to \"travel to the Internet in 2046, where knowledge is under  siege \". [ 23 ] [ 24 ] The Wayback Machine's software has been developed to \" crawl \" the Web and download all publicly accessible information and data files on webpages, the  Gopher  hierarchy, the  Netnews  (Usenet) bulletin board system, and downloadable software. [ 25 ]  The information collected by these \"crawlers\" does not include all the information available on the Internet, since much of the data is restricted by the publisher or stored in databases that are not accessible. To overcome inconsistencies in partially cached websites, Archive-It.org was developed in 2005 by the Internet Archive as a means of allowing institutions and content creators to voluntarily harvest and preserve collections of digital content, and create digital archives. [ 26 ] Crawls are contributed from various sources, some imported from third parties and others generated internally by the Archive. [ 19 ]  For example, crawls are contributed by the  Sloan Foundation  and  Alexa , crawls run by Internet Archive on behalf of  NARA  and the  Internet Memory Foundation , mirrors of  Common Crawl . [ 19 ]  The \"Worldwide Web Crawls\" have been running since 2010 and capture the global Web. [ 19 ] [ 27 ]  In September 2020, the Internet Archive announced a partnership with  Cloudflare  – an American  content delivery network  service provider – to automatically index websites served via its \"Always Online\" services. [ 28 ] Documents and resources are stored with time stamp URLs such as  20241203072243 . Pages' individual resources such as images and style sheets and scripts, as well as outgoing  hyperlinks , are linked to with the time stamp of the currently viewed page, so they are redirected automatically to their individual captures that are the closest in time. [ 29 ] The frequency of snapshot captures varies per website. [ 19 ]  Websites in the \"Worldwide Web Crawls\" are included in a \"crawl list\", with the site archived once per crawl. [ 19 ]  A crawl can take months or even years to complete, depending on size. [ 19 ]  For example, \"Wide Crawl Number 13\" started on January 9, 2015, and completed on July 11, 2016. [ 30 ]  However, there may be multiple crawls ongoing at any one time, and a site might be included in more than one crawl list, so how often a site is crawled varies widely. [ 19 ] A \"Save Page Now\" archiving feature was made available in October 2013, [ 31 ]  accessible on the lower right of the Wayback Machine's main page. [ 32 ]  Once a target URL is entered and saved, the web page will become part of the Wayback Machine. [ 31 ]  Through the Internet address web.archive.org, [ 33 ]  users can upload to the Wayback Machine a large variety of contents, including  PDF  and  data compression  file formats. The Wayback Machine creates a permanent local URL of the upload content, that is accessible in the web, even if not listed while searching in the https://archive.org official website. [ jargon ] Starting in October 2019, users were  limited  to 15 archival requests and retrievals per minute. [ 34 ] As technology has developed over the years, the storage capacity of the Wayback Machine has grown. In 2003, after only two years of public access, the Wayback Machine was growing at a rate of 12 terabytes per month. The data is stored on  PetaBox  rack systems custom designed by Internet Archive staff. The first 100TB rack became fully operational in June 2004, although it soon became clear that they would need much more storage than that. [ 35 ] [ 36 ] The Internet Archive migrated its customized storage architecture to  Sun Open Storage  in 2009, and hosts a new data centre in a  Sun Modular Datacenter  on  Sun Microsystems ' California campus. [ 37 ]  As of 2009 [update] , the Wayback Machine contained approximately three  petabytes  of data and was growing at a rate of 100  terabytes  each month. [ 38 ] A new, improved version of the Wayback Machine, with an updated interface and a fresher index of archived content, was made available for public testing in 2011, where captures appear in a calendar layout with circles whose width visualizes the number of crawls each day, but no marking of duplicates with asterisks or an advanced search page. [ 39 ] [ 40 ]  A top  toolbar  was added to facilitate navigating between captures. A bar chart visualizes the frequency of captures per month over the years. [ 41 ]  Features like \"Changes\", \"Summary\", and a graphical site map were added subsequently. In March that year, it was said on the Wayback Machine forum that \"the Beta of the new Wayback Machine has a more complete and up-to-date index of all crawled materials into 2010, and will continue to be updated regularly. The index driving the classic Wayback Machine only has a little bit of material past 2008, and no further index updates are planned, as it will be phased out this year.\" [ 42 ]  Also in 2011, the Internet Archive installed their sixth pair of PetaBox racks which increased the Wayback Machine's storage capacity by 700 terabytes. [ 43 ] In January 2013, the company announced a milestone of 240 billion URLs. [ 44 ] In October 2013, the company introduced the \"Save a Page\" feature, which allows any Internet user to archive the contents of a URL, and quickly generates a  permanent link  unlike the preceding  liveweb  feature. [ 45 ] [ 46 ] In December 2014, the Wayback Machine contained 435  billion  web pages—almost nine petabytes of data, and was growing at about 20 terabytes a week. [ 18 ] [ 47 ] [ 48 ] In July 2016, the Wayback Machine reportedly contained around 15 petabytes of data. [ 49 ]  In October 2016, it was announced that the way web pages are counted would be changed, resulting in the decrease of the archived pages counts shown. Embedded objects such as pictures, videos, style sheets, JavaScripts are no longer counted as a \"web page\", whereas HTML, PDF, and plain text documents remain counted. [ 50 ] In September 2018, the Wayback Machine contained over 25 petabytes of data. [ 51 ] [ 52 ]  As of December 2020, the Wayback Machine contained over 70 petabytes of data. [ 53 ] The Wayback Machine service offers three public APIs, SavePageNow, Availability, and CDX. [ 56 ]  SavePageNow can be used to archive web pages. Availability API for checking the archive availability status for a web page, [ 57 ]  checking whether an archive for the web page exists or not. CDX API is for complex querying, filtering, and analysis of captured data. [ 58 ] [ 59 ] Historically, the Wayback Machine has respected the  robots exclusion standard  (robots.txt) in determining if a website would be crawled – or if already crawled, if its archives would be publicly viewable. Website owners had the option to opt out of Wayback Machine through the use of robots.txt. It applied robots.txt rules retroactively; if a site blocked the Internet Archive, any previously archived pages from the domain were immediately rendered unavailable as well. In addition, the Internet Archive stated that \"Sometimes, a website owner will contact us directly and ask us to stop crawling or archiving a site. We comply with these requests.\" [ 60 ]  In addition, the website says: \"The Internet Archive is not interested in preserving or offering access to Web sites or other internet documents of persons who do not want their materials in the collection.\" [ 61 ] [ 62 ] On April 17, 2017, reports surfaced of sites that had gone defunct and became  parked domains  that were using robots.txt to exclude themselves from search engines, resulting in them being inadvertently excluded from the Wayback Machine. [ 63 ]  Following this, the Internet Archive changed the policy to require an explicit exclusion request to remove sites from the Wayback Machine. [ 29 ] Wayback's retroactive exclusion policy is based in part upon  Recommendations for Managing Removal Requests and Preserving Archival Integrity , known as  The Oakland Archive Policy , published by the School of Information Management and Systems at  University of California, Berkeley  in 2002, which gives a website owner the right to block access to the site's archives. [ 64 ]  Wayback has complied with this policy to help avoid expensive litigation. [ 65 ] The Wayback retroactive exclusion policy began to relax in 2017, when it stopped honoring robots on U.S. government and military web sites for both crawling and displaying web pages. As of April 2017, Wayback is ignoring robots.txt more broadly, not just for U.S. government websites. [ 66 ] [ 67 ] [ 68 ] [ 69 ] From its public launch in 2001, the Wayback Machine has been studied by scholars both for the ways it stores and collects data as well as for the actual pages contained in its archive. As of 2013, scholars had written about 350 articles on the Wayback Machine, mostly from the  information technology ,  library science , and  social science  fields. Social science scholars have used the Wayback Machine to analyze how the development of websites from the mid-1990s to the present has affected the company's growth. [ 18 ] When the Wayback Machine archives a page, it usually includes most of the hyperlinks, keeping those links active when they just as easily could have been broken by the Internet's instability. Researchers in India studied the effectiveness of the Wayback Machine's ability to save hyperlinks in online scholarly publications and found that it saved slightly more than half of them. [ 70 ] \"Journalists use the Wayback Machine to view dead websites, dated news reports, and changes to website contents. Its content has been used to hold politicians accountable and expose battlefield lies.\" [ 71 ]  In 2014, an archived social media page of  Igor Girkin , a separatist rebel leader in Ukraine, showed him boasting about his troops having shot down a suspected Ukrainian military airplane before it became known that the plane actually was a civilian Malaysian Airlines jet ( Malaysia Airlines Flight 17 ), after which he deleted the post and blamed Ukraine's military for downing the plane. [ 71 ] [ 72 ]  In 2017, the  March for Science  originated from a discussion on  Reddit  that indicated someone had visited Archive.org and discovered that all references to  climate change  had been deleted from the White House website. In response, a user commented, \"There needs to be a Scientists' March on Washington\". [ 73 ] [ 74 ] [ 75 ] The site is used heavily for verification, providing access to references and content creation by  Wikipedia editors . [ 76 ]  When new URLs are added to Wikipedia, the Internet Archive has been archiving them. [ 76 ] In September 2020, a partnership was announced with  Cloudflare  to automatically archive websites served via its \"Always Online\" service, which will also allow it to direct users to its copy of the site if it cannot reach the original host. [ 77 ] In 2014, there was a six-month lag time between when a website was crawled and when it became available for viewing in the Wayback Machine. [ 78 ]  As of 2024, the lag time is 3 to 10 hours. [ 29 ]  The Wayback Machine offers only limited search facilities. Its \"Site Search\" feature allows users to find a site based on words describing the site, rather than words found on the web pages themselves. [ 79 ] The Wayback Machine does not include every web page ever made due to the limitations of its web crawler. The Wayback Machine cannot completely archive web pages that contain interactive features such as Flash platforms and forms written in JavaScript and  progressive web applications , because those functions require interaction with the host website. This means that, since approximately July 9, 2013, the Wayback Machine has been unable to display YouTube comments when saving videos' watch pages, as, according to the Archive Team, comments are no longer \"loaded within the page itself.\" [ 80 ]  The Wayback Machine's web crawler has difficulty extracting anything not coded in HTML or one of its variants, which can often result in broken hyperlinks and missing images. Due to this, the web crawler cannot archive \"orphan pages\" that are not linked to by other pages. [ 79 ] [ 81 ]  The Wayback Machine's crawler only follows a predetermined number of hyperlinks based on a preset depth limit, so it cannot archive every hyperlink on every page. [ 27 ] In a 2009 case,  Netbula, LLC v. Chordiant Software Inc. , defendant Chordiant filed a motion to compel Netbula to disable the  robots.txt  file on its website that was causing the Wayback Machine to retroactively remove access to previous versions of pages it had archived from Netbula's site, pages that Chordiant believed would support its case. [ 82 ] Netbula objected to the motion on the ground that defendants were asking to alter Netbula's website and that they should have subpoenaed Internet Archive for the pages directly. [ 83 ]  An employee of Internet Archive filed a sworn statement supporting Chordiant's motion, however, stating that it could not produce the web pages by any other means \"without considerable burden, expense and disruption to its operations.\" [ 82 ] Magistrate Judge Howard Lloyd in the Northern District of California, San Jose Division, rejected Netbula's arguments and ordered them to disable the robots.txt blockage temporarily in order to allow Chordiant to retrieve the archived pages that they sought. [ 82 ] In an October 2004 case,  Telewizja Polska USA, Inc.  v. Echostar Satellite , No. 02 C 3293, 65 Fed. R. Evid. Serv. 673 (N.D. Ill. October 15, 2004), a litigant attempted to use the Wayback Machine archives as a source of admissible evidence, perhaps for the first time. Telewizja Polska is the provider of  TVP Polonia  and  EchoStar  operates the  Dish Network . Prior to the trial proceedings, EchoStar indicated that it intended to offer Wayback Machine snapshots as proof of the past content of Telewizja Polska's website. Telewizja Polska brought a motion  in limine  to suppress the snapshots on the grounds of  hearsay  and unauthenticated source, but Magistrate Judge Arlander Keys rejected Telewizja Polska's assertion of hearsay and denied TVP's motion  in limine  to exclude the evidence at trial. [ 84 ] [ 85 ]  At the trial, however, District Court Judge Ronald Guzman, the trial judge, overruled Magistrate Keys' findings, and held that neither the affidavit of the Internet Archive employee nor the underlying pages (i.e., the Telewizja Polska website) were admissible as evidence. Judge Guzman reasoned that the employee's affidavit contained both hearsay and inconclusive supporting statements, and the purported web page, printouts were not self-authenticating. [ 86 ] [ 87 ] The  United States Patent and Trademark Office  and the  European Patent Office  will accept date stamps from the Internet Archive as evidence of when a given Web page was accessible to the public. These dates are used to determine if a Web page is available as  prior art  for instance in examining a patent application. [ 88 ] There are technical limitations to archiving a website, and as a consequence, opposing parties in litigation can misuse the results provided by website archives. This problem can be exacerbated by the practice of submitting screenshots of web pages in complaints, answers, or expert witness reports when the underlying links are not exposed and therefore, can contain errors. For example, archives such as the Wayback Machine do not fill out forms and therefore, do not include the contents of non- RESTful  e-commerce databases in their archives. [ 89 ] In Europe, the Wayback Machine could be interpreted as violating  copyright  laws. Only the content creator can decide where their content is published or duplicated so the Archive would have to delete pages from its system upon request of the creator. [ 90 ]  The exclusion policies for the Wayback Machine may be found in the FAQ section of the site. [ 91 ] Some cases have been brought against the Internet Archive specifically for its Wayback Machine archiving efforts. In late 2002, the Internet Archive removed various sites that were critical of  Scientology  from the Wayback Machine. [ 92 ]  An error message stated that this was in response to a \"request by the site owner\". [ 93 ]  Later, it was clarified that lawyers from the  Church of Scientology  had demanded the removal and that the site owners did not want their material removed. [ 94 ] In 2003, Harding Earley Follmer & Frailey defended a client from a trademark dispute using the Archive's Wayback Machine. The attorneys were able to demonstrate that the claims made by the plaintiff were invalid, based on the content of their website from several years prior. The plaintiff, Healthcare Advocates, then amended their complaint to include the Internet Archive, accusing the organization of copyright infringement as well as violations of the  DMCA  and the  Computer Fraud and Abuse Act . Healthcare Advocates claimed that, since they had installed a  robots.txt  file on their website, even if after the initial lawsuit was filed, the Archive should have removed all previous copies of the plaintiff website from the Wayback Machine, however, some material continued to be publicly visible on Wayback. [ 95 ]  The lawsuit was settled out of court after Wayback fixed the problem. [ 96 ] Activist  Suzanne Shell  filed suit in December 2005, demanding Internet Archive pay her US$100,000 for archiving her website profane-justice.org between 1999 and 2004. [ 97 ] [ 98 ]  Internet Archive filed a  declaratory judgment  action in the  United States District Court for the Northern District of California  on January 20, 2006, seeking a judicial determination that Internet Archive did not violate Shell's  copyright . Shell responded and brought a  countersuit  against Internet Archive for archiving her site, which she alleges is in violation of her  terms of service . [ 99 ]  On February 13, 2007, a judge for the  United States District Court for the District of Colorado  dismissed all counterclaims except  breach of contract . [ 98 ]  The Internet Archive did not move to dismiss the  copyright infringement  claims that Shell asserted arose out of its copying activities, which would also go forward. [ 100 ] On April 25, 2007, Internet Archive and Suzanne Shell jointly announced the settlement of their lawsuit. [ 97 ]  The Internet Archive said it \"...has no interest in including materials in the Wayback Machine of persons who do not wish to have their Web content archived. We recognize that Ms. Shell has a valid and enforceable copyright in her Web site and we regret that the inclusion of her Web site in the Wayback Machine resulted in this litigation.\" Shell said, \"I respect the historical value of Internet Archive's goal. I never intended to interfere with that goal nor cause it any harm.\" [ 101 ] Between 2013 and 2016, a  pornographic actor  named Daniel Davydiuk tried to remove archived images of himself from the Wayback Machine's archive, first by sending multiple  DMCA requests  to the archive, and then by appealing to the  Federal Court of Canada . [ 102 ] [ 103 ] [ 104 ]  The images were removed from the website in 2017. In 2018, archives of  stalkerware application  FlexiSpy's website were removed from the Wayback Machine. The company claimed to have contacted the Internet Archive, presumably to remove the archives of its website. [ 105 ] Archive.org is  blocked in China . [ 106 ] [ 107 ] [ 108 ]  The Internet Archive was  blocked in its entirety in Russia  in 2015–16, ostensibly for hosting a Jihad outreach video. [ 71 ] [ 109 ] [ 110 ]  Since 2016, the website has been back, available in its entirety, although in 2016 Russian commercial lobbyists were suing the Internet Archive to ban it on copyright grounds. [ 111 ] In March 2015, it was published that security researchers became aware of the threat posed by the service's unintentional  hosting of malicious binaries  from archived sites. [ 112 ] [ 113 ] Alison Macrina , director of the  Library Freedom Project , notes that \"while librarians deeply value individual privacy, we also strongly oppose censorship\". [ 71 ] There is at least one case in which an article was removed from the archive shortly after it had been removed from its original website. A  Daily Beast  reporter had written an article that outed several gay Olympian athletes in 2016 after the reporter had made a fake profile posing as a gay man on a dating app.  The Daily Beast  removed the article after it was met with widespread furor; not long after, the Internet Archive soon did as well, but emphatically stated that they did so for no other reason than to protect the safety of the outed athletes. [ 71 ] Other threats include natural disasters, [ 114 ]  destruction (both remote and physical), [ 115 ]  manipulation of the archive's contents, problematic copyright laws, [ 116 ]  and surveillance of the site's users. [ 117 ] Alexander Rose, executive director of the  Long Now Foundation , suspects that in the long term of multiple generations \"next to nothing\" will survive in a useful way, stating, \"If we have continuity in our technological civilization, I suspect a lot of the bare data will remain findable and searchable. But I suspect almost nothing of the format in which it was delivered will be recognizable\" because sites \"with deep back-ends of content-management systems like Drupal and Ruby and Django\" are harder to archive. [ 118 ] In 2016, in an article reflecting on the preservation of human knowledge,  The Atlantic  has commented that the Internet Archive, which describes itself to be built for the long-term, [ 119 ]  \"is working furiously to capture data before it disappears without any long-term infrastructure to speak of.\" [ 120 ] In September 2024, the Internet Archive suffered a data breach that exposed 31 million records containing personal information, including email addresses and  hashed  passwords. [ citation needed ]  On October 9, 2024, the site went down due to a  distributed denial-of-service attack . [ 121 ] [ 122 ]  On October 14, the site returned online, but it remained in read-only mode until November 4. \"Save Page Now\" was disabled, replaced with a \"Temporarily Unavailable\" banner. [ 123 ]"
  },
  {
    "id": 142,
    "title": "Ted Nelson",
    "content": "Theodor Holm Nelson  (born June 17, 1937) is an American pioneer of  information technology , philosopher, and sociologist. He coined the terms  hypertext  and  hypermedia  in 1963 [ 1 ]  and published them in 1965. [ 2 ]  According to a 1997  Forbes  profile, Nelson \"sees himself as a literary romantic, like a  Cyrano de Bergerac , or 'the  Orson Welles  of software'.\" [ 3 ] Nelson is the son of  Emmy Award -winning director  Ralph Nelson  and  Academy Award -winning actress  Celeste Holm . [ 4 ]  His parents' marriage was brief and he was mostly raised by his grandparents, first in Chicago and later in  Greenwich Village . [ 5 ] Nelson earned a  B.A.  in philosophy from  Swarthmore College  in 1959. While there, he made an experimental humorous student film,  The Epiphany of Slocum Furlow , in which the titular hero discovers the meaning of life. His contemporary at the college, musician and composer  Peter Schickele , scored the film. [ 6 ]  Following a year of graduate study in sociology at the  University of Chicago , Nelson began graduate work in Social Relations, then a department at Harvard University specializing in sociology, ultimately earning a M.A in sociology from the Department of Social Relations in 1962. [ 7 ]  After Harvard, Nelson was a photographer and filmmaker for a year at  John C. Lilly 's Communication Research Institute in Miami, Florida, where he briefly shared an office with  Gregory Bateson . From 1964 to 1966, he was an instructor in sociology at Vassar College. During college and graduate school, he began to envision a computer-based writing system that would provide a lasting repository for the world's knowledge, and also permit greater flexibility of drawing connections between ideas. This came to be known as  Project Xanadu . Much later in life, in 2002, he obtained his  PhD  in media and governance from  Keio University . Nelson founded Project Xanadu in 1960, with the goal of creating a computer network with a simple user interface. The effort is documented in the books  Computer Lib / Dream Machines  (1974),  The Home Computer Revolution  (1977) and  Literary Machines  (1981). Much of his adult life has been devoted to working on Xanadu and advocating for it. Throughout his career, Nelson supported his work on the project through a variety of administrative, academic and research positions and consultancies, including stints at  Harcourt Brace and Company  (a technology consultancy and assistantship where he met  Douglas Engelbart , who later became a close friend; 1966-1967), [ citation needed ]   Brown University  (a tumultuous consultancy on the Nelson-inspired  Hypertext Editing System  and  File Retrieval and Editing System  with Swarthmore friend  Andries van Dam 's group; c. 1967-1969), [ citation needed ]   Bell Labs  (hypertext-related defense research; 1968-1969), [ 8 ]   CBS Laboratories  (\"writing and photographing interactive slide shows for their AVS-10 instructional device\"; 1968-1969), [ 8 ]  the  University of Illinois at Chicago  (an interdisciplinary staff position; 1973-1976) [ 8 ]  and Swarthmore College (visiting lecturer in computing; 1977). [ 8 ] Nelson also conducted research and development under the auspices of the Nelson Organization (founder and president; 1968-1972) and the Computopia Corporation (co-founder; 1977-1978). Clients of the former firm included  IBM , Brown University,  Western Electric , the  University of California , the  Jewish Museum , the Fretheim Chartering Corporation and the  Deering-Milliken Research Corporation . He has alleged that the Nelson Organization was envisaged as a clandestine funding conduit for the  Central Intelligence Agency , which expressed interest in Project Xanadu at an early juncture; however, the promised funds failed to materialize after several benchmarks were met. From 1980 to 1981, he was the editor of  Creative Computing . At the behest of Xanadu developers  Mark S. Miller  and Stuart Greene, Nelson joined  San Antonio, Texas -based  Datapoint  as chief software designer (1981–1982), remaining with the company as a media specialist and technical writer until its  Asher Edelman -driven restructuring in 1984. Following several San Antonio-based consultancies and the acquisition of Xanadu technology by  Autodesk  in 1988, he continued working on the project as a non-managerial Distinguished Fellow in the  San Francisco Bay Area  until the divestiture of the Xanadu Operating Group in 1992–1993. After holding visiting professorships in  media  and  information science  at  Hokkaido University  (1995-1996),  Keio University  (1996-2002), the  University of Southampton  and the  University of Nottingham , he was a fellow (2004–2006) and visiting fellow (2006–2008) of the  Oxford Internet Institute  in conjunction with  Wadham College, Oxford . [ 9 ]  More recently, he has taught classes at  Chapman University  and the  University of California, Santa Cruz . The Xanadu project itself failed to flourish, for a variety of reasons which are disputed. Journalist  Gary Wolf  published an unflattering history of Nelson and his project in the June 1995 issue of  Wired , calling it \"the longest-running  vaporware  project in the history of computing\". [ 10 ]  On his own website, Nelson expressed his disgust with the criticisms, referring to Wolf as \"Gory Jackal\", and threatened to sue him. [ 11 ]  He also outlined his objections in a letter to  Wired , [ 12 ]  and released a detailed rebuttal of the article. [ 13 ] As early as 1972, a demonstration iteration developed by Cal Daniels failed to reach fruition when Nelson was forced to return the project's rented  Data General Nova   minicomputer  due to financial exigencies. Nelson has stated that some aspects of his vision are being fulfilled by  Tim Berners-Lee 's invention of the  World Wide Web , but he dislikes the World Wide Web,  XML  and all embedded  markup  – regarding Berners-Lee's work as a gross over-simplification of his original vision: HTML is precisely what we were trying to PREVENT— ever-breaking links, links going outward only, quotes you can't follow to their origins, no  version management , no rights management. [ 14 ] Jaron Lanier  explained the difference between the World Wide Web and Nelson's vision, and the implications: A core technical difference between a Nelsonian network and what we have become familiar with online is that [Nelson's] network links were two-way instead of one-way. In a network with two-way links, each node knows what other nodes are linked to it. ... Two-way linking would preserve context. It's a small simple change in how online information should be stored that couldn't have vaster implications for culture and the economy. [ 15 ] In 1957, while a student, Nelson co-wrote and co-produced what he describes as a pioneering rock musical. Entitled \"Anything and Everything\", it was produced and performed at  Swarthmore College . [ 16 ] In 1965, he presented the paper \"Complex Information Processing: A File Structure for the Complex, the Changing, and the Indeterminate\" at the  ACM  National Conference, in which he coined the term \"hypertext\". [ 2 ] In 1976, Nelson co-founded and briefly served as the advertising director of the \"itty bitty machine company\", or \"ibm\", a small computer retail store that operated from 1977 to 1980 in  Evanston, Illinois . In 1978, he had a significant impact upon  IBM 's thinking when he outlined his vision of the potential of personal computing to the team that three years later launched the  IBM PC . [ 17 ] From the 1960s to the mid-2000s, Nelson built an extensive collection of direct advertising mail he received in his mailbox, mainly from companies selling products in IT, print/publishing, aerospace, and engineering. In 2017, the  Internet Archive  began to publish it online in scanned form, in a collection titled \"Ted Nelson's Junk Mail Cartons\". [ 18 ] [ 19 ] [ 20 ] As of 2011, Nelson was working on a new information structure, ZigZag, [ 21 ]  which is described on the Xanadu project website, which also hosts two versions of the Xanadu code. He also developed XanaduSpace, a system for the exploration of connected parallel documents (an early version of this software may be freely downloaded). [ 22 ] In January 1988  Byte  magazine  published an article about Nelson's ideas, titled \"Managing Immense Storage\". [ 23 ]  This stimulated discussions within the computer industry, and encouraged people to experiment with Hypertext features. [ citation needed ] In 1998, at the Seventh WWW Conference in  Brisbane , Australia, Nelson was awarded the  Yuri Rubinsky Memorial Award . In 2001, he was knighted by France as  Officier des Arts et Lettres . In 2007, he celebrated his 70th birthday by giving an invited lecture at the  University of Southampton . [ 24 ]  In 2014, ACM  SIGCHI  honored him with a Special Recognition Award. [ 25 ] In 2014, Nelson was conferred with a Doctor of Science degree, honoris causa, by  Chapman University . The ceremony took place during the 'Intertwingled' conference, featuring Nelson and other prominent figures in the field, including Apple Computer founder  Steve Wozniak  and former Association for Computing Machinery president  Wendy Hall . At the conference, Nelson stated confidence in the potential of his Xanadu system, saying 'The world would have been a better place if I had succeeded, but I ain't dead yet.' [ 26 ] Nelson is credited with coining several new words that have come into common usage especially in the world of computing. Among them are: Many of his books are published through his own company, Mindful Press. [ 27 ]"
  },
  {
    "id": 143,
    "title": "Hypertext",
    "content": "Hypertext  is  text  displayed on a  computer display  or other  electronic devices  with references ( hyperlinks ) to other text that the reader can immediately access. [ 1 ]  Hypertext documents are interconnected by  hyperlinks , which are typically activated by a  mouse  click, keypress set, or screen touch. Apart from text, the term \"hypertext\" is also sometimes used to describe tables, images, and other presentational  content formats  with integrated hyperlinks. Hypertext is one of the key underlying concepts of the  World Wide Web , [ 2 ]  where  Web pages  are often written in the  Hypertext Markup Language  (HTML). As implemented on the Web, hypertext enables the easy-to-use publication of information over the  Internet . \"(...)'Hypertext' is a recent coinage. 'Hyper-' is used in the mathematical sense of extension and generality (as in 'hyperspace,' 'hypercube') rather than the medical sense of 'excessive' ('hyperactivity'). There is no implication about  size — a hypertext could contain only 500 words or so. 'Hyper-' refers to structure and not size.\" The English prefix \"hyper-\" comes from the  Greek  prefix \"ὑπερ-\" and means \"over\" or \"beyond\"; it has a common origin with the prefix \"super-\" which comes from Latin. It signifies the overcoming of the previous linear constraints of written text. The term \"hypertext\" is often used where the term \" hypermedia \" might seem appropriate. In 1992, author  Ted Nelson  – who coined both terms in 1963  [ 3 ] [ 4 ] – wrote: By now the word \"hypertext\" has become generally accepted for branching and responding text, but the corresponding word \"hypermedia\", meaning complexes of branching and responding graphics, movies and sound – as well as text – is much less used. Instead they use the strange term \"interactive multimedia\": this is four syllables longer, and does not express the idea of extending hypertext. Hypertext documents can either be static (prepared and stored in advance) or dynamic (continually changing in response to user input, such as  dynamic web pages ). Static hypertext can be used to  cross-reference  collections of data in documents,  software applications , or books on  CDs . A well-constructed system can also incorporate other user-interface conventions, such as menus and command lines.  Links  used in a hypertext document usually replace the current piece of hypertext with the destination document. A lesser known feature is  StretchText , which expands or contracts the content in place, thereby giving more control to the reader in determining the level of detail of the displayed document. Some implementations support  transclusion , where text or other content is included by reference and automatically rendered in place. Hypertext can be used to support very complex and dynamic systems of linking and cross-referencing. The most famous implementation of hypertext is the  World Wide Web , written in the final months of 1990 and released on the  Internet  in 1991. In 1941,  Jorge Luis Borges  published \" The Garden of Forking Paths \", a  short story  that is often considered an inspiration for the concept of hypertext. [ 5 ] In 1945,  Vannevar Bush  wrote an article in  The Atlantic Monthly  called \" As We May Think \", about a futuristic proto-hypertext device he called a  Memex . A Memex would hypothetically store — and record — content on reels of microfilm, using electric photocells to read coded symbols recorded next to individual microfilm frames while the reels spun at high speed, and stopping on command. The coded symbols would enable the Memex to index, search, and link content to create and follow associative trails. Because the Memex was never implemented and could only link content in a relatively crude fashion — by creating chains of entire microfilm frames — the Memex is regarded only as a proto-hypertext device, but it is fundamental to the history of hypertext because it directly inspired the invention of hypertext by Ted Nelson and Douglas Engelbart. In 1965,  Ted Nelson  coined the terms 'hypertext' and 'hypermedia' as part of a model he developed for creating and using linked content (first published reference 1965). [ 7 ]  He later worked with  Andries van Dam  to develop the  Hypertext Editing System  (text editing) in 1967 at  Brown University . It was implemented using the terminal  IBM 2250  with a  light pen  which was provided as a  pointing device . [ 8 ]  By 1976, its successor  FRESS  was used in a poetry class in which students could browse a hyperlinked set of poems and discussion by experts, faculty and other students, in what was arguably the world's first online scholarly community [ 9 ]  which van Dam says \"foreshadowed wikis, blogs and communal documents of all kinds\". [ 10 ]  Ted Nelson said in the 1960s that he began implementation of a hypertext system he theorized, which was named  Project Xanadu , but his first and incomplete public release was finished much later, in 1998. [ 6 ] Douglas Engelbart  independently began working on his  NLS  system in 1962 at Stanford Research Institute, although delays in obtaining funding, personnel, and equipment meant that its key features were not completed until 1968. In December of that year, Engelbart demonstrated a 'hypertext' (meaning editing) interface to the public for the first time, in what has come to be known as \" The Mother of All Demos \". In 1971 a system called  Scrapbook , produced by David Yates and his team at the UK's  National Physical Laboratory , went live. It was an information storage and retrieval system that included what would now be called word processing, e-mail and hypertext. ZOG , an early hypertext system, was developed at Carnegie Mellon University during the 1970s, used for documents on Nimitz class aircraft carriers, and later evolving as   KMS  (Knowledge Management System). The first hypermedia application is generally considered to be the  Aspen Movie Map , implemented in 1978. The Movie Map allowed users to arbitrarily choose which way they wished to drive in a virtual cityscape, in two seasons (from actual photographs) as well as  3-D polygons . In 1980,  Tim Berners-Lee  created  ENQUIRE , an early hypertext database system somewhat like a  wiki  but without hypertext punctuation, which was not invented until 1987. The early 1980s also saw a number of experimental \"hyperediting\" functions in word processors and  hypermedia  programs, many of whose features and terminology were later analogous to the  World Wide Web .  Guide , the first significant hypertext system for  personal computers , was developed by Peter J. Brown at the  University of Kent  in 1982. In 1980,  Roberto Busa , [ 11 ]  an Italian  Jesuit  priest and one of the pioneers in the usage of computers for linguistic and literary analysis, [ 12 ]  published the  Index Thomisticus , as a tool for performing text searches within the massive corpus of  Aquinas 's works. [ 13 ]  Sponsored by the founder of IBM,  Thomas J. Watson , [ 14 ]  the project lasted about 30 years (1949–1980), and eventually produced the 56 printed volumes of the  Index Thomisticus  the first important hypertext work about  Saint Thomas Aquinas  books and of a few related authors. [ 15 ] In 1983,  Ben Shneiderman  at the  University of Maryland Human - Computer Interaction Lab  led a group that developed the  HyperTies  system that was commercialized by  Cognetics Corporation . They studied many designs before adopting the  blue color for links . Hyperties was used to create the July 1988 issue of the  Communications of the ACM  as a hypertext document and then the first commercial electronic book  Hypertext Hands-On! . In August 1987,  Apple Computer  released  HyperCard  for the  Macintosh  line at the  MacWorld convention . Its impact, combined with interest in Peter J. Brown's  GUIDE  (marketed by  OWL  and released earlier that year) and Brown University's  Intermedia , led to broad interest in and enthusiasm for hypertext, hypermedia, databases, and new media in general. The first ACM Hypertext (hyperediting and databases)  academic conference  took place in November 1987, in Chapel Hill NC, where many other applications, including the branched literature writing software  Storyspace , were also demonstrated. [ 16 ] Meanwhile, Nelson (who had been working on and advocating his  Xanadu  system for over two decades) convinced  Autodesk  to invest in his revolutionary ideas. The project continued at Autodesk for four years, but no product was released. In 1989, Tim Berners-Lee, then a scientist at  CERN , proposed and later prototyped a new hypertext project in response to a request for a simple, immediate, information-sharing facility, to be used among physicists working at CERN and other academic institutions. He called the project \"WorldWideWeb\". [ 17 ] HyperText is a way to link and access information of various kinds as a web of nodes in which the user can browse at will. Potentially, HyperText provides a single user-interface to many large classes of stored information, such as reports, notes, data-bases, computer documentation and on-line systems help. We propose the implementation of a simple scheme to incorporate several different servers of machine-stored information already available at CERN, including an analysis of the requirements for  information access  needs by experiments... A program which provides access to the hypertext world we call a browser. ― T. Berners-Lee, R. Cailliau, 12 November 1990, CERN [ 17 ] In 1992,  Lynx  was born as an early Internet web browser. Its ability to provide hypertext links within documents that could reach into documents anywhere on the Internet began the creation of the Web on the Internet. As new web browsers were released, traffic on the World Wide Web quickly exploded from only 500 known web servers in 1993 to over 10,000 in 1994. As a result, all previous hypertext systems were overshadowed by the success of the Web, even though it lacked many features of those earlier systems, such as integrated browsers/editors (a feature of the original WorldWideWeb browser, which was not carried over into most of the other early Web browsers). Besides the already mentioned  Project Xanadu ,  Hypertext Editing System ,  NLS ,  HyperCard , and World Wide Web, there are other noteworthy early implementations of hypertext, with different feature sets: Among the top academic conferences for new research in hypertext is the annual  ACM Conference on Hypertext and Social Media . [ 18 ]  The  Electronic Literature Organization  hosts annual conferences discussing  hypertext fiction , poetry and other forms of  electronic literature . Although not exclusively about hypertext, the World Wide Web series of conferences, organized by  IW3C2 , [ 19 ]  also include many papers of interest. There is a list on the Web with links to all conferences in the series. [ 20 ] Hypertext writing has developed its own style of fiction, coinciding with the growth and proliferation of hypertext development software and the emergence of electronic networks. Hypertext fiction is one of earliest genres of  electronic literature , or literary works that are designed to be read in digital media. Two software programs specifically designed for literary hypertext,  Storyspace  and  Intermedia , became available in the 1990s.  Judy Malloy 's  Uncle Roger  (1986) and  Michael Joyce 's  afternoon, a story  (1987) are generally considered the first works of hypertext fiction. [ 21 ] [ 22 ] An advantage of writing a narrative using hypertext technology is that the meaning of the story can be conveyed through a sense of spatiality and perspective that is arguably unique to digitally networked environments. An author's creative use of nodes, the self-contained units of meaning in a hypertextual narrative, can play with the reader's orientation and add meaning to the text. One of the most successful computer games,  Myst , was first written in HyperCard. The game was constructed as a series of Ages, each Age consisting of a separate HyperCard stack. The full stack of the game consists of over 2500 cards. In some ways,  Myst  redefined interactive fiction, using puzzles and exploration as a replacement for hypertextual narrative. [ 23 ] Critics of hypertext claim that it inhibits the old, linear, reader experience by creating several different tracks to read on. This can also been seen as contributing to a  postmodernist  fragmentation of worlds. In some cases, hypertext may be detrimental to the development of appealing stories (in the case of hypertext  Gamebooks ), where ease of linking fragments may lead to non-cohesive or incomprehensible narratives. [ 24 ]  However, they do see value in its ability to present several different views on the same subject in a simple way. [ 25 ]  This echoes the arguments of 'medium theorists' like  Marshall McLuhan  who look at the social and psychological impacts of the media. New media can become so dominant in public culture that they effectively create a \"paradigm shift\" [ 26 ]  as people have shifted their perceptions, understanding of the world, and ways of interacting with the world and each other in relation to new technologies and media. So hypertext signifies a change from linear, structured and hierarchical forms of representing and understanding the world into fractured, decentralized and changeable media based on the technological concept of hypertext links. In the 1990s, women and feminist artists took advantage of hypertext and produced dozens of works.  Linda Dement 's  Cyberflesh Girlmonster  a hypertext  CD-ROM  that incorporates images of women's body parts and remixes them to create new monstrous yet beautiful shapes. Caitlin Fisher's award-winning online hypertext novella  These Waves of Girls  (2001) is set in three time periods of the protagonist exploring polymorphous perversity enacted in her queer identity through memory. The story is written as a reflection diary of the interconnected memories of childhood, adolescence, and adulthood. It consists of an associated multi-modal collection of nodes includes linked text, still and moving images, manipulable images, animations, and sound clips. Adrienne Eisen (pen name for  Penelope Trunk ) wrote hypertexts that were subversive narrative journeys into the mind of a woman whose erotic encounters were charged with a post-feminist satirical edge that cuts deep into the American psyche. There are various forms of hypertext fiction, each of which is structured differently. Below are four:"
  },
  {
    "id": 144,
    "title": "Nick Jardine",
    "content": "Nicholas Jardine   FBA  (born 4 September 1943) is a British mathematician, philosopher of science and its history, historian of astronomy and natural history, and amateur  mycologist . He is  Emeritus Professor  at the  Department of History and Philosophy of Science  (HPS) at the  University of Cambridge . [ 1 ] Jardine was educated at  Monkton Combe School  in  Somerset  and read natural sciences at  King's College, Cambridge .  He then worked as a King's College and Royal Society Research Fellow on the automation of classification and information retrieval and its applications to biological taxonomy and diagnosis.  In 1975 he moved to  Darwin College, Cambridge  and to the Department of History and Philosophy of Science.  Since then he has developed a question-based pragmatic philosophy of science (inspired by the work of  Ian Hacking ), as well as studying the history of early-modern astronomy and natural history, and reflecting on the methodology of the history of the sciences. From 1987–2011 he was Senior Editor of Studies in History and Philosophy of Science and from 1998 of Studies in History and Philosophy of Biological and Biomedical Sciences.  Since his retirement in 2010 he has been Senior Consultant to the Cambridge Scientific Heritage Project (2010–13) and principal investigator of the project Diagrams, Figures and the Transformation of Astronomy, 1450–1650 (2008–14)  http://www.astronomicalimages.group.cam.ac.uk Jardine was a founding member in 1988 of the popular research seminar \"The Cabinet of Natural History (Cambridge Group for the History of Natural History and the Environmental Sciences)\".  This is organised by staff and students of the Department of History and Philosophy of Science, and in term time it holds weekly seminars led by academic speakers. Jardine is a keen amateur mycologist and for over twenty years has led the annual HPS  fungus  hunt. [ 2 ] Jardine married historian  Lisa Bronowski  in 1969, divorcing in 1979; the couple had two children. [ 3 ]"
  },
  {
    "id": 145,
    "title": "C. J. van Rijsbergen",
    "content": "C. J. \"Keith\" van Rijsbergen   FREng  ( Cornelis Joost van Rijsbergen ; born 1943) [ 1 ]  is a professor of  computer science  at the  University of Glasgow , where he founded the Glasgow Information Retrieval Group. [ 2 ]  He is one of the founders of modern  Information Retrieval  and the author of the seminal monograph  Information Retrieval  and of the textbook  The Geometry of Information Retrieval . He was born in  Rotterdam , and educated in the  Netherlands ,  Indonesia ,  Namibia  and  Australia .\nHis first degree is in mathematics from the  University of Western Australia , and in 1972 he completed a\nPhD in computer science at the  University of Cambridge . He spent three years lecturing in information retrieval and artificial intelligence at  Monash University [ 1 ]  before returning to  Cambridge  to hold a  Royal Society  Information Research Fellowship. \nIn 1980 he was appointed to the chair of computer science at  University College Dublin ; from there he moved in 1986 to  Glasgow University . He chaired the Scientific Board of the  Information Retrieval Facility  from 2007 to 2012. In 2003 he was inducted as a Fellow of the  Association for Computing Machinery . In 2004 he was awarded the  Tony Kent Strix award .\nIn 2004 he was appointed a  Fellow  of the  Royal Academy of Engineering . [ 3 ]  In 2006, he was awarded the  Gerard Salton Award  for  Quantum haystacks . In 2009, he was made an  honorary professor  at the  University of Edinburgh . [ 3 ] This article about a Dutch scientist is a  stub . You can help Wikipedia by  expanding it . This biographical article relating to a  computer scientist  is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 146,
    "title": "Term discrimination",
    "content": "Term discrimination  is a way to rank keywords in how useful they are for  information retrieval . This is a method similar to  tf-idf  but it deals with finding keywords suitable for  information retrieval  and ones that are not.  Please refer to  Vector Space Model  first. This method uses the concept of  Vector Space Density  that the less dense an  occurrence matrix  is, the better an information retrieval query will be. An optimal index term is one that can distinguish two different documents from each other and relate two similar documents.  On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents. The discrimination value is the difference in the occurrence matrix's vector-space density versus the same matrix's vector-space without the index term's density. Given an  occurrency matrix :  \n \n \n \n A \n \n \n {\\displaystyle A} \n \n  and one keyword:  \n \n \n \n k \n \n \n {\\displaystyle k} A higher value is better because including the keyword will result in better information retrieval. Keywords that are  sparse  should be poor discriminators because they have poor  recall , \nwhereas\nkeywords that are  frequent  should be poor discriminators because they have poor  precision ."
  },
  {
    "id": 147,
    "title": "Journal of the Association for Information Science and Technology",
    "content": "The  Journal of the Association for Information Science and Technology  is a monthly  peer-reviewed   academic journal  covering all aspects of  information science  published by  Wiley-Blackwell  on behalf of the  Association for Information Science and Technology . The journal publishes original research and rapid communications, as well as  book reviews  and announcements of the association. Occasional special issues appear with contents focused on a single topic area. Established in 1950 as the quarterly journal  American Documentation , the new publication was produced by the  American Documentation Institute  (ADI), which had formed in 1937 around a group of researchers and practitioners interested in the emerging technology of  microfilm  as a medium for the preservation and dissemination of documents and knowledge. Many of the same people and institutions were involved in a pre-war  American Library Association  journal called  The Journal of Documentary Reproduction , which ran from 1938 to 1943, before being discontinued due to the imperatives of the war. [ 1 ] American Documentation  was an explicit continuation of and extension upon  The Journal of Documentary Reproduction , with a broader brief to cover documentation as a whole, then defined as \"...the creation, transmission, collection, classification and use of 'documents'; documents may be broadly defined as recorded knowledge in any format.\" [ 2 ] In the postwar years, rapid technological and social changes ushered in an \" information explosion \" which created many new problems and opportunities of special interest to documentation specialists, and in time documentation found itself at the center of the emerging field of  information science . The ADI's membership and scope increased rapidly, and in 1968 the members voted to change the organization's name to \"American Society for Information Science\", to reflect the changes in their membership and focus. As their official journal,  American Documentation  followed suit, and beginning with the first issue of 1970 it changed its name to  The Journal of the American Society for Information Science  and began publishing bimonthly. [ 3 ] In 1991, the publication frequency increased to 10 issues yearly and by 1996, the journal was publishing monthly.  [ 4 ]  In 2000, ASIS again voted to change its name, this time to The \"American Society for Information Science and Technology\", in order to recognize the further changes in membership and interests brought on by the rise of the internet and the mainstreaming of networked computing and information technology. The journal's name was subsequently also changed in January 2001 to  Journal of the American Society for Information Science and Technology . It obtained its current name in January 2014. The journal is abstracted and indexed in: According to the  Journal Citation Reports , the journal has a 2020  impact factor  of 2.687. [ 12 ]"
  },
  {
    "id": 148,
    "title": "Communications of the ACM",
    "content": "Communications of the ACM  is the monthly  journal  of the  Association for Computing Machinery  (ACM). It was established in 1958, [ 2 ]  with  Saul Rosen  as its first managing editor. It is sent to all ACM members. [ 3 ] [ 4 ] \nArticles are intended for readers with backgrounds in all areas of  computer science  and  information systems . The focus is on the practical implications of advances in information technology and associated management issues; ACM also publishes a variety of more theoretical journals. The magazine straddles the boundary of a  science magazine ,  trade magazine , and a  scientific journal . While the content is subject to  peer review , the articles published are often summaries of research that may also be published elsewhere. Material published must be accessible and relevant to a broad readership. [ 5 ] From 1960 onward,  CACM  also published  algorithms , expressed in  ALGOL . The collection of algorithms later became known as the Collected Algorithms of the ACM. [ 6 ] CACM announced a transition to entirely open access in February 2024, [ 1 ]  as part of ACM's commitment to make all articles open access. According to the  Journal Citation Reports , the journal has a 2023  impact factor  of 22.7. [ 7 ] This article about a  computer science   journal  is a  stub . You can help Wikipedia by  expanding it . See tips for writing articles about academic journals . Further suggestions might be found on the article's  talk page ."
  },
  {
    "id": 149,
    "title": "Association for Computing Machinery",
    "content": "The  Association for Computing Machinery  ( ACM ) is a US-based international  learned society  for  computing . It was founded in 1947 and is the world's largest scientific and educational computing society. [ 1 ]  The ACM is a  non-profit  professional membership group, [ 2 ]  reporting nearly 110,000 student and professional members as of 2022 [update] . Its headquarters are in  New York City . The ACM is an  umbrella organization  for academic and scholarly interests in  computer science  ( informatics ). Its motto is \"Advancing Computing as a Science & Profession\". In 1947, a notice was sent to various people: [ 3 ] [ 4 ] On January 10, 1947, at the Symposium on Large-Scale Digital Calculating Machinery at the Harvard computation Laboratory, Professor  Samuel H. Caldwell  of Massachusetts Institute of Technology spoke of the need for an association of those interested in computing machinery, and of the need for communication between them.\n[...]\nAfter making some inquiries during May and June, we believe there is ample interest to start an informal association of many of those interested in the new machinery for computing and reasoning. Since there has to be a beginning, we are acting as a temporary committee to start such an association: The committee (except for Curtiss) had gained experience with computers during  World War II : Berkeley, Campbell, and Goheen helped build  Harvard Mark I  under  Howard H. Aiken , Mauchly and Sharpless were involved in building  ENIAC , Tompkins had used \"the secret Navy code-breaking machines\", and Taylor had worked on  Bush 's  Differential analyzers . [ 4 ] The ACM was then founded in 1947 under the name  Eastern Association for Computing Machinery , which was changed the following year to the Association for Computing Machinery. [ 5 ] [ 6 ] [ 7 ]  The ACM History Committee since 2016 has published the A.M.Turing Oral History project, the ACM Key Award Winners Video Series, and the India Industry Leaders Video project. [ 8 ] ACM is organized into over 180 local professional chapters [ 9 ]  and 38  Special Interest Groups  (SIGs), [ 10 ]  through which it conducts most of its activities. Additionally, there are over 680 student chapters. [ 9 ]  The first student chapter was founded in 1961 at the  University of Louisiana at Lafayette . [ 11 ] [ 12 ] Many of the SIGs, such as  SIGGRAPH ,  SIGDA ,  SIGPLAN ,  SIGCSE  and  SIGCOMM , sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters. [ 13 ] ACM also sponsors other computer science related events such as the worldwide  ACM International Collegiate Programming Contest  (ICPC), and has sponsored some other events such as the chess match between  Garry Kasparov  and the  IBM Deep Blue  computer. [ 14 ] ACM publishes over 50 journals [ 15 ]  including the prestigious [ 16 ]   Journal of the ACM , and two general magazines for computer professionals,  Communications of the ACM  (also known as  Communications  or  CACM ) and  Queue . Other publications of the ACM include: Although  Communications  no longer publishes primary research, and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages. ACM has made almost all of its publications available to paid  subscribers  online at its  Digital Library  and also has a  Guide to Computing Literature . ACM also offers insurance, online courses, and other services to its members. In 1997, ACM Press published  Wizards and Their Wonders: Portraits in Computing  ( ISBN   0897919602 ), written by Christopher Morgan, with new photographs by  Louis Fabian Bachrach . The book is a collection of historic and current portrait photographs of figures from the computer industry. The  ACM Portal  is an online service of the ACM. [ 19 ]  Its core are two main sections:  ACM Digital Library  and the  ACM Guide to Computing Literature . [ 20 ] The ACM Digital Library was launched in October 1997. [ 21 ]  It is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries. [ 19 ]  The ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization's journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying  bibliographic database  containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature. ACM adopted a hybrid  Open Access  (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement. [ 22 ] ACM was a \"green\" publisher before the term was invented. [ 23 ]  Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library's permanently maintained Version of Record. All metadata in the Digital Library is open to the world, including  abstracts , linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription. In addition, starting on April 7, 2022, ACM made its publications from 1951 to 2000 open access through the Digital Library in celebration of the 75th anniversary of the organisation's founding. [ 24 ] In 2020, ACM launched a major push to become a fully open access publisher by 2026. ACM restructured its pricing for the ACM Digital Library on the basis of publishing activity by affiliated  lead authors  in ACM's journals, magazines, and conference proceedings. Under this model, termed \"ACM Open,\" institutions pay set fees for full access to ACM Digital Library contents as well as unlimited open access publishing by their affiliated authors. Authors not affiliated with a participating institution will be expected to pay an  article processing charge . [ 25 ] [ 26 ]  As of May 2024, ACM reported that more than 1,340 institutions worldwide had signed on for ACM Open, putting ACM at just over halfway to meeting its target of 2,500 participating institutions by 2026. [ 27 ] In addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and \"demonstrated performance that sets them apart from their peers\". [ 28 ] The number of Fellows, Distinguished Members, and Senior Members cannot exceed 1%, 10%, and 25% of the total number of professional members, respectively. [ 29 ] The  ACM Fellows  Program was established by Council of the Association for Computing Machinery in 1993 \"to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM.\" There are 1,310 Fellows as of 2020 [update] [ 30 ]  out of about 100,000 members. In 2006, ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and \"have made a significant impact on the computing field\". In 2006 when the Distinguished Members first came out, one of the three levels was called \"Distinguished Member\" and was changed about two years later to \"Distinguished Educator\". Those who already had the Distinguished Member title had their titles changed to one of the other three titles. List of Distinguished Members of the Association for Computing Machinery  [ 31 ] Also in 2006, ACM began recognizing Senior Members. According to the ACM, \"The Senior Members Grade recognizes those ACM members with at least 10 years of professional experience and 5 years of continuous Professional Membership who have demonstrated performance through technical leadership, and technical or professional contributions\". [ 32 ]  Senior membership also requires 3 letters of reference While not technically a membership grade, the ACM recognizes distinguished speakers on topics in computer science. A distinguished speaker is appointed for a three-year period. There are usually about 125 current distinguished speakers. The ACM website describes these people as 'Renowned International Thought Leaders'. [ 33 ]  The distinguished speakers program (DSP) has been in existence for over 20 years and serves as an outreach program that brings renowned experts from Academia, Industry and Government to present on the topic of their expertise. [ 34 ]   The DSP is overseen by a committee  [ 35 ] ACM has three kinds of chapters:  Special Interest Groups , [ 36 ]  Professional Chapters, and  Student Chapters . [ 37 ] As of 2022 [update] , ACM has professional & SIG Chapters in 56 countries. [ 38 ] As of 2022 [update] , there exist ACM student chapters in 41 countries. [ 39 ] ACM and its Special Interest Groups (SIGs) sponsors numerous conferences worldwide. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example,  SIGGRAPH  2007 attracted about 30000 attendees, while CIKM 2005 and  RecSys  2022 had paper acceptance rates of only accepted 15% and 17% respectively. [ 41 ] The ACM is a co–presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the  Anita Borg Institute for Women and Technology . [ 47 ] Some conferences are hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM. [ 48 ]  In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended. For additional non-ACM conferences, see this  list of computer science conferences . The ACM presents or co–presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology. [ 49 ] [ 50 ] [ 51 ] Over 30 of ACM's Special Interest Groups also award individuals for their contributions with a few listed below. [ 55 ] The President of ACM for 2022–2024 is  Yannis Ioannidis , Professor at the  National and Kapodistrian University of Athens . [ 56 ]  He is successor of  Gabriele Kotsis  (2020–2022), Professor at the  Johannes Kepler University Linz ;  Cherri M. Pancake  (2018–2020), professor emeritus at  Oregon State University  and Director of the Northwest Alliance for Computational Science and Engineering (NACSE); Vicki L. Hanson (2016–2018), Distinguished Professor at the  Rochester Institute of Technology  and visiting professor at the  University of Dundee ;  Alexander L. Wolf  (2014–2016), Dean of the  Jack Baskin School of Engineering  at the  University of California, Santa Cruz ;  Vint Cerf  (2012–2014), American computer scientist and Internet pioneer; Alain Chesnais (2010–2012); and  Dame Wendy Hall  of the  University of Southampton , UK (2008–2010). [ 57 ] ACM is led by a council consisting of the president, vice-president, treasurer, past president, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members-At-Large. This institution is often referred to simply as \"Council\" in  Communications of the ACM . ACM has numerous boards, committees, and task forces which run the organization: [ 58 ] ACM-W , [ 59 ]  the ACM council on  women in computing , supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM–W's main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively.  ACM-W collaborates with organizations such as the  Anita Borg Institute , the  National Center for Women & Information Technology (NCWIT) , and  Committee on the Status of Women in Computing Research (CRA-W) .\nThe ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science. [ 60 ]  This program began in 2006. Speakers are nominated by SIG officers. [ 61 ] ACM's primary partner has been the  IEEE Computer Society  (IEEE-CS), which is the largest subgroup of the  Institute of Electrical and Electronics Engineers  (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical  computer science , but there is considerable overlap with ACM's agenda. They have many joint activities including conferences, publications and awards. [ 62 ]  ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE. [ 63 ]   Eckert-Mauchly Award  and  Ken Kennedy Award , both major awards in computer science, are given jointly by ACM and the IEEE-CS. [ 64 ]  They occasionally cooperate on projects like developing computing curricula. [ 65 ] ACM has also jointly sponsored on events with other professional organizations like the  Society for Industrial and Applied Mathematics  (SIAM). [ 66 ] In December 2019, the ACM co-signed a letter with over one hundred other publishers to President  Donald Trump  saying that an  open access  mandate would increase costs to taxpayers or researchers and hurt  intellectual property . This was in response to rumors that he was considering issuing an  executive order  that would require federally funded research be made freely available online immediately after being published. It is unclear how these rumors started. [ 67 ]  Many ACM members opposed the letter, leading ACM to issue a statement clarifying that they remained committed to open access, [ 68 ]  and they wanted to see communication with stakeholders about the potential mandate. The statement did not significantly assuage criticism from ACM members. [ 69 ] The  SoCG conference , while originally an ACM conference, parted ways with ACM in 2014 [ 70 ]  because of problems when organizing conferences abroad. [ 71 ]"
  },
  {
    "id": 150,
    "title": "Special Interest Group on Information Retrieval",
    "content": "SIGIR  is the  Association for Computing Machinery 's  Special Interest Group on Information Retrieval . The scope of the group's specialty is the theory and application of computers to the acquisition, organization, storage,  retrieval  and distribution of information; emphasis is placed on working with non-numeric information, ranging from  natural language  to highly structured  data bases . The annual international SIGIR conference, which began in 1978, is considered the most important in the field of information retrieval. SIGIR also sponsors the annual  Joint Conference on Digital Libraries  (JCDL) in association with  SIGWEB , the  Conference on Information and Knowledge Management  (CIKM), and the  International Conference on Web Search and Data Mining  (WSDM) in association with  SIGKDD ,  SIGMOD , and  SIGWEB . The group gives out several awards to contributions to the field of information retrieval. The most important award is the  Gerard Salton Award  (named after the  computer scientist   Gerard Salton ), which is awarded every three years to an individual who has made \"significant, sustained and continuing contributions to research in  information retrieval \". Additionally, SIGIR presents a Best Paper Award  [ 1 ]  to recognize the highest quality paper at each conference. \"Test of time\" Award  [ 2 ]  is a recent award that is given to a paper that has had \"long-lasting influence, including impact on a subarea of information retrieval research, across subareas of information retrieval research, and outside of the information retrieval research community\". This award is selected from a set of full papers presented at the main SIGIR conference 10–12 years before. The  ACM SIGIR Academy [ 3 ] [ 4 ]  is a group of researchers honored by SIGIR. Each year, 3-5 new members are elected (in addition to other \"very senior members of the IR community\" who will be \"automatically\" inducted) for having made significant, cumulative contributions to the development of the field of  information retrieval  and influencing the research of others. These are the principal leaders of the field, whose efforts have shaped the discipline and/or industry through significant research, innovation, and/or service. Here are the inductees into the SIGIR Academy by year:"
  },
  {
    "id": 151,
    "title": "Natural-language user interface",
    "content": "Natural-language user interface  ( LUI  or  NLUI ) is a type of  computer human interface  where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications. In  interface design , natural-language interfaces are sought after for their speed and ease of use, but most suffer the challenges to  understanding  wide varieties of  ambiguous input . [ 1 ] \nNatural-language interfaces are an active area of study in the field of  natural-language processing  and  computational linguistics . An intuitive general natural-language interface is one of the active goals of the  Semantic Web . Text interfaces are \"natural\" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional  keyword search  engine could be described as a \"shallow\" natural-language user interface. A natural-language search engine would in theory find targeted  answers to user questions  (as opposed to keyword search). For example, when confronted with a question of the form 'which  U.S.  state has the highest  income tax ?', conventional search engines ignore the question and instead search on the  keywords  'state', 'income' and 'tax'. Natural-language search, on the other hand, attempts to use natural-language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine, due to the question being included. [ citation needed ] Prototype Nl interfaces had already appeared in the late sixties and early seventies. [ 2 ] Natural-language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the  AI winter  of the 1970s and 80s. A  1995 paper  titled 'Natural Language Interfaces to Databases – An Introduction', describes some challenges: [ 2 ] Other goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market. In addition, localisation across multiple language sites requires extra consideration - this is based on differing sentence structure and language syntax variations between most languages. Finally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome. The natural-language interface gives rise to technology used for many different applications. Some of the main uses are: Below are named and defined some of the applications that use natural-language recognition, and so have integrated utilities listed above. Ubiquity, an  add-on  for  Mozilla Firefox , is a collection of quick and easy natural-language-derived commands that act as  mashups  of web services, thus allowing users to get information and relate it to current and other webpages. Wolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a  search engine  would. [ 5 ]  It was announced in March 2009 by  Stephen Wolfram , and was released to the public on May 15, 2009. [ 6 ] Siri is an  intelligent personal assistant  application integrated with operating system  iOS . The application uses  natural language processing  to answer questions and make recommendations. Siri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab. [ 7 ]"
  },
  {
    "id": 152,
    "title": "Nicholas J. Belkin",
    "content": "Nicholas J. Belkin  is a professor at the  School of Communication and Information  at  Rutgers University . Among the main themes of his research are  digital libraries ; information-seeking behaviors; and interaction between humans and  information retrieval systems . Belkin is best known for his work on human-centered Information Retrieval and the hypothesis of Anomalous State of Knowledge (ASK). Belkin realized that in many cases, users of search systems are unable to precisely formulate what they need. They miss some vital knowledge to formulate their queries. In such cases it is more suitable to attempt to describe a user's anomalous state of knowledge than to ask the user to specify her/his need as a request to the system. [ 1 ] [ 2 ] Belkin was the chair of  SIGIR  in 1995-99, and the president of  American Society for Information Science and Technology  in 2005. [ 3 ]  In 2015, Belkin received the  Gerard Salton Award . [ 4 ] Nicholas Belkin studied Slavic  Philology  at the  University of Washington , graduating in 1968. He graduated from the same college in  Library Science  2 years later (1970), and read his doctoral thesis in 1977 in the  University of London . He worked in the Information Science department of this university from 1975 to 1985. That year, he signed for the Faculty of Communication and Information at  Rutgers University  (USA). He has been a visiting professor at  Western Ontario University (Canada),  Dhirubhai Ambani Institute of Information and Communication Technology  (India) and  Free University of Berlin . He has been a visiting researcher at the  National University of Singapore  in 1996. He has given more than 200 lectures around the world. He has been president of  Association for Computing Machinery   SIGIR  (Special Interest Group on Information Retrieval) during the period 1995-1999, and president of the  American Society of Information Science and Technology  (ASIST) in 2005. Nicholas Belkin has served on numerous editorial boards of numerous scientific journals. Among the most prestigious are \"Information Processing and Management\" and \"Information Retrieval\". Nicholas Belkin has approached information retrieval from the so-called   cognitive models  , that is, those focused on users who access document systems. Belkin approached his research from 3 basic lines: In 1977, Belkin read his thesis where he developed a new  theory  of the concept   documentary information  . This would be a structure that would allow the user to transform his   anomalous state of knowledge   (Anomalous State of Knowledge or ASK), when the need for information is satisfied, producing an adequate connection between the two ends of the documentary process: the producer and the receiver or user. For Belkin, the purpose towards which Documentation works is to make this effective communication possible, which would imply the study of documentary information in human and cognitive  communication  systems, the connection between this information and its producer, the connection between information and user, gives the idea of the requested information and the effectiveness between information and  document  and its transmission process. Belkin concludes that the concept of documentary information is the combination of a cognitive  communication  system, a structural representation of knowledge, the implementation of the project via user when he recognizes the need for information (ASK9, the meaning of the text (  message ) and the interest in solving the problem of information science. This theory has also been developed by Oddy and Brooks. Nicholas Belkin proposed a novel  cognitive model  of information retrieval, referred to as   'episodic'  . In this, Belkin defines a set of interactions that occur between the user and the system during the consultation to \"conceptualize, label and transcribe the need for information, as well as make relevant judgments about one or more documents.\" The components would be the same as those used in the traditional model: navigation ( browsing ), query (querying),  display ,  indexing , representation and matching. This model pays very little attention to the structure of documents and their retrieval, because it focuses on the anomalous state of knowledge of the individual, how to represent it, how to retrieve it, so it is based on the storage, retrieval and interaction of the search strategy. Nicholas Belkin has been awarded numerous times, obtaining in 2003 the  Award of Merit , and the  Gerard Salton Award  in 2015. Belkin has published numerous articles in the most prestigious magazines in the field of Information and Documentation, some awarded by the ASIST. He is also the author of the book:   Interaction in Information Systems: A Review of Research from Document Retrieval to Knowledge-Based Systems   (1985) co-authored with  Alina Vickery ."
  },
  {
    "id": 153,
    "title": "Melvin Earl Maron",
    "content": "Melvin Earl \"Bill\" Maron  (Jan 23, 1924 - September 28, 2016) was an American computer scientist and emeritis professor of  University of California, Berkeley . [ 1 ]  He studied mechanical engineering and physics at the University of Nebraska and received his Ph.D. in philosophy from the University of California in 1951. [ 2 ]  Maron is best known for his work on probabilistic  information retrieval  which he published together with his friend and colleague Lary Kuhns. [ 3 ] [ 4 ] \nQuite remarkably, Maron also pioneered relational databases, proposing a system called the Relational Data File in 1967, on which  Ted Codd  based his  Relational model  of data. [ 5 ] This article about an American scientist in academia is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 154,
    "title": "Robert R. Korfhage",
    "content": "Robert Roy Korfhage  (December 2, 1930 – November 20, 1998) was an American computer scientist, famous for his contributions to  information retrieval  and several textbooks. He was son of Dr. Roy Korfhage who was a chemist at  Nestlé  in  Fulton, Oswego County, New York .  Korfhage earned his bachelor's degree (1952) in engineering  mathematics  at  University of Michigan , while working part-time at  United Aircraft and Transport Corporation  in  East Hartford  as  programmer .  At the same university, he earned a master's degree and  Ph.D.  (1962) in  mathematics ,\nhis PhD dissertation being  On Systems of Distinct Representatives for Several Collections of Sets \nadvised by  Bernard Galler  (1962). Korfhage taught mathematics at  North Carolina State University  (1962–64),  Purdue University  (1964–70), [ 1 ]   Southern Methodist University  (1970–86) and the  University of Pittsburgh School of Information Sciences  (1986–98). Korfhage's research focused on  graph theory  and  information retrieval . For instance, his  Information Storage and Retrieval  (1997) was winner of  American Society for Information Science and Technology   Best information science book  award (1998). [ 2 ] In his later years, he worked on new ways of  information visualization  and also  genetic algorithms  to optimize text queries. He died of  cancer  in  Pittsburgh . This biographical article relating to a  computer scientist  is a  stub . You can help Wikipedia by  expanding it ."
  },
  {
    "id": 155,
    "title": "World Wide Web",
    "content": "The  World Wide Web  ( WWW  or simply the  Web ) is an  information system  that enables  content  sharing over the  Internet  through user-friendly ways meant to appeal to users beyond  IT  specialists and hobbyists. [ 1 ]  It allows documents and other  web resources  to be accessed over the Internet according to specific rules of the  Hypertext Transfer Protocol  (HTTP). [ 2 ] The Web was invented by English computer scientist  Tim Berners-Lee  while at  CERN  in 1989 and opened to the public in 1991. It was conceived as a \"universal linked information system\". [ 3 ] [ 4 ]  Documents and other media content are made available to the network through  web servers  and can be accessed by programs such as  web browsers . Servers and resources on the World Wide Web are identified and located through character strings called  uniform resource locators  (URLs). The original and still very common document type is a  web page  formatted in  Hypertext Markup Language  (HTML). This markup language supports  plain text ,  images , embedded  video  and  audio  contents, and  scripts  (short programs) that implement complex user interaction. The HTML language also supports  hyperlinks  (embedded URLs) which provide immediate access to other web resources.  Web navigation , or web surfing, is the common practice of following such hyperlinks across multiple websites.  Web applications  are web pages that function as  application software . The information in the Web is transferred across the Internet using HTTP. Multiple web resources with a common theme and usually a common  domain name  make up a  website . A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and  individual users ; and comprises an enormous amount of educational, entertainment, commercial, and government information. The Web has become the world's dominant  information systems platform . [ 5 ] [ 6 ] [ 7 ] [ 8 ]  It is the primary tool that billions of people worldwide use to interact with the Internet. [ 2 ] The Web was invented by English computer scientist  Tim Berners-Lee  while working at  CERN . [ 9 ] [ 10 ]  He was motivated by the problem of storing, updating, and finding documents and data files in that large and constantly changing organization, as well as distributing them to collaborators outside CERN. In his design, Berners-Lee dismissed the common  tree structure  approach, used for instance in the existing CERNDOC documentation system and in the  Unix filesystem , as well as approaches that relied in tagging files with  keywords , as in the VAX/NOTES system. Instead he adopted concepts he had put into practice with his private  ENQUIRE  system (1980) built at CERN. When he became aware of  Ted Nelson 's  hypertext  model (1965), in which documents can be linked in unconstrained ways through  hyperlinks  associated with \"hot spots\" embedded in the text, it helped to confirm the validity of his concept. [ 11 ] [ 12 ] The model was later popularized by  Apple 's  HyperCard  system. Unlike Hypercard, Berners-Lee's new system from the outset was meant to support links between multiple databases on independent computers, and to allow simultaneous access by many users from any computer on the Internet. He also specified that the system should eventually handle other media besides text, such as graphics, speech, and video. Links could refer to  mutable data files, or even fire up programs on their server computer. He also conceived \"gateways\" that would allow access through the new system to documents organized in other ways (such as traditional computer  file systems  or the  Usenet ). Finally, he insisted that the system should be decentralized, without any central control or coordination over the creation of links. [ 3 ] [ 13 ] [ 9 ] [ 10 ] Berners-Lee submitted a proposal to CERN in May 1989, without giving the system a name. [ 3 ]  He got a working system implemented by the end of 1990, including a browser called   WorldWideWeb  (which became the name of the project and of the network) and  an HTTP server  running at CERN. As part of that development he defined the first version of the HTTP protocol, the basic URL syntax, and implicitly made HTML the primary document format. [ 14 ]  The technology was released outside CERN to other research institutions starting in January 1991, and then to the whole Internet on 23 August 1991. The Web was a success at CERN, and began to spread to other scientific and academic institutions. Within the next two years,  there were 50 websites created . [ 15 ] [ 16 ] CERN made the Web protocol and code available royalty free in 1993, enabling its widespread use. [ 17 ] [ 18 ]  After the  NCSA  released the  Mosaic web browser  later that year, the Web's popularity grew rapidly as  thousands of websites  sprang up in less than a year. [ 19 ] [ 20 ]  Mosaic was a graphical browser that could display inline images and submit  forms  that  were processed by the  HTTPd server . [ 21 ] [ 22 ]   Marc Andreessen  and  Jim Clark  founded  Netscape  the following year and released the  Navigator browser , which introduced  Java  and  JavaScript  to the Web. It quickly became the dominant browser. Netscape  became a public company  in 1995 which triggered a frenzy for the Web and started the  dot-com bubble . [ 23 ]  Microsoft responded by developing its own browser,  Internet Explorer , starting the  browser wars . By bundling it with Windows, it became the dominant browser for 14 years. [ 24 ] Berners-Lee founded the  World Wide Web Consortium  (W3C) which created  XML  in 1996 and recommended replacing HTML with stricter  XHTML . [ 25 ]  In the meantime, developers began exploiting an IE feature called  XMLHttpRequest  to make  Ajax  applications and launched the  Web 2.0  revolution.  Mozilla ,  Opera , and Apple rejected XHTML and created the  WHATWG  which developed  HTML5 . [ 26 ]  In 2009, the W3C conceded and abandoned XHTML. [ 27 ]  In 2019, it ceded control of the HTML specification to the WHATWG. [ 28 ] The World Wide Web has been central to the development of the  Information Age  and is the primary tool billions of people use to interact on the  Internet . [ 29 ] [ 30 ] [ 31 ] [ 8 ] Tim Berners-Lee states that  World Wide Web  is officially spelled as three separate words, each capitalised, with no intervening hyphens. [ 32 ]  Nonetheless, it is often called simply  the Web , and also often  the web ; see  Capitalization of  Internet  for details. In Mandarin Chinese,  World Wide Web  is commonly translated via a  phono-semantic matching  to  wàn wéi wǎng  ( 万维网 ), which satisfies  www  and literally means \"10,000-dimensional net\", a translation that reflects the design concept and proliferation of the World Wide Web. Use of the www prefix has been declining, especially when  web applications  sought to brand their domain names and make them easily pronounceable. As the  mobile Web  grew in popularity, [ citation needed ]  services like  Gmail .com,  Outlook.com ,  Myspace .com,  Facebook .com and  Twitter .com are most often mentioned without adding \"www.\" (or, indeed, \".com\") to the domain. [ 33 ] In English,  www  is usually read as  double-u double-u double-u . [ 34 ]  Some users pronounce it  dub-dub-dub , particularly in New Zealand. [ 35 ]   Stephen Fry , in his \"Podgrams\" series of podcasts, pronounces it  wuh wuh wuh . [ 36 ]  The English writer  Douglas Adams  once quipped in  The Independent  on Sunday  (1999): \"The World Wide Web is the only thing I know of whose shortened form takes three times longer to say than what it's short for\". [ 37 ] The terms  Internet  and  World Wide Web  are often used without much distinction. However, the two terms do not mean the same thing. The Internet is a global system of  computer networks  interconnected through telecommunications and  optical networking . In contrast, the World Wide Web is a global collection of documents and other  resources , linked by hyperlinks and  URIs . Web resources are accessed using  HTTP  or  HTTPS , which are application-level Internet protocols that use the Internet transport protocols. [ 2 ] Viewing a  web page  on the World Wide Web normally begins either by typing the  URL  of the page into a web browser or by following a hyperlink to that page or resource. The web browser then initiates a series of background communication messages to fetch and display the requested page. In the 1990s, using a browser to view web pages—and to move from one web page to another through hyperlinks—came to be known as 'browsing,' 'web surfing' (after  channel surfing ), or 'navigating the Web'. Early studies of this new behaviour investigated user patterns in using web browsers. One study, for example, found five user patterns: exploratory surfing, window surfing, evolved surfing, bounded navigation and targeted navigation. [ 38 ] The following example demonstrates the functioning of a web browser when accessing a page at the URL  http://example.org/home.html . The browser resolves the server name of the URL ( example.org ) into an  Internet Protocol address  using the globally distributed  Domain Name System  (DNS). This lookup returns an IP address such as  203.0.113.4  or  2001:db8:2e::7334 . The browser then requests the resource by sending an  HTTP  request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. HTTP normally uses  port number 80  and for HTTPS it normally uses  port number 443 . The content of the HTTP request can be as simple as two lines of text: The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the web server can fulfil the request it sends an HTTP response back to the browser indicating success: followed by the content of the requested page. Hypertext Markup Language ( HTML ) for a basic web page might look like this: The web browser  parses  the HTML and interprets the markup ( < title > ,  < p >  for paragraph, and such) that surrounds the words to format the text on the screen. Many web pages use HTML to reference the URLs of other resources such as images, other embedded media,  scripts  that affect page behaviour, and  Cascading Style Sheets  that affect page layout. The browser makes additional HTTP requests to the web server for these other  Internet media types . As it receives their content from the web server, the browser progressively  renders  the page onto the screen as specified by its HTML and these additional resources. Hypertext Markup Language (HTML) is the standard  markup language  for creating  web pages  and  web applications . With  Cascading Style Sheets  (CSS) and  JavaScript , it forms a triad of  cornerstone  technologies for the World Wide Web. [ 39 ] Web browsers  receive HTML documents from a  web server  or from local storage and  render  the documents into multimedia web pages. HTML describes the structure of a web page  semantically  and originally included cues for the appearance of the document. HTML elements  are the building blocks of HTML pages. With HTML constructs,  images  and other objects such as  interactive forms  may be embedded into the rendered page. HTML provides a means to create  structured documents  by denoting structural  semantics  for text such as headings, paragraphs, lists,  links , quotes and other items. HTML elements are delineated by  tags , written using  angle brackets . Tags such as  < img   />  and  < input   />  directly introduce content into the page. Other tags such as  < p >  surround and provide information about document text and may include other tags as sub-elements. Browsers do not display the HTML tags, but use them to interpret the content of the page. HTML can embed programs written in a  scripting language  such as  JavaScript , which affects the behaviour and content of web pages. Inclusion of CSS defines the look and layout of content. The  World Wide Web Consortium  (W3C), maintainer of both the HTML and the CSS standards, has encouraged the use of CSS over explicit presentational HTML since 1997. [update] [ 40 ] Most web pages contain hyperlinks to other related pages and perhaps to downloadable files, source documents, definitions and other web resources. In the underlying HTML, a hyperlink looks like this:\n < a   href = \"http://example.org/home.html\" > Example.org Homepage </ a > . Such a collection of useful, related resources, interconnected via hypertext links is dubbed a  web  of information. Publication on the Internet created what Tim Berners-Lee first called the  WorldWideWeb  (in its original  CamelCase , which was subsequently discarded) in November 1990. [ 41 ] The hyperlink structure of the web is described by the  webgraph : the nodes of the web graph correspond to the web pages (or URLs) the directed edges between them to the hyperlinks. Over time, many web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This makes hyperlinks obsolete, a phenomenon referred to in some circles as link rot, and the hyperlinks affected by it are often called  \"dead\" links . The ephemeral nature of the Web has prompted many efforts to archive websites. The  Internet Archive , active since 1996, is the best known of such efforts. Many hostnames used for the World Wide Web begin with  www  because of the long-standing practice of naming  Internet  hosts according to the services they provide. The  hostname  of a  web server  is often  www , in the same way that it may be  ftp  for an  FTP server , and  news  or  nntp  for a  Usenet   news server . These hostnames appear as Domain Name System (DNS) or  subdomain  names, as in  www.example.com . The use of  www  is not required by any technical or policy standard and many websites do not use it; the first web server was  nxoc01.cern.ch . [ 42 ]  According to Paolo Palazzi, who worked at CERN along with Tim Berners-Lee, the popular use of  www  as subdomain was accidental; the World Wide Web project page was intended to be published at www.cern.ch while info.cern.ch was intended to be the CERN home page; however the DNS records were never switched, and the practice of prepending  www  to an institution's website domain name was subsequently copied. [ 43 ] [ better source needed ]  Many established websites still use the prefix, or they employ other subdomain names such as  www2 ,  secure  or  en  for special purposes. Many such web servers are set up so that both the main domain name (e.g., example.com) and the  www  subdomain (e.g., www.example.com) refer to the same site; others require one form or the other, or they may map to different web sites. The use of a subdomain name is useful for  load balancing  incoming web traffic by creating a  CNAME record  that points to a cluster of web servers. Since, currently [ as of? ] , only a subdomain can be used in a CNAME, the same result cannot be achieved by using the bare domain root. [ 44 ] [ dubious  –  discuss ] When a user submits an incomplete domain name to a web browser in its address bar input field, some web browsers automatically try adding the prefix \"www\" to the beginning of it and possibly \".com\", \".org\" and \".net\" at the end, depending on what might be missing. For example, entering \"microsoft\" may be transformed to  http://www.microsoft.com/  and \"openoffice\" to  http://www.openoffice.org . This feature started appearing in early versions of  Firefox , when it still had the working title 'Firebird' in early 2003, from an earlier practice in browsers such as  Lynx . [ 45 ]   [ unreliable source? ]  It is reported that Microsoft was granted a US patent for the same idea in 2008, but only for mobile devices. [ 46 ] The scheme specifiers  http://  and  https://  at the start of a web  URI  refer to  Hypertext Transfer Protocol  or  HTTP Secure , respectively. They specify the communication protocol to use for the request and response. The HTTP protocol is fundamental to the operation of the World Wide Web, and the added encryption layer in HTTPS is essential when browsers send or retrieve confidential data, such as passwords or banking information. Web browsers usually automatically prepend http:// to user-entered URIs, if omitted. A  web page  (also written as  webpage ) is a document that is suitable for the World Wide Web and  web browsers . A web browser displays a web page on a  monitor  or  mobile device . The term  web page  usually refers to what is visible, but may also refer to the contents of the  computer file  itself, which is usually a  text file  containing  hypertext  written in  HTML  or a comparable  markup language . Typical web pages provide  hypertext  for browsing to other web pages via  hyperlinks , often referred to as  links . Web browsers will frequently have to access multiple  web resource  elements, such as reading  style sheets ,  scripts , and images, while presenting each web page. On a network, a web browser can retrieve a web page from a remote  web server . The web server may restrict access to a private network such as a corporate intranet. The web browser uses the  Hypertext Transfer Protocol  (HTTP) to make such requests to the  web server . A  static  web page  is delivered exactly as stored, as  web content  in the web server's  file system . In contrast, a  dynamic  web page  is generated by a  web application , usually driven by  server-side software . Dynamic web pages are used when each user may require completely different information, for example, bank websites, web email etc. A  static web page  (sometimes called a  flat page/stationary page ) is a  web page  that is delivered to the user exactly as stored, in contrast to  dynamic web pages  which are generated by a  web application . Consequently, a static web page displays the same information for all users, from all contexts, subject to modern capabilities of a  web server  to  negotiate   content-type  or language of the document where such versions are available and the server is configured to do so. A  server-side dynamic web page  is a  web page  whose construction is controlled by an  application server  processing server-side scripts. In server-side scripting,  parameters  determine how the assembly of every new web page proceeds, including the setting up of more client-side processing. A  client-side dynamic web page  processes the web page using JavaScript running in the browser. JavaScript programs can interact with the document via  Document Object Model , or DOM, to query page state and alter it. The same client-side techniques can then dynamically update or change the DOM in the same way. A dynamic web page is then reloaded by the user or by a  computer program  to change some variable content. The updating information could come from the server, or from changes made to that page's DOM. This may or may not truncate the browsing history or create a saved version to go back to, but a  dynamic web page update  using  Ajax  technologies will neither create a page to go back to nor truncate the  web browsing history  forward of the displayed page. Using Ajax technologies the end  user  gets  one dynamic page  managed as a single page in the  web browser  while the actual  web content  rendered on that page can vary. The Ajax engine sits only on the browser requesting parts of its DOM,  the  DOM, for its client, from an application server. Dynamic HTML, or DHTML, is the umbrella term for technologies and methods used to create web pages that are not  static web pages , though it has fallen out of common use since the popularization of  AJAX , a term which is now itself rarely used. [ citation needed ]  Client-side-scripting, server-side scripting, or a combination of these make for the dynamic web experience in a browser. JavaScript  is a  scripting language  that was initially developed in 1995 by  Brendan Eich , then of  Netscape , for use within web pages. [ 47 ]  The standardised version is  ECMAScript . [ 47 ]  To make web pages more interactive, some web applications also use JavaScript techniques such as  Ajax  ( asynchronous  JavaScript and  XML ).  Client-side script  is delivered with the page that can make additional HTTP requests to the server, either in response to user actions such as mouse movements or clicks, or based on elapsed time. The server's responses are used to modify the current page rather than creating a new page with each response, so the server needs only to provide limited, incremental information. Multiple Ajax requests can be handled at the same time, and users can interact with the page while data is retrieved. Web pages may also regularly  poll  the server to check whether new information is available. [ 48 ] A  website [ 49 ]  is a collection of related web resources including  web pages ,  multimedia  content, typically identified with a common  domain name , and published on at least one  web server . Notable examples are  wikipedia .org,  google .com, and  amazon.com . A website may be accessible via a public  Internet Protocol  (IP) network, such as the  Internet , or a private  local area network  (LAN), by referencing a  uniform resource locator  (URL) that identifies the site. Websites can have many functions and can be used in various fashions; a website can be a  personal website , a corporate website for a company, a government website, an organization website, etc. Websites are typically dedicated to a particular topic or purpose, ranging from entertainment and  social networking  to providing news and education. All publicly accessible websites collectively constitute the World Wide Web, while private websites, such as a company's website for its employees, are typically a part of an  intranet . Web pages, which are the building blocks of websites, are  documents , typically composed in  plain text  interspersed with  formatting instructions  of Hypertext Markup Language ( HTML ,  XHTML ). They may incorporate elements from other websites with suitable  markup anchors . Web pages are accessed and transported with the  Hypertext Transfer Protocol  (HTTP), which may optionally employ encryption ( HTTP Secure , HTTPS) to provide security and privacy for the user. The user's application, often a  web browser , renders the page content according to its HTML markup instructions onto a  display terminal . Hyperlinking  between web pages conveys to the reader the  site structure  and guides the navigation of the site, which often starts with a  home page  containing a directory of the site  web content . Some websites require user registration or  subscription  to access content. Examples of  subscription websites  include many business sites, news websites,  academic journal  websites, gaming websites, file-sharing websites,  message boards , web-based  email ,  social networking  websites, websites providing real-time price quotations for different types of markets, as well as sites providing various other services.  End users  can access websites on a range of devices, including  desktop  and  laptop computers ,  tablet computers ,  smartphones  and  smart TVs . A  web browser  (commonly referred to as a  browser ) is a  software   user agent  for accessing information on the World Wide Web. To connect to a website's  server  and display its pages, a user needs to have a web browser program. This is the program that the user runs to download, format, and display a web page on the user's computer. In addition to allowing users to find, display, and move between web pages, a web browser will usually have features like keeping bookmarks, recording history, managing cookies (see below), and home pages and may have facilities for recording passwords for logging into web sites. The most popular browsers are  Chrome ,  Firefox ,  Safari ,  Internet Explorer , and  Edge . A  Web server  is  server software , or hardware dedicated to running said software, that can satisfy World Wide Web client requests. A web server can, in general, contain one or more websites. A web server processes incoming network requests over  HTTP  and several other related protocols. The primary function of a web server is to store, process and deliver  web pages  to  clients . [ 50 ]  The communication between client and server takes place using the  Hypertext Transfer Protocol (HTTP) . Pages delivered are most frequently  HTML documents , which may include  images ,  style sheets  and  scripts  in addition to the text content. A  user agent , commonly a  web browser  or  web crawler , initiates communication by making a  request  for a specific resource using HTTP and the server responds with the content of that resource or an  error message  if unable to do so. The resource is typically a real file on the server's  secondary storage , but this is not necessarily the case and depends on how the webserver is  implemented . While the primary function is to serve content, full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting  web forms , including  uploading  of files. Many generic web servers also support  server-side scripting  using  Active Server Pages  (ASP),  PHP  (Hypertext Preprocessor), or other  scripting languages . This means that the behaviour of the webserver can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents  dynamically  (\"on-the-fly\") as opposed to returning  static documents . The former is primarily used for retrieving or modifying information from  databases . The latter is typically much faster and more easily  cached  but cannot deliver  dynamic content . Web servers can also frequently be found  embedded  in devices such as  printers ,  routers ,  webcams  and serving only a  local network . The web server may then be used as a part of a system for monitoring or administering the device in question. This usually means that no additional software has to be installed on the client computer since only a web browser is required (which now is included with most  operating systems ). An  HTTP cookie  (also called  web cookie ,  Internet cookie ,  browser cookie , or simply  cookie ) is a small piece of data sent from a website and stored on the user's computer by the user's  web browser  while the user is browsing. Cookies were designed to be a reliable mechanism for websites to remember  stateful  information (such as items added in the shopping cart in an online store) or to record the user's browsing activity (including clicking particular buttons,  logging in , or recording which pages were visited in the past). They can also be used to remember arbitrary pieces of information that the user previously entered into form fields such as names, addresses, passwords, and credit card numbers. Cookies perform essential functions in the modern web. Perhaps most importantly,  authentication cookies  are the most common method used by web servers to know whether the user is logged in or not, and which account they are logged in with. Without such a mechanism, the site would not know whether to send a page containing sensitive information or require the user to authenticate themselves by logging in. The security of an authentication cookie generally depends on the security of the issuing website and the user's  web browser , and on whether the cookie data is encrypted. Security vulnerabilities may allow a cookie's data to be read by a  hacker , used to gain access to user data, or used to gain access (with the user's credentials) to the website to which the cookie belongs (see  cross-site scripting  and  cross-site request forgery  for examples). [ 51 ] Tracking cookies, and especially third-party tracking cookies, are commonly used as ways to compile long-term records of individuals' browsing histories – a potential  privacy concern  that prompted European [ 52 ]  and U.S. lawmakers to take action in 2011. [ 53 ] [ 54 ]  European law requires that all websites targeting  European Union  member states gain \"informed consent\" from users before storing non-essential cookies on their device. Google  Project Zero  researcher Jann Horn describes ways cookies can be read by  intermediaries , like  Wi-Fi  hotspot providers. When in such circumstances, he recommends using the browser in  private browsing  mode (widely known as  Incognito mode  in Google Chrome). [ 55 ] A  web search engine  or  Internet search engine  is a  software system  that is designed to carry out  web search  ( Internet search ), which means to search the World Wide Web in a systematic way for particular information specified in a  web search query . The search results are generally presented in a line of results, often referred to as  search engine results pages  (SERPs). The information may be a mix of  web pages , images, videos, infographics, articles, research papers, and other types of files. Some search engines also  mine data  available in  databases  or  open directories . Unlike  web directories , which are maintained only by human editors, search engines also maintain  real-time  information by running an  algorithm  on a  web crawler . Internet content that is not capable of being searched by a web search engine is generally described as the  deep web . The deep web, [ 56 ]   invisible web , [ 57 ]  or  hidden web [ 58 ]  are parts of the World Wide Web whose contents are not  indexed  by standard  web search engines . The opposite term to the deep web is the  surface web , which is accessible to anyone using the Internet. [ 59 ]   Computer scientist  Michael K. Bergman is credited with coining the term  deep web  in 2001 as a search indexing term. [ 60 ] The content of the deep web is hidden behind  HTTP  forms, [ 61 ] [ 62 ]  and includes many very common uses such as  web mail ,  online banking , and services that users must pay for, and which is protected by a  paywall , such as  video on demand , some online magazines and newspapers, among others. The content of the deep web can be located and accessed by a direct  URL  or  IP address  and may require a password or other security access past the public website page. A  web cache  is a server computer located either on the public Internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request. Most web browsers also implement a  browser cache  by writing recently obtained data to a local data storage device. HTTP requests by a browser may ask only for data that has changed since the last access. Web pages and resources may contain expiration information to control caching to secure sensitive data, such as in  online banking , or to facilitate frequently updated sites, such as news media. Even sites with highly dynamic content may permit basic resources to be refreshed only occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. Enterprise  firewalls  often cache Web resources requested by one user for the benefit of many users. Some  search engines  store cached content of frequently accessed websites. For  criminals , the Web has become a venue to spread  malware  and engage in a range of  cybercrimes , including (but not limited to)  identity theft ,  fraud ,  espionage  and  intelligence gathering . [ 63 ]  Web-based  vulnerabilities  now outnumber traditional computer security concerns, [ 64 ] [ 65 ]  and as measured by  Google , about one in ten web pages may contain malicious code. [ 66 ]  Most web-based  attacks  take place on legitimate websites, and most, as measured by  Sophos , are hosted in the United States, China and Russia. [ 67 ]  The most common of all malware  threats  is  SQL injection  attacks against websites. [ 68 ]  Through HTML and URIs, the Web was vulnerable to attacks like  cross-site scripting  (XSS) that came with the introduction of JavaScript [ 69 ]  and were exacerbated to some degree by  Web 2.0  and Ajax  web design  that favours the use of scripts. [ 70 ]  In one 2007 estimate, 70% of all websites are open to XSS attacks on their users. [ 71 ]   Phishing  is another common threat to the Web. In February 2013, RSA (the security division of EMC) estimated the global losses from phishing at $1.5 billion in 2012. [ 72 ]  Two of the well-known phishing methods are Covert Redirect and Open Redirect. Proposed solutions vary. Large security companies like  McAfee  already design governance and compliance suites to meet post-9/11 regulations, [ 73 ]  and some, like  Finjan Holdings  have recommended active real-time inspection of programming code and all content regardless of its source. [ 63 ]  Some have argued that for enterprises to see Web security as a business opportunity rather than a  cost centre , [ 74 ]  while others call for \"ubiquitous, always-on  digital rights management \" enforced in the infrastructure to replace the hundreds of companies that secure data and networks. [ 75 ]   Jonathan Zittrain  has said users sharing responsibility for computing safety is far preferable to locking down the Internet. [ 76 ] Every time a client requests a web page, the server can identify the request's  IP address . Web servers usually log IP addresses in a  log file . Also, unless set not to do so, most web browsers record requested web pages in a viewable  history  feature, and usually  cache  much of the content locally. Unless the server-browser communication uses HTTPS encryption, web requests and responses travel in plain text across the Internet and can be viewed, recorded, and cached by intermediate systems. Another way to hide  personally identifiable information  is by using a  virtual private network . A VPN  encrypts  traffic between the client and VPN server, and masks the original IP address, lowering the chance of user identification. When a web page asks for, and the user supplies, personally identifiable information—such as their real name, address, e-mail address, etc. web-based entities can associate current web traffic with that individual. If the website uses  HTTP cookies , username, and password authentication, or other tracking techniques, it can relate other web visits, before and after, to the identifiable information provided. In this way, a web-based organization can develop and build a profile of the individual people who use its site or sites. It may be able to build a record for an individual that includes information about their leisure activities, their shopping interests, their profession, and other aspects of their  demographic profile . These profiles are of potential interest to marketers, advertisers, and others. Depending on the website's  terms and conditions  and the local laws that apply information from these profiles may be sold, shared, or passed to other organizations without the user being informed. For many ordinary people, this means little more than some unexpected emails in their inbox or some uncannily relevant advertising on a future web page. For others, it can mean that time spent indulging an unusual interest can result in a deluge of further targeted marketing that may be unwelcome. Law enforcement, counterterrorism, and espionage agencies can also identify, target, and track individuals based on their interests or proclivities on the Web. Social networking  sites usually try to get users to use their real names, interests, and locations, rather than pseudonyms, as their executives believe that this makes the social networking experience more engaging for users. On the other hand, uploaded photographs or unguarded statements can be identified to an individual, who may regret this exposure. Employers, schools, parents, and other relatives may be influenced by aspects of social networking profiles, such as text posts or digital photos, that the posting individual did not intend for these audiences.  Online bullies  may make use of personal information to harass or  stalk  users. Modern social networking websites allow fine-grained control of the privacy settings for each posting, but these can be complex and not easy to find or use, especially for beginners. [ 77 ]  Photographs and videos posted onto websites have caused particular problems, as they can add a person's face to an online profile. With modern and potential  facial recognition technology , it may then be possible to relate that face with other, previously anonymous, images, events, and scenarios that have been imaged elsewhere. Due to image caching, mirroring, and copying, it is difficult to remove an image from the World Wide Web. Web standards include many interdependent standards and specifications, some of which govern aspects of the  Internet , not just the World Wide Web. Even when not web-focused, such standards directly or indirectly affect the development and administration of websites and  web services . Considerations include the  interoperability ,  accessibility  and  usability  of web pages and web sites. Web standards, in the broader sense, consist of the following: Web standards are not fixed sets of rules but are constantly evolving sets of finalized technical specifications of web technologies. [ 84 ]  Web standards are developed by  standards organizations —groups of interested and often competing parties chartered with the task of standardization—not technologies developed and declared to be a standard by a single individual or company. It is crucial to distinguish those specifications that are under development from the ones that already reached the final development status (in the case of  W3C  specifications, the highest maturity level). There are methods for accessing the Web in alternative mediums and formats to facilitate use by individuals with  disabilities . These disabilities may be visual, auditory, physical, speech-related, cognitive, neurological, or some combination. Accessibility features also help people with temporary disabilities, like a broken arm, or ageing users as their abilities change. [ 85 ]  The Web is receiving information as well as providing information and interacting with society. The World Wide Web Consortium claims that it is essential that the Web be accessible, so it can provide equal access and  equal opportunity  to people with disabilities. [ 86 ]  Tim Berners-Lee once noted, \"The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect.\" [ 85 ]  Many countries regulate web accessibility as a requirement for websites. [ 87 ]  International co-operation in the W3C  Web Accessibility Initiative  led to simple guidelines that web content authors as well as software developers can use to make the Web accessible to persons who may or may not be using  assistive technology . [ 85 ] [ 88 ] The W3C  Internationalisation  Activity assures that web technology works in all languages, scripts, and cultures. [ 89 ]  Beginning in 2004 or 2005,  Unicode  gained ground and eventually in December 2007 surpassed both  ASCII  and Western European as the Web's most frequently used  character map . [ 90 ]  Originally  RFC   3986  allowed resources to be identified by  URI  in a subset of US-ASCII.  RFC   3987  allows more characters—any character in the  Universal Character Set —and now a resource can be identified by  IRI  in any language. [ 91 ]"
  },
  {
    "id": 156,
    "title": "Tim Berners-Lee",
    "content": "Sir Timothy John Berners-Lee  (born 8 June 1955), [ 1 ]  also known as  TimBL , is an English  computer scientist  best known as the inventor of the  World Wide Web , the  HTML   markup language , the  URL  system, and  HTTP . He is a professorial research fellow at the  University of Oxford [ 2 ]  and a professor emeritus at the  Massachusetts Institute of Technology  (MIT). [ 3 ] [ 4 ] Berners-Lee proposed an information management system on 12 March 1989 [ 5 ] [ 6 ]  and implemented the first successful communication between a Hypertext Transfer Protocol (HTTP) client and  server  via the  Internet  in mid-November. [ 7 ] [ 8 ] [ 9 ] [ 10 ] [ 11 ] \nHe devised and implemented the first Web browser and Web server and helped foster the Web's subsequent explosive development. He is the founder and director of the  World Wide Web Consortium  (W3C), which oversees the continued development of the Web. He co-founded (with  Rosemary Leith ) the  World Wide Web Foundation . In April 2009, he was elected as  Foreign Associate of the National Academy of Sciences . [ 12 ] [ 13 ] Berners-Lee was previously a senior researcher and holder of the  3Com  founder's chair at the  MIT Computer Science and Artificial Intelligence Laboratory  (CSAIL). [ 14 ]  He is a director of the  Web Science Research Initiative  (WSRI) [ 15 ]  and a member of the advisory board of the  MIT Center for Collective Intelligence . [ 16 ] [ 17 ]  In 2011, he was named as a member of the board of trustees of the  Ford Foundation . [ 18 ]  He is a founder and president of the  Open Data Institute  and is currently an advisor at social network  MeWe . [ 19 ]  In 2004, Berners-Lee was knighted by Queen  Elizabeth II  for his pioneering work. [ 20 ] [ 21 ]  He received the 2016  Turing Award  \"for inventing the World Wide Web, the first web browser, and the fundamental protocols and algorithms allowing the Web to scale\". [ 22 ]  He was named in  Time  magazine's list of the  100 Most Important People of the 20th century  and has received a  number of other accolades  for his invention. [ 23 ] Berners-Lee was born in  London  on 8 June 1955, [ 24 ]  the son of mathematicians and computer scientists  Mary Lee Woods  (1924–2017) and  Conway Berners-Lee  (1921–2019). His parents were both from  Birmingham  and worked on the  Ferranti Mark 1 , the first commercially-built computer. He has three younger siblings; his brother,  Mike , is a professor of ecology and  climate change  management. Berners-Lee attended Sheen Mount Primary School, then attended  Emanuel School  (a  direct grant grammar school  at the time) from 1969 to 1973. [ 1 ] [ 20 ]  A keen  trainspotter  as a child, he learnt about electronics from tinkering with a  model railway . [ 25 ] From 1973 to 1976, he studied at  The Queen's College, Oxford , where he received a  first-class   BA  in physics. [ 1 ] [ 24 ]  While there, he made a computer out of an old television set he had purchased from a repair shop. [ 26 ] After graduation, Berners-Lee worked as an engineer at the telecommunications company  Plessey  in  Poole , Dorset. [ 24 ]  In 1978, he joined D. G. Nash in  Ferndown , Dorset, where he helped create typesetting software for printers. [ 24 ] Berners-Lee worked as an  independent contractor  at  CERN  from June to December 1980. While in  Geneva , he proposed a project based on the concept of  hypertext , to facilitate sharing and updating information among researchers. [ 27 ]  To demonstrate it, he built a prototype system named  ENQUIRE . [ 28 ] After leaving CERN in late 1980, he went to work at John Poole's Image Computer Systems, Ltd, in Bournemouth, Dorset. [ 29 ]  He ran the company's technical side for three years. [ 30 ]  The project he worked on was a \" real-time   remote procedure call \" which gave him experience in  computer networking . [ 29 ]  In 1984, he returned to CERN as a fellow. [ 28 ] In 1989, CERN was the largest Internet node in Europe and Berners-Lee saw an opportunity to join hypertext with the Internet: I just had to take the hypertext idea and connect it to the  TCP  and  DNS  ideas and—ta-da!—the World Wide Web. Creating the web was really an act of desperation, because the situation without it was very difficult when I was working at CERN later. Most of the technology involved in the web, like the hypertext, like the Internet, multifont text objects, had all been designed already. I just had to put them together. It was a step of generalising, going to a higher level of abstraction, thinking about all the documentation systems out there as being possibly part of a larger imaginary documentation system. Berners-Lee wrote his proposal in March 1989 and, in 1990, redistributed it. It then was accepted by his manager, Mike Sendall, who called his proposals \"vague, but exciting\". [ 33 ]   Robert Cailliau  had independently proposed a project to develop a hypertext system at CERN, and joined Berners-Lee as a partner in his efforts to get the web off the ground. [ 28 ]  They used similar ideas to those underlying the  ENQUIRE  system to create the  World Wide Web , for which Berners-Lee designed and built the first  web browser . His software also functioned as an editor (called  WorldWideWeb , running on the  NeXTSTEP  operating system), and the first Web server,  CERN HTTPd  (short for Hypertext Transfer Protocol  daemon ). Berners-Lee published the first web site, which described the project itself, on 20 December 1990; it was available to the Internet from the CERN network. The site provided an explanation of what the World Wide Web was, and how people could use a browser and set up a web server, as well as how to get started with your own website. [ 34 ] [ 35 ] [ 36 ] [ 26 ]  On 6 August 1991, Berners-Lee first posted, on  Usenet , a public invitation for collaboration with the WorldWideWeb project. [ 37 ] In a list of 80 cultural moments that shaped the world, chosen by a panel of 25 eminent scientists, academics, writers and world leaders, the invention of the World Wide Web was ranked number one, with the entry stating, \"The fastest growing communications medium of all time, the Internet has changed the shape of modern life forever. We can connect with each other instantly, all over the world.\" [ 38 ] In 1994, Berners-Lee founded the W3C at the  Massachusetts Institute of Technology . It comprised various companies that were willing to create standards and recommendations to improve the quality of the Web. Berners-Lee made his idea available freely, with no patent and no royalties due. The World Wide Web Consortium decided that its standards should be based on royalty-free technology, so that they easily could be adopted by anyone. [ 39 ] Berners-Lee participated in Curl Corp's attempt to develop and promote the  Curl programming language . [ 40 ] In 2001, Berners-Lee became a patron of the East Dorset Heritage Trust, having previously lived in  Colehill  in  Wimborne ,  East Dorset . [ 41 ]  In December 2004, he accepted a chair in computer science at the School of Electronics and Computer Science,  University of Southampton , Hampshire, to work on the  Semantic Web . [ 42 ] [ 43 ] In a  Times  article in October 2009, Berners-Lee admitted that  the initial pair of slashes  (\"//\") in a web address were \"unnecessary\". He told the newspaper that he easily could have designed web addresses without the slashes. \"There you go, it seemed like a good idea at the time,\" he said in his lighthearted apology. [ 44 ] By 2010, he created  data.gov.uk  alongside  Nigel Shadbolt . Commenting on the  Ordnance Survey  data in April 2010, Berners-Lee said: \"The changes signal a wider cultural change in government based on an assumption that information should be in the public domain unless there is a good reason not to—not the other way around.\" He went on to say: \"Greater openness, accountability and transparency in Government will give people greater choice and make it easier for individuals to get more directly involved in issues that matter to them.\" [ 45 ] In November 2009, Berners-Lee launched the  World Wide Web Foundation  (WWWF) in order to campaign to \"advance the Web to empower humanity by launching transformative programs that build local capacity to leverage the Web as a medium for positive change\". [ 46 ] Berners-Lee is one of the pioneer voices in favour of  net neutrality , [ 47 ]  and has expressed the view that  ISPs  should supply \"connectivity with no strings attached\", and should neither control nor monitor the browsing activities of customers without their expressed consent. [ 48 ] [ 49 ]  He advocates the idea that net neutrality is a kind of human network right: \"Threats to the Internet, such as companies or governments that interfere with or snoop on Internet traffic, compromise basic human network rights.\" [ 50 ]  Berners-Lee participated in an open letter to the US Federal Communications Commission (FCC). He and 20 other Internet pioneers urged the FCC to cancel a vote on 14 December 2017 to uphold net neutrality. The letter was addressed to Senator  Roger Wicker , Senator  Brian Schatz , Representative  Marsha Blackburn  and Representative Michael F. Doyle. [ 51 ] Berners-Lee was honoured as the \"Inventor of the World Wide Web\" during the  2012 Summer Olympics opening ceremony , in which he appeared working with a vintage  NeXT Computer . [ 52 ]  He tweeted \"This is for everyone\" [ 53 ]  which appeared in LED lights attached to the chairs of the audience. [ 52 ] Berners-Lee joined the board of advisors of start-up  State.com , based in London. [ 54 ]  As of May 2012, he is president of the  Open Data Institute , [ 55 ]  which he co-founded with  Nigel Shadbolt  in 2012. The  Alliance for Affordable Internet  (A4AI) was launched in October 2013, and Berners-Lee is leading the coalition of public and private organisations that includes  Google ,  Facebook ,  Intel  and  Microsoft . The A4AI seeks to make Internet access more affordable so that access is broadened in the developing world, where only 31% of people are online. Berners-Lee will work with those aiming to decrease Internet access prices so that they fall below the  UN Broadband Commission 's worldwide target of 5% of monthly income. [ 56 ] Berners-Lee holds the founders chair in Computer Science at the  Massachusetts Institute of Technology , where he heads the Decentralized Information Group and is leading  Solid , a joint project with the  Qatar Computing Research Institute  that aims to radically change the way Web applications work today, resulting in true data ownership as well as improved privacy. [ 57 ]  In October 2016, he joined the  Department of Computer Science  at  Oxford University  as a professorial research fellow [ 58 ]  and as a  fellow  of  Christ Church , one of the Oxford colleges. [ 59 ] From the mid-2010s, Berners-Lee initially remained neutral on the emerging  Encrypted Media Extensions  (EME) proposal with its controversial  digital rights management  (DRM) implications. [ 60 ]  In March 2017 he felt he had to take a position which was to support the EME proposal. [ 60 ]   He reasoned EME's virtues whilst noting DRM was inevitable. [ 60 ]  As W3C director, he went on to approve the finalised specification in July 2017. [ 61 ] [ 60 ]  His stance was opposed by some including  Electronic Frontier Foundation  (EFF), the anti-DRM campaign  Defective by Design  and the  Free Software Foundation . [ 61 ]  Varied concerns raised included being not supportive of the Internet's open philosophy against commercial interests and risks of users being forced to use a particular  web browser  to view specific DRM content. [ 60 ]  The EFF raised a formal appeal which did not succeed and the EME specification became a formal W3C recommendation in September 2017. [ 62 ] On 30 September 2018, Berners-Lee announced his new  open-source  startup  Inrupt  to fuel a commercial ecosystem around the  Solid  project, which aims to give users more control over their personal data and lets them choose where the data goes, who's allowed to see certain elements and which apps are allowed to see that data. [ 63 ] [ 64 ] In November 2019, at the  Internet Governance Forum  in Berlin, Berners-Lee and the WWWF launched  Contract for the Web , a campaign initiative to persuade governments, companies and citizens to commit to nine principles to stop \"misuse\", with the warning that \"if we don't act now – and act together – to prevent the web being misused by those who want to exploit, divide and undermine, we are at risk of squandering [its potential for good]\". [ 65 ] \"He wove the World Wide Web and created a mass medium for the 21st century. The World Wide Web is Berners-Lee's alone. He designed it. He loosed it on the world. And he more than anyone else has fought to keep it open, nonproprietary and free.\" Berners-Lee has received many awards and honours. He was  knighted  by Queen  Elizabeth II  in the  2004 New Year Honours  \"for services to the global development of the Internet\", and was invested formally on 16 July 2004. [ 20 ] [ 21 ] On 13 June 2007, he was appointed to the  Order of Merit  (OM), an order restricted to 24 living members, plus any honorary members. [ 66 ]  Bestowing membership of the Order of Merit is within the personal purview of the Sovereign and does not require recommendation by ministers or the Prime Minister. He was elected a  Fellow of the Royal Society (FRS) in 2001 . [ 67 ]  He was also elected as a member into the  American Philosophical Society  in 2004 [ 68 ]  and the  National Academy of Engineering  in 2007. He has been conferred honorary degrees from a number of universities around the world, including  Manchester  (his parents worked on the  Manchester Mark 1  in the 1940s),  Harvard  and  Yale . [ 69 ] [ 70 ] [ 71 ] In 2012, Berners-Lee was among the  British cultural icons  selected by artist  Sir Peter Blake  to appear in a new version of his most famous artwork – the Beatles'  Sgt. Pepper's Lonely Hearts Club Band  album cover – to celebrate the British cultural figures of his life that he most admires to mark his 80th birthday. [ 72 ] [ 73 ] In 2013, he was awarded the inaugural  Queen Elizabeth Prize for Engineering . [ 74 ]  On 4 April 2017, he received the 2016  Association for Computing Machinery 's  Turing Award  for his invention of the World Wide Web, the first web browser, and their fundamental protocols and algorithms. [ 22 ] Berners-Lee has said \"I like to keep work and personal life separate.\" [ 75 ] Berners-Lee married Nancy Carlson, an American computer programmer, in 1990. She was also working in Switzerland at the  World Health Organization . [ 76 ]  They had two children and divorced in 2011. In 2014, he married  Rosemary Leith  at the  Chapel Royal ,  St. James's Palace  in London. [ 77 ]  Leith is a Canadian Internet and banking entrepreneur and a founding director of Berners-Lee's  World Wide Web Foundation . [ 78 ]  The couple also collaborate on venture capital to support artificial intelligence companies. [ 79 ] Berners-Lee was raised as an  Anglican , but he turned away from religion in his youth. After he became a parent, he became a  Unitarian Universalist  (UU). [ 80 ]  When asked whether he believes in God, he stated: \"Not in the sense of most people, I'm atheist and Unitarian Universalist.\" [ 81 ] The  web 's  source code  was auctioned by  Sotheby's  in  London  during 23–30 June 2021, as a  non-fungible token  (NFT) by TimBL. [ 82 ] [ 83 ] [ 84 ]  Selling for US$5,434,500, [ 85 ]  it was reported the proceeds would be used to fund initiatives by TimBL and Leith. [ 84 ] [ 82 ]"
  },
  {
    "id": 157,
    "title": "CERN",
    "content": "The  European Organization for Nuclear Research , known as  CERN  ( / s ɜːr n / ;  French pronunciation:   [sɛʁn] ;  Organisation européenne pour la recherche nucléaire ), is an  intergovernmental organization  that operates the largest  particle physics  laboratory in the world. Established in 1954, it is based in  Meyrin , western suburb of  Geneva , on the  France–Switzerland border . It comprises  24 member states . [ 4 ]  Israel, admitted in 2013, is the only non-European full member. [ 5 ] [ 6 ]  CERN is an official  United Nations General Assembly observer . [ 7 ] The acronym CERN is also used to refer to the laboratory; in 2019, it had 2,660 scientific, technical, and administrative staff members, and hosted about 12,400 users from institutions in more than 70 countries. [ 8 ]  In 2016, CERN generated 49  petabytes  of data. [ 9 ] CERN's main function is to provide the  particle accelerators  and other infrastructure needed for high-energy physics research – consequently, numerous experiments have been constructed at CERN through international collaborations. CERN is the site of the  Large Hadron Collider  (LHC), the world's largest and highest-energy particle collider. [ 10 ]  The main site at  Meyrin  hosts a large computing facility, which is primarily used to store and analyze data from experiments, as well as simulate  events . As researchers require remote access to these facilities, the lab has historically been a major  wide area network  hub. CERN is also the birthplace of the  World Wide Web . [ 11 ] [ 12 ] The convention establishing CERN [ 14 ]  was ratified on 29 September 1954 by 12 countries in Western Europe. [ 15 ]  The acronym CERN originally represented the French words for  Conseil Européen pour la Recherche Nucléaire  ('European Council for Nuclear Research'), which was a provisional council for building the laboratory, established by 12 European governments in 1952. During these early years, the council worked at the  University of Copenhagen  under the direction of  Niels Bohr  before moving to its present site near Geneva. [ 16 ] [ 17 ] The acronym was retained for the new laboratory after the provisional council was dissolved, even though the name changed to the current  Organisation européenne pour la recherche nucléaire  ('European Organization for Nuclear Research') in 1954. [ 18 ] [ 19 ]  According to  Lew Kowarski , a former director of CERN, when the name was changed, the abbreviation could have become the awkward OERN, [ 20 ]  and  Werner Heisenberg  said that this could \"still be CERN even if the name is [not]\". [ 21 ] CERN's first president was Sir  Benjamin Lockspeiser .  Edoardo Amaldi  was the general secretary of CERN at its early stages when operations were still provisional, and the first Director-General (1954) was  Felix Bloch . [ 22 ] The laboratory was originally devoted to the study of  atomic nuclei , but was soon applied to  higher-energy physics , concerned mainly with the study of interactions between  subatomic particles . Therefore, the laboratory operated by CERN is commonly referred to as the  European laboratory for particle physics  ( Laboratoire européen pour la physique des particules ), which better describes the research being performed there. [ citation needed ] At the sixth session of the CERN Council in Paris from 29 June to 1 July 1953, the convention establishing the organization was signed, subject to ratification, by 12 states. The convention was gradually ratified by the 12 founding Member States: Belgium, Denmark, France, the  Federal Republic of Germany , Greece, Italy, the Netherlands, Norway, Sweden, Switzerland, the United Kingdom, and  Yugoslavia . [ 23 ] Several important achievements in particle physics have been made through experiments at CERN. They include: In September 2011, CERN attracted media attention when the  OPERA Collaboration  reported the detection of possibly  faster-than-light neutrinos . [ 39 ]  Further tests showed that the results were flawed due to an incorrectly connected  GPS  synchronization cable. [ 40 ] The 1984  Nobel Prize for Physics  was awarded to  Carlo Rubbia  and  Simon van der Meer  for the developments that resulted in the discoveries of the W and Z bosons. [ 41 ]  The 1992 Nobel Prize for Physics was awarded to CERN staff researcher  Georges Charpak  \"for his invention and development of particle detectors, in particular the  multiwire proportional chamber \". The 2013 Nobel Prize for Physics was awarded to  François Englert  and  Peter Higgs  for the theoretical description of the Higgs mechanism in the year after the Higgs boson was found by CERN experiments. CERN pioneered the introduction of  TCP/IP  for its  intranet , beginning in 1984. This played an influential role in the adoption of the TCP/IP in Europe (see  History of the Internet  and  Protocol Wars ). [ 42 ] In 1989, the  World Wide Web  was invented at CERN by  Tim Berners-Lee . Based on the concept of  hypertext , the idea was designed to facilitate information sharing between researchers. [ 43 ] [ 44 ]  This stemmed from Berners-Lee's earlier work at CERN on a database named  ENQUIRE . A colleague,  Robert Cailliau , became involved in 1990. [ 45 ] [ 46 ] [ 47 ] [ 48 ] In 1995, Berners-Lee and Cailliau were jointly honoured by the  Association for Computing Machinery  for their contributions to the development of the World Wide Web. [ 49 ]  A copy of the first webpage, created by Berners-Lee, is still published on the  World Wide Web Consortium 's website as a historical document. [ 50 ]  The first website was activated in 1991. On 30 April 1993, CERN announced that the World Wide Web would be free to anyone. It became the dominant way through which most users interact with the  Internet . [ 51 ] [ 52 ] More recently, CERN has become a facility for the development of  grid computing , hosting projects including the  Enabling Grids for E-sciencE  (EGEE) and  LHC Computing Grid . It also hosts the  CERN Internet Exchange Point  (CIXP), one of the two main  internet exchange points  in Switzerland. As of 2022 [update] , CERN employs ten times more engineers and technicians than research physicists. [ 53 ] CERN operates a network of seven accelerators and two decelerators, and some additional small accelerators. Each machine in the chain increases the energy of particle beams before delivering them to experiments or to the next more powerful accelerator. The decelerators naturally decrease the energy of particle beams before delivering them to experiments or further accelerators/decelerators. Before an experiment is able to use the network of accelerators, it must be approved by the various  Scientific Committees of CERN . [ 54 ]  Currently (as of 2022) active machines are the LHC accelerator and: Many activities at CERN currently involve operating the  Large Hadron Collider  (LHC) and the experiments for it. The LHC represents a large-scale, worldwide scientific cooperation project. [ 71 ] The LHC tunnel is located 100 metres underground, in the region between  Geneva International Airport  and the nearby  Jura mountains . The majority of its length is on the French side of the border. It uses the 27 km circumference circular tunnel previously occupied by the  Large Electron–Positron Collider  (LEP), which was shut down in November 2000. CERN's existing PS/SPS accelerator complexes are used to pre-accelerate protons and lead ions which are then injected into the LHC. Eight experiments ( CMS , [ 72 ]   ATLAS , [ 73 ]   LHCb , [ 74 ]   MoEDAL , [ 75 ]   TOTEM , [ 76 ]   LHCf , [ 77 ]   FASER [ 78 ]  and  ALICE [ 79 ] ) are located along the collider; each of them studies particle collisions from a different aspect, and with different technologies. Construction for these experiments required an extraordinary engineering effort. For example, a special  crane  was rented from Belgium to lower pieces of the CMS detector into its cavern, since each piece weighed nearly 2,000 tons. The first of the approximately 5,000 magnets necessary for construction was lowered down a special shaft at in March 2005. The LHC has begun to generate vast quantities of data, which CERN streams to laboratories around the world for distributed processing, making use of a specialized  grid  infrastructure, the  LHC Computing Grid . In April 2005, a trial successfully streamed 600 MB/s to seven different sites across the world. In August 2008, the initial particle beams were injected into the LHC. [ 80 ]  The first beam was circulated through the entire LHC on 10 September 2008, [ 81 ]  but the system failed 10 days later because of a faulty magnet connection, and it was stopped for repairs on 19 September 2008. The LHC resumed operation on 20 November 2009 by successfully circulating two beams, each with an energy of 3.5  teraelectronvolts  (TeV). The challenge for the engineers was then to line up the two beams so that they smashed into each other. This is like \"firing two needles across the Atlantic and getting them to hit each other\" according to Steve Myers, director for accelerators and technology. On 30 March 2010, the LHC successfully collided two proton beams with 3.5 TeV of energy per proton, resulting in a 7 TeV collision energy. This was enough to start the main research program, including the search for the  Higgs boson . When the 7 TeV experimental period ended, the LHC increased to 8 TeV (4 TeV per proton) starting March 2012, and soon began particle collisions at that energy. In July 2012, CERN scientists announced the discovery of a new sub-atomic particle that was later confirmed to be the  Higgs boson . [ 82 ] In March 2013, CERN announced that the measurements performed on the newly found particle allowed it to conclude that it was a Higgs boson. [ 83 ]  In early 2013, the LHC was deactivated for a two-year maintenance period, to strengthen the electrical connections between magnets inside the accelerator and for other upgrades. On 5 April 2015, after two years of maintenance and consolidation, the LHC restarted for a second run. The first ramp to the record-breaking energy of 6.5 TeV was performed on 10 April 2015. [ 84 ] [ 85 ]  In 2016, the design collision rate was exceeded for the first time. [ 86 ]  A second two-year period of shutdown begun at the end of 2018. [ 87 ] [ 88 ] As of October 2019, the construction is on-going to upgrade the LHC's luminosity in a project called  High Luminosity LHC  (HL–LHC). This project should see the LHC accelerator upgraded by 2026 to an order of magnitude higher luminosity. [ 89 ] As part of the HL–LHC upgrade project, also other CERN accelerators and their subsystems are receiving upgrades. Among other work, the LINAC 2 linear accelerator injector was decommissioned and replaced by a new injector accelerator, the  LINAC4 . [ 90 ] CERN, in collaboration with groups worldwide, is investigating two main concepts for future accelerators: A linear electron-positron collider with a new acceleration concept to increase the energy ( CLIC ) and a larger version of the LHC, a project currently named  Future Circular Collider . [ 107 ] The smaller accelerators are on the main  Meyrin  site, also known as the West Area, which was originally built in Switzerland alongside the French border, but has been extended to span the border since 1965. The French side is under Swiss jurisdiction and there is no obvious border within the site, apart from a line of marker stones. The SPS and LEP/LHC tunnels are almost entirely outside the main site, and are mostly buried under French farmland and invisible from the surface. They have surface sites at points around them, either as the location of buildings associated with experiments or other facilities needed to operate the colliders such as  cryogenic  plants and access shafts. The experiments are located at the same underground level as the tunnels at these sites. Three of these experimental sites are in France, with ATLAS in Switzerland, and some of the ancillary cryogenic and access sites are in Switzerland. The largest of the experimental sites is the  Prévessin  site, also known as the North Area, which is the target station for non-collider experiments on the SPS accelerator. Other sites are the ones which were used for the  UA1 ,  UA2  and the LEP experiments. The latter are used by LHC experiments. Outside of the LEP and LHC experiments, most are officially named and numbered after the site where they were located. For example,  NA32  was an experiment looking at the production of so-called \" charmed \" particles and located at the Prévessin (North Area) site.  WA22  used the  Big European Bubble Chamber  (BEBC) at the Meyrin (West Area) site to examine neutrino interactions. The  UA1  and  UA2  experiments were considered to be in the Underground Area, i.e. situated underground at sites on the SPS accelerator. Most of the  roads on the CERN Meyrin and Prévessin sites  are named after famous physicists, such as  Wolfgang Pauli , who pushed for CERN's creation. Other notable names are  Richard Feynman ,  Albert Einstein , and  Bohr . Since its foundation by 12 members in 1954, CERN regularly accepted new members. All new members have remained in the organization continuously since their accession, except Spain and Yugoslavia. Spain first joined CERN in 1961, withdrew in 1969, and rejoined in 1983. Yugoslavia was a founding member of CERN but quit in 1961. Of the 24 members, Israel joined CERN as a full member in January 2014, [ 108 ]  becoming the first, and currently only, non-European full member. [ 109 ] The budget contributions of member states are computed based on their GDP. [ 110 ] Associate Members, Candidates: Three countries have observer status: [ 139 ] Also observers are the following international organizations: Non-Member States (with dates of Co-operation Agreements) currently involved in CERN programmes are: [ 142 ] [ 143 ] CERN also has scientific contacts with the following other countries: [ 142 ] [ 149 ] International research institutions, such as CERN, can aid in science diplomacy. [ 150 ] A large number of institutes around the world are  associated to CERN  through current collaboration agreements and/or historical links. [ 152 ]  The list below contains organizations represented as observers to the CERN Council, organizations to which CERN is an observer and organizations based on the CERN model: .cern  is a  top-level domain  for CERN. [ 160 ] [ 161 ]  It was registered on 13 August 2014. [ 162 ] [ 163 ]  On 20 October 2015, CERN moved its main Website to  https://home.cern . [ 164 ] [ 165 ] The  Open Science movement  focuses on making scientific research openly accessible and on creating knowledge through open tools and processes.  Open access ,  open data ,  open source software  and  hardware ,  open licenses ,  digital preservation  and  reproducible research  are primary components of open science and areas in which CERN has been working towards since its formation. CERN has developed policies and official documents that enable and promote open science, starting with CERN's founding convention in 1953 which indicated that all its results are to be published or made generally available. [ 14 ]  Since then, CERN published its open access policy in 2014, [ 166 ]  which ensures that all publications by CERN authors will be published with  gold open access  and most recently an open data policy that was endorsed by the four main LHC collaborations ( ALICE ,  ATLAS ,  CMS  and  LHCb ). [ 167 ] The open data policy complements the open access policy, addressing the public release of scientific data collected by LHC experiments after a suitable embargo period. Prior to this open data policy, guidelines for data preservation, access and reuse were implemented by each collaboration individually through their own policies which are updated when necessary. [ 168 ] [ 169 ] [ 170 ] [ 171 ] The European Strategy for Particle Physics, a document mandated by the CERN Council that forms the cornerstone of Europe's decision-making for the future of particle physics, was last updated in 2020 and affirmed the organisation's role within the open science landscape by stating: \"The particle physics community should work with the relevant authorities to help shape the emerging consensus on open science to be adopted for publicly-funded research, and should then implement a policy of open science for the field\". [ 172 ] Beyond the policy level, CERN has established a variety of services and tools to enable and guide open science at CERN, and in particle physics more generally. On the publishing side, CERN has initiated and operates a global cooperative project, the  Sponsoring Consortium for Open Access Publishing in Particle Physics , SCOAP3, to convert scientific articles in high-energy physics to open access. In 2018, the SCOAP3 partnership represented 3,000+ libraries from 44 countries and 3 intergovernmental organizations who have worked collectively to convert research articles in high-energy physics across 11 leading journals in the discipline to open access. [ 173 ] [ 174 ] Public-facing results can be served by various CERN-based services depending on their use case: the  CERN Open Data portal , [ 175 ]   Zenodo , the  CERN Document Server , [ 176 ]   INSPIRE  and  HEPData [ 177 ]  are the core services used by the researchers and community at CERN, as well as the wider high-energy physics community for the publication of their documents, data, software, multimedia, etc. CERN's efforts towards preservation and reproducible research are best represented by a suite of services addressing the entire physics analysis lifecycle, such as data, software and computing environment.  CERN Analysis Preservation [ 178 ]  helps researchers to preserve and document the various components of their physics analyses.  REANA  (Reusable Analyses) [ 179 ]  enables the instantiating of preserved research data analyses on the cloud. All services are built using  open source software  and strive towards compliance with best effort principles, such as the  FAIR principles , the  FORCE11 guidelines  and  Plan S , while taking into account relevant activities carried out by the  European Commission . [ 180 ] The CERN Science Gateway, opened in October 2023, [ 181 ]  is CERN's latest facility for science outreach and education. It is home to a range of immersive exhibits, workshops, and shows. The Globe of Science and Innovation , which opened in late 2005, is open to the public. It is used four times a week for special exhibits. The  Microcosm museum  previously hosted another on-site exhibition on  particle physics  and CERN history. It closed permanently on 18 September 2022, in preparation for the installation of the exhibitions in Science Gateway. [ 182 ] CERN also provides daily tours to certain facilities such as the Synchro-cyclotron (CERNs first particle accelerator) and the superconducting magnet workshop. In 2004, a two-metre statue of the  Nataraja , the dancing form of the Hindu god  Shiva , was unveiled at CERN. The statue, symbolizing Shiva's cosmic dance of creation and destruction, was presented by the  Indian government  to celebrate the research center's long association with India. [ 183 ]  A special plaque next to the statue explains the metaphor of Shiva's cosmic dance with quotations from physicist  Fritjof Capra : Hundreds of years ago, Indian artists created visual images of dancing Shivas in a beautiful series of bronzes. In our time, physicists have used the most advanced technology to portray the patterns of the cosmic dance. The metaphor of the cosmic dance thus unifies ancient mythology, religious art and modern physics. [ 184 ] CERN launched its Cultural Policy for engaging with the arts in 2011. [ 185 ] [ 186 ]  The initiative provided the essential framework and foundations for establishing  Arts at CERN , the arts programme of the Laboratory. Since 2012, Arts at CERN has fostered creative dialogue between art and physics through residencies, art commissions, exhibitions and events. Artists across all creative disciplines have been invited to CERN to experience how fundamental science pursues the big questions about our universe. Even before the arts programme officially started, several highly regarded artists visited the laboratory, drawn to physics and fundamental science. In 1972,  James Lee Byars  was the first artist to visit the laboratory and the only one, so far, to feature on the cover of the CERN Courier. [ 187 ]   Mariko Mori , [ 188 ]   Gianni Motti , [ 189 ]   Cerith Wyn Evans , [ 190 ]   John Berger [ 191 ]  and  Anselm Kiefer [ 192 ]  are among the artists who came to CERN in the years that followed. The programmes of Arts at CERN are structured according to their values and vision to create bridges between cultures. Each programme is designed and formed in collaboration with cultural institutions, other partner laboratories, countries, cities and artistic communities eager to connect with CERN's research, support their activities, and contribute to a global network of art and science. They comprise research-led artistic residencies that take place on-site or remotely. More than 200 artists from 80 countries have participated in the residencies to expand their creative practices at the Laboratory, benefiting from the involvement of 400 physicists, engineers and CERN staff. Between 500 and 800 applications are received every year. The programmes comprise Collide, the international residency programme organised in partnership with a city; Connect, a programme of residencies to foster experimentation in art and science at CERN and in scientific organisations worldwide in collaboration with  Pro Helvetia , and Guest Artists, a short stay for artists to stay to engage with CERN's research and community. [ 193 ] [ 194 ] International: General:"
  },
  {
    "id": 158,
    "title": "Ricardo Baeza-Yates",
    "content": "Ricardo A. Baeza-Yates  (born March 21, 1961) is a  Chilean   computer scientist  that currently is the Director of Research of the Institute for Experiential AI at  Northeastern University  in the Silicon Valley campus. He is also part-time professor at  Universitat Pompeu Fabra  in Barcelona and  Universidad de Chile  in Santiago. He is an expert member of the  Global Partnership on Artificial Intelligence , a member of the  Association for Computing Machinery 's US Technology Policy Committee as well as  IEEE 's Ethics Committee. He is member of the Chilean Academy of Sciences (2002), [ 1 ]  founding member of the Chilean Academy of Engineering (2010), corresponding member of the Brazilian Academy of Sciences (2018), [ 2 ]  and member of the  Academia Europaea  (2023). [ 3 ] \nHe is an  ACM Fellow  (2009). [ 4 ]  and an  IEEE Fellow  (2011). [ 5 ]  He is a former member of  Spain 's Advisory Council on AI (2019-2023). From June 2016 until June 2020 he was CTO of NTENT, a semantic search technology company. [ 6 ]  Before, until February 2016, he was VP of Research for  Yahoo! Labs , leading teams in United States, Europe, Asia and Latin America. [ 7 ] He obtained a Ph.D. from the  University of Waterloo  with  Efficient Text Searching , supervised by  Gaston Gonnet  and granted in 1989. [ 8 ] His research interests include: Dr. Baeza-Yates was awarded one of the Spanish national Computer Science awards in 2018 [ 15 ] \nas well as the  J.W. Graham Medal  in Computing and Innovation by the University of Waterloo, Canada, in 2007. \nIn August 2008, Dr. Baeza-Yates was proposed for the first time to the Chilean National Prize in Applied Sciences  (Premio Nacional de Ciencias Aplicadas) [ citation needed ] . He has been proposed again most of even years when this award is given. In 2024, he won the award. [ 16 ]"
  },
  {
    "id": 159,
    "title": "European Conference on Information Retrieval",
    "content": "The  European Conference on Information Retrieval  (ECIR) is the main \nEuropean research conference for the presentation of new results in the field of  information retrieval  (IR).\nIt is organized by the  Information Retrieval Specialist Group  of the  British Computer Society  (BCS-IRSG). The event started its life as the  Annual Colloquium on Information Retrieval Research  in 1978 and was \nheld in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has\nalternated between the United Kingdom and continental Europe. To mark the metamorphosis\nfrom a small informal colloquium to a major event in the IR research calendar, the \nBCS-IRSG later renamed the event to  European Conference on Information Retrieval . In recent years,\nECIR has continued to grow and has become the major European forum for the discussion\nof research in the field of Information Retrieval. Some of the topics dealt with include: The ECIR is generally held in Spring, near the Easter weekend. A list of locations and planned venues are presented below. *as the Annual Colloquium on Information Retrieval Research"
  },
  {
    "id": 160,
    "title": "Conference on Information and Knowledge Management",
    "content": "The  ACM   Conference on Information and Knowledge Management  ( CIKM , pronounced  /ˈsikəm/ ) is an annual  computer science   research conference  dedicated to  information management  (IM) and  knowledge management  (KM). Since the first event in 1992, the conference has evolved into one of the major forums for research on  database management ,  information retrieval , and knowledge management. [ 1 ] [ 2 ]  The conference is noted for its  interdisciplinarity , as it brings together communities that otherwise often publish at separate venues. Recent editions have attracted well beyond 500 participants. [ 3 ]  In addition to the main research program, the conference also features a number of workshops, tutorials, and industry presentations. [ 4 ] For many years, the conference was held in the US. Since 2005, venues in other countries have been selected as well."
  },
  {
    "id": 161,
    "title": "The Web Conference",
    "content": "The ACM Web Conference  (formerly known as  International World Wide Web Conference , abbreviated as  WWW ) is a yearly international  academic conference  on the topic of the future direction of the  World Wide Web . The first conference of many was held and organized by  Robert Cailliau  in 1994 at  CERN  in  Geneva ,  Switzerland .  The conference has been organized by the  International World Wide Web Conference Committee  (IW3C2), also founded by Robert Cailliau and colleague Joseph Hardin, every year since. [ 1 ]  In 2020, the Web Conference series became affiliated with the  Association for Computing Machinery  (ACM), [ 2 ]  where it is supported by  ACM SIGWEB . The conference's location rotates among North America, Europe, and Asia and its events usually span a period of five days. The conference aims to provide a forum in which \"key influencers, decision makers, technologists, businesses and  standards bodies \" can both present their ongoing work, research, and opinions as well as receive feedback from some of the most knowledgeable people in the field. [ 1 ] The web conference series is aimed at providing a global forum for discussion and debate in regard to the standardization of its associated technologies and the impact of said technologies on society and culture. Developers, researchers, internet users as well as commercial ventures and organizations come together at the conference to discuss the latest advancements of the Web and its evolving uses and trends, such as the development and popularization of the eTV and eBusiness. [ 3 ]   The conferences usually include a variety of events, such as tutorials and workshops, as well as the main conference and special dedications of space in memory of the history of the Web and specific notable events. [ 4 ]  The conferences are organized by the IW3C2 in collaboration with the  World Wide Web Consortium  (W3C), Local Organizing Committees, and Technical Program Committees. [ 5 ] Robert Cailliau , a founder of the World Wide Web himself, lobbied inside  CERN  and at conferences like the  Hypertext  1991 in  San Antonio ,  Texas , and  Hypertext  1993 in  Seattle ,  Washington . [ 6 ]  As he came back from the conference 1993 he announced a new conference called  World Wide Web Conference 1  and was actually 23 hours faster than the  NCSA  announced  Mosaic and the Web . [ 6 ]  After founding the IW3C2 with Joseph Hardin from the NCSA they decided the next Conferences in Geneva. [ 6 ] Though the way in which its content is organized varies from year to year, the World-Wide Web Conference continues to call itself the \"premiere venue for researchers, academics, businesses, and standard bodies to come together and discuss [the] latest updates and the state and evolutionary path of the Web\". [ 7 ]   People from all across the world come together and submit their own new research to be peer-reviewed by some of the World Wide Web community's most knowledgeable members. At the 2014 conference, WWW's largest program, peer-reviewed research paper presentations, fell into one of eleven categories: Those papers accepted were to be presented at the conference itself, and appear in the online  conference proceedings  published by the  ACM Digital Library  as well as the conference's website.  Furthermore, many of these papers are submitted to other  peer-reviewed journals  after the conference. [ 7 ] In addition to presenting breakthrough research on the Web and its associated technologies, the Conference acts as a stage for developers to demonstrate and receive feedback on their ongoing work in a dedicated session. The Demo Track allows researchers and practitioners to demonstrate new systems in a dedicated session. The Developer Track is a track dedicated solely to web development, a stage upon which web developers can present \"new trends and interesting ideas [as well as the] code and APIs of emerging applications, platforms, and standards.\" [ 8 ] Though peer-reviewed research paper presentations, demo, and developer tracks are a large portion of the conference's program, it is not merely a launch pad for individuals who have completed cutting-edge research in the field.  Students studying the Web and its associated technologies can submit unfinished work for review.  Beginner as well as senior PhD students are encouraged to present their ideas to the PhD Symposium for review.  This is a unique opportunity to receive feedback on their work from experienced researchers as well as other senior PhD students working in related research areas.  All applications and submissions are looked over by the Symposium Program Committee.  This committee includes other experienced researches.  These people are able to help the applicants and guide them in their work. [ 9 ]  Researchers and practitioners are also encouraged to submit their new and innovative work-in-progress.  Providing them with a unique opportunity to gain feedback from their peers in an informal setting, the Poster Track provides its presenters invaluable feedback from knowledgeable sources as well as other conference attendees with an opportunity to learn about novel ongoing research projects whose results already appear promising, despite their incompletion. [ 10 ] Lastly, the Conference allows for a series of co-located workshops to its attendees dedicated to emergent Web topics. These workshops work to not only create an open dialogue amongst all researchers and practitioners of Web technologies but also a potential means of collaboration in present and future endeavors. [ 10 ] Past and future conferences include: [ 11 ]"
  },
  {
    "id": 162,
    "title": "Tony Kent Strix award",
    "content": "The UKeiG [ 1 ]   Strix  award is an annual award for outstanding contributions to the field of  information retrieval  and is presented in memory of Dr. Tony Kent, a past Fellow of the Institute of Information Scientists (IIS), who died in 1997. Tony Kent made a major contribution to the development of information science and information services both in the UK and internationally, particularly in the field of chemistry. The name 'Strix' was chosen to reflect Tony's interest in ornithology, and as the name of the last and most successful information retrieval packages that he created. The Award is made in partnership with the International Society for Knowledge Organisation UK (ISKO UK [ 2 ] ), the Royal Society of Chemistry Chemical Information and Computer Applications Group (RSC [ 3 ] ) and the British Computer Society Information Retrieval Specialist Group (BCS IRSG [ 4 ] ). The Strix Award is given in recognition of an outstanding contribution to the field of information retrieval that meets one of the following criteria: Recipients so far have been: Source [ 5 ] Since 2014, the winner of the Tony Kent Strix Award in year _n_ is giving the Tony Kent Strix Annual Lecture in year _n+1_.  Annual lectures so far: UKeiG is a special interest group of the  Chartered Institute of Library and Information Professionals . [ 14 ]"
  },
  {
    "id": 163,
    "title": "Gerard Salton Award",
    "content": "The  Gerard Salton Award  is presented by the  Association for Computing Machinery  (ACM)  Special Interest Group on Information Retrieval  (SIGIR) every three years to an individual who has made \"significant, sustained and continuing contributions to research in information retrieval\". [ 1 ]  SIGIR also co-sponsors (with  SIGWEB ) the  Vannevar Bush Award , for the best paper at the  Joint Conference on Digital Libraries . [ 2 ] Source:  SIGIR"
  },
  {
    "id": 164,
    "title": "Karen Spärck Jones Award",
    "content": "To commemorate the achievements of  Karen Spärck Jones , the  Karen Spärck Jones Award  was created in 2008 by the  British Computer Society  (BCS) and its  Information Retrieval Specialist Group  (BCS IRSG). Since 2024, the award has been sponsored by  Bloomberg . [ 1 ]  Prior to 2024, it was sponsored by  Microsoft Research . The winner of the award is invited to present a keynote talk the following year alternately at the  European Conference on Information Retrieval (ECIR)  or the Conference of the European Chapter of the Association for Computational Linguistics (EACL)."
  },
  {
    "id": 165,
    "title": "Computer memory",
    "content": "Computer memory  stores information, such as data and programs, for immediate use in the  computer . [ 2 ]  The term  memory  is often synonymous with the terms  RAM ,   main memory ,  or  primary storage .  Archaic synonyms for main memory include  core  (for magnetic core memory) and  store . [ 3 ] Main memory operates at a high speed compared to  mass storage  which is slower but less expensive per bit and higher in capacity. Besides storing opened programs and data being actively processed, computer memory serves as a  mass storage cache  and  write buffer  to improve both reading and writing performance. Operating systems borrow  RAM  capacity for caching so long as it is not needed by running software. [ 4 ]  If needed, contents of the computer memory can be transferred to storage; a common way of doing this is through a memory management technique called  virtual memory . Modern computer memory is implemented as  semiconductor memory , [ 5 ] [ 6 ]  where data is stored within  memory cells  built from  MOS transistors  and other components on an  integrated circuit . [ 7 ]  There are two main kinds of semiconductor memory:  volatile  and  non-volatile . Examples of  non-volatile memory  are  flash memory  and  ROM ,  PROM ,  EPROM , and  EEPROM  memory. Examples of  volatile memory  are  dynamic random-access memory  (DRAM) used for primary storage and  static random-access memory  (SRAM) used mainly for  CPU cache . Most semiconductor memory is organized into  memory cells  each storing one  bit  (0 or 1).  Flash memory  organization includes both one bit per memory cell and a  multi-level cell  capable of storing multiple bits per cell. The memory cells are grouped into words of fixed  word length , for example, 1, 2, 4, 8, 16, 32, 64 or 128 bits. Each word can be accessed by a binary address of  N  bits, making it possible to store 2 N  words in the memory. In the early 1940s, memory technology often permitted a capacity of a few bytes. The first electronic programmable  digital computer , the  ENIAC , using thousands of  vacuum tubes , could perform simple calculations involving 20 numbers of ten decimal digits stored in the vacuum tubes. The next significant advance in computer memory came with acoustic  delay-line memory , developed by  J. Presper Eckert  in the early 1940s. Through the construction of a glass tube filled with  mercury  and plugged at each end with a quartz crystal, delay lines could store  bits of information  in the form of sound waves propagating through the mercury, with the quartz crystals acting as  transducers  to read and write bits. Delay-line memory was limited to a capacity of up to a few thousand bits. Two alternatives to the delay line, the  Williams tube  and  Selectron tube , originated in 1946, both using electron beams in glass tubes as means of storage. Using  cathode-ray tubes , Fred Williams invented the Williams tube, which was the first  random-access computer memory . The Williams tube was able to store more information than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and was less expensive.  The Williams tube was nevertheless frustratingly sensitive to environmental disturbances. Efforts began in the late 1940s to find  non-volatile memory .  Magnetic-core memory  allowed for memory recall after power loss. It was developed by Frederick W. Viehe and  An Wang  in the late 1940s, and improved by  Jay Forrester  and  Jan A. Rajchman  in the early 1950s, before being commercialized with the  Whirlwind I  computer in 1953. [ 8 ]  Magnetic-core memory was the dominant form of memory until the development of  MOS   semiconductor memory  in the 1960s. [ 9 ] The first  semiconductor memory  was implemented as a  flip-flop  circuit in the early 1960s using  bipolar transistors . [ 9 ]  Semiconductor memory made from  discrete devices  was first shipped by  Texas Instruments  to the  United States Air Force  in 1961. In the same year, the concept of  solid-state  memory on an  integrated circuit  (IC) chip was proposed by  applications engineer  Bob Norman at  Fairchild Semiconductor . [ 10 ]  The first bipolar semiconductor memory IC chip was the SP95 introduced by  IBM  in 1965. [ 9 ]  While semiconductor memory offered improved performance over magnetic-core memory, it remained larger and more expensive and did not displace magnetic-core memory until the late 1960s. [ 9 ] [ 11 ] The invention of the metal–oxide–semiconductor field-effect transistor ( MOSFET ) enabled the practical use of  metal–oxide–semiconductor  (MOS) transistors as  memory cell  storage elements. MOS memory was developed by John Schmidt at  Fairchild Semiconductor  in 1964. [ 12 ]  In addition to higher performance, MOS  semiconductor memory  was cheaper and consumed less power than magnetic core memory. [ 13 ]  In 1965, J. Wood and R. Ball of the  Royal Radar Establishment  proposed digital storage systems that use  CMOS  (complementary MOS) memory cells, in addition to MOSFET  power devices  for the  power supply , switched cross-coupling,  switches  and  delay-line storage . [ 14 ]  The development of  silicon-gate   MOS integrated circuit  (MOS IC) technology by  Federico Faggin  at Fairchild in 1968 enabled the production of MOS  memory chips . [ 15 ]   NMOS  memory was commercialized by  IBM  in the early 1970s. [ 16 ]  MOS memory overtook magnetic core memory as the dominant memory technology in the early 1970s. [ 13 ] The two main types of volatile  random-access memory  (RAM) are  static random-access memory  (SRAM) and  dynamic random-access memory  (DRAM). Bipolar SRAM was invented by Robert Norman at Fairchild Semiconductor in 1963, [ 9 ]  followed by the development of MOS SRAM by John Schmidt at Fairchild in 1964. [ 13 ]  SRAM became an alternative to magnetic-core memory, but requires six transistors for each  bit  of data. [ 17 ]  Commercial use of SRAM began in 1965, when IBM introduced their SP95 SRAM chip for the  System/360 Model 95 . [ 9 ] Toshiba  introduced bipolar DRAM  memory cells  for its Toscal BC-1411  electronic calculator  in 1965. [ 18 ] [ 19 ]  While it offered improved performance, bipolar DRAM could not compete with the lower price of the then dominant magnetic-core memory. [ 20 ]  MOS technology is the basis for modern DRAM. In 1966,  Robert H. Dennard  at the  IBM Thomas J. Watson Research Center  was working on MOS memory. While examining the characteristics of MOS technology, he found it was possible to build  capacitors , and that storing a charge or no charge on the MOS capacitor could represent the 1 and 0 of a bit, while the MOS transistor could control writing the charge to the capacitor. This led to his development of a single-transistor DRAM memory cell. [ 17 ]  In 1967, Dennard filed a patent for a single-transistor DRAM memory cell based on MOS technology. [ 21 ]  This led to the first commercial DRAM IC chip, the  Intel 1103  in October 1970. [ 22 ] [ 23 ] [ 24 ]   Synchronous dynamic random-access memory  (SDRAM) later debuted with the  Samsung  KM48SL2000 chip in 1992. [ 25 ] [ 26 ] The term  memory  is also often used to refer to  non-volatile memory  including  read-only memory  (ROM) through modern  flash memory .  Programmable read-only memory  (PROM) was invented by  Wen Tsing Chow  in 1956, while working for the Arma Division of the American Bosch Arma Corporation. [ 27 ] [ 28 ]  In 1967, Dawon Kahng and  Simon Sze  of Bell Labs proposed that the  floating gate  of a MOS  semiconductor device  could be used for the cell of a reprogrammable ROM, which led to  Dov Frohman  of  Intel  inventing  EPROM  (erasable PROM) in 1971. [ 29 ]   EEPROM  (electrically erasable PROM) was developed by Yasuo Tarui, Yutaka Hayashi and Kiyoko Naga at the  Electrotechnical Laboratory  in 1972. [ 30 ]  Flash memory was invented by  Fujio Masuoka  at  Toshiba  in the early 1980s. [ 31 ] [ 32 ]  Masuoka and colleagues presented the invention of  NOR flash  in 1984, [ 33 ]  and then  NAND flash  in 1987. [ 34 ]  Toshiba commercialized NAND flash memory in 1987. [ 35 ] [ 36 ] [ 37 ] Developments in technology and economies of scale have made possible so-called  very large memory  (VLM) computers. [ 37 ] Volatile memory is computer memory that requires power to maintain the stored information. Most modern  semiconductor  volatile memory is either  static RAM  (SRAM) or  dynamic RAM  (DRAM). [ a ]  DRAM dominates for desktop system memory. SRAM is used for  CPU cache . SRAM is also found in small  embedded systems  requiring little memory. SRAM retains its contents as long as the power is connected and may use a simpler interface, but  commonly uses six transistors per bit . Dynamic RAM is more complicated for interfacing and control, needing regular refresh cycles to prevent losing its contents, but uses only one transistor and one capacitor per bit, allowing it to reach much higher densities and much cheaper per-bit costs. [ 2 ] [ 23 ] [ 37 ] Non-volatile memory can retain the stored information even when not powered. Examples of non-volatile memory include  read-only memory ,  flash memory , most types of magnetic computer storage devices (e.g.  hard disk drives ,  floppy disks  and  magnetic tape ),  optical discs , and early computer storage methods such as  magnetic drum ,  paper tape  and  punched cards . [ 37 ] Non-volatile memory technologies under development include  ferroelectric RAM ,  programmable metallization cell ,  Spin-transfer torque magnetic RAM ,  SONOS ,  resistive random-access memory ,  racetrack memory ,  Nano-RAM ,  3D XPoint , and  millipede memory . A third category of memory is  semi-volatile . The term is used to describe a memory that has some limited non-volatile duration after power is removed, but then data is ultimately lost. A typical goal when using a semi-volatile memory is to provide the high performance and durability associated with volatile memories while providing some benefits of non-volatile memory. For example, some non-volatile memory types experience wear when written. A  worn  cell has increased volatility but otherwise continues to work. Data locations which are written frequently can thus be directed to use worn circuits.  As long as the location is updated within some known retention time, the data stays valid.  After a period of time without update, the value is copied to a less-worn circuit with longer retention.  Writing first to the worn area allows a high write rate while avoiding wear on the not-worn circuits. [ 38 ] As a second example, an  STT-RAM  can be made non-volatile by building large cells, but doing so raises the cost per bit and power requirements and reduces the write speed. Using small cells improves cost, power, and speed, but leads to semi-volatile behavior. In some applications, the increased volatility can be managed to provide many benefits of a non-volatile memory, for example by removing power but forcing a wake-up before data is lost; or by caching read-only data and discarding the cached data if the power-off time exceeds the non-volatile threshold. [ 39 ] The term semi-volatile is also used to describe semi-volatile behavior constructed from other memory types, such as  nvSRAM , which combines  SRAM  and a non-volatile memory on the same  chip , where an external signal copies data from the volatile memory to the non-volatile memory, but if power is removed before the copy occurs, the data is lost. Another example is  battery-backed RAM , which uses an external  battery  to power the memory device in case of external power loss. If power is off for an extended period of time, the battery may run out, resulting in data loss. [ 37 ] Proper management of memory is vital for a computer system to operate properly. Modern  operating systems  have complex systems to properly manage memory. Failure to do so can lead to bugs or slow performance. Improper management of memory is a common cause of bugs and security vulnerabilities, including the following types: Virtual memory is a system where  physical memory  is managed by the operating system typically with assistance from a  memory management unit , which is part of many modern  CPUs . It allows multiple types of memory to be used. For example, some data can be stored in RAM while other data is stored on a  hard drive  (e.g. in a  swapfile ), functioning as an extension of the  cache hierarchy . This offers several advantages. Computer programmers no longer need to worry about where their data is physically stored or whether the user's computer will have enough memory. The operating system will place actively used data in RAM, which is much faster than hard disks. When the amount of RAM is not sufficient to run all the current programs, it can result in a situation where the computer spends more time moving data from RAM to disk and back than it does accomplishing tasks; this is known as  thrashing . Protected memory is a system where each program is given an area of memory to use and is prevented from going outside that range. If the operating system detects that a program has tried to alter memory that does not belong to it, the program is terminated (or otherwise restricted or redirected). This way, only the offending program crashes, and other programs are not affected by the misbehavior (whether accidental or intentional). Use of protected memory greatly enhances both the reliability and security of a computer system. Without protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system's memory is corrupted, the entire computer system may crash and need to be  rebooted . At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers. It may also be used benignly by desirable programs which are intended to modify other programs,  debuggers , for example, to insert breakpoints or hooks."
  },
  {
    "id": 166,
    "title": "Controlled vocabulary",
    "content": "Controlled vocabularies  provide a way to organize knowledge for subsequent retrieval.  They are used in  subject indexing  schemes,  subject headings ,  thesauri , [ 1 ] [ 2 ]   taxonomies  and other  knowledge organization systems . Controlled vocabulary schemes mandate the use of predefined, preferred terms that have been preselected by the designers of the schemes, in contrast to  natural language  vocabularies, which have no such restriction. [ 3 ] In  library and information science , controlled vocabulary is a carefully selected list of  words  and  phrases , which are used to  tag  units of information (document or work) so that they may be more easily retrieved by a search. [ 4 ] [ 5 ]  Controlled vocabularies solve the problems of  homographs ,  synonyms  and  polysemes  by a  bijection  between concepts and preferred terms. In short, controlled vocabularies reduce unwanted ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency. [ 3 ] For example, in the  Library of Congress Subject Headings [ 6 ]  (a subject heading system that uses a controlled vocabulary), preferred terms—subject headings in this case—have to be chosen to handle choices between variant spellings of the same word (American versus British), choice among scientific and popular terms ( cockroach  versus  Periplaneta americana ), and choices between synonyms ( automobile  versus  car ), among other difficult issues. Choices of preferred terms are based on the principles of  user warrant  (what terms users are likely to use),  literary warrant  (what terms are generally used in the literature and documents), and  structural warrant  (terms chosen by considering the structure, scope of the controlled vocabulary). Controlled vocabularies also typically handle the problem of  homographs  with qualifiers. For example, the term  pool  has to be qualified to refer to either  swimming pool  or the game  pool  to ensure that each preferred term or heading refers to only one concept. [ 7 ] There are two main kinds of controlled vocabulary tools used in libraries: subject headings [ 8 ]  and  thesauri . While the differences between the two are diminishing, there are still some minor differences. The  terms  are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the  Library of Congress system ,  Medical Subject Headings  (MeSH) created by the  United States National Library of Medicine , and  Sears . Well known thesauri include the  Art and Architecture Thesaurus  and the  ERIC  Thesaurus. When selecting terms for a controlled vocabulary, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-coordination (in which case the degree of enumeration versus synthesis becomes an issue) and post-coordination in the system is another important issue. Controlled vocabulary elements (terms/phrases) employed as  tags , to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as  metadata . There are three main types of indexing languages. When indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example, using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each document. In recent years  free text search  as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is  indexed ). These methods have been compared in some studies, such as the 2007 article, \"A Comparative Evaluation of Full-text, Concept-based, and Context-sensitive Search\". [ 9 ] Controlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce  irrelevant  items in the retrieval list. These irrelevant items ( false positives ) are often caused by the inherent ambiguity of  natural language . Take the English word  football  for example.  Football  is the name given to a number of different  team sports . Worldwide the most popular of these team sports is  association football , which also happens to be called  soccer  in several countries. The word  football  is also applied to  rugby football  ( rugby union  and  rugby league ),  American football ,  Australian rules football ,  Gaelic football , and  Canadian football . A search for  football  therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by  tagging  the documents in such a way that the ambiguities are eliminated. Compared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually  relevant  to the search topic). In some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct preferred term is searched, there is no need to search for other terms that might be synonyms of that term. A controlled vocabulary search may lead to unsatisfactory  recall , in that it will fail to retrieve some documents that are actually relevant to the search question. This is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with that of the indexer. Another possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example, an article might mention football as a secondary focus, and the indexer might decide not to tag it with \"football\" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A  free text search  would automatically pick up that article regardless. On the other hand, free text searches have high exhaustivity (every word is searched) so although it has much lower precision, it has potential for high recall as long as the searcher overcome the problem of synonyms by entering every combination. Controlled vocabularies may become outdated rapidly in fast developing fields of knowledge, unless the preferred terms are updated regularly. Even in an ideal scenario, a controlled vocabulary is often less specific than the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while this precise problem is not a factor in a free text, as it uses the author's own words. The use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision. Numerous methodologies have been developed to assist in the creation of controlled vocabularies, including  faceted classification , which enables a given data record or document to be described in multiple ways. Word choice in chosen vocabularies is not neutral, and the indexer must carefully consider the ethics of their word choices. For example, traditionally colonialist terms have often been the preferred terms in chosen vocabularies when discussing First Nations issues, which has caused controversy. [ 10 ] Controlled vocabularies, such as the  Library of Congress Subject Headings , are an essential component of  bibliography , the study and classification of books. They were initially developed in  library and information science . In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the  Medical Subject Headings  (MeSH) developed by the  U.S. National Library of Medicine . Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup  X.25  networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first  full text  databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library. In large organizations, controlled vocabularies may be introduced to improve  technical communication . The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in  technical writing  and  knowledge management , where effort is expended to use the same word throughout a  document  or  organization  instead of slightly different ones to refer to the same thing. Web searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a  Semantic Web , in which the content of Web pages is described using a machine-readable  metadata  scheme. One of the first proposals for such a scheme is the  Dublin Core  Initiative. An example of a controlled vocabulary which is usable for  indexing web pages  is  PSH . It is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web. [ 11 ]  To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The eXchangeable Faceted Metadata Language (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on  faceted classification  principles. [ 12 ] [ non-primary source needed ] Controlled vocabularies of the  Semantic Web  define the concepts and relationships (terms) used to describe a field of interest or area of concern. For instance, to declare a person in a machine-readable format, a vocabulary is needed that has the formal definition of \"Person\", such as the Friend of a Friend ( FOAF ) vocabulary, which has a Person class that defines typical properties of a person including, but not limited to, name, honorific prefix, affiliation, email address, and homepage, or the Person vocabulary of  Schema.org . [ 13 ]  Similarly, a book can be described using the Book vocabulary of  Schema.org [ 14 ]  and general publication terms from the  Dublin Core  vocabulary, [ 15 ]  an event with the Event vocabulary of  Schema.org , [ 16 ]  and so on. To use machine-readable terms from any controlled vocabulary, web designers can choose from a variety of annotation formats, including RDFa,  HTML5 Microdata , or  JSON-LD  in the markup, or  RDF  serializations (RDF/XML, Turtle, N3, TriG, TriX) in external files."
  },
  {
    "id": 167,
    "title": "Data mining",
    "content": "Data mining  is the process of extracting and discovering patterns in large  data sets  involving methods at the intersection of  machine learning ,  statistics , and  database systems . [ 1 ]  Data mining is an  interdisciplinary  subfield of  computer science  and  statistics  with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. [ 1 ] [ 2 ] [ 3 ] [ 4 ]  Data mining is the analysis step of the \" knowledge discovery in databases \" process, or KDD. [ 5 ]  Aside from the raw analysis step, it also involves database and  data management  aspects,  data pre-processing ,  model  and  inference  considerations, interestingness metrics,  complexity  considerations, post-processing of discovered structures,  visualization , and  online updating . [ 1 ] The term \"data mining\" is a  misnomer  because the goal is the extraction of  patterns  and knowledge from large amounts of data, not the  extraction ( mining ) of data itself . [ 6 ]  It also is a  buzzword [ 7 ]  and is frequently applied to any form of large-scale data or  information processing  ( collection ,  extraction ,  warehousing , analysis, and statistics) as well as any application of  computer decision support system , including  artificial intelligence  (e.g., machine learning) and  business intelligence . Often the more general terms ( large scale )  data analysis  and  analytics —or, when referring to actual methods,  artificial intelligence  and  machine learning —are more appropriate. The actual data mining task is the semi- automatic  or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records ( cluster analysis ), unusual records ( anomaly detection ), and  dependencies  ( association rule mining ,  sequential pattern mining ). This usually involves using database techniques such as  spatial indices . These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and  predictive analytics . For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a  decision support system . Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps. The difference between  data analysis  and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a  marketing campaign , regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data. [ 8 ] The related terms  data dredging ,  data fishing , and  data snooping  refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations. In the 1960s, statisticians and economists used terms like  data fishing  or  data dredging  to refer to what they considered the bad practice of analyzing data without an  a-priori  hypothesis. The term \"data mining\" was used in a similarly critical way by economist  Michael Lovell  in an article published in the  Review of Economic Studies  in 1983. [ 9 ] [ 10 ]  Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative). The term  data mining  appeared around 1990 in the database community, with generally positive connotations. For a short time in 1980s, the phrase \"database mining\"™, was used, but since it was trademarked by HNC, a  San Diego –based company, to pitch their Database Mining Workstation; [ 11 ]  researchers consequently turned to  data mining . Other terms used include  data archaeology ,  information harvesting ,  information discovery ,  knowledge extraction , etc.  Gregory Piatetsky-Shapiro  coined the term \"knowledge discovery in databases\" for the first workshop on the same topic  (KDD-1989)  and this term became more popular in the  AI  and  machine learning  communities. However, the term data mining became more popular in the business and press communities. [ 12 ]  Currently, the terms  data mining  and  knowledge discovery  are used interchangeably. The manual extraction of patterns from  data  has occurred for centuries. Early methods of identifying patterns in data include  Bayes' theorem  (1700s) and  regression analysis  (1800s). [ 13 ]  The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As  data sets  have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as  neural networks ,  cluster analysis ,  genetic algorithms  (1950s),  decision trees  and  decision rules  (1960s), and  support vector machines  (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns. [ 14 ]  in large data sets. It bridges the gap from  applied statistics  and artificial intelligence (which usually provide the mathematical background) to  database management  by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets. The  knowledge discovery in databases (KDD) process  is commonly defined with the stages: It exists, however, in many variations on this theme, such as the  Cross-industry standard process for data mining  (CRISP-DM) which defines six phases: or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation. Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. [ 15 ] [ 16 ] [ 17 ] [ 18 ] The only other data mining standard named in these polls was  SEMMA . However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, [ 19 ]  and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008. [ 20 ] Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a  data mart  or  data warehouse . Pre-processing is essential to analyze the  multivariate  data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing  noise  and those with  missing data . Data mining involves six common classes of tasks: [ 5 ] Data mining can unintentionally be misused, producing results that appear to be significant but which do not actually predict future behavior and cannot be  reproduced  on a new sample of data, therefore bearing little use. This is sometimes caused by investigating too many hypotheses and not performing proper  statistical hypothesis testing . A simple version of this problem in  machine learning  is known as  overfitting , but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening. [ 21 ] The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called  overfitting . To overcome this, the evaluation uses a  test set  of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" e-mails would be trained on a  training set  of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had  not  been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as  ROC curves . If the learned patterns do not meet the desired standards, it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge. The premier professional body in the field is the  Association for Computing Machinery 's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining ( SIGKDD ). [ 22 ] [ 23 ]  Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings, [ 24 ]  and since 1999 it has published a biannual  academic journal  titled \"SIGKDD Explorations\". [ 25 ] Computer science conferences on data mining include: Data mining topics are also present in many  data management/database conferences  such as the ICDE Conference,  SIGMOD Conference  and  International Conference on Very Large Data Bases . There have been some efforts to define standards for the data mining process, for example, the 1999 European  Cross Industry Standard Process for Data Mining  (CRISP-DM 1.0) and the 2004  Java Data Mining  standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft. For exchanging the extracted models—in particular for use in  predictive analytics —the key standard is the  Predictive Model Markup Language  (PMML), which is an  XML -based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example)  subspace clustering  have been proposed independently of the DMG. [ 26 ] Data mining is used wherever there is digital data available. Notable  examples of data mining  can be found throughout business, medicine, science, finance, construction, and surveillance. While the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to  user behavior  (ethical and otherwise). [ 27 ] The ways in which data mining can be used can in some cases and contexts raise questions regarding  privacy , legality, and  ethics . [ 28 ]  In particular, data mining government or commercial data sets for  national security  or  law enforcement  purposes, such as in the  Total Information Awareness  Program or in  ADVISE , has raised privacy concerns. [ 29 ] [ 30 ] Data mining requires data preparation which uncovers information or patterns which compromise  confidentiality  and  privacy  obligations. A common way for this to occur is through  data aggregation .  Data aggregation  involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent). [ 31 ]  This is not data mining  per se , but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous. [ 32 ] It is recommended [ according to whom? ]  to be aware of the following  before  data are collected: [ 31 ] Data may also be modified so as to  become  anonymous, so that individuals may not readily be identified. [ 31 ]  However, even \" anonymized \" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL. [ 33 ] The inadvertent revelation of  personally identifiable information  leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,\nemotional, or bodily harm to the indicated individual.  In one instance of  privacy violation , the patrons of Walgreens filed a lawsuit against the company in 2011 for selling\nprescription information to data mining companies who in turn provided the data\nto pharmaceutical companies. [ 34 ] Europe  has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the  U.S.–E.U. Safe Harbor Principles , developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of  Edward Snowden 's  global surveillance disclosure , there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the  National Security Agency , and attempts to reach an agreement with the United States have failed. [ 35 ] In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places. [ 36 ] In the United States, privacy concerns have been addressed by the  US Congress  via the passage of regulatory controls such as the  Health Insurance Portability and Accountability Act  (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in  Biotech Business Week , \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.\" [ 37 ]  This underscores the necessity for data anonymity in data aggregation and mining practices. U.S. information privacy legislation such as HIPAA and the  Family Educational Rights and Privacy Act  (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation. Under  European copyright   database laws , the mining of in-copyright works (such as by  web mining ) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist, so data mining becomes subject to  intellectual property  owners' rights that are protected by the  Database Directive . On the recommendation of the  Hargreaves review , this led to the UK government to amend its copyright law in 2014 to allow content mining as a  limitation and exception . [ 38 ]  The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the  Information Society Directive  (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions.\nSince 2020 also Switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art. 24d of the Swiss Copyright Act. This new article entered into force on 1 April 2020. [ 39 ] The  European Commission  facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. [ 40 ]  The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and  open access  publishers to leave the stakeholder dialogue in May 2013. [ 41 ] US copyright law , and in particular its provision for  fair use , upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the  Google Book settlement  the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining. [ 42 ] The following applications are available under free/open-source licenses. Public access to application source code is also available. The following applications are available under proprietary licenses. For more information about extracting information out of data (as opposed to  analyzing  data), see:"
  },
  {
    "id": 168,
    "title": "Data retrieval",
    "content": "Data retrieval  means obtaining data from a  database management system  (DBMS), like for example an  object-oriented database  (ODBMS). In this case, it is considered that data is represented in a  structured  way, and there is no  ambiguity  in data. In order to retrieve the desired data the user presents a set of criteria by a  query . Then the database management system selects the demanded data from the database. The retrieved data may be stored in a file, printed, or viewed on the screen. A  query language , like for example  Structured Query Language  (SQL), is used to prepare the queries. SQL is an  American National Standards Institute  (ANSI) standardized query language developed specifically to write database queries. Each database management system may have its own language, but most are relational. [ clarification needed ] Reports  and queries are the two primary forms of the retrieved data from a database. There are some overlaps between them, but queries generally select a relatively small portion of the database, while reports show larger amounts of data. Queries also present the data in a standard format and usually display it on the monitor; whereas reports allow formatting of the output however you like and is normally printed. Reports are designed using a  report generator  built into the database management system."
  },
  {
    "id": 169,
    "title": "European Summer School in Information Retrieval",
    "content": "The  European Summer School in Information Retrieval  ( ESSIR ) is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high-quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.\nThe aim of ESSIR is to give to its participants a common ground in different aspects of  Information Retrieval  (IR) .  Maristella Agosti  in 2008 stated that: \" The term IR identifies the activities that a person – the user – has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need. \" [ 1 ] IR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from  computer science  to  information science  and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the  Internet . ESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the Internet. Two books have been prepared as readings in IR from editions of ESSIR, the first one is  Lectures on Information Retrieval , [ 2 ]  the second one is  Advanced Topics in Information Retrieval . [ 3 ] ESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by  Maristella Agosti ,  University of Padua , Italy and  Nick Belkin ,  Rutgers University , U.S.A., for an Italian audience in 1989."
  },
  {
    "id": 170,
    "title": "Human–computer information retrieval",
    "content": "Human–computer information retrieval  ( HCIR ) is the study and engineering of  information retrieval  techniques that bring human intelligence into the  search  process. It combines the fields of  human-computer interaction  (HCI) and information retrieval (IR) and creates systems that improve search by taking into account the human context, or through a multi-step search process that provides the opportunity for human feedback. This term  human–computer information retrieval  was coined by  Gary Marchionini  in a series of lectures delivered between 2004 and 2006. [ 1 ]  Marchionini's main thesis is that \"HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy.\" In 1996 and 1998, a pair of workshops at the  University of Glasgow  on  information retrieval  and  human–computer interaction  sought to address the overlap between these two fields. Marchionini notes the impact of the  World Wide Web  and the sudden increase in  information literacy  – changes that were only embryonic in the late 1990s. A few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the  University of Maryland Human-Computer Interaction Lab  in 2005, alternates between the  Association for Computing Machinery   Special Interest Group on Information Retrieval  (SIGIR) and  Special Interest Group on Computer-Human Interaction  (CHI) conferences. Also in 2005, the  European Science Foundation  held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the  Massachusetts Institute of Technology . HCIR includes various aspects of IR and HCI. These include  exploratory search , in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as \"the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system.\" [ 2 ] A key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users. [ 3 ] Most modern IR systems employ a  ranked  retrieval model, in which the documents are scored based on the  probability  of the document's  relevance  to the query. [ 4 ]  In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their  mean average precision  over a set of benchmark queries from organizations like the  Text Retrieval Conference  (TREC). Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models – one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and  Nicholas J. Belkin 's 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of  precision  and  recall  but apply them to the results of multiple iterations of user interaction, rather than to a single query response. [ 5 ]  Other HCIR research, such as  Pia Borlund 's IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc. [ 6 ] HCIR researchers have put forth the following goals towards a system where the user has more control in determining relevant results. [ 1 ] [ 7 ] Systems should In short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process — the  information professional  — to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs). The techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift. Many  search engines  have features that incorporate HCIR techniques.  Spelling suggestions  and  automatic query reformulation  provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the user's hands. Faceted search  enables users to navigate information  hierarchically , going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional  taxonomies  in which the hierarchy of categories is fixed and unchanging.  Faceted navigation , like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking. [ 8 ] Lookahead  provides a general approach to penalty-free exploration. For example, various  web applications  employ  AJAX  to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g.,  metadata  about the objects) and \"snippets\" of document text that are most pertinent to the words in the search query. Relevance feedback  allows users to guide an IR system by indicating whether particular results are more or less relevant. [ 9 ] Summarization and  analytics  help users digest the results that come back from the query. Summarization here is intended to encompass any means of  aggregating  or  compressing  the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is  clustering , which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for \"java\" might return clusters for  Java (programming language) ,  Java (island) , or  Java (coffee) . Visual representation of data  is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of  information visualization  that allow users access to summary views of search results include  tag clouds  and  treemapping ."
  },
  {
    "id": 171,
    "title": "Information extraction",
    "content": "Information extraction  ( IE ) is the task of automatically extracting  structured information  from  unstructured  and/or semi-structured  machine-readable  documents and other electronically represented sources. Typically, this involves processing human language texts by means of  natural language processing  (NLP). [ 1 ]  Recent activities in  multimedia  document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction. Recent advances in NLP techniques have allowed for significantly improved performance compared to previous years. [ 2 ]  An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: from an online news sentence such as: A broad goal of IE is to allow computation to be done on the previously unstructured data.  A more specific goal is to allow  automated reasoning  about the  logical form  of the input data.  Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and  context . Information extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of  information retrieval  (IR) [ 3 ]  has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of  natural language processing  (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis,  IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events  in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to \"understand\" an attack article only enough to find data corresponding to the slots in this template. Information extraction dates back to the late 1970s in the early days of NLP. [ 4 ]  An early commercial system from the mid-1980s was JASPER built for  Reuters  by the Carnegie Group Inc with the aim of providing  real-time financial news  to financial traders. [ 5 ] Beginning in 1987, IE was spurred by a series of  Message Understanding Conferences . MUC is a competition-based conference [ 6 ]  that focused on the following domains: Considerable support came from the U.S. Defense Advanced Research Projects Agency ( DARPA ), who wished to automate mundane tasks performed by government analysts, such as scanning newspapers for possible links to terrorism. [ citation needed ] The present significance of IE pertains to the growing amount of information available in unstructured form.  Tim Berners-Lee , inventor of the  World Wide Web , refers to the existing  Internet  as the web of  documents   [ 7 ]  and advocates that more of the content be made available as a  web of  data . [ 8 ]   Until this transpires, the web largely consists of unstructured documents lacking semantic  metadata .  Knowledge contained within these documents can be made more accessible for machine processing by means of transformation into  relational form , or by marking-up with  XML  tags.  An intelligent agent monitoring a news data feed requires IE to transform unstructured data into something that can be reasoned with.  A typical application of IE is to scan a set of documents written in a  natural language  and populate a database with the information extracted. [ 9 ] Applying information extraction to text is linked to the problem of  text simplification  in order to create a structured view of the information present in free text. The overall goal being to create a more easily machine-readable text to process the sentences. Typical IE tasks and subtasks include: Note that this list is not exhaustive and that the exact meaning of IE activities is not commonly accepted and that many approaches combine multiple sub-tasks of IE in order to achieve a wider goal. Machine learning, statistical analysis and/or natural language processing are often used in IE. IE on non-text documents is becoming an increasingly interesting topic [ when? ]  in research, and information extracted from multimedia documents can now [ when? ]  be expressed in a high level structure as it is done on text. This naturally leads to the fusion of extracted information from multiple kinds of documents and sources. IE has been the focus of the MUC conferences. The proliferation of the  Web , however, intensified the need for developing IE systems that help people to cope with the  enormous amount of data  that are available online. Systems that perform IE from online text should meet the requirements of low cost, flexibility in development and easy adaptation to new domains. MUC systems fail to meet those criteria. Moreover, linguistic analysis performed for unstructured text does not exploit the HTML/ XML  tags and the layout formats that are available in online texts. As a result, less linguistically intensive approaches have been developed for IE on the Web using  wrappers , which are sets of highly accurate rules that extract a particular page's content. Manually developing wrappers has proved to be a time-consuming task, requiring a high level of expertise.  Machine learning  techniques, either  supervised  or  unsupervised , have been used to induce such rules automatically. Wrappers  typically handle highly structured collections of web pages, such as product catalogs and telephone directories. They fail, however, when the text type is less structured, which is also common on the Web. Recent effort on  adaptive information extraction  motivates the development of IE systems that can handle different types of text, from well-structured to almost free text -where common wrappers fail- including mixed types. Such systems can exploit shallow natural language knowledge and thus can be also applied to less structured texts. A recent [ when? ]  development is Visual Information Extraction, [ 16 ] [ 17 ]  that relies on rendering a webpage in a browser and creating rules based on the proximity of regions in the rendered web page. This helps in extracting entities from complex web pages that may exhibit a visual pattern, but lack a discernible pattern in the HTML source code. The following standard approaches are now widely accepted: Numerous other approaches exist for IE including hybrid approaches that combine some of the standard approaches previously listed."
  },
  {
    "id": 172,
    "title": "Information seeking",
    "content": "Information seeking  is the process or activity of attempting to obtain information in both human and technological contexts. Information seeking is related to, but different from,  information retrieval  (IR). Traditionally, IR tools have been designed for IR professionals to enable them to effectively and efficiently retrieve information from a source. It is assumed that the information exists in the source and that a well-formed query will retrieve it (and nothing else). It has been argued that  laypersons'  information seeking on the internet is very different from information retrieval as performed within the IR discourse. Yet, internet search engines are built on IR principles. Since the late 1990s a body of research on how casual users interact with internet search engines has been forming, but the topic is far from fully understood. IR can be said to be technology-oriented, focusing on  algorithms  and issues such as  precision  and  recall . Information seeking may be understood as a more human-oriented and open-ended process than information retrieval. In information seeking, one does not know whether there exists an answer to one's query, so the process of seeking may provide the learning required to satisfy one's  information need . Much library and information science (LIS) research has focused on the information-seeking practices of practitioners within various fields of professional work. Studies have been carried out into the information-seeking behaviors of librarians, [ 1 ]  academics, [ 2 ]  medical professionals, [ 3 ]  engineers, [ 4 ]  lawyers [ 5 ] [ 6 ]  and mini-publics [ 7 ] (among others). Much of this research has drawn on the work done by Leckie, Pettigrew (now Fisher) and Sylvain, who in 1996 conducted an extensive review of the LIS literature (as well as the literature of other academic fields) on professionals' information seeking. The authors proposed an analytic model of professionals' information seeking behaviour, intended to be generalizable across the professions, thus providing a platform for future research in the area. The model was intended to \"prompt new insights... and give rise to more refined and applicable theories of information seeking\" (1996, p. 188). The model has been adapted by Wilkinson (2001) who proposes a model of the information seeking of lawyers. Recent studies in this topic address the concept of information-gathering that \"provides a broader perspective that adheres better to professionals' work-related reality and desired skills.\" [ 8 ]  (Solomon & Bronstein, 2021). A variety of theories of information behavior – e.g.  Zipf 's  Principle of Least Effort ,  Brenda Dervin 's Sense Making,  Elfreda Chatman 's Life in the Round – seek to understand the processes that surround information seeking. In addition, many theories from other disciplines have been applied in investigating an aspect or whole process of information seeking behavior. [ 9 ] [ 10 ] A review of the literature on information seeking behavior shows that information seeking has generally been accepted as dynamic and non-linear (Foster, 2005; Kuhlthau 2006). People experience the information search process as an interplay of thoughts, feelings and actions ( Kuhlthau, 2006 ). Donald O. Case (2007) also wrote a good book that is a review of the literature. Information seeking has been found to be linked to a variety of interpersonal communication behaviors beyond question-asking, to include strategies such as candidate answers. Robinson's (2010) [ 11 ]  research suggests that when seeking information at work, people rely on both other people and information repositories (e.g., documents and databases), and spend similar amounts of time consulting each (7.8% and 6.4% of work time, respectively; 14.2% in total). However, the distribution of time among the constituent information seeking stages differs depending on the source. When consulting other people, people spend less time locating the information source and information within that source, similar time understanding the information, and more time problem solving and decision making, than when consulting information repositories. Furthermore, the research found that people spend substantially more time receiving information passively (i.e., information that they have not requested) than actively (i.e., information that they have requested), and this pattern is also reflected when they provide others with information. The concepts of information seeking, information retrieval, and information behaviour are objects of investigation of  information science . Within this scientific discipline a variety of studies has been undertaken analyzing the interaction of an individual with  information sources  in case of a specific  information need , task, and context. The research models developed in these studies vary in their level of scope.  Wilson  (1999) therefore developed a nested model of conceptual areas, which visualizes the interrelation of the here mentioned central concepts. Wilson defines models of information behavior to be \"statements, often in the form of diagrams, that attempt to describe an information-seeking activity, the causes and consequences of that activity, or the relationships among stages in information-seeking behaviour\" (1999: 250)."
  },
  {
    "id": 173,
    "title": "Collaborative information seeking",
    "content": "Collaborative information seeking  ( CIS ) is a field of research that involves studying situations, motivations, and methods for people working in collaborative groups for information seeking projects, as well as building systems for supporting such activities. Such projects often involve information searching or  information retrieval  (IR), information gathering, and  information sharing . Beyond that, CIS can extend to collaborative information synthesis and collaborative  sense-making . Seeking for information is often considered a solo activity, but there are many situations that call for people working together for  information seeking . Such situations are typically complex in nature, and involve working through several sessions exploring, evaluating, and gathering relevant information. Take for example, a couple going on a trip. They have the same goal, and in order to accomplish their goal, they need to seek out several kinds of information, including flights, hotels, and sightseeing. This may involve them working together over multiple sessions, exploring and collecting useful information, and collectively making decisions that help them move toward their common goal. It is a common knowledge that collaboration is either necessary or highly desired in many activities that are complex or difficult to deal with for an individual. Despite its natural appeal and situational necessity, collaboration in information seeking is an understudied domain. The nature of the available information and its role in our lives have changed significantly, but the methods and tools that are used to access and share that information in collaboration have remained largely unaltered. People still use general-purpose systems such as email and IM for doing CIS projects, and there is a lack of specialized tools and techniques to support CIS explicitly. There are also several models to explain  information seeking  and  information behavior , [ 1 ]  but the areas of collaborative information seeking and collaborative information behavior remain understudied. On the theory side, Shah has presented C5 Model [ 2 ] [ 3 ]  for studying collaborative situations, including  information seeking . On the practical side, a few specialized systems for supporting CIS have emerged in the recent past, but their usage and evaluations have underwhelmed. Despite such limitations, the field of CIS has been getting a lot of attention lately, and several promising theories and tools have come forth. Multiple reviews of CIS related literature are written by Shah. [ 4 ]  Shah's book [ 5 ]  provides a comprehensive review of this field, including theories, models, systems, evaluation, and future research directions. Other books in this area include one by Morris and  Teevan , [ 6 ]  as well as Foster's book on collaborative information behavior. [ 7 ]  and Hansen, Shah, and Klas's edited book on CIS. [ 8 ] Depending upon what one includes or excludes while talking about CIS, we have many or hardly any theories. If we consider the past work on the  groupware  systems, many interesting insights can be obtained about people working on collaborative projects, the issues they face, and the guidelines for system designers. One of the notable works is by Grudin, [ 9 ]  who laid out eight design principles for developers of  groupware  systems. The discussion below is primarily based on some of the recent works in the field of computer supported cooperative work  CSCW , collaborative IR, and CIS. The literature is filled with works that use terms such as  collaborative information retrieval , [ 10 ] [ 11 ]   social searching , [ 12 ]   concurrent search , [ 13 ]   collaborative exploratory search , [ 14 ]   co-browsing , [ 15 ]   collaborative information behavior , [ 16 ] [ 17 ]   collaborative information synthesis , [ 18 ]  and  collaborative information seeking , [ 19 ] [ 20 ]  which are often used interchangeably. There are several definitions of such related or similar terms in the literature. For instance, Foster [ 21 ]  defined collaborative IR as  \"the study of the systems and practices that enable individuals to collaborate during the seeking, searching, and retrieval of information.\"  Shah [ 22 ]  defined CIS as a process of collaboratively seeking information that is  \"defined explicitly among the participants, interactive, and mutually beneficial.\"  While there is still a lack of a definition or a terminology that is universally accepted, but most agree that CIS is an active process, as opposed to  collaborative filtering , where a system connects the users based on their passive involvement (e.g., buying similar products on Amazon). Foley and Smeaton [ 23 ]   defined two key aspects of collaborative information seeking as  division of labor  and the  sharing of knowledge . Division of labor allows collaborating searchers to tackle larger problems by reducing the duplication of effort (e.g., finding documents that one's collaborator has already discovered). The sharing of knowledge allows searchers to influence each other's activities as they interact with the retrieval system in pursuit of their (often evolving) information need. This influence can occur in real time if the collaborative search system supports it, or it can occur in a turn-taking, asynchronous manner if that is how interaction is structured. Teevan   et al. [ 24 ]   characterized two classes of collaboration, task-based vs. trait-based. Task-based collaboration corresponds to intentional collaboration; trait-based collaboration facilitates the sharing of knowledge through inferred similarity of information need. One of the important issues to study in CIS is the instance, reason, and the methods behind a collaboration. For instance, Morris, [ 25 ]  using a survey with 204  knowledge workers  at a large technology company found that people often like and want to collaborate, but they do not find specialized tools to help them in such endeavors. Some of the situations for doing collaborative information seeking in this survey were travel planning, shopping, and literature search. Shah, [ 26 ]  similarly, using personal interviews, identified three main reasons why people collaborate. As far as the tools and/or methods for CIS are concerned, both Morris and Shah found that email is still the most used tool. Other popular methods are face-to-face meetings, IM, and phone or conference calls. In general, the choice of the method or tool for our respondents depended on their situation (co-located or remote), and objective (brainstorming or working on independent parts). The classical way of organizing collaborative activities is based on two factors: location and time. [ 27 ]  Recently Hansen & Jarvelin [ 28 ]  and Golovchinsky, Pickens, & Back [ 29 ]  also classified approaches to collaborative IR using these two dimensions of space and time. See \"Browsing is a Collaborative Process\", [ 30 ]  where the authors depict various library activities on these two dimensions. [ 31 ] As we can see from this figure, the majority of collaborative activities in conventional libraries are co-located and synchronous, whereas collaborative activities relating to digital libraries are more remote and synchronous. Social information filtering, or collaborative filtering, as we saw earlier, is a process benefitting from other users' actions in the past; thus, it falls under asynchronous and mostly remote domain. These days email also serves as a tool for doing asynchronous collaboration among users who are not co-located. Chat or IM (represented as 'internet' in the figure) helps to carry out synchronous and remote collaboration. Rodden, [ 27 ]  similarly, presented a classification of CSCW systems using the form of interaction and the geographical nature of cooperative systems. Further, Rodden & Blair [ 32 ]  presented an important characteristic to all CSCW systems – control. According to the authors, two predominant control mechanisms have emerged within CSCW systems: speech act theory systems, and procedure based systems. These mechanisms are tightly coupled with the kind of control the system can support in a collaborative environment (discussed later). Often researchers also talk about other dimensions, such as intentionality and depth of mediation (system mediated or user mediated), [ 29 ]  while classifying various CIS systems. Three components specific to group-work or collaboration that are highly predominant in the CIS or CSCW literature are control, communication, and awareness. In this section key definitions and related works for these components will be highlighted. Understanding their roles can also help us address various design issues with CIS systems. Rodden identified the value of control in CSCW systems and listed a number of projects with their corresponding schemes for implementing for control. For instance, the COSMOS project [ 33 ]  had a formal structure to represent control in the system. They used roles to represent people or automatons, and rules to represent the flow and processes. The roles of the people could be a supervisor, processor, or analyst. Rules could be a condition that a process needs to satisfy in order to start or finish. Due to such a structure seen in projects like COSMOS, Rodden classified these control systems as procedural based systems.\nThe control penal was every effort to seeking people and control others in this method used for highly responsible people take control of another network system was supply chine managements or transformation into out connection processor information This is one of the most critical components of any collaboration. In fact, Rodden (1991) identified message or communication systems as the class of systems in CSCW that is most mature and most widely used. Since the focus here is on CIS systems that allow its participants to engage in an intentional and interactive collaboration, there must be a way for the participants to communicate with each other. What is interesting to note is that often, collaboration could begin by letting a group of users communicate with each other. For instance, Donath & Robertson [ 34 ]  presented a system that allows a user to know that others were currently viewing the same webpage and communicate with those people to initiate a possible collaboration or at least a co-browsing experience. Providing communication capabilities even in an environment that was not originally designed for carrying out collaboration is an interesting way of encouraging collaboration. Awareness, in the context of CSCW, has been defined as  \"an understanding of the activities of others, which provides a context for your own activity\" . [ 35 ]  The following four kinds of awareness are often discussed and addressed in the CSCW literature: [ 36 ] Shah and Marchionini [ 37 ]  studied awareness as provided by interface in collaborative information seeking. They found that one needs to provide \"right\" (not too little, not too much, and appropriate for the task at hand) kind of awareness to reduce the cost of coordination and maximize the benefits of collaboration. A number of specialized systems have been developed back from the days of the  groupware  systems to today's Web 2.0 interfaces. A few such examples, in chronological order, are given below. Twidale et al. [ 38 ]  developed Ariadne to support the collaborative learning of database browsing skills. In addition to enhancing the opportunities and effectiveness of the collaborative learning that already occurred, Ariadne was designed to provide the facilities that would allow collaborations to persist as people increasingly searched information remotely and had less opportunity for spontaneous face-to-face collaboration. Ariadne was developed in the days when Telnet-based access to library catalogs was a common practice. Building on top of this command-line interface, Ariadne could capture the users’ input and the database’s output, and form them into a search history that consisted of a series of command-output pairs. Such a separation of capture and display allowed Ariadne to work with various forms of data capture methods. To support complex browsing processes in collaboration, Ariadne presented a visualization of the search process. [ 39 ]  This visualization consisted of thumbnails of screens, looking like playing cards, which represented command-output pairs. Any such card can be expanded to reveal its details. The horizontal axis on Ariadne’s display represented time, and the vertical axis showed information on the semantics of the action it represented: the top row for the top level menus, the middle row for specifying a search, and the bottom row for looking at particular book details. This visualization of the search process in Ariadne makes it possible to annotate, discuss with colleagues around the screen, and distribute to remote collaborators for asynchronous commenting easily and effectively. As we saw in the previous section, having access to one’s history as well as the history of one’s collaborators are very crucial to effective collaboration. Ariadne implements these requirements with the features that let one visualize, save, and share a search process. In fact, the authors found one of the advantages of search visualization was the ability to recap previous searching sessions easily in a multi-session exploratory searching. More recently, one of the collaborative information seeking tools that have caught a lot of attention is SearchTogether, developed by Morris and  Horvitz . [ 40 ]  The design of this tool was motivated by a survey that the researchers did with 204 knowledge workers, [ 25 ]  in which they discovered the following. Based on the survey responses, and the current and desired practices for collaborative search, the authors of SearchTogether identified three key features for supporting people’s collaborative information behavior while searching on the Web: awareness, division of labor, and persistence. Let us look at how these three features are implemented. SearchTogether instantiates  awareness  in several ways, one of which is per-user query histories. This is done by showing each group member’s screen name, his/her photo and queries in the “Query Awareness” region. The access to the query histories is immediate and interactive, as clicking on a query brings back the results of that query from when it was executed. The authors identified query awareness as a very important feature in collaborative searching, which allows group members to not only share their query terms, but also learn better query formulation techniques from one another. Another component of SearchTogether that facilitates awareness is the display of page-specific metadata. This region includes several pieces of information about the displayed page, including group members who viewed the given page, and their comments and ratings. The authors claim that such visitation information can help one either choose to avoid a page already visited by someone in the group to reduce the duplication of efforts, or perhaps choose to visit such pages, as they provide a sign of promising leads as indicated by the presence of comments and/or ratings. Division of labor  in SearchTogether is implemented in three ways: (1) “Split Search” allows one to split the search results among all online group members in a round-robin fashion, (2) “Multi-Engine Search” takes a query and runs it on n different search engines, where  n  is the number of online group members, (3) manual division of labor can be facilitated using integrated IM. Finally, the  persistence  feature in SearchTogether is instantiated by storing all the objects and actions, including IM conversations, query histories, recommendation queues, and page-specific metadata. Such data about all the group members are available to each member when he/she logs in. This allows one to easily carry a multi-session collaborative project. Cerchiamo [ 41 ] [ 42 ]   is a collaborative information seeking tool that explores issues related to algorithmic mediation of information seeking activities and how collaborators' roles can be used to structure the user interface. Cerchiamo introduced the notion of algorithmic mediation, that is, the ability of the system to collect input asynchronously from multiple collaborating searchers, and to use these multiple streams of input to affect the information that is being retrieved and displayed to the searchers. Cerchiamo collected judgments of relevance from multiple collaborating searchers and used those judgments to create a ranked list of items that were potentially relevant to the information need. This algorithm prioritized items that were retrieved by multiple queries and that were retrieved by queries that also retrieved many other relevant documents. This rank fusion is just one way in which a search system that manages activities of multiple collaborating searchers can combine their inputs to generate results that are better than those produced by individuals working independently. Cerchiamo implemented two roles—Prospector and Miner—that searchers could assume. Each role had an associated interface. The Prospector role/interface focused on running many queries and making a few judgments of relevance for each query to explore the information space. The Miner role/interface focused on making relevance judgments on a ranked list of items selected from items retrieved by all queries in the current session. This combination of roles allowed searchers to explore and exploit the information space, and led teams to discover more unique relevant documents than pairs of individuals working separately. [ 41 ] Coagmento  (Latin for \"working together\") is a new and unique system that allows a group of people work together for their information seeking tasks without leaving their browsers.  Coagmento  has been developed with a client-server architecture, where the client is implemented as a Firefox plug-in that helps multiple people working in collaboration to communicate, and search, share and organize information. The server component stores and provides all the objects and actions collected from the client. Due to this decoupling,  Coagmento  provides a flexible architecture that allows its users to be co-located or remote, working synchronously or asynchronously, and use different platforms. Coagmento  includes a toolbar and a sidebar. The toolbar has several buttons that helps one collect information and be aware of the progress in a given collaboration. The toolbar has three major parts: The sidebar features a chat window, under which there are three tabs with the history of search engine queries, saved pages and snippets. With each of these objects, the user who created or collected that object is shown. Anyone in the group can access an object by clicking on it. For instance, one can click on a query issued by anyone in the group to re-run that query and bring up the results in the main browser window. An  Android (operating system)  app for Coagmento can be found in the  Android Market . Fernandez-Luna et al. [ 43 ]  introduce Cosme (COde Search MEeting) as a NetBeans IDE plug-in that enables remote team of software developers to collaborate in real time during source-code search sessions. The COSME design was motivated by early studies of C. Foley, M. R. Morris, C. Shah, among others researchers, and by habits of software developers identified in a survey of 117 universities students and professors related with projects of software development, as well as to computer programmers of some companies. The five more commons collaborative search habits (or related to it) of the interviewees was: COSME is designed to enable either synchronous or asynchronous, but explicit remote collaboration among team developers with shared technical information needs. Its client user interface include a search panel that lets developers to specify queries, division of labor principle (possible combination include the use of different search engines, ranking fusion, and split algorithms), searching field (comments, source-code, class or methods declaration), and the collection type (source-code files or digital documentation). The sessions panel wraps the principal options to management the collaborative search sessions, which consists in a team of developers working together to satisfy their shared technical information needs. For example, a developer can use the embedded chat room to negotiate the creation of a collaborative search session, and show comments of the current and historical search results. The implementation of Cosme was based on CIRLab (Collaborative Information Retrieval Laboratory) instantiation, a groupware framework for CIS research and experimentation, Java as programming language, NetBeans IDE Platform as plug-in base, and Amenities (A MEthodology for aNalysis and desIgn of cooperaTIve systEmS) as software engineering methodology. CIS systems development is a complex task, which involves software technologies and Know-how  in different areas such as distributed programming, information search and retrieval, collaboration among people, task coordination and many others according to the context. This situation is not ideal because it requires great programming efforts. Fortunately, some CIS application frameworks and toolkits are increasing their popularity since they have a high reusability impact for both developers and researchers, like Coagmento Collaboratory and DrakkarKeel. Many interesting and important questions remain to be addressed in the field of CIS, including"
  },
  {
    "id": 174,
    "title": "Social information seeking",
    "content": "Social information seeking  is a field of research that involves studying situations, motivations, and methods for people seeking and sharing information in participatory online social sites, such as  Yahoo! Answers , Answerbag,  WikiAnswers  and  Twitter  as well as building systems for supporting such activities. Highly related topics involve traditional and  virtual reference  services,  information retrieval ,  information extraction , and  knowledge representation . [ 1 ] Social information seeking is often materialized in online question-answering (QA) websites, which are driven by a community. Such QA sites have emerged in the past few years as an enormous market, so to speak, for the fulfillment of information needs. Estimates of the volume of questions answered are difficult to come by, but it is likely that the number of questions answered on social/community QA (cQA) sites far exceeds the number of questions answered by library reference services, [ 2 ]  which until recently were one of the few institutional sources for such  question answering . cQA sites make their content – questions and associated answers submitted on the site – available on the open web, and indexable by search engines, thus enabling web users to find answers provided for previously asked questions in response to new queries. The popularity of such sites have been increasing dramatically for the past several years. Major sites that provide a general platform for questions of all types include  Yahoo! Answers , Answerbag and  Quora . While other sites that focus on particular fields; for example,  StackOverflow  (computing). StackOverflow has 3.45 million questions, 1.3 million users and over 6.86 million answers since July 2008 while Quora has 437 thousand questions, 264 thousand users and 979 thousand answers. [ 3 ] Social Q&A or cQA, according to Shah et al., [ 4 ]  consists of three components: a mechanism for users to submit questions in natural language, a venue for users to submit answers to questions, and a community built around this exchange. Viewed in that light, online communities have performed a question answering function perhaps since the advent of Usenet and Bulletin Board Systems, so in one sense cQA is nothing new. Websites dedicated to cQA, however, have emerged on the web only within the past few years: the first cQA site was the Korean Naver Knowledge iN, launched in 2002, while the first English-language CQA site was Answerbag, launched in April 2003. Despite this short history, however, cQA has already attracted a great deal of attention from researchers investigating information seeking behaviors, [ 5 ]  selection of resources, [ 6 ]  social annotations, [ 7 ]  user motivations, [ 8 ]  comparisons with other types of question answering services, [ 9 ]  and a range of other information-related behaviors. Some of the interesting and important research questions in this area include: Shah et al. [ 10 ]  provide a detailed research agenda for social Q&A. A new book by Shah [ 11 ]  presents a more recent and comprehensive information pertaining to social information seeking. Friendsourcing is an important component of social question and answering, including how to route questions to friends or others who will most likely answer the question. [ 12 ]  The important questions include what people's behaviors are in social networks, especially what kinds of questions people ask from their social networks and how different question types affect the frequency, speed and quality of answers they receive. Morris et al. (2010) [ 13 ]  conducted a survey of question and answering within social networks with 624 people, and gathered detailed data about the behavior of Q&A, including frequency, types of questions and answers, and motivations. They found that half (50.6%) of respondents reported having used their status messages to ask a question, which indicated that Q&A on social networks is popular. Also, the types of questions people asked include recommendation, opinion, factual knowledge, rhetorical, etc. And motivations for asking include trust, asking subjective questions, etc. Their analysis also explored the relationships between answer speed and quality, questions’ property and participants’ property. Only a very small portion (6.5%) of the questions were answered, but the 89.3% of the respondents were satisfied with the response time they experienced even though there's a discrepancy between that and expectation. Also, the responses gathered via social networks appear to be very valuable. Their findings implied design for search tools that could combine the speed and breadth of traditional search engines with the trustworthiness, personalization, and the high engagement of social media Q&A. Paul et al. (2011) [ 14 ]  did a study on question and answering on Twitter, and found that out of the 1152 questions they examined, the most popular question types asked on Twitter were rhetorical (42%) and factual (16%). Surprisingly, along with entertainment (29%) and technology (29%) questions, people asked personal and health-related questions (11%). Only 18.7% questions received response, while a handful of questions received a high number of responses. The larger the askers’ network, the more responses she received; however, posting more tweets or posting more frequently did not increase chances of receiving a response. Most often the “follow” relationship between asker and answerer was one-way. Paul et al. also examined what factors of the askers would increase the chance of getting a response and found that more relevant responses are received when there is a mutual relationship between askers and answerers. Intuitively, we would expect this, as mutual relationship would indicate stronger tie strength and hence, more number of relevant answers. Existing social Q&A services can be characterized from the three perspectives, by the definition of social Q&A as a service involving (1) a method for presenting information needs, (2) a place for responding to information need, and (3) participation as a community. These social networks support various friendsourcing behavior, provide information benefits that oftentimes traditional search tools cannot, and also may reinforce social bonds through the process. However, there are many questions and limitations that may prevent people from asking questions on their social networks. For example, they may feel uncomfortable asking questions that are too private, might not want to cost too much other people's time and effort, or might feel the burden of social debts. Rzeszotarski and Morris (2014) [ 15 ]  took a novel approach to explore the perceived social costs of friendsourcing on Twitter via monetary choices. They modeled friendsourcing costs across users, and compared it with crowdsourcing on Amazon Mechanical Turk. Their findings suggested interesting design considerations for minimizing social cost by building a hybrid system combining friendsourcing and crowdsourcing with microtask markets. Sometimes, only asking question from people's own social networks or friends is not enough. If the question is obscure or time sensitive, no members of their social networks may know the answer. For example, this person's friends might not have expertise in providing evaluations for a specific model of digital camera. Also asking the current wait time for security at the local airport might not be possible if none of this person's friends are currently at the airport. Nichols and Kang (2012) [ 16 ]  leveraged Twitter for question and answering with targeted strangers by taking advantage of its public accessibility. In their approach, they mined the public status updates posted on Twitter to find strangers with potentially useful information, and send questions to these strangers to collect responses. As a feasibility study, they collected information regarding response rate, and response time. 42% of users responded to questions from strangers, and 44% of the responses arrived within 30 minutes. Another important and unique component of social Q&A system is that it is a community which allows members to form relationships and bonds, so that their behavior in these social Q&A services will also add to their social capital. Gray et al. (2013) [ 17 ]  explored how bridging social capital, question type and relational closeness influence the perceived usefulness and satisfaction of information obtained through questions asked on Facebook. Their results indicated that bridging social capital could positively predict the perceived utility of the acquired information, meaning that information exchanges on social networks is an effective way of social capital conversion. Also, useful answers are more likely to be received from weak ties than strong ties. In order to recommend the most appropriate users to provide answers in a social network, we need to find approaches to detect users' authority in a social network. In the field of information retrieval, there has been a trend of research investigating ways to detect users' authority effectively and accurately in a social network. Cha et al. [ 18 ]  investigate possible metrics for determining authority users on popular social network Twitter. They propose the following three simple network-based metrics and discuss their usefulness in determining a user's influence. An initial analysis of the three aforementioned metrics showed that the users with the highest indegrees and the users with the highest retweet/mention counts were not the same. The top 1% of users by indegree are shown to have very low correlation with the same percentile of users by retweets and by mentions. This implies that follower count is not useful in determining whether a user's tweets get retweeted or whether the other users engage with them. Pal et al. [ 19 ]  designed features to measure a user's authority on a certain topic. For example, retweet impact refers to how many times a certain user has been retweeted on a certain topic. The impact is dampened by a factor measuring how many times the user had been retweeted by a unique author to avoid the cases when a user has fans who retweet regardless of the content. They first used a clustering approach to find the target cluster which has the highest average score across all features, and used a ranking algorithm to find the most authoritative users within the cluster. With these authority detection methods, social Q&A could be more effective in providing accurate answers to askers. People associated with social information seeking include:"
  },
  {
    "id": 175,
    "title": "Information Retrieval Facility",
    "content": "The  Information Retrieval Facility  ( IRF ), founded 2006 and located in  Vienna ,  Austria , was a research platform for networking and collaboration for professionals in the field of  information retrieval . It ceased operations in 2012."
  },
  {
    "id": 176,
    "title": "Visualization (graphics)",
    "content": "Visualization  (or  visualisation  (see  spelling differences )), also known as Graphics Visualization, is any technique for creating  images ,  diagrams , or  animations  to communicate a message. Visualization through visual imagery has been an effective way to communicate both abstract and concrete ideas since the dawn of humanity. from history include  cave paintings ,  Egyptian hieroglyphs , Greek  geometry , and  Leonardo da Vinci 's revolutionary methods of technical drawing for engineering purposes that actively involve scientific requirements. Visualization today has ever-expanding applications in science, education, engineering (e.g., product visualization),  interactive multimedia ,  medicine , etc. Typical of a visualization application is the field of  computer graphics . The invention of computer graphics (and  3D computer graphics ) may be the most important development in visualization since the invention of  central perspective  in the  Renaissance  period. The development of  animation  also helped advance visualization. The use of visualization to present information is not a new phenomenon. It has been used in maps, scientific drawings, and data plots for over a thousand years. Examples from  cartography  include  Ptolemy's Geographia  (2nd century AD), a map of China (1137 AD), and  Minard 's map (1861) of  Napoleon 's  invasion of Russia  a century and a half ago. Most of the concepts learned in devising these images carry over in a straightforward manner to computer visualization.  Edward Tufte  has written three critically acclaimed books that explain many of these principles. [ 1 ] [ 2 ] [ 3 ] Computer graphics has from its beginning been used to study scientific problems. However, in its early days the lack of graphics power often limited its usefulness. The recent emphasis on visualization started in 1987 with the publication of Visualization in Scientific Computing, a special issue of Computer Graphics. [ 4 ]  Since then, there have been several conferences and workshops, co-sponsored by the  IEEE Computer Society  and  ACM SIGGRAPH , devoted to the general topic, and special areas in the field, for example volume visualization. Most people are familiar with the digital animations produced to present  meteorological  data during weather reports on  television , though few can distinguish between those models of reality and the  satellite photos  that are also shown on such programs. TV also offers scientific visualizations when it shows computer drawn and animated reconstructions of road or airplane accidents. Some of the most popular examples of scientific visualizations are  computer-generated images  that show real  spacecraft  in action, out in the void far beyond Earth, or on other  planets . [ citation needed ]  Dynamic forms of visualization, such as  educational animation  or  timelines , have the potential to enhance learning about systems that change over time. Apart from the distinction between interactive visualizations and animation, the most useful categorization is probably between abstract and model-based scientific visualizations. The abstract visualizations show completely conceptual constructs in 2D or 3D. These generated shapes are completely arbitrary. The model-based visualizations either place overlays of data on real or digitally constructed images of reality or make a digital construction of a real object directly from the scientific data. Scientific visualization is usually done with specialized  software , though there are a few exceptions, noted below. Some of these specialized programs have been released as  open source  software, having very often its origins in universities, within an academic environment where sharing software tools and giving access to the source code is common. There are also many  proprietary software  packages of scientific visualization tools. Models and frameworks for building visualizations include the  data flow  models popularized by systems such as AVS, IRIS Explorer, and  VTK  toolkit, and data state models in spreadsheet systems such as the Spreadsheet for Visualization and Spreadsheet for Images. As a subject in  computer science ,  scientific visualization  is the use of interactive, sensory representations, typically visual, of abstract data to reinforce  cognition ,  hypothesis  building, and  reasoning .\n Scientific visualization  is the transformation, selection, or representation of data from simulations or experiments, with an implicit or explicit geometric structure, to allow the exploration, analysis, and understanding of the data. Scientific visualization focuses and emphasizes the representation of higher order data using primarily graphics and animation techniques. [ 5 ] [ 6 ]  It is a very important part of visualization and maybe the first one, as the visualization of experiments and phenomena is as old as  science  itself. Traditional areas of scientific visualization are  flow visualization ,  medical visualization ,  astrophysical visualization , and  chemical visualization . There are several different techniques to visualize scientific data, with  isosurface reconstruction  and  direct volume rendering  being the more common. Data visualization is a related subcategory of visualization dealing with  statistical graphics  and  geospatial data  (as in  thematic cartography ) that is abstracted in schematic form. [ 7 ] Information visualization concentrates on the use of computer-supported tools to explore large amount of abstract data. The term \"information visualization\" was originally coined by the User Interface Research Group at Xerox PARC and included  Jock Mackinlay . [ citation needed ]  Practical application of information visualization in computer programs involves selecting,  transforming , and representing abstract data in a form that facilitates human interaction for exploration and understanding. Important aspects of information visualization are dynamics of visual representation and the interactivity. Strong techniques enable the user to modify the visualization in real-time, thus affording unparalleled perception of patterns and structural relations in the abstract data in question. Educational visualization is using a  simulation  to create an image of something so it can be taught about. This is very useful when teaching about a topic that is difficult to otherwise see, for example,  atomic structure , because atoms are far too small to be studied easily without expensive and difficult to use scientific equipment. The use of visual representations to transfer knowledge between at least two persons aims to improve the transfer of  knowledge  by using  computer  and non-computer-based visualization methods complementarily. [ 8 ]  Thus properly designed visualization is an important part of not only data analysis but knowledge transfer process, too. [ 9 ]  Knowledge transfer may be significantly improved using hybrid designs as it enhances information density but may decrease clarity as well. For example, visualization of a 3D  scalar field  may be implemented using iso-surfaces for field distribution and textures for the gradient of the field. [ 10 ]  Examples of such visual formats are  sketches ,  diagrams ,  images , objects, interactive visualizations, information visualization applications, and imaginary visualizations as in  stories . While information visualization concentrates on the use of computer-supported tools to derive new insights, knowledge visualization focuses on transferring insights and creating new  knowledge  in  groups . Beyond the mere transfer of  facts , knowledge visualization aims to further transfer  insights ,  experiences ,  attitudes ,  values ,  expectations ,  perspectives ,  opinions , and  predictions  by using various complementary visualizations.\nSee also:  picture dictionary ,  visual dictionary Product visualization  involves visualization software technology for the viewing and manipulation of 3D models, technical drawing and other related documentation of manufactured components and large assemblies of products. It is a key part of  product lifecycle management . Product visualization software typically provides high levels of photorealism so that a product can be viewed before it is actually manufactured. This supports functions ranging from design and styling to sales and marketing.  Technical visualization  is an important aspect of product development. Originally  technical drawings  were made by hand, but with the rise of advanced  computer graphics  the  drawing board  has been replaced by  computer-aided design  (CAD). CAD-drawings and models have several advantages over hand-made drawings such as the possibility of  3-D  modeling,  rapid prototyping , and  simulation . 3D product visualization promises more interactive experiences for online shoppers, but also challenges retailers to overcome hurdles in the production of 3D content, as large-scale 3D content production can be extremely costly and time-consuming. [ 11 ] Visual communication  is the  communication  of  ideas  through the visual display of  information . Primarily associated with  two dimensional   images , it includes:  alphanumerics ,  art ,  signs , and  electronic  resources. Recent research in the field has focused on  web design  and graphically oriented  usability . Visual analytics  focuses on human interaction with visualization systems as part of a larger process of data analysis. Visual analytics has been defined as \"the science of analytical reasoning supported by the interactive visual interface\". [ 12 ] Its focus is on human information discourse (interaction) within massive, dynamically changing information spaces. Visual analytics research concentrates on support for perceptual and cognitive operations that enable users to detect the expected and discover the unexpected in complex information spaces. Technologies resulting from visual analytics find their application in almost all fields, but are being driven by critical needs (and funding) in biology and national security. Interactive visualization  or  interactive visualisation  is a branch of  graphic visualization  in  computer science  that involves studying how humans interact with computers to create graphic illustrations of information and how this process can be made more efficient. For a visualization to be considered interactive it must satisfy two criteria: One particular type of interactive visualization is  virtual reality  (VR), where  the visual representation of information is presented using an immersive display device such as a stereo projector (see  stereoscopy ). VR is also characterized by the use of a spatial metaphor, where some aspect of the information is represented in three dimensions so that humans can explore the information as if it were present (where instead it was remote), sized appropriately (where instead it was on a much smaller or larger scale than humans can sense directly), or had shape (where instead it might be completely abstract). Another type of interactive visualization is collaborative visualization, in which multiple people interact with the same computer visualization to communicate their ideas to each other or to explore information cooperatively. Frequently, collaborative visualization is used when people are physically separated. Using several networked computers, the same visualization can be presented to each person simultaneously. The people then make annotations to the visualization as well as communicate via audio (i.e., telephone), video (i.e., a video-conference), or text (i.e.,  IRC ) messages. The Programmer's Hierarchical Interactive Graphics System ( PHIGS ) was one of the first programmatic efforts at interactive visualization and provided an enumeration of the types of input humans provide. People can: All of these actions require a physical device. Input devices range from the common –  keyboards ,  mice ,  graphics tablets ,  trackballs , and  touchpads  – to the esoteric –  wired gloves ,  boom arms , and even  omnidirectional treadmills . These input actions can be used to control both the  unique information  being represented or the way that the information is presented. When the information being presented is altered, the visualization is usually part of a  feedback loop . For example, consider an aircraft avionics system where the pilot inputs roll, pitch, and yaw and the visualization system provides a rendering of the aircraft's new attitude. Another example would be a scientist who changes a simulation while it is running in response to a visualization of its current progress. This is called  computational steering . More frequently, the representation of the information is changed rather than the information itself. Experiments have shown that a delay of more than 20  ms  between when input is provided and a visual representation is updated is noticeable by most people  [ citation needed ] . Thus it is desirable for an interactive visualization to provide a  rendering  based on human input within this time frame. However, when large amounts of data must be processed to create a visualization, this becomes hard or even impossible with current technology. Thus the term \"interactive visualization\" is usually applied to systems that provide feedback to users within several seconds of input. The term  interactive  framerate  is often used to measure how interactive a visualization is. Framerates measure the frequency with which an image (a frame) can be generated by a visualization system. A framerate of 50 frames per second (frame/s) is considered good while 0.1 frame/s would be considered poor. The use of framerates to characterize interactivity is slightly misleading however, since framerate is a measure of  bandwidth  while humans are more sensitive to  latency . Specifically, it is possible to achieve a good framerate of 50 frame/s but if the images generated refer to changes to the visualization that a person made more than 1 second ago, it will not feel interactive to a person. The rapid response time required for interactive visualization is a difficult constraint to meet and there are several approaches that have been explored to provide people with rapid visual feedback based on their input. Some include Many conferences occur where interactive visualization academic papers are presented and published."
  },
  {
    "id": 177,
    "title": "Multimedia information retrieval",
    "content": "Multimedia information retrieval  ( MMIR  or  MIR ) is a research discipline of  computer science  that aims at extracting semantic information from  multimedia  data sources. [ 1 ] [ failed verification ]  Data sources include directly perceivable media such as  audio ,  image  and  video , indirectly perceivable sources such as  text , semantic descriptions, [ 2 ]   biosignals  as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups: Feature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness. [ 1 ] : 2  [ failed verification ]  Generally, two possible goals can be achieved by feature extraction: Multimedia Information Retrieval implies that multiple channels are employed for the understanding of media content. [ 5 ]  Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions – as they frequently occur in motion description – have to be normalized to a fixed length first. Frequently used methods for description filtering include  factor analysis  (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the  Kalman filter  are used for merging of descriptions. Generally, all forms of machine learning can be employed for the categorization of multimedia descriptions [ 1 ] : 125  [ failed verification ]  though some methods are more frequently used in one area than another. For example,  hidden Markov models  are state-of-the-art in  speech recognition , while  dynamic time warping  – a semantically related method – is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following: The selection of the best classifier for a given problem (test set with descriptions and class labels, so-called  ground truth ) can be performed automatically, for example, using the  Weka  Data Miner. Models of Multimedia Information Retrieval\nSpoken Language Audio Retrieval\nSpoken Language Audio Retrieval focuses on audio content containing spoken words. It involves the transcription of spoken content into text using Automatic Speech Recognition (ASR) and indexing the transcriptions for text-based search. Key Features:\nTechniques: ASR for transcription and text indexing.\nQuery Types: Text-based queries.\nApplications:\nSearching podcast transcripts.\nAnalyzing customer service call logs.\nFinding specific phrases in meeting recordings.\nChallenges:\nErrors in ASR can reduce retrieval accuracy.\nMultilingual and accent variability requires robust systems.\nNon-Speech Audio Retrieval\nNon-Speech Audio Retrieval handles audio content without spoken words, such as music, environmental sounds, or sound effects. This model relies on extracting audio features like pitch, rhythm, and timbre to identify relevant audio. Key Features:\nTechniques: Acoustic feature extraction (e.g., spectrograms, MFCCs).\nQuery Types: Audio samples or textual descriptions.\nApplications:\nMusic recommendation systems.\nEnvironmental sound detection (e.g., gunshots, animal calls).\nSound effect retrieval in media production.\nChallenges:\nDifficulty in bridging the semantic gap between user queries and low-level audio features.\nEfficient indexing of large datasets.\nGraph Retrieval\nGraph Retrieval retrieves information represented as graphs, which consist of nodes (entities) and edges (relationships). It is widely used in social networks, knowledge graphs, and bioinformatics. Key Features:\nTechniques: Graph matching, adjacency list/matrix storage, and graph databases (e.g., Neo4j).\nQuery Types: Subgraphs, patterns, or textual queries.\nApplications:\nSocial network analysis.\nSearching knowledge graphs.\nMolecular structure retrieval.\nChallenges:\nComputationally intensive subgraph matching.\nScalability for large, complex graphs.\nImagery Retrieval\nImagery Retrieval retrieves images based on user input, such as textual descriptions or visual samples. It leverages both low-level features and semantic analysis for search. Key Features:\nTechniques: Content-Based Image Retrieval (CBIR), visual feature extraction, semantic analysis.\nQuery Types: Text, sketches, or example images.\nApplications:\nStock image search.\nE-commerce product matching.\nMedical imaging analysis.\nChallenges:\nBridging the semantic gap between user queries and image content.\nEfficient indexing of large-scale image datasets.\nVideo Retrieval\nVideo Retrieval is the process of finding specific video content based on user queries. It involves analyzing both the visual and temporal features of videos. Key Features:\nTechniques: Keyframe extraction, motion pattern analysis, temporal indexing.\nQuery Types: Textual descriptions, sample clips, or temporal queries.\nApplications:\nStreaming service recommendations.\nSurveillance footage analysis.\nSports analytics.\nChallenges:\nManaging the large file sizes of video content.\nEfficient analysis of temporal sequences and multimodal features.\nComparison of Retrieval Models\nModel\tData Type\tQuery Types\tApplications\nSpoken Language Audio\tSpeech recordings\tText queries\tPodcasts, meeting logs, call centers\nNon-Speech Audio\tMusic, sound effects\tAudio samples or text\tMusic apps, environmental sounds\nGraph Retrieval\tGraph structures\tSubgraphs, patterns\tKnowledge graphs, bioinformatics\nImagery Retrieval\tImages\tText, sketches, or images\tE-commerce, medical imaging\nVideo Retrieval\tVideos (visual + temporal)\tText, clips, or time queries\tSurveillance, sports analysis\nConclusion\nMultimedia Information Retrieval plays a crucial role in organizing and accessing vast multimedia data repositories. The variety of retrieval models ensures that users can effectively interact with and extract insights from complex multimedia datasets. Future advancements in artificial intelligence and machine learning are expected to improve the accuracy and scalability of MIR systems. MMIR provides an overview over methods employed in the areas of information retrieval. [ 6 ] [ 7 ]  Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as: The  International Journal of Multimedia Information Retrieval [ 8 ]  documents the development of MMIR as a research discipline that is independent of these areas. See also  Handbook of Multimedia Information Retrieval [ 9 ]  for a complete overview over this research discipline."
  },
  {
    "id": 178,
    "title": "Personal information management",
    "content": "Personal information management  ( PIM ) is the study and implementation of the activities that people perform in order to acquire or create, store, organize, maintain, retrieve, and use  informational  items such as  documents  (paper-based and digital),  web pages , and  email  messages for everyday use to complete tasks (work-related or not) and fulfill a person's various roles (as parent, employee, friend, member of community, etc.); [ 1 ] [ 2 ]  it is  information management  with intrapersonal scope.  Personal knowledge management  is by some definitions a subdomain. One ideal of PIM is that people should always have the right information in the right place, in the right form, and of sufficient completeness and quality to meet their current need. Technologies and tools can help so that people spend less time with time-consuming and error-prone clerical activities of PIM (such as looking for and organising information). But tools and technologies can also overwhelm people with too much information leading to  information overload . A special focus of PIM concerns how people organize and maintain personal information collections, and methods that can help people in doing so. People may manage information in a variety of settings, for a variety of reasons, and with a variety of types of information. For example, a traditional office worker might manage physical documents in a filing cabinet by placing them in hanging folders organized alphabetically by project name. More recently, this office worker might organize digital documents into the virtual folders of a local, computer-based  file system  or into a cloud-based store using a  file hosting service  (e.g.,  Dropbox ,  Microsoft OneDrive ,  Google Drive ). People manage information in many more private, personal contexts as well. A parent may, for example, collect and organize photographs of their children into a photo album which might be paper-based or digital. PIM considers not only the methods used to store and organize information, but also is concerned with how people  retrieve information  from their collections for re-use.  For example, the office worker might re-locate a physical document by remembering the name of the project and then finding the appropriate folder by an alphabetical search.  On a computer system with a  hierarchical file system , a person might need to remember the top-level folder in which a document is located, and then browse through the folder contents to navigate to the desired document.  Email systems often support additional methods for re-finding such as fielded search (e.g., search by sender, subject, date).  The characteristics of the document types, the data that can be used to describe them (meta-data), and features of the systems used to store and organize them (e.g. fielded search) are all components that may influence how users accomplish personal information management. The purview of PIM is broad. A person's perception of and ability to effect change in the world is determined, constrained, and sometimes greatly extended, by an ability to receive, send and otherwise manage information. Research in the field of personal information management has considered six senses in which information can be personal (to “me”) and so an object of that person's PIM activities: [ 2 ] An encyclopaedic review of PIM literature suggests that all six senses of personal information listed above and the tools and technologies used to work with such information (from email applications and word processors to  personal information managers  and  virtual assistants ) combine to form a  personal space of information  (PSI, pronounced as in the  Greek letter , alternately referred to as a  personal information space ) that is unique for each individual. [ 3 ]  Within a person's PSI are  personal information collections  (PICs) or, simply, collections. Examples include: Activities of PIM – i.e., the actions people take to manage information that is personal to them in one or more of the ways listed above – can be seen as an effort to establish, use, and maintain a mapping between information and need. [ 2 ] Two activities of PIM occur repeatedly throughout a person's day and are often prompted by external events. Meta-level activities focus more broadly on aspects of the mapping itself. PIM activities overlap with one another. For example, the effort to keep an email attachment as a document in a personal file system may prompt an activity to organize the file system e.g., by creating a new folder for the document. Similarly, activities to organize may be prompted by a person's efforts to find a document as when, for example, a person discovers that two folders have overlapping content and should be consolidated. Meta-level activities overlap not only with finding and keeping activities but, even more so, with each other. For example, efforts to re-organize a personal file system can be motivated by the evaluation that the current file organization is too time-consuming to maintain and doesn't properly highlight the information most in need of attention. Information sent and received takes many different information forms in accordance with a growing list of communication modes, supporting tools, and people's customs, habits, and expectations. People still send paper-based letters, birthday cards, and thank you notes. But increasingly, people communicate using digital forms of information including  emails, digital documents shared (as attachments or via a  file hosting service   such as  Dropbox ),  blog posts  and  social media  updates (e.g., using a service such as  Facebook ),  text messages  and links, text, photos, and videos shared via services such as  Twitter ,  Snapchat ,  Reddit , and  Instagram . People work with information items as packages of information with properties that vary depending upon the information form involved. Files, emails, \"tweets\", Facebook updates, blog posts, etc. are each examples of the information item. The ways in which an information item can be manipulated depend upon its underlying form. Items can be created but not always deleted (completely). Most items can be copied, sent and transformed as in, for example, when a digital photo is taken of a paper document (transforming from paper to digital) and then possibly further transformed as when optical character recognition is used to extract text from the digital photo, and then transformed yet again when this information is sent to others via a text message. Information fragmentation [ 4 ] [ 2 ]  is a key problem of PIM often made worse by the many information forms a person must work with. Information is scattered widely across information forms on different devices, in different formats, in different organizations, with different supporting tools. Information fragmentation creates problems for each kind of PIM activity. Where to keep new information? Where to look for (re-find) information already kept? Meta-level activities, such as maintaining and organizing, are also more difficult and time-consuming when different stores on different devices must be separately maintained. Problems of information fragmentation are especially manifest when a person must look across multiple devices and applications to gather together the information needed to complete a project. [ 5 ] PIM is a new field with ancient roots. When the  oral  rather than the written word dominated, human memory was the primary means for information preservation. [ 6 ]  As information was increasingly rendered in paper form, tools were developed over time to meet the growing challenges of management. For example, the vertical  filing cabinet , now such a standard feature of home and workplace offices, was first commercially available in 1893. [ 7 ] With the increasing availability of computers in the 1950s came an interest in the computer as a source of metaphors and a test bed for efforts to understand the human ability to  process information  and to  solve problems .  Newell  and  Simon  pioneered the computer's use as a tool to model human thought. [ 8 ] [ 9 ]  They produced \"The  Logic Theorist \", generally thought to be the first running artificial intelligence (AI) program. The computer of the 1950s was also an inspiration for the development of an information processing approach to human behavior and performance. [ 10 ] After the 1950s research showed that the computer, as a symbol processor, could \"think\" (to varying degrees of fidelity) like people do, the 1960s saw an increasing interest in the use of the computer to help people to think better and to process information more effectively. Working with  Andries van Dam  and others,  Ted Nelson , who coined the word \" hypertext \", [ 11 ]  developed one of the first hypertext systems, The Hypertext Editing System, in 1968. [ 12 ]  That same year,  Douglas Engelbart  also completed work on a hypertext system called NLS (oN-Line System). [ 13 ]  Engelbart advanced the notion that the computer could be used to augment the human intellect. [ 14 ] [ 15 ]  As heralded by the publication of  Ulric Neisser 's book  Cognitive Psychology , [ 16 ]  the 1960s also saw the emergence of cognitive psychology as a discipline that focused primarily on a better understanding of the human ability to think, learn, and remember. The computer as aid to the individual, rather than remote  number cruncher  in a refrigerated room, gained further validity from work in the late 1970s and through the 1980s to produce  personal computers  of increasing power and portability. These trends continue:  computational power  roughly equivalent to that of a  desktop computer  of a decade ago can now be found in devices that fit into the palm of a hand.\nThe phrase \"Personal Information Management\" was itself apparently first used in the 1980s in the midst of general excitement over the potential of the personal computer to greatly enhance the human ability to process and manage information. [ 17 ]  The 1980s also saw the advent of so-called \"PIM tools\" that provided limited support for the management of such things as appointments and scheduling, to-do lists, phone numbers, and addresses. A community dedicated to the study and improvement of human–computer interaction also emerged in the 1980s. [ 18 ] [ 19 ] As befits the \"information\" focus of PIM, PIM-relevant research of the 1980s and 1990s extended beyond the study of a particular device or application towards larger ecosystems of information management to include, for example, the organization of the physical office and the management of paperwork. [ 20 ] [ 21 ]  Malone characterized personal organization strategies as 'neat' or 'messy' and described 'filing' and 'piling' approaches to the organization of information. [ 22 ]  Other studies showed that people vary their methods for keeping information according to anticipated uses of that information in the future. [ 23 ]  Studies explored the practical implications that human memory research might carry in the design of, for example, personal filing systems, [ 24 ] [ 25 ] [ 26 ]  and information retrieval systems. [ 27 ]  Studies demonstrated a preference for navigation (browsing, \"location-based finding) in the return to personal files, [ 28 ]  a preference that endures today notwithstanding significant improvements in search support. [ 29 ] [ 30 ] [ 31 ] [ 32 ]  and an increasing use of search as the preferred method of return to e-mails. [ 33 ] [ 34 ] PIM, as a contemporary field of inquiry with a self-identified community of researchers, traces its origins to a Special Interest Group (SIG) session on PIM at the CHI 2004 conference and to a special  National Science Foundation  (NSF)-sponsored workshop held in Seattle in 2005. [ 35 ] [ 36 ] Much PIM research can be grouped according to the PIM activity that is the primary focus of the research. These activities are reflected in the two main models of PIM, i.e., that primary PIM activities are finding/re-finding, keeping and meta-level activities [ 37 ] [ 2 ]   (see section   Activities of PIM ) or, alternatively, keeping, managing, and exploiting. [ 38 ] [ 39 ]  Important research is also being done under the special topics: Personality, mood, and emotion both as impacting and impacted by a person's practice of PIM, the management of personal health information and the management of personal information over the long run and for legacy. Throughout a typical day, people repeatedly experience the need for information in large amounts and small (e.g., \"When is my next meeting?\"; \"What's the status of the budget forecast?\" \"What's in the news today?\") prompting activities to find and re-find. A large body of research in  information seeking ,  information behavior , and  information retrieval  relates and especially to efforts to find information in public spaces such as the Web or a traditional library. There is a strong personal component even in efforts to find new information, never before experienced, from a public store such as the Web. For example, efforts to find information may be directed by a personally created outline, self-addressed email reminder or a to-do list. In addition, information inside a person's PSI can be used to support a more targeted, personalized search of the web. [ 40 ] A person's efforts to find useful information are often a sequence of interactions rather than a single transaction. Under a \"berry picking\" model of finding, information is gathered in bits and pieces through a series of interactions, and during this time, a person's expression of need, as reflected in the current query, evolves. [ 41 ]  People may favor stepwise approach to finding needed information to preserve a greater sense of control and context over the finding process and smaller steps may also reduce the cognitive burden associated with query formulation. [ 42 ]  In some cases, there simply is not a \"direct\" way to access the information. For example, a person's remembrance for a needed Web site may only be through an email message sent by a colleague i.e., a person may not recall a Web address nor even keywords that might get be used in a Web search but the person does recall that the Web site was mentioned recently in an email from a colleague). People may find (rather than re-find) information even when this information is ostensibly under their control. For example, items may be \"pushed\" into the PSI (e.g., via the inbox, podcast subscriptions, downloads). If these items are discovered later, it is through an act of finding not re-finding (since the person has no remembrance for the information). Lansdale [ 17 ]  characterized the retrieval of information as a two-step process involving interplay between actions to  recall  and  recognize . The steps of recall and recognition can iterate to progressively narrow the efforts to find the desired information. This interplay happens, for example, when people move through a folder hierarchy to a desired file or e-mail message or navigate through a website to a desired page. But re-finding begins first with another step:  Remember  to look in the first place. People may take the trouble to create Web bookmarks or to file away documents and then forget about this information so that, in worst case, the original effort is wasted. [ 43 ] [ 44 ] [ 45 ] [ 46 ] Also, finding/re-finding often means not just assembling a single item of information but rather a set of information. The person may need to  repeat  the finding sequence several times. A challenge in tool support is to provide people with ways to group or interrelate information items so that their chances improve of retrieving a complete set of the information needed to complete a task. [ 3 ] Over the years, PIM studies have determined that people prefer to return to personal information, most notably the information kept in personal digital files, by navigating rather than searching. [ 28 ] [ 30 ] [ 32 ] Support for searching personal information has improved dramatically over the years most notably in the provision for full-text indexing to improve search speed. [ 47 ]  With these improvements, preference may be shifting to search as a primary means for locating email messages (e.g., search on subject or sender, for messages not in view). [ 48 ] [ 49 ] However, a preference persists for navigation as the primary means of re-finding personal files (e.g., stepwise folder traversal; scanning a list of files within a folder for the desired file), notwithstanding ongoing improvements in search support. [ 30 ]  The enduring preference for navigation as a primary means of return to files may have a neurological basis [ 50 ]  i.e., navigation to files appears to use mental facilities similar to those people use to navigate in the physical world. Preference for navigation is also in line with a  primacy effect  repeatedly observed in psychological research such that preferred method of return aligns with initial exposure. Under a  first impressions  hypothesis, if a person's initial experience with a file included its placement in a folder, where the folder itself was reached by navigating through a hierarchy of containing folders, then the person will prefer a similar method – navigation – for return to the file later. [ 49 ] There have been some prototyping efforts to explore an in-context creation e.g., creation in the context of a project the person is working on, of not only files, but also other forms of information such as web references and email. [ 51 ]  Prototyping efforts have also explored ways to improve support for navigation e.g., by highlighting and otherwise making it easier to follow, the paths people are more likely to take in their navigation back to a file. [ 52 ] Many events of daily life are roughly the converse of finding events: People encounter information and try to determine what, if anything, they should do with this information, i.e., people must match the information encountered to current or anticipated needs. Decisions and actions relating to encountered information are collectively referred to as keeping activities. The ability to effectively handle information that is encountered by happenstance is essential to a person's ability to discover new material and make new connections. [ 53 ]  People also keep information that they have actively sought but do not have time to process currently. A search on the web, for example, often produces much more information than can be consumed in the current session. Both the decision to keep this information for later use and the steps to do so are keeping activities. Keeping activities are also triggered when people are interrupted during a current task and look for ways of preserving the current state so that work can be quickly resumed later. [ 54 ]  People keep appointments by entering reminders into a calendar and keep good ideas or \"things to pick up at the grocery store\" by writing down a few cryptic lines on a loose piece of paper. People keep not only to ensure they have the information later, but also to build reminders to look for and use this information. Failure to remember to use information later is one kind of  prospective memory  failure. [ 55 ]  In order to avoid such a failure, people may, for example, self-e-mail a web page reference in addition to or instead of making a bookmark because the e-mail message with the reference appears in the inbox where it is more likely to be noticed and used. [ 56 ] The keeping decision can be characterized as a signal detection task subject to errors of two kinds: 1) an incorrect rejection (\"miss\") when information is ignored that later is needed and should have been kept (e.g., proof of charitable donations needed now to file a tax return) and 2) a false positive when information kept as useful (incorrectly judged as \"signal\") turns out not to be used later. [ 57 ]  Information kept and never used only adds to the clutter – digital and physical – in a person's life. [ 58 ] Keeping can be a difficult and error prone effort. Filing i.e., placing information items such as paper documents, digital documents and emails, into folders, can be especially so. [ 59 ] [ 60 ]  To avoid, or delay filing information (e.g., until more is known concerning where the information might be used), people may opt to put information in \"piles\" instead. [ 22 ]  (Digital counterparts to physical piling include leaving information in the email inbox or placing digital documents and web links into a holding folder such as \"stuff to look at later\".) But information kept in a pile, physical or virtual, is easily forgotten as the pile fades into a background of clutter and research indicates that a typical person's ability to keep track of different piles, by location alone, is limited. [ 61 ] Tagging provides another alternative to filing information items into folders. A strict folder hierarchy does not readily allow for the flexible classification of information even though, in a person's mind, an information item might fit in several different categories. [ 62 ]  A number of tag-related prototypes for PIM have been developed over the years. [ 63 ] [ 64 ]  A tagging approach has also been pursued in commercial systems, most notably Gmail (as \"labels\"), but the success of tags so far is mixed. Bergman et al. found that users, when provided with options to use folders or tags, preferred folders to tags and, even when using tags, they typically refrained from adding more than a single tag per information item. [ 65 ] [ 66 ]  Civan et al., through an engagement of participants in critical, comparative observation of both tagging and the use of folders were able to elicit some limitations of tagging not previously discussed openly such as, for example, that once a person decides to use multiple tags, it is usually important to continue doing so (else the tag not applied consistently becomes ineffective as a means of retrieving a complete set of items). [ 67 ] Technologies may help to reduce the costs, in personal time and effort, of keeping and the likelihood of error. For example, the ability to take a digital photo of a sign, billboard announcement or the page of a paper document can obviate the task of otherwise transcribing (or photocopying) the information. A person's ongoing use of a smartphone through the day can create a time-stamped record of events as a kind of automated keeping and especially of information \"experienced by me\" (see section, \"The senses in which information is personal\") with potential use in a person's efforts to journal or to return to information previously experienced (\"I think I read the email while in the taxi on the way to the airport...\").  Activity tracking  technology can further enrich the record of a person's daily activity with tremendous potential use for people to enrich their understanding of their daily lives and the healthiness of their diet and their activities. [ 68 ] Technologies to automate the keeping of personal information segue to personal informatics and the  quantified self  movement, life logging, in the extreme, a 'total capture\" of information. [ 69 ]  Tracking technologies raise serious issues of privacy (see \" Managing privacy and the flow of information \"). Additional questions arise concerning the utility and even the practical accessibility of \"total capture\". [ 70 ] Activities of finding and, especially, keeping can segue into activities to maintain and organize as when, for example, efforts to keep a document in the file system prompt the creation of a new folder or efforts to re-find a document highlight the need to consolidate two folders with overlapping content and purpose. Differences between people are especially apparent in their approaches to the maintenance and organization of information. Malone [ 22 ]  distinguished between \"neat\" and \"messy\" organizations of paper documents. \"Messy\" people had more piles in their offices and appeared to invest less effort than \"neat\" people in filing information. Comparable differences have been observed in the ways people organize digital documents, emails, and web references. [ 71 ] Activities of keeping correlate with activities of organizing so that, for example, people with more elaborate folder structures tend to file information more often and sooner. [ 71 ]  However, people may be selective in the information forms for which they invest efforts to organize. The schoolteachers who participated in one study, for example, reported having regular \"spring cleaning\" habits for organization and maintenance of paper documents but no comparable habits for digital information. [ 72 ] Activities of organization (e.g., creating and naming folders) segue into activities of maintenance such as consolidating redundant folders, archiving information no longer in active use, and ensuring that information is properly  backed up  and otherwise  secured . (See also section, \"Managing privacy and the flow of information\"). Studies of people's folder organizations for digital information indicate that these have uses going far beyond the organization of files for later retrieval. Folders are information in their own right – representing, for example, a person's evolving understanding of a project and its components. A folder hierarchy can sometimes represent an informal problem decomposition with a parent folder representing a project and subfolders representing major components of the project (e.g., \"wedding reception\" and \"church service\" for a \"wedding\" project). [ 73 ] However, people generally struggle to keep their information organized [ 74 ]  and often do not have reliable backup routines. [ 75 ]  People have trouble maintaining and organizing many distinct forms of information (e.g., digital documents, emails, and web references) [ 76 ]  and are sometimes observed to make special efforts to consolidate different information forms into a single organization. [ 56 ] With ever increasing stores of personal digital information, people face challenges of  digital curation  for which they are not prepared. [ 77 ] [ 78 ] [ 79 ]  At the same time, these stores offer their owners the opportunity, with the right training and tool support, for  exploitation  of their information in new, useful ways. [ 80 ] Empirical observations of PIM studies motivate prototyping efforts towards information tools to provide better support for the maintenance, organization and, going further, curation of personal information. For example,  GrayArea [ 81 ]  applies the demotion principle of the user-subjective approach to allow people to move less frequently used files in any given folder to a gray area at the bottom\tend of the listing of this folder. These files can still be accessed but are less visible and so less distracting of a person's attention. The  Planz [ 51 ]  prototype supports an in-context creation and integration of project-related files, emails, web references, informal notes and other forms of information into a simplified, document-like interface meant to represent the project with headings corresponding to folders in the personal file system and subheadings (for tasks, sub-projects, or other project components) corresponding to subfolders. The intention is that a single, useful organization should emerge incidentally as people focus on the planning and completion of their projects. People face a continual evaluation of tradeoffs in deciding what information \"flows\" into and out of their PSI. Each interaction poses some degree of risk to privacy and security. Letting out information to the wrong recipients can lead to  identity theft . Letting in the wrong kind of information can mean that a person's devices are \"infected\" and the person's data is corrupted or \"locked\" for  ransom . By some estimates, 30% or more of the computers in the United States are infected. [ 82 ]  \nHowever, the exchange of information, incoming and outgoing, is an essential part of living in the modern world. To order goods and services online, people must be prepared to \"let out\" their credit card information. To try out a potentially useful, new information tool, people may need to \"let in\" a download that could potentially make unwelcome changes to the web browser or the desktop. Providing for adequate control over the information, coming into and out of a PSI, is a major challenge. Even more challenging is the user interface to make clear the implications for various privacy choices particularly regarding  Internet privacy . What, for example, are the personal information privacy implications of clicking the \"Sign Up\" button for use of social media services such as Facebook. [ 83 ] People seek to understand how they might improve various aspects of their PIM practices with questions such as \"Do I really need to keep all this information?\"; \"Is this tool (application, applet, device) worth the troubles (time, frustration) of its use?\" and, perhaps most persistent, \"Where did the day go? Where has the time gone? What did I accomplish?\". These last questions may often be voiced in reflection, perhaps on the commute home from work at the end of the workday. \nBut there is increasing reason to expect that answers will be based on more than remembrance and reflection. Increasingly data incidentally, automatically captured over the course of a person's day and the person's interactions with various information tools to work with various forms of information (files, emails, texts, pictures, etc.) can be brought to bear in evaluations of a person's PIM practice and the identification of possible ways to improve. [ 84 ] Efforts to make sense of information represent another set of meta-level activity that operate on personal information and the mapping between information and need. People must often assemble and analyze a larger collection of information to decide what to do next. \"Which job applicant is most likely to work best for us?\", \"Which retirement plan to choose?\", \"What should we pack for our trip?\". These and many other decisions are generally based not on a single information item but on a collection of information items – documents, emails (e.g., with advice or impressions from friends and colleagues), web references, etc.\nMaking sense of information is \"meta\" not only for its broader focus on information collections but also because it permeates most PIM activity even when the primary purpose may ostensibly be something else. For example, as people organize information into folders, ostensibly to ensure its subsequent retrieval, people may also be making sense and coming to a deeper understanding of this information. Personality  and  mood  can impact a person's practice of PIM and, in turn, a person's emotions can be impacted by the person's practice of PIM. In particular,  personality traits  (e.g., \"conscientiousness\" or \"neuroticism\") have, in certain circumstances, been shown to correlate with the extent to which a person keeps and organizes information into a personal archive such as a personal filing system. [ 85 ]  However, another recent study found personality traits were not correlated with any aspects of personal filing systems, suggesting that PIM practices are influenced less by personality than by external factors such as the operating system used (i.e. Mac OS or Windows), which were seen to be much more predictive. [ 86 ] Aside from the correlation between practices of PIM and more enduring personality traits, there is evidence to indicate that a person's (more changeable) mood impacts activities of PIM so that, for example, a person experiencing negative moods, when organizing personal information, is more likely to create a structure with more folders where folders, on average, contain fewer files. [ 87 ] Conversely, the information a person keeps or routinely encounters (e.g., via social media), can profoundly impact a person's mood. Even as explorations continue into the potential for the automatic, incidental capture of information (see section  Keeping ) there is growing awareness for the need to design for forgetting as well as for remembrance as, for example, when a person realizes the need to dispose of digital belongings in the aftermath of a romantic breakup or the death of a loved one. [ 88 ] Beyond the negative feelings induced by information associated with a failed relationship, people experience negative feelings about their PIM practices, per se. People are shown in general to experience anxiety and dissatisfaction with respect to their personal information archives including both concerns of possible loss of the information and also express concerns about their ability and effectiveness in managing and organizing their information. [ 89 ] [ 90 ] Traditional, personal health information resides in various  information systems  in healthcare institutions (e.g., clinics, hospitals, insurance providers), often in the form of  medical records . People often have difficulty managing or even navigating a variety of paper or  electronic medical records  across multiple health services in different specializations and institutions. [ 91 ]  Also referred to as  personal health records , this type of personal health information usually requires people (i.e., patients) to engage in additional PIM finding activities to locate and gain access to health information and then to generate a comprehensible summary for their own use. With the rise of consumer-facing health products including  activity trackers  and health-related  mobile apps , people are able to access new types of personal health data (e.g., physical activity, heart rate) outside healthcare institutions. PIM behavior also changes. Much of the effort to keep information is automated. But people may experience difficulties making sense of a using the information later, e.g., to plan future physical activities based on activity tracker data. People are also frequently engaged in other meta-level activities, such as maintaining and organizing (e.g., syncing data across different health-related mobile apps). [ 92 ] The purpose of PIM study is both descriptive and prescriptive. PIM research seeks to understand what people do now and the problems they encounter i.e., in the management of information and the use of information tools. This understanding is useful on its own but should also have application to understand what might be done in techniques, training and, especially, tool design to improve a person's practice of PIM. The nature of PIM makes its study challenging. [ 93 ]  The techniques and preferred methods of a person's PIM practice can vary considerably with information form (e.g., files vs. emails) and over time. [ 71 ] [ 49 ] [ 94 ]  The  operating system  and the default  file manager  are also shown to impact PIM practices especially in the management of files. [ 32 ] [ 95 ]  A person's practice is also observed to vary in significant ways with gender, age and current life circumstances. [ 96 ] [ 97 ] [ 98 ] [ 99 ]  Certainly, differences among people on different sides of the so-called \" digital divide \" will have profound impact on PIM practices. And, as noted in section \" Personality, mood, and emotion \", personality traits and even a person's current mood can impact PIM behavior. For research results to generalize, or else to be properly qualified, PIM research, at least in aggregate, should include the study of people, with a diversity of backgrounds and needs, over time as they work in many different situations, with different forms of information and different tools of information management. At the same time, PIM research, at least in initial exploratory phases, must often be done in situ (e.g., in a person's workplace or office or at least where people have access to their laptops, smartphones and other devices of information management) so that people can be observed as they manage information that is \"personal\" to them (see section \" The senses in which information is personal \").  Exploratory methods are demanding in the time of both observer and participant and can also be intrusive for the participants. Consequently, the number and nature of participants is likely to be limited i.e., participants may often be people \"close at hand\" to the observer as family, friends, colleagues or other members of the observer's community. For example, the  guided tour , in which the participant is asked to give an interviewer a \"tour\" of the participant's various information collections (e.g., files, emails, Web bookmarks, digital photographs, paper documents, etc.), has proven a very useful, but expensive method of study with results bound by caveats reflecting the typically small number and narrow sampling of participants. The guided tour method is one of several methods that are excellent for exploratory work but expensive and impractical to do with a larger, more diverse sampling of people. Other exploratory methods include the use of  think aloud protocols  collected, for example, as a participant completes a keeping or finding task, [ 56 ]  and the  experience sampling method  wherein participants report on their PIM actions and experiences over time possibly as prompted (e.g., by a beep or a text on a smartphone). A challenge is to combine, within or across studies, time-consuming (and often demographically biased) methods of exploratory observation with other methods that have broader, more economical reach. The exploratory methods bring out interesting patterns; the follow-on methods add in numbers and diversity of participants. Among these methods are: Another method using the  Delphi technique  for achieving consensus has been used to leverage the expertise and experience of PIM researchers as means of extending, indirectly, the number and diversity of PIM practices represented. [ 102 ] The purview of PIM tool design applies to virtually any tool people use to work with their information including \" sticky notes \" and  hanging folders  for paper-based information to a wide range of computer-based applications for the management of digital information, ranging from applications people use every day such as  Web browsers ,  email applications  and  texting applications  to personal information managers. With respect to methods for the evaluation of alternatives in PIM tools design, PIM researchers again face an \"in situ\" challenge. How to evaluate an alternative, as nearly as possible, in the working context of a person's PSI? One \"let it lie\" approach [ 103 ]  would provide for  interfaces  between the tool under evaluation and a participant's PSI so that the tool can work with a participant's other tools and the participant's personal information (as opposed to working in a separate environment with \"test\" data). Dropbox and other file hosting services exemplify this approach: Users can continue to work with their files and folders locally on their computers through the file manager even as an installed applet works to seamlessly synchronize the users files and folders with a Web store for the added benefits of a backup and options to synchronize this information with other devices and share this information with other users. As what is better described as a methodology of tool design rather than a method, Bergman reports good success in the application of a  user-subjective approach . The user-subjective approach advances three design principles. In brief, the design should allow the following: 1) all project-related items no matter their form (or format) are to be organized together (the subjective project classification principle); 2) the importance of information (to the user) should determine its visual salience and accessibility (the subjective importance principle); and 3) information should be retrieved and used by the user in the same context as it was previously used in (the subjective context principle). The approach may suggest design principles that serve not only in evaluating and improving existing systems but also in creating new implementations. For example, according to the demotion principle, information items of lower subjective importance should be demoted (i.e., by making them less visible) so as not to distract the user but be kept within their original context just in case they are needed. The principle has been applied in the creation of several interesting prototypes. [ 104 ] [ 81 ] Finally, a simple \"checklist\" methodology of tool design\", [ 3 ]  follows from an assessment of a proposed tool design with respect to each of the six senses in which information can be personal (see section \" The senses in which information is personal \") and each of the six activities of PIM (finding, keeping and the four meta-level activities, see section \" Activities of PIM \"). A tool that is good with respect to one kind of personal information or one PIM activity, may be bad with respect to another. For example, a new smartphone app that promises to deliver information potentially \"relevant to me\" (the \"6th sense\" in which information is personal) may do so only at the cost of a distracting increase in the information \"directed to me\" and by keeping too much personal information \"about me\" in a place not under the person's control. PIM is a practical meeting ground for many disciplines including  cognitive psychology ,  cognitive science ,  human-computer interaction  (HCI),  human information interaction  (HII),  library and information science  (LIS),  artificial intelligence  (AI), information retrieval, information behavior, organizational  information management , and  information science . Cognitive psychology, as the study of how people learn and remember, problem solve, and make decisions, necessarily also includes the study of how people make smart use of available information. The related field of cognitive science, in its efforts to apply these questions more broadly to the study and simulation of intelligent behavior, is also related to PIM.  (Cognitive science, in turn, has significant overlap with the field of artificial intelligence). There is great potential for a mutually beneficial interplay between cognitive science and PIM. Sub-areas of cognitive science of clear relevance to PIM include problem solving and  decision making . For example, folders created to hold information for a big project such as \"plan my wedding\" may sometimes resemble a  problem-decomposition . [ 105 ]  To take another example, the  signal detection task [ 106 ]  has long been used to frame and explain human behavior and has recently been used as a basis for analyzing our choices concerning what information to keep and how – a key activity of PIM. [ 57 ]  Similarly, there is interplay between the psychological study of  categorization  and  concept formation  and the PIM study of how people use tags and folders to describe and organize their information. Now large portions of a document may be the product of  \"copy-and-paste\" operations  (from our previous writings) rather than a product of original writing. Certainly, management of text pieces pasted for re-use is a PIM activity, and this raises several interesting questions. How do we go about deciding when to re-use and when to write from scratch? We may sometimes spend more time chasing down a paragraph we have previously written than it would have taken to simply write a new paragraph expressing the same thoughts. Beyond this, we can wonder at what point a reliance on an increasing (and increasingly available) supply of previously written material begins to impact our creativity. As people do PIM they work in an external environment that includes other people, available technology, and, often, an organizational setting. This means that  situated cognition ,  distributed cognition , and  social cognition  all relate to the study of PIM. The study of PIM is also related to the field of human–computer interaction (HCI). Some of the more influential papers on PIM over the years have been published in HCI journals and conference proceedings. However, the \"I\" in PIM is for information – in various forms, paper-based and digital (e.g., books, digital documents, emails and,  even, the letter magnets on a refrigerator in the kitchen). The \"I\" in HCI stands for \"interaction\" as this relates to the \"C\" – computers. (An argument has been advanced that HCI should be focused more on information rather than computers. [ 107 ] ) Group information management  (GIM, usually pronounced with a soft \"G\") has been written about elsewhere in the context of PIM. [ 108 ] [ 109 ]  The study of GIM, in turn, has clear relevance to the study of  computer-supported cooperative work  (CSCW). GIM is to CSCW as PIM is to HCI. Just as concerns of PIM substantially overlap with but are not fully subsumed by concerns of HCI (nor vice versa), concerns of GIM overlap with but are not subsumed by concerns of CSCW. Information in support of GIM activities can be in non-digital forms such as paper calendars and bulletin boards that do not involve computers. Group and social considerations frequently enter into a person's PIM strategy. [ 110 ]  For example, one member of a household may agree to manage medical information for everyone in the household (e.g., shot records) while another member of the household manages financial information for the household. But the collaborative organization and sharing of information is often difficult because, for example, the people working together in a group may have many different perspectives on how best to organize information. [ 111 ] [ 112 ] In larger organizational settings, the GIM goals of the organization may conflict with the PIM goals of individuals working within the organization, where the goals of different individuals may also conflict. [ 113 ]  Individuals may, for example, keep copies of secure documents on their private laptops for the sake of convenience even though doing so violates group (organizational) security. [ 114 ]   Given drawbacks—real or perceived—in the use of web services that support a shared use of folders, [ 115 ] [ 116 ]   people working in a group may opt to share information instead through the use of e-mail attachments. [ 117 ] Concerns of data management relate to PIM especially with respect to the safe, secure, long-term preservation of personal information in digital form. The study of information management and knowledge management in organizations also relates to the study of PIM and issues seen first at an organizational level often migrate to the PIM domain. [ 118 ] Concerns of knowledge management on a personal (vs. organizational) level have given rise to arguments for a field of  personal knowledge management  (PKM). However, knowledge is not a \"thing\" to be managed directly but rather indirectly e.g., through items of information such as Web pages, emails and paper documents. PKM is best regarded as a useful subset of  PIM [ 118 ]  with special focus on important issues that might otherwise be overlooked such as self-directed efforts of knowledge elicitation (\"What do I know? What have I learned?\") and knowledge instillation (\"how better to learn what it is I want to know?\") Both  time management  and  task management  on a personal level make heavy use of information tools and external forms of information such as to-do lists, calendars, timelines, and email exchange. These are another form of information to be managed. Over the years, email, in particular, has been used in an ad hoc manner in support of task management. [ 119 ] [ 120 ] Much of the useful information a person receives comes, often unprompted, through a person's network of family, friends and colleagues. People reciprocate and much of the information a person sends to others reflects an attempt to build relationships and influence the behavior of others. As such,  personal network management   (PNM) is a crucial aspect of PIM and can be understood as the practice of managing the links and connections to other people for social and professional benefits."
  },
  {
    "id": 179,
    "title": "Pearl growing",
    "content": "Pearl growing  is a  metaphor  taken from the process of small bits of sand growing to make a beautiful pearl, which is used in  information literacy . This is also called \"snowballing\", [ 1 ]  alluding to the process of how a snowball can grow into a big snow-man by accumulating snow. In this context this refers to the process of using one information item (like a  subject term  or  citation ) to find content that provides more information items. This search strategy is most successfully employed at the beginning of the research process as the searcher uncovers new  pearls  about his or her topic. Citation pearl growing is the act of using one relevant source, or  citation , to find more relevant sources on a topic. The searcher usually has a document that matches a topic or information need. From this document, the searcher is able to find other keywords, descriptors and themes to use in a subsequent search. [ 2 ]  Citation Pearl Growing is a popular search and retrieval method used by  librarians . [ 3 ] Subject pearl growing is a strategy used in an  electronic database  that has  subject  or  keyword  descriptors. By clicking on one  subject , the searcher is able to find other related  subjects  and subdivisions that may or may not be useful to the search. Searchers use the pearl growing technique when surfing the  Internet . Using the theory that websites that link to each other are similar, a searcher can move from site to site, collecting information. Ramer (2005) suggests pearl growing by using the  pearl  as a search term in  search engines  or even in the  URL . In systematic literature reviews, pearl growing is a technique used to ensure all relevant articles are included. Pearl growing involves identifying a primary article that meets the inclusion criteria for the review. From this primary article, the researcher works backwards to find all the articles cited in the bibliography and checks them for eligibility for inclusion in the review. The researcher then works forwards to search for any articles that have cited the primary article. It is estimated that up to 51% of references in a systematic review are identified by pearl growing. [ 4 ]  There is evidence that using pearl growing for systematic reviews is a more comprehensive approach and more likely to identify all relevant articles compared to online database searches. [ 5 ] Pearl growing, when applied to scientific literature, may also be referred to as citation mining or snowballing."
  },
  {
    "id": 180,
    "title": "Query understanding",
    "content": "Query understanding  is the process of inferring the  intent  of a  search engine  user by extracting semantic meaning from the searcher’s keywords. [ 1 ]  Query understanding methods generally take place before the search engine  retrieves  and  ranks  results. It is related to  natural language processing  but specifically focused on the understanding of search queries. Many languages  inflect  words to reflect their role in the utterance they appear in. The variation between various forms of a word is likely to be of little importance for the relatively coarse-grained model of meaning involved in a retrieval system, and for this reason the task of conflating the various forms of a word is a potentially useful technique to increase recall of a retrieval system. [ 2 ] Stemming  algorithms, also known as stemmers, typically use a collection of simple rules to remove  suffixes  intended to model the language’s inflection rules. [ 3 ] For some languages, there are simple  lemmatisation  methods to reduce a word in query to its  lemma  or  root  form or its  stem ; for others, this operation involves non-trivial string processing and may require recognizing the word's  part of speech  or referencing a   lexical database . The effectiveness of stemming and lemmatization varies across languages.\n [ 4 ] \n [ 5 ] Query segmentation is a key component of query understanding, aiming to divide a query into meaningful segments. Traditional approaches, such as the  bag-of-words model , treat individual words as independent units, which can limit interpretative accuracy. For languages like Chinese, where words are not separated by spaces, segmentation is essential, as individual characters often lack standalone meaning. Even in English, the BOW model may not capture the full meaning, as certain phrases—such as \"New York\"—carry significance as a whole rather than as isolated terms. By identifying phrases or entities within queries, query segmentation enhances interpretation, enabling search engines to apply proximity and ordering constraints, ultimately improving search accuracy and user satisfaction. [ 6 ] Entity recognition is the process of locating and classifying entities within a text string.  Named-entity recognition  specifically focuses on  named entities , such as names of people, places, and organizations. In addition, entity recognition includes identifying concepts in queries that may be represented by multi-word phrases. Entity recognition systems typically use grammar-based linguistic techniques or statistical  machine learning  models. [ 7 ] Query rewriting is the process of automatically reformulating a search query to more accurately capture its intent.  Query expansion  adds additional query terms, such as synonyms, in order to retrieve more documents and thereby increase recall. Query relaxation removes query terms to reduce the requirements for a document to match the query, thereby also increasing  recall . Other forms of query rewriting, such as automatically converting consecutive query terms into  phrases  and restricting query terms to specific  fields , aim to increase  precision . Automatic  spelling correction  is a critical feature of modern search engines, designed to address common spelling errors in user queries. Such errors are especially frequent as users often search for unfamiliar topics. By correcting misspelled queries, search engines enhance their understanding of user intent, thereby improving the relevance and quality of search results and overall user experience. [ 8 ]"
  },
  {
    "id": 181,
    "title": "Relevance feedback",
    "content": "Relevance feedback  is a feature of some  information retrieval  systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user  feedback , and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or \"pseudo\" feedback. Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as  relevance  judgments. Users may indicate relevance explicitly using a  binary  or  graded  relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as \"not relevant\", \"somewhat relevant\", \"relevant\", or \"very relevant\"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the  SearchWiki  feature implemented by  Google  on their search website. The relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known  Rocchio algorithm . A  performance metric  which became popular around 2005 to measure the usefulness of a ranking  algorithm  based on the explicit relevance feedback is  normalized discounted cumulative gain . Other measures include  precision  at  k  and  mean average precision . Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions. [ 1 ]  There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response. [ 2 ] [ 3 ] The key differences of implicit relevance feedback from that of explicit include: [ 4 ] An example of this is  dwell time , which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results. Pseudo relevance feedback, also known as blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top \"k\" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is: Some experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments. This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis. [ 5 ]  Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task  [ citation needed ] . But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents. [ 6 ]  Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required. Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the  Rocchio algorithm ."
  },
  {
    "id": 182,
    "title": "Nearest centroid classifier",
    "content": "In  machine learning , a  nearest centroid classifier  or  nearest prototype classifier  is a  classification model  that assigns to observations the label of the class of training samples whose  mean  ( centroid ) is closest to the observation. When applied to  text classification  using  word vectors  containing  tf*idf  weights to represent documents, the nearest centroid classifier is known as the  Rocchio classifier  because of its similarity to the  Rocchio algorithm  for  relevance feedback . [ 1 ] An extended version of the nearest centroid classifier has found applications in the medical domain, specifically classification of  tumors . [ 2 ] Given labeled training samples  \n \n \n \n \n { \n ( \n \n \n \n \n x \n → \n \n \n \n \n 1 \n \n \n , \n \n y \n \n 1 \n \n \n ) \n , \n … \n , \n ( \n \n \n \n \n x \n → \n \n \n \n \n n \n \n \n , \n \n y \n \n n \n \n \n ) \n } \n \n \n \n {\\displaystyle \\textstyle \\{({\\vec {x}}_{1},y_{1}),\\dots ,({\\vec {x}}_{n},y_{n})\\}} \n \n  with class labels  \n \n \n \n \n y \n \n i \n \n \n ∈ \n \n Y \n \n \n \n {\\displaystyle y_{i}\\in \\mathbf {Y} } \n \n , compute the per-class centroids  \n \n \n \n \n \n \n \n \n μ \n → \n \n \n \n \n ℓ \n \n \n = \n \n \n 1 \n \n \n | \n \n \n C \n \n ℓ \n \n \n \n | \n \n \n \n \n \n \n ∑ \n \n i \n ∈ \n \n C \n \n ℓ \n \n \n \n \n \n \n \n \n \n x \n → \n \n \n \n \n i \n \n \n \n \n \n {\\displaystyle \\textstyle {\\vec {\\mu }}_{\\ell }={\\frac {1}{|C_{\\ell }|}}{\\underset {i\\in C_{\\ell }}{\\sum }}{\\vec {x}}_{i}} \n \n  where  \n \n \n \n \n C \n \n ℓ \n \n \n \n \n {\\displaystyle C_{\\ell }} \n \n  is the set of indices of samples belonging to class  \n \n \n \n ℓ \n ∈ \n \n Y \n \n \n \n {\\displaystyle \\ell \\in \\mathbf {Y} } \n \n . The class assigned to an observation  \n \n \n \n \n \n \n x \n → \n \n \n \n \n \n {\\displaystyle {\\vec {x}}} \n \n  is  \n \n \n \n \n \n \n y \n ^ \n \n \n \n = \n \n \n arg \n ⁡ \n min \n \n \n ℓ \n ∈ \n \n Y \n \n \n \n ‖ \n \n \n \n \n μ \n → \n \n \n \n \n ℓ \n \n \n − \n \n \n \n x \n → \n \n \n \n ‖ \n \n \n {\\displaystyle {\\hat {y}}={\\arg \\min }_{\\ell \\in \\mathbf {Y} }\\|{\\vec {\\mu }}_{\\ell }-{\\vec {x}}\\|} \n \n ."
  },
  {
    "id": 183,
    "title": "Search engine indexing",
    "content": "Search engine indexing  is the collecting,  parsing , and storing of data to facilitate fast and accurate  information retrieval . Index design incorporates interdisciplinary concepts from  linguistics ,  cognitive psychology , mathematics,  informatics , and  computer science . An alternate name for the process, in the context of  search engines  designed to find  web pages  on the Internet, is  web indexing . Popular search engines focus on the  full-text  indexing of online,  natural language  documents. [ 1 ]   Media types  such as pictures, video, [ 2 ]  audio, [ 3 ]  and graphics [ 4 ]  are also searchable. Meta search engines  reuse the indices of other services and do not store a local index whereas cache-based search engines permanently store the index along with the  corpus . Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while  agent -based search engines index in  real time . The purpose of storing an index is to optimize speed and performance in finding  relevant  documents for a search query. Without an index, the search engine would  scan  every document in the  corpus , which would require considerable time and computing power. For example, while an index of 10,000 documents can be queried within milliseconds, a sequential scan of every word in 10,000 large documents could take hours. The additional  computer storage  required to store the index, as well as the considerable increase in the time required for an update to take place, are traded off for the time saved during information retrieval. Major factors in designing a search engine's architecture include: Search engine architectures vary in the way indexing is performed and in methods of index storage to meet the various design factors. A major challenge in the design of search engines is the management of serial computing processes. There are many opportunities for  race conditions  and coherent faults. For example, a new document is added to the corpus and the index must be updated, but the index simultaneously needs to continue responding to search queries. This is a collision between two competing tasks. Consider that authors are producers of information, and a  web crawler  is the consumer of this information, grabbing the text and storing it in a cache (or  corpus ). The forward index is the consumer of the information produced by the corpus, and the inverted index is the consumer of information produced by the forward index. This is commonly referred to as a  producer-consumer model . The indexer is the producer of searchable information and users are the consumers that need to search.  The challenge is magnified when working with distributed storage and distributed processing. In an effort to scale with larger amounts of indexed information, the search engine's architecture may involve  distributed computing , where the search engine consists of several machines operating in unison. This increases the possibilities for incoherency and makes it more difficult to maintain a fully synchronized, distributed, parallel architecture. [ 14 ] Many search engines incorporate an  inverted index  when evaluating a  search query  to quickly locate documents containing the words in a query and then rank these documents by relevance. Because the inverted index stores a list of the documents containing each word, the search engine can use direct  access  to find the documents associated with each word in the query in order to retrieve the matching documents quickly. The following is a simplified illustration of an inverted index: This index can only determine whether a word exists within a particular document, since it stores no information regarding the frequency and position of the word; it is therefore considered to be a  Boolean  index. Such an index determines which documents match a query but does not rank matched documents. In some designs the index includes additional information such as the frequency of each word in each document or the positions of a word in each document. [ 15 ]  Position information enables the search algorithm to identify word proximity to support searching for phrases; frequency can be used to help in ranking the relevance of documents to the query. Such topics are the central research focus of  information retrieval . The inverted index is a  sparse matrix , since not all words are present in each document. To reduce  computer storage  memory requirements, it is stored differently from a two dimensional  array . The index is similar to the  term document matrices  employed by  latent semantic analysis . The inverted index can be considered a form of a hash table. In some cases the index is a form of a  binary tree , which requires additional storage but may reduce the lookup time. In larger indices the architecture is typically a  distributed hash table . [ 16 ] For phrase searching, a specialized form of an inverted index called a positional index is used. A positional index not only stores the ID of the document containing the token but also the exact position(s) of the token within the document in the  postings list . The occurrences of the phrase specified in the query are retrieved by navigating these postings list and identifying the indexes at which the desired terms occur in the expected order (the same as the order in the phrase). So if we are searching for occurrence of the phrase \"First Witch\", we would: The postings lists can be navigated using a binary search in order to minimize the time complexity of this procedure. [ 17 ] The inverted index is filled via a merge or rebuild. A rebuild is similar to a merge but first deletes the contents of the inverted index. The architecture may be designed to support incremental indexing, [ 18 ]  where a merge identifies the document or documents to be added or updated and then parses each document into words. For technical accuracy, a merge conflates newly indexed documents, typically residing in virtual memory, with the index cache residing on one or more computer hard drives. After parsing, the indexer adds the referenced document to the document list for the appropriate words. In a larger search engine, the process of finding each word in the inverted index (in order to report that it occurred within a document) may be too time consuming, and so this process is commonly split up into two parts, the development of a forward index and a process which sorts the contents of the forward index into the inverted index. The inverted index is so named because it is an inversion of the forward index. The forward index stores a list of words for each document. The following is a simplified form of the forward index: The rationale behind developing a forward index is that as documents are parsed, it is better to intermediately store the words per document.  The delineation enables asynchronous system processing, which partially circumvents the inverted index update  bottleneck . [ 19 ]  The forward index is  sorted  to transform it to an inverted index. The forward index is essentially a list of pairs consisting of a document and a word, collated by the document. Converting the forward index to an inverted index is only a matter of sorting the pairs by the words. In this regard, the inverted index is a word-sorted forward index. Generating or maintaining a large-scale search engine index represents a significant storage and processing challenge. Many search engines utilize a form of  compression  to reduce the size of the indices on  disk . [ 20 ]  Consider the following scenario for a full text, Internet search engine. Given this scenario, an uncompressed index (assuming a non- conflated , simple, index) for 2 billion web pages would need to store 500 billion word entries. At 1 byte per character, or 5 bytes per word, this would require 2500 gigabytes of storage space alone. [ citation needed ]  This space requirement may be even larger for a fault-tolerant distributed storage architecture. Depending on the compression technique chosen, the index can be reduced to a fraction of this size. The tradeoff is the time and processing power required to perform compression and decompression. [ citation needed ] Notably, large scale search engine designs incorporate the cost of storage as well as the costs of electricity to power the storage. Thus compression is a measure of cost. [ citation needed ] Document parsing breaks apart the components (words) of a document or other form of media for insertion into the forward and inverted indices. The words found are called  tokens , and so, in the context of search engine indexing and  natural language processing , parsing is more commonly referred to as  tokenization . It is also sometimes called  word boundary disambiguation ,  tagging ,  text segmentation ,  content analysis , text analysis,  text mining ,  concordance  generation,  speech segmentation ,  lexing , or  lexical analysis . The terms 'indexing', 'parsing', and 'tokenization' are used interchangeably in corporate slang. Natural language processing is the subject of continuous research and technological improvement. Tokenization presents many challenges in extracting the necessary information from documents for indexing to support quality searching. Tokenization for indexing involves multiple technologies, the implementation of which are commonly kept as corporate secrets. [ citation needed ] Unlike  literate  humans, computers do not understand the structure of a natural language document and cannot automatically recognize words and sentences. To a computer, a document is only a sequence of bytes. Computers do not 'know' that a space character separates words in a document. Instead, humans must program the computer to identify what constitutes an individual or distinct word referred to as a token. Such a program is commonly called a  tokenizer  or  parser  or  lexer . Many search engines, as well as other natural language processing software, incorporate  specialized programs  for parsing, such as  YACC  or  Lex . During tokenization, the parser identifies sequences of characters that represent words and other elements, such as punctuation, which are represented by numeric codes, some of which are non-printing control characters. The parser can also identify  entities  such as  email  addresses, phone numbers, and  URLs . When identifying each token, several characteristics may be stored, such as the token's case (upper, lower, mixed, proper), language or encoding, lexical category (part of speech, like 'noun' or 'verb'), position, sentence number, sentence position, length, and line number. If the search engine supports multiple languages, a common initial step during tokenization is to identify each document's language; many of the subsequent steps are language dependent (such as  stemming  and  part of speech  tagging).  Language recognition  is the process by which a computer program attempts to automatically identify, or categorize, the  language  of a document. Other names for language recognition include language classification, language analysis, language identification, and language tagging. Automated language recognition is the subject of ongoing research in  natural language processing . Finding which language the words belongs to may involve the use of a  language recognition chart . If the search engine supports multiple  document formats , documents must be prepared for tokenization. The challenge is that many document formats contain formatting information in addition to textual content.  For example,  HTML  documents contain HTML tags, which specify formatting information such as new line starts, bold emphasis, and  font  size or  style .  If the search engine were to ignore the difference between content and 'markup', extraneous information would be included in the index, leading to poor search results. Format analysis is the identification and handling of the formatting content embedded within documents which controls the way the document is rendered on a computer screen or interpreted by a software program. Format analysis is also referred to as structure analysis, format parsing, tag stripping, format stripping, text normalization, text cleaning and text preparation. The challenge of format analysis is further complicated by the intricacies of various file formats. Certain file formats are proprietary with very little information disclosed, while others are well documented. Common, well-documented file formats that many search engines support include: Options for dealing with various formats include using a publicly available commercial parsing tool that is offered by the organization which developed, maintains, or owns the format, and writing a custom  parser . Some search engines support inspection of files that are stored in a  compressed  or encrypted file format.  When working with a compressed format, the indexer first decompresses the document; this step may result in one or more files, each of which must be indexed separately. Commonly supported  compressed file formats  include: Format analysis can involve quality improvement methods to avoid including 'bad information' in the index.  Content can manipulate the formatting information to include additional content. Examples of abusing document formatting for  spamdexing : Some search engines incorporate section recognition, the identification of major parts of a document, prior to tokenization. Not all the documents in a corpus read like a well-written book, divided into organized chapters and pages.  Many documents on the  web , such as newsletters and corporate reports, contain erroneous content and side-sections that do not contain primary material (that which the document is about). For example, articles on the Wikipedia website display a side menu with links to other web pages. Some file formats, like HTML or PDF, allow for content to be displayed in columns. Even though the content is displayed, or rendered, in different areas of the view, the raw markup content may store this information sequentially. Words that appear sequentially in the raw source content are indexed sequentially, even though these sentences and paragraphs are rendered in different parts of the computer screen. If search engines index this content as if it were normal content, the quality of the index and search quality may be degraded due to the mixed content and improper word proximity. Two primary problems are noted: Section analysis may require the search engine to implement the rendering logic of each document, essentially an abstract representation of the actual document, and then index the representation instead. For example, some content on the Internet is rendered via JavaScript. If the search engine does not render the page and evaluate the JavaScript within the page, it would not 'see' this content in the same way and would index the document incorrectly. Given that some search engines do not bother with rendering issues, many web page designers avoid displaying content via JavaScript or use the  Noscript  tag to ensure that the web page is indexed properly.  At the same time, this fact can also be  exploited  to cause the search engine indexer to 'see' different content than the viewer. Indexing often has to recognize the  HTML  tags to organize priority. Indexing low priority to high margin to labels like  strong  and  link  to optimize the order of priority if those labels are at the beginning of the text could not prove to be relevant. Some indexers like  Google  and  Bing  ensure that the  search engine  does not take the large texts as relevant source due to strong type system compatibility. [ 23 ] Meta tag indexing plays an important role in organizing and categorizing web content. Specific documents often contain embedded meta information such as author, keywords, description, and language. For HTML pages, the  meta tag  contains keywords which are also included in the index. Earlier Internet  search engine technology  would only index the keywords in the meta tags for the forward index; the full document would not be parsed. At that time full-text indexing was not as well established, nor was  computer hardware  able to support such technology.  The design of the HTML markup language initially included support for meta tags for the very purpose of being properly and easily indexed, without requiring tokenization. [ 24 ] As the Internet grew through the 1990s, many  brick-and-mortar corporations  went 'online' and established corporate websites. The keywords used to describe webpages (many of which were corporate-oriented webpages similar to product brochures) changed from descriptive to marketing-oriented keywords designed to drive sales by placing the webpage high in the search results for specific search queries. The fact that these keywords were subjectively specified was leading to  spamdexing , which drove many search engines to adopt full-text indexing technologies in the 1990s. Search engine designers and companies could only place so many 'marketing keywords' into the content of a webpage before draining it of all interesting and useful information.  Given that conflict of interest with the business goal of designing user-oriented websites which were 'sticky', the  customer lifetime value  equation was changed to incorporate more useful content into the website in hopes of retaining the visitor. In this sense, full-text indexing was more objective and increased the quality of search engine results, as it was one more step away from subjective control of search engine result placement, which in turn furthered research of full-text indexing technologies. In  desktop search , many solutions incorporate meta tags to provide a way for authors to further customize how the search engine will index content from various files that is not evident from the file content. Desktop search is more under the control of the user, while Internet search engines must focus more on the full text index."
  },
  {
    "id": 184,
    "title": "Subject indexing",
    "content": "Subject indexing  is the act of describing or  classifying  a  document  by  index terms , keywords, or other symbols in order to indicate what different documents are  about , to summarize their  contents  or to increase  findability . In other words, it is about identifying and describing the  subject  of documents. Indexes are constructed, separately, on three distinct levels: terms in a document such as a book; objects in a collection such as a library; and documents (such as books and articles) within a field of knowledge. Subject indexing is used in  information retrieval  especially to create  bibliographic indexes  to retrieve documents on a particular subject. Examples of academic indexing services are  Zentralblatt MATH ,  Chemical Abstracts  and  PubMed . The index terms were mostly assigned by experts but author keywords are also common. The process of indexing begins with any analysis of the subject of the document. The indexer must then identify terms which appropriately identify the subject either by extracting words directly from the document or assigning words from a  controlled vocabulary . [ 1 ]  The terms in the index are then presented in a systematic order. Indexers must decide how many terms to include and how specific the terms should be. Together this gives a depth of indexing. The first step in indexing is to decide on the subject matter of the document. In manual indexing, the indexer would consider the subject matter in terms of answer to a set of questions such as \"Does the document deal with a specific product, condition or phenomenon?\". [ 2 ]  As the analysis is influenced by the knowledge and experience of the indexer, it follows that two indexers may analyze the content differently and so come up with different index terms. This will impact on the success of retrieval. Automatic indexing  follows set processes of analyzing frequencies of word patterns and comparing results to other documents in order to assign to subject categories. This requires no understanding of the material being indexed. This leads to more uniform indexing but at the expense of the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analyzing the full text in depth is costly and time-consuming. [ 3 ]  An automated system takes away the time limit and allows the entire document to be analyzed, but also has the option to be directed to particular parts of the document. The second stage of indexing involves the translation of the subject analysis into a set of  index terms . This can involve extracting from the document or assigning from a  controlled vocabulary . With the ability to conduct a  full text search  widely available, many people have come to rely on their own expertise in conducting information searches and full text search has become very popular. Subject indexing and its experts, professional indexers,  catalogers , and  librarians , remains crucial to information organization and retrieval. These experts understand controlled vocabularies and are able to find information that cannot be located by full text search. The cost of expert analysis to create subject indexing is not easily compared to the cost of hardware, software and labor to manufacture a comparable set of full-text, fully searchable materials. With new web applications that allow every user to annotate documents,  social tagging  has gained popularity especially in the Web. [ 4 ] One application of indexing, the  book index , remains relatively unchanged despite the  information revolution . Extraction indexing involves taking words directly from the document. It uses  natural language  and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A stop-list containing common words (such as \"the\", \"and\") would be referred to and such  stop words  would be excluded as index terms. Automated extraction indexing may lead to loss of meaning of terms by indexing single words as opposed to phrases. Although it is possible to extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases. Automated extraction indexing also has the problem that, even with use of a stop-list to remove common words, some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore, use of this term would likely return most or all the documents in the database. Post-coordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore, a term that occurs more often in a document than might be expected based on the rest of the database could then be used as an index term, and terms that occur equally frequently throughout will be excluded. Another problem with automated extraction is that it does not recognize when a concept is discussed but is not identified in the text by an indexable keyword. [ 5 ] Since this process is based on simple string matching and involves no intellectual analysis, the resulting product is more appropriately known as a  concordance  than an index. An alternative is assignment indexing where index terms are taken from a controlled vocabulary. This has the advantage of controlling for  synonyms  as the preferred term is indexed and synonyms or related terms direct the user to the preferred term. This means the user can find articles regardless of the specific term used by the author and saves the user from having to know and check all possible synonyms. [ 6 ]  It also removes any confusion caused by  homographs  by inclusion of a qualifying term. A third advantage is that it allows the linking of related terms whether they are linked by hierarchy or association, e.g. an index entry for an oral medication may list other oral medications as related terms on the same level of the hierarchy but would also link to broader terms such as treatment. Assignment indexing is used in manual indexing to improve inter-indexer consistency as different indexers will have a controlled set of terms to choose from. Controlled vocabularies do not completely remove inconsistencies as two indexers may still interpret the subject differently. [ 2 ] The final phase of indexing is to present the entries in a systematic order. This may involve linking entries. In a pre-coordinated index the indexer determines the order in which terms are linked in an entry by considering how a user may formulate their search. In a post-coordinated index, the entries are presented singly and the user can link the entries through searches, most commonly carried out by computer software. Post-coordination results in a loss of precision in comparison to pre-coordination. [ 7 ] Indexers must make decisions about what entries should be included and how many entries an index should incorporate. The depth of indexing describes the thoroughness of the indexing process with reference to exhaustivity and specificity. [ 8 ] An exhaustive index is one which lists all possible index terms. Greater exhaustivity gives a higher  recall , or more likelihood of all the relevant articles being retrieved, however, this occurs at the expense of  precision . This means that the user may retrieve a larger number of irrelevant documents or documents which only deal with the subject in little depth. In a manual system a greater level of exhaustivity brings with it a greater cost as more man-hours are required. The additional time taken in an automated system would be much less significant. At the other end of the scale, in a selective index only the most important aspects are covered. [ 9 ]  Recall is reduced in a selective index as if an indexer does not include enough terms, a highly relevant article may be overlooked. Therefore, indexers should strive for a balance and consider what the document may be used. They may also have to consider the implications of time and expense. The specificity describes how closely the index terms match the topics they represent [ 10 ]  An index is said to be specific if the indexer uses parallel descriptors to the concept of the document and reflects the concepts precisely. [ 11 ]  Specificity tends to increase with exhaustivity as the more terms you include, the narrower those terms will be. Hjørland  (2011) [ 12 ]  found that theories of indexing are at the deepest level connected to different theories of knowledge: The core of indexing is, as stated by Rowley and Farrow [ 16 ]  to evaluate a paper's contribution to knowledge and index it accordingly. Or, in the words of Hjørland (1992, [ 17 ]  1997) to index its informative potentials.\n\"In order to achieve good consistent indexing, the indexer must have a thorough appreciation of the structure of the subject and the nature of the contribution that the document is making to the advancement of knowledge\" (Rowley & Farrow, 2000, [ 16 ]  p. 99)."
  },
  {
    "id": 185,
    "title": "Temporal information retrieval",
    "content": "Temporal information retrieval  ( T-IR ) is an emerging area of research related to the field of  information retrieval  (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs. According to  information theory  science (Metzger, 2007), [ 1 ]  timeliness or currency is one of the key five aspects that determine a document's credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company's earnings or information on already-happened or invalid predictions. T-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for  query understanding , query disambiguation, query classification, result diversification and so on. This article contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail."
  },
  {
    "id": 186,
    "title": "tf–idf",
    "content": "In  information retrieval ,  tf–idf  (also  TF*IDF ,  TFIDF ,  TF–IDF , or  Tf–idf ), short for  term frequency–inverse document frequency ,  is a measure of importance of a word to a  document  in a collection or  corpus , adjusted for the fact that some words appear more frequently in general. [ 1 ]  Like the bag-of-words model, it models a document as a  multiset  of words, without  word order . It is a refinement over the simple  bag-of-words model , by allowing the weight of words to depend on the rest of the corpus. It was often used as a  weighting factor  in searches of information retrieval,  text mining , and  user modeling . A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries used tf–idf. [ 2 ]  Variations of the tf–idf weighting scheme were often used by  search engines  as a central tool in scoring and ranking a document's  relevance  given a user  query . One of the simplest  ranking functions  is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model. Karen Spärck Jones  (1972) conceived a statistical interpretation of term-specificity called Inverse Document Frequency (idf), which became a cornerstone of term weighting: [ 3 ] The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs. For example, the df (document frequency) and idf for some words in Shakespeare's 37 plays are as follows: [ 4 ] We see that \" Romeo \", \" Falstaff \", and \"salad\" appears in very few plays, so seeing these words, one could get a good idea as to which play it might be. In contrast, \"good\" and \"sweet\" appears in every play and are completely uninformative as to which play it is. Term frequency,  tf( t , d ) , is the relative frequency of term  t  within document  d , where  f t , d  is the  raw count  of a term in a document, i.e., the number of times that term  t  occurs in document  d . Note the denominator is simply the total number of terms in document  d  (counting each occurrence of the same term separately). There are various other ways to define term frequency: [ 5 ] : 128 The  inverse document frequency  is a measure of how much information the word provides, i.e., how common or rare it is across all documents. It is the  logarithmically scaled  inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient): with Then tf–idf is calculated as A high weight in tf–idf is reached by a high term  frequency  (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf–idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf–idf closer to 0. Idf was introduced as \"term specificity\" by  Karen Spärck Jones  in a 1972 paper. Although it has worked well as a  heuristic , its theoretical foundations have been troublesome for at least three decades afterward, with many researchers trying to find  information theoretic  justifications for it. [ 7 ] Spärck Jones's own explanation did not propose much theory, aside from a connection to  Zipf's law . [ 7 ]  Attempts have been made to put idf on a  probabilistic  footing, [ 8 ]  by estimating the probability that a given document  d  contains a term  t  as the relative document frequency, so that we can define idf as Namely, the inverse document frequency is the logarithm of \"inverse\" relative document frequency. This probabilistic interpretation in turn takes the same form as that of  self-information . However, applying such information-theoretic notions to problems in information retrieval leads to problems when trying to define the appropriate  event spaces  for the required  probability distributions : not only documents need to be taken into account, but also queries and terms. [ 7 ] Both term frequency and inverse document frequency can be formulated in terms of  information theory ; it helps to understand why their product has a meaning in terms of joint informational content of a document. A characteristic assumption about the distribution  \n \n \n \n p \n ( \n d \n , \n t \n ) \n \n \n {\\displaystyle p(d,t)} \n \n  is that: This assumption and its implications, according to Aizawa: \"represent the heuristic that tf–idf employs.\" [ 9 ] The  conditional entropy  of a \"randomly chosen\" document in the corpus  \n \n \n \n D \n \n \n {\\displaystyle D} \n \n , conditional to the fact it contains a specific term  \n \n \n \n t \n \n \n {\\displaystyle t} \n \n  (and assuming that all documents have equal probability to be chosen) is: In terms of notation,  \n \n \n \n \n \n D \n \n \n \n \n {\\displaystyle {\\cal {D}}} \n \n  and  \n \n \n \n \n \n T \n \n \n \n \n {\\displaystyle {\\cal {T}}} \n \n  are \"random variables\" corresponding to respectively draw a document or a term. The  mutual information  can be expressed as The last step is to expand  \n \n \n \n \n p \n \n t \n \n \n \n \n {\\displaystyle p_{t}} \n \n , the unconditional probability to draw a term, with respect to the (random) choice of a document, to obtain: This expression shows that summing the Tf–idf of all possible terms and documents recovers the mutual information between documents and term taking into account all the specificities of their joint distribution. [ 9 ]  Each Tf–idf hence carries the \"bit of information\" attached to a term x document pair. Suppose that we have term count tables of a corpus consisting of only two documents, as listed on the right. The calculation of tf–idf for the term \"this\" is performed as follows: In its raw frequency form, tf is just the frequency of the \"this\" for each document. In each document, the word \"this\" appears once; but as the document 2 has more words, its relative frequency is smaller. An idf is constant per corpus, and  accounts  for the ratio of documents that include the word \"this\". In this case, we have a corpus of two documents and all of them include the word \"this\". So tf–idf is zero for the word \"this\", which implies that the word is not very informative as it appears in all documents. The word \"example\" is more interesting - it occurs three times, but only in the second document: Finally, (using the  base 10 logarithm ). The idea behind tf–idf also applies to entities other than terms. In 1998, the concept of idf was applied to citations. [ 10 ]  The authors argued that \"if a very uncommon citation is shared by two documents, this should be weighted more highly than a citation made by a large number of documents\". In addition, tf–idf was applied to \"visual words\" with the purpose of conducting object matching in videos, [ 11 ]  and entire sentences. [ 12 ]  However, the concept of tf–idf did not prove to be more effective in all cases than a plain tf scheme (without idf). When tf–idf was applied to citations, researchers could find no improvement over a simple citation-count weight that had no idf component. [ 13 ] A number of term-weighting schemes have derived from tf–idf. One of them is TF–PDF (term frequency * proportional document frequency). [ 14 ]  TF–PDF was introduced in 2001 in the context of identifying emerging topics in the media. The PDF component measures the difference of how often a term occurs in different domains. Another derivate is TF–IDuF. In TF–IDuF, [ 15 ]  idf is not calculated based on the document corpus that is to be searched or recommended. Instead, idf is calculated on users' personal document collections. The authors report that TF–IDuF was equally effective as tf–idf but could also be applied in situations when, e.g., a user modeling system has no access to a global document corpus."
  },
  {
    "id": 187,
    "title": "XML retrieval",
    "content": "XML retrieval , or  XML information retrieval , is the content-based retrieval of documents structured with  XML  (eXtensible Markup Language). As such it is used for computing  relevance  of XML documents. [ 1 ] Most XML retrieval approaches do so based on techniques from the  information retrieval  (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain  structural   hints . So-called \"content and structure\" (CAS) queries enable users to specify what structure the requested content can or must have. Taking advantage of the  self-describing  structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments. Ranking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request. [ 2 ] An overview of two potential approaches is available. [ 3 ] [ 4 ]  The INitiative for the Evaluation of XML-Retrieval ( INEX ) was founded in 2002 and provides a platform for evaluating such  algorithms . [ 2 ]  Three different areas influence XML-Retrieval: [ 5 ] Query languages  such as the  W3C  standard  XQuery [ 6 ]  supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents'  schemas . [ 7 ] Classic  database  systems have adopted the possibility to store  semi-structured data [ 5 ]  and resulted in the development of  XML databases . Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries. Classic information retrieval models such as the  vector space model  provide relevance ranking, but do not include document structure; only flat queries are  supported. Also, they apply a static document concept, so retrieval units usually are entire documents. [ 7 ]  They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document  subtrees  (index terms plus structure) as dimensions of the vector space. [ 8 ] For data-centric XML datasets, the unique and distinct keyword search method, namely, XDMA [ 9 ]  for XML databases is designed and developed based on dual indexing and mutual summation."
  },
  {
    "id": 188,
    "title": "Data mining",
    "content": "Data mining  is the process of extracting and discovering patterns in large  data sets  involving methods at the intersection of  machine learning ,  statistics , and  database systems . [ 1 ]  Data mining is an  interdisciplinary  subfield of  computer science  and  statistics  with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. [ 1 ] [ 2 ] [ 3 ] [ 4 ]  Data mining is the analysis step of the \" knowledge discovery in databases \" process, or KDD. [ 5 ]  Aside from the raw analysis step, it also involves database and  data management  aspects,  data pre-processing ,  model  and  inference  considerations, interestingness metrics,  complexity  considerations, post-processing of discovered structures,  visualization , and  online updating . [ 1 ] The term \"data mining\" is a  misnomer  because the goal is the extraction of  patterns  and knowledge from large amounts of data, not the  extraction ( mining ) of data itself . [ 6 ]  It also is a  buzzword [ 7 ]  and is frequently applied to any form of large-scale data or  information processing  ( collection ,  extraction ,  warehousing , analysis, and statistics) as well as any application of  computer decision support system , including  artificial intelligence  (e.g., machine learning) and  business intelligence . Often the more general terms ( large scale )  data analysis  and  analytics —or, when referring to actual methods,  artificial intelligence  and  machine learning —are more appropriate. The actual data mining task is the semi- automatic  or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records ( cluster analysis ), unusual records ( anomaly detection ), and  dependencies  ( association rule mining ,  sequential pattern mining ). This usually involves using database techniques such as  spatial indices . These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and  predictive analytics . For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a  decision support system . Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps. The difference between  data analysis  and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a  marketing campaign , regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data. [ 8 ] The related terms  data dredging ,  data fishing , and  data snooping  refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations. In the 1960s, statisticians and economists used terms like  data fishing  or  data dredging  to refer to what they considered the bad practice of analyzing data without an  a-priori  hypothesis. The term \"data mining\" was used in a similarly critical way by economist  Michael Lovell  in an article published in the  Review of Economic Studies  in 1983. [ 9 ] [ 10 ]  Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative). The term  data mining  appeared around 1990 in the database community, with generally positive connotations. For a short time in 1980s, the phrase \"database mining\"™, was used, but since it was trademarked by HNC, a  San Diego –based company, to pitch their Database Mining Workstation; [ 11 ]  researchers consequently turned to  data mining . Other terms used include  data archaeology ,  information harvesting ,  information discovery ,  knowledge extraction , etc.  Gregory Piatetsky-Shapiro  coined the term \"knowledge discovery in databases\" for the first workshop on the same topic  (KDD-1989)  and this term became more popular in the  AI  and  machine learning  communities. However, the term data mining became more popular in the business and press communities. [ 12 ]  Currently, the terms  data mining  and  knowledge discovery  are used interchangeably. The manual extraction of patterns from  data  has occurred for centuries. Early methods of identifying patterns in data include  Bayes' theorem  (1700s) and  regression analysis  (1800s). [ 13 ]  The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As  data sets  have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as  neural networks ,  cluster analysis ,  genetic algorithms  (1950s),  decision trees  and  decision rules  (1960s), and  support vector machines  (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns. [ 14 ]  in large data sets. It bridges the gap from  applied statistics  and artificial intelligence (which usually provide the mathematical background) to  database management  by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets. The  knowledge discovery in databases (KDD) process  is commonly defined with the stages: It exists, however, in many variations on this theme, such as the  Cross-industry standard process for data mining  (CRISP-DM) which defines six phases: or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation. Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. [ 15 ] [ 16 ] [ 17 ] [ 18 ] The only other data mining standard named in these polls was  SEMMA . However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, [ 19 ]  and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008. [ 20 ] Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a  data mart  or  data warehouse . Pre-processing is essential to analyze the  multivariate  data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing  noise  and those with  missing data . Data mining involves six common classes of tasks: [ 5 ] Data mining can unintentionally be misused, producing results that appear to be significant but which do not actually predict future behavior and cannot be  reproduced  on a new sample of data, therefore bearing little use. This is sometimes caused by investigating too many hypotheses and not performing proper  statistical hypothesis testing . A simple version of this problem in  machine learning  is known as  overfitting , but the same problem can arise at different phases of the process and thus a train/test split—when applicable at all—may not be sufficient to prevent this from happening. [ 21 ] The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called  overfitting . To overcome this, the evaluation uses a  test set  of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" e-mails would be trained on a  training set  of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had  not  been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as  ROC curves . If the learned patterns do not meet the desired standards, it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge. The premier professional body in the field is the  Association for Computing Machinery 's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining ( SIGKDD ). [ 22 ] [ 23 ]  Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings, [ 24 ]  and since 1999 it has published a biannual  academic journal  titled \"SIGKDD Explorations\". [ 25 ] Computer science conferences on data mining include: Data mining topics are also present in many  data management/database conferences  such as the ICDE Conference,  SIGMOD Conference  and  International Conference on Very Large Data Bases . There have been some efforts to define standards for the data mining process, for example, the 1999 European  Cross Industry Standard Process for Data Mining  (CRISP-DM 1.0) and the 2004  Java Data Mining  standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft. For exchanging the extracted models—in particular for use in  predictive analytics —the key standard is the  Predictive Model Markup Language  (PMML), which is an  XML -based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example)  subspace clustering  have been proposed independently of the DMG. [ 26 ] Data mining is used wherever there is digital data available. Notable  examples of data mining  can be found throughout business, medicine, science, finance, construction, and surveillance. While the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to  user behavior  (ethical and otherwise). [ 27 ] The ways in which data mining can be used can in some cases and contexts raise questions regarding  privacy , legality, and  ethics . [ 28 ]  In particular, data mining government or commercial data sets for  national security  or  law enforcement  purposes, such as in the  Total Information Awareness  Program or in  ADVISE , has raised privacy concerns. [ 29 ] [ 30 ] Data mining requires data preparation which uncovers information or patterns which compromise  confidentiality  and  privacy  obligations. A common way for this to occur is through  data aggregation .  Data aggregation  involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent). [ 31 ]  This is not data mining  per se , but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous. [ 32 ] It is recommended [ according to whom? ]  to be aware of the following  before  data are collected: [ 31 ] Data may also be modified so as to  become  anonymous, so that individuals may not readily be identified. [ 31 ]  However, even \" anonymized \" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL. [ 33 ] The inadvertent revelation of  personally identifiable information  leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,\nemotional, or bodily harm to the indicated individual.  In one instance of  privacy violation , the patrons of Walgreens filed a lawsuit against the company in 2011 for selling\nprescription information to data mining companies who in turn provided the data\nto pharmaceutical companies. [ 34 ] Europe  has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the  U.S.–E.U. Safe Harbor Principles , developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of  Edward Snowden 's  global surveillance disclosure , there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the  National Security Agency , and attempts to reach an agreement with the United States have failed. [ 35 ] In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places. [ 36 ] In the United States, privacy concerns have been addressed by the  US Congress  via the passage of regulatory controls such as the  Health Insurance Portability and Accountability Act  (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in  Biotech Business Week , \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.\" [ 37 ]  This underscores the necessity for data anonymity in data aggregation and mining practices. U.S. information privacy legislation such as HIPAA and the  Family Educational Rights and Privacy Act  (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation. Under  European copyright   database laws , the mining of in-copyright works (such as by  web mining ) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright—but database rights may exist, so data mining becomes subject to  intellectual property  owners' rights that are protected by the  Database Directive . On the recommendation of the  Hargreaves review , this led to the UK government to amend its copyright law in 2014 to allow content mining as a  limitation and exception . [ 38 ]  The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the  Information Society Directive  (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions.\nSince 2020 also Switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art. 24d of the Swiss Copyright Act. This new article entered into force on 1 April 2020. [ 39 ] The  European Commission  facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. [ 40 ]  The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and  open access  publishers to leave the stakeholder dialogue in May 2013. [ 41 ] US copyright law , and in particular its provision for  fair use , upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the  Google Book settlement  the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed—one being text and data mining. [ 42 ] The following applications are available under free/open-source licenses. Public access to application source code is also available. The following applications are available under proprietary licenses. For more information about extracting information out of data (as opposed to  analyzing  data), see:"
  },
  {
    "id": 189,
    "title": "Digital object identifier",
    "content": "A  digital object identifier  ( DOI ) is a  persistent identifier  or  handle  used to uniquely identify various objects, standardized by the  International Organization for Standardization  (ISO). [ 2 ]  DOIs are an implementation of the  Handle System ; [ 3 ] [ 4 ]  they also fit within the URI system ( Uniform Resource Identifier ). They are widely used to identify academic, professional, and government information, such as  journal  articles, research reports, data sets, and official  publications . A DOI aims to resolve to its target, the information object to which the DOI refers. This is achieved by binding the DOI to  metadata  about the object, such as a  URL  where the object is located. Thus, by being actionable and  interoperable , a DOI differs from  ISBNs  or  ISRCs  which are identifiers only. The DOI system uses the  indecs Content Model  to represent  metadata . The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI should provide a more stable link than directly using its URL. But if its URL changes, the publisher must update the metadata for the DOI to maintain the link to the URL. [ 5 ] [ 6 ] [ 7 ]  It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a  dead link , leaving the DOI useless. [ 8 ] The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000. [ 9 ]  Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs. [ 10 ]  The DOI system is implemented through a federation of registration agencies coordinated by the IDF. [ 11 ]  By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations, [ 12 ]  and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations. [ citation needed ]   Fake registries have even appeared. [ 13 ] A DOI is a type of Handle System handle, which takes the form of a  character string  divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the identifier and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal  Unicode  characters are allowed in these strings, which are interpreted in a  case-insensitive  manner. The prefix usually takes the form  10.NNNN , where  NNNN  is a number greater than or equal to  1000 , whose limit depends only on the total number of registrants. [ 14 ] [ 15 ]  The prefix may be further subdivided with periods, like  10.NNNN.N . [ 16 ] For example, in the DOI name  10.1000/182 , the prefix is  10.1000  and the suffix is  182 . The \"10\" part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace, [ A ]  and the characters  1000  in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself.  182  is the suffix, or item ID, identifying a single object (in this case, the latest version of the  DOI Handbook ). DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms,  performances , and abstract works [ 17 ]  such as licenses, parties to a transaction, etc. The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a  data dictionary  based on the  indecs Content Model . The official  DOI Handbook  explicitly states that DOIs should be displayed on screens and in print in the format  doi:10.1000/182 . [ 18 ] Contrary to the  DOI Handbook ,  Crossref , a major DOI registration agency, recommends displaying a URL (for example,  https://doi.org/10.1000/182 ) instead of the officially specified format. [ 19 ] [ 20 ]  This URL is persistent (there is a contract that ensures persistence in the doi.org domain, [ citation needed ] ) so it is a  PURL —providing the location of an  name resolver  which will redirect  HTTP requests  to the correct online location of the linked item. [ 10 ] [ 21 ] The Crossref recommendation is primarily based on the assumption that the DOI is being displayed without being hyperlinked to its appropriate URL—the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their  browser  in order to go to the appropriate page for the document the DOI represents. [ 22 ] Major content of the DOI system currently includes: In the  Organisation for Economic Co-operation and Development 's publication service  OECD iLibrary , each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned. [ 24 ] Other registries include  Crossref  and the  multilingual European DOI Registration Agency (mEDRA) . [ 25 ]  Since 2015,  RFCs  can be referenced as  doi:10.17487/rfc ... . [ 26 ] The IDF designed the DOI system to provide a form of  persistent identification , in which each DOI name permanently and unambiguously identifies the object to which it is associated (although when the publisher of a journal changes, sometimes all the DOIs will be changed, with the old DOIs no longer working). It also associates  metadata  with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the  Handle System  and the  indecs Content Model  with a social infrastructure. The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the  URI  specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on  open architectures , incorporates  trust mechanisms , and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system. [ 27 ]  DOI name-resolution may be used with  OpenURL  to select the most appropriate among multiple locations for a given object, according to the location of the user making the request. [ 28 ]  However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents, that would have been available for no additional fee from alternative locations. [ 29 ] The  indecs Content Model  as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL. The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as  GS1  and  ISBN . A DOI name differs from commonly used Internet pointers to material, such as the  Uniform Resource Locator  (URL), in that it identifies an object itself as a  first-class entity , rather than the specific place where the object is located at a certain time. It implements the  Uniform Resource Identifier  ( Uniform Resource Name ) concept and adds to it a data model and social infrastructure. [ 30 ] A DOI name also differs from standard identifier registries such as the  ISBN ,  ISRC , etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections. [ 31 ] The DOI system offers persistent,  semantically interoperable  resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing both social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as \"identifiers\" does not mean that they can be compared easily. Other \"identifier systems\" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include  Persistent Uniform Resource Locator  (PURL), URLs,  Globally Unique Identifiers  (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g.,  ARK ). A DOI name does not depend on the object's location and, in this way, is similar to a  Uniform Resource Name  (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name. DOI name resolution is provided through the  Handle System , developed by  Corporation for National Research Initiatives , and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its  <type>  field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues. To resolve a DOI name, it may be input to a DOI resolver, such as  doi.org . Another approach, which avoids typing or  copying and pasting  into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as  https://doi.org/  (preferred) [ 32 ]  or  http://dx.doi.org/ , both of which support HTTPS. For example, the DOI  10.1000/182  can be included in a reference or  hyperlink  as  https://doi.org/10.1000/182 . This approach allows users to click on the DOI as a normal  hyperlink . Indeed, as previously mentioned, this is how Crossref recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable. Other DOI resolvers and HTTP Proxies include the  Handle System  and  PANGAEA . At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a  non-paywalled  (often  author archived ) version of a title and redirects the user to that instead of the  publisher's version . [ 33 ] [ 34 ]  Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016 [ 35 ]  (later  Unpaywall ). While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as  BASE  (Bielefeld Academic Search Engine). [ 33 ] [ 35 ] An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for  browsers , thereby avoiding the conversion of the DOIs to URLs, [ 36 ]  which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser,  mail reader , or other software which does not have one of these plug-ins installed. The  International DOI Foundation  ( IDF ), a non-profit organization created in 1997, is the governance body of the DOI system. [ 37 ]  It safeguards all  intellectual property rights  relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system. The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues. Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level  10  prefix. [ 38 ] Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a  not-for-profit  cost recovery basis. The DOI system is an international standard developed by the  International Organization for Standardization  in its technical committee on identification and description, TC46/SC9. [ 39 ]  The Draft International Standard ISO/DIS 26324,  Information and documentation – Digital Object Identifier System  met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot, [ 40 ]  which was approved by 100% of those voting in a ballot closing on 15 November 2010. [ 41 ]  The final standard was published on 23 April 2012. [ 2 ] DOI is a registered URI under the  info URI scheme  specified by IETF  RFC   4452 . info:doi/ is the infoURI Namespace of Digital Object Identifiers. [ 42 ] The DOI syntax is a  NISO  standard, first standardized in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier. [ 43 ] The maintainers of the DOI system have deliberately not registered a DOI namespace for  URNs , stating that: URN architecture assumes a DNS-based  Resolution Discovery Service  (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string  urn:doi:10.1000/1  rather than the simpler  doi:10.1000/1 ) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN."
  },
  {
    "id": 190,
    "title": "Handle System",
    "content": "The  Handle System  is a proprietary registry assigning  persistent identifiers , or  handles , to information resources, and for resolving \"those handles into the information necessary to locate, access, and otherwise make use of the resources\". [ 1 ] \nAs with handles used elsewhere in computing, Handle System handles are opaque, and encode no information about the underlying resource, being bound only to  metadata  regarding the resource. Consequently, the handles are not rendered invalid by changes to the metadata. The system was developed by  Bob Kahn  at the  Corporation for National Research Initiatives  (CNRI) as a part of the  Digital Object Architecture  (DOA). The original work was funded by the  Defense Advanced Research Projects Agency  (DARPA) between 1992 and 1996, as part of a wider framework for distributed digital object services, [ 2 ]  and was thus contemporaneous with the early deployment of the  World Wide Web , with similar goals. The Handle System was first implemented in autumn 1994, and was administered and operated by CNRI until December 2015, when a new \"multi-primary administrator\" (MPA) mode of operation was introduced. The DONA Foundation [ 3 ]  now administers the system's Global Handle Registry and accredits MPAs, including CNRI and the  International DOI Foundation . [ 4 ] \nThe system currently provides the underlying infrastructure for such handle-based systems as  Digital Object Identifiers  (DOI) and  DSpace , which are mainly used to provide access to scholarly, professional and government documents and other information resources. CNRI provides specifications and the source code for reference implementations for the servers and protocols used in the system under a royalty-free \"Public License\", similar to an open source license. [ 5 ] \nThousands of handle services are currently running. Over 1000 of these are at universities and libraries, but they are also in operation at national laboratories, research groups, government agencies, and commercial enterprises, receiving over 200 million resolution requests per month. [ citation needed ] The Handle System is defined in informational  RFCs  3650, [ 1 ]  3651 [ 6 ]  and 3652 [ 7 ]  of the  Internet Engineering Task Force (IETF) ; it includes an open set of protocols, a namespace, and a reference implementation of the protocols. Documentation, software, and related information is provided by CNRI on a dedicated website [ 8 ] Handles consist of a prefix which identifies a \"naming authority\" and a suffix which gives the \"local name\" of a resource. Similar to domain names, prefixes are issued to naming authorities by one of the \"multi-primary administrators\" of the system upon payment of a fee, which must be renewed annually. A naming authority may create any number of handles, with unique \"local names\", within their assigned prefixes. Two example of handles are: In the first example, which is the handle for the HANDLE.NET software license,  20.1000  is the prefix assigned to the naming authority (in this case, Handle.net itself) and  100  is the local name within that namespace. The local name may consist of any characters from the  Unicode  UCS-2 character set. The prefix also consists of any  UCS-2  characters, other than \"/\". The prefixes consist of one or more naming authority segments, separated by periods, representing a hierarchy of naming authorities. Thus, in the example  20  is the naming authority prefix for CNRI, while  1000  designates a subordinate naming authority within the 20 prefix. Other examples of top-level prefixes for the federated naming authorities of the DONA Foundation are  10  for DOI handles;  11  for handles assigned by the  ITU ;  21  for handles issued by the German Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG), the scientific computing center of the  University of Göttingen ; and  86  for the Coalition of Handle Services – China. Older \"legacy\" prefixes issued by CNRI before the \"multi-primary administrator\" (MPA) structure was instituted are typically four of five digits, as in the second example above, a handle administered by the  University of Leicester . All prefixes must be registered in the Global Handle Registry through an DONA Foundation approved registrar, normally for a fee. As with other uses of  handles  in computing, the handle is opaque; that is, it encodes no information about the underlying resource and provides only the means to retrieve metadata about the resource. This may be contrasted with a  Uniform Resource Locator  (URL), which may encode within the identifier such attributes of the resource as the protocol to be used to access the server holding the resource, the server host name and port number, and perhaps even location specifics such as the name of a file in the server file system containing the resource. In the Handle System, these specifics are not encoded in the handle, but are found in the metadata to which the handle is bound. The metadata may include many attributes of the information resource, such as its locations, the forms in which it is available, the types of access (e.g. \"free\" versus \"paid\") offered, and to whom. The processing of the metadata to determine how and where the resource should be accessed, and the provision of the resource to the user, are performed in a separate step, called \"resolution\", using a Resolver, a server which may be different from the ones involved in exchanging the handle for the metadata. Unlike URLs, which may become invalid if the metadata embedded within them becomes invalid, handles do not become invalid and do not need to change when locations or other metadata attributes change. This helps to prevent  link rot , as changes in the information resource (such as location) need only be reflected in changes to the metadata, rather than in changes in every reference to the resource. Each handle may have its own administrator and administration of the handles can be done in a distributed environment, similar to DNS domain names. The name-to-value bindings may also be secured, both via signatures to verify the data and via challenge response to verify the transmission of the data, allowing handles to be used in trust management applications. It is possible for the same underlying information resource to be associated with multiple handles, as when two university libraries generate handles (and therefore possibly different sets of metadata) for the same book. The Handle System is compatible with the  Domain Name System  (DNS), but does not require it, unlike persistent identifiers such as  PURLs  or  ARKs , which are similar to handles, but which utilise domain names. However, unlike these domain-name based approaches, handles do require a separate prefix registration process and handle servers separate from the domain name servers. Handles can be used natively, or expressed as  Uniform Resource Identifiers  (URIs) through a namespace within the  info URI scheme ; [ 9 ] [ 10 ]  for example,  20.1000/100  may be written as the URI,  info:hdl/20.1000/100 . Some Handle System namespaces, such as Digital Object Identifiers, are \"info:\" URI namespaces in their own right; for example,  info:doi/10.1000/182  is another way of writing the handle for the current revision of the DOI Handbook [ 11 ]  as a URI. Some Handle System namespaces define special presentation rules. For example,  Digital Object Identifiers , which represent a high percentage of the extant handles, are usually presented with a \"doi:\" prefix:  doi:10.1000/182 . Any Handle may be expressed as a Uniform Resource Locator (URL) through the use of the generic  HTTP proxy server : [ 12 ] Some Handle-based systems offer an HTTP proxy server that is intended for use with their own system such as: Implementation of the Handle System consists of Local Handle Services, each of which is made up of one or more sites that provide the servers that store specific handles. The Global Handle Registry is a unique Local Handle Service which stores information on the prefixes (also known as naming authorities) within the Handle System and can be queried to find out where specific handles are stored on other Local Handle Services within this distributed system. The Handle System website provides a series of implementation tools, notably the HANDLE.NET Software [ 13 ]  and HANDLE.NET Client Libraries. [ 14 ]  Handle clients can be embedded in end user software (e.g., a web browser) or in server software (e.g., a web server) and extensions are already available for  Adobe Acrobat [ 15 ]  and  Firefox . [ 16 ] Handle client software libraries are available in both C and Java. Some applications have developed specific add-on tools, e.g., for the DOI System. [ 17 ] The interoperable network of distributed handle resolver servers (also known as the Proxy Server System) are linked through a Global Resolver (which is one logical entity though physically decentralised and mirrored). Users of Handle System technology obtain a handle prefix created in the Global Handle Registry. The Global Handle Registry maintains and resolves the prefixes of locally maintained handle services. Any local handle service can, therefore, resolve any handle through the Global Resolver. Handles (identifiers) are passed by a client, as a query of the naming authority/prefix, to the Handle System's Global Handle Registry (GHR). The GHR responds by sending the client the location information for the relevant Local Handle Service (which may consist of multiple servers in multiple sites); a query is then sent to the relevant server within the Local Handle Service. The Local Handle Service returns the information needed to acquire the resource, e.g., a URL which can then be turned into an HTTP redirect. (Note: if the client already has information on the appropriate LHS to query, the initial query to GHR is omitted) Though the original model from which the Handle System derives dealt with management of digital objects, the Handle System does not mandate any particular model of relationships between the identified entities, nor is it limited to identifying only digital objects: non-digital entities may be represented as a corresponding digital object for the purposes of digital object management. Some care is needed in the definition of such objects and how they relate to non-digital entities; there are established models that can aid in such definitions e.g.,  Functional Requirements for Bibliographic Records (FRBR) ,  CIDOC CRM , and  indecs content model . Some applications have found it helpful to marry such a framework to the handle application: for example, the Advanced Distributed Learning (ADL) Initiative [ 18 ]  brings together Handle System application with existing standards for distributed learning content, using a Shareable Content Object Reference Model (SCORM), [ 19 ]  and the  Digital Object Identifier (DOI) system  implementation of the Handle System has adopted it together with the  indecs  framework to deal with  semantic interoperability . The Handle System also makes explicit the importance of organizational commitment to a persistent identifier scheme, but does not mandate one model for ensuring such commitment. Individual applications may choose to establish their own sets of rules and social infrastructure to ensure persistence (e.g., when used in the  DSpace  application, and the DOI application). [ 20 ] The Handle system is designed to meet the following requirements to contribute to persistence [ 21 ] The identifier string: The identifier resolution mechanism: Among the objects that are currently identified by handles are journal articles, technical reports, books, theses and dissertations, government documents, metadata, distributed learning content, and data sets. Handles are being used in  digital watermarking  applications,  GRID  applications, repositories, and more. Although individual users may download and use the HANDLE.NET software independently, many users have found it beneficial to collaborate in developing applications in a federation, using common policy or additional technology to provide shared services. As one of the first persistent identifier schemes, the Handle System has been widely adopted by public and private institutions and proven over several years. (See Paradigm, Persistent identifiers.) [ 22 ] Handle System applications may use handles as simple persistent identifiers (as most commonly used, to resolve to the current URL of an object), or may choose to take advantage of other features. Its support for the simultaneous return as output of multiple pieces of current information related to the object, in defined data structures, enables priorities to be established for the order in which the multiple resolutions will be used. Handles can, therefore, resolve to different digital versions of the same content, to mirror sites, or to different business models (pay vs. free, secure vs. open, public vs. private). They can also resolve to different digital versions of differing content, such as a mix of objects required for a distance-learning course. There are thousands of handle services running today, located in 71 countries, on 6 continents; over 1000 of them run at universities and libraries. Handle services are being run by user federations, national laboratories, universities, computing centers, libraries (national and local), government agencies, contractors, corporations, and research groups. Major publishers use the Handle System for persistent identification of commercially traded and Open Access content through its implementation with the  Digital Object Identifier (DOI) system . The number of prefixes, which allow users to assign handles, is growing and stands at over 12,000 as of early 2014. There are six top-level Global Handle Registry servers that receive (on average) 68 million resolution requests per month. Proxy servers known to CNRI, passing requests to the system on the Web, receive (on average) 200 million resolution requests per month. (Statistics from Handle Quick Facts.) In 2010, CNRI and  ITU  (International Telecommunication Union) entered into an agreement to collaborate on use of the Handle System (and the Digital Object Architecture more generally) and are working on the specific details of that collaboration; in April 2009 ITU listed the Handle System as an \"emerging trend\". [ 23 ] Conversely, implementing Handle System can be highly advantageous for gathering alternative metrics. By assigning persistent identifiers, companies specializing in altmetrics can effectively monitor the spread of content across social platforms and other non-traditional channels of scientific communication. [ 24 ] Handle System, HANDLE.NET and Global Handle Registry are trademarks of the  Corporation for National Research Initiatives  (CNRI), a non-profit research and development corporation in the US. The Handle System is the subject of patents by CNRI, which licenses its Handle System technology through a public license, [ 25 ]  similar to an open source license, in order to enable broader use of the technology. Handle System infrastructure is supported by prefix registration and service fees, with the majority coming from single prefix holders. The largest current single contributor is the  International DOI Foundation . The Public License allows commercial and non-commercial use at low cost of both its patented technology and the reference implementation of the software, and allows the software to be freely embedded in other systems and products. A Service Agreement [ 5 ]  is also available for users who intend to provide identifier or resolution services using the Handle System technology under the Handle System public license. The Handle System represents several components of a long-term digital object architecture. In January 2010 CNRI released its general-purpose Digital Object Repository software, [ 26 ]  another major component of this architecture. More information [ 27 ]  about the release, including protocol specification, source code and ready-to-use system, clients and utilities, is available. [ 28 ] [ 29 ]"
  },
  {
    "id": 191,
    "title": "Semantic Scholar",
    "content": "Semantic Scholar  is a research tool for scientific literature powered by  artificial intelligence . It is developed at the  Allen Institute for AI  and was publicly released in November 2015. [ 2 ]  Semantic Scholar uses modern techniques in  natural language processing  to support the research process, for example by providing automatically generated summaries of scholarly papers. [ 3 ]  The Semantic Scholar team is actively researching the use of artificial intelligence in  natural language processing ,  machine learning ,  human–computer interaction , and  information retrieval . [ 4 ] Semantic Scholar began as a database for the topics of  computer science ,  geoscience , and  neuroscience . [ 5 ]  In 2017, the system began including  biomedical literature  in its corpus. [ 5 ]  As of September 2022 [update] , it includes over 200 million publications from all fields of science. [ 6 ] Semantic Scholar provides a one-sentence summary of  scientific literature . One of its aims was to address the challenge of reading numerous titles and lengthy abstracts on mobile devices. [ 7 ]  It also seeks to ensure that the three million scientific papers published yearly reach readers, since it is estimated that only half of this literature is ever read. [ 8 ] Artificial intelligence is used to capture the essence of a paper, generating it through an \"abstractive\" technique. [ 3 ]  The project uses a combination of  machine learning ,  natural language processing , and  machine vision  to add a layer of  semantic analysis  to the traditional methods of  citation analysis , and to extract relevant figures,  tables , entities, and venues from papers. [ 9 ] [ 10 ] Another key AI-powered feature is Research Feeds, an adaptive research recommender that uses AI to quickly learn what papers users care about reading and recommends the latest research to help scholars stay up to date. It uses a state-of-the-art paper embedding model trained using contrastive learning to find papers similar to those in each Library folder. [ 11 ] Semantic Scholar also offers Semantic Reader, an augmented reader with the potential to revolutionize scientific reading by making it more accessible and richly contextual. [ 12 ]  Semantic Reader provides in-line citation cards that allow users to see citations with  TLDR  (short for Too Long, Didn't Read) automatically generated short summaries as they read and skimming highlights that capture key points of a paper so users can digest faster. In contrast with  Google Scholar  and  PubMed , Semantic Scholar is designed to highlight the most important and influential elements of a paper. [ 13 ]  The AI technology is designed to identify hidden connections and links between research topics. [ 14 ]  Like the previously cited search engines, Semantic Scholar also exploits graph structures, which include the  Microsoft Academic Knowledge Graph , Springer Nature's  SciGraph , and the Semantic Scholar Corpus (originally a 45 million papers corpus in computer science, neuroscience and biomedicine). [ 15 ] [ 16 ] Each paper hosted by Semantic Scholar is assigned a unique  identifier  called the Semantic Scholar Corpus ID (abbreviated S2CID). The following entry is an example: Liu, Ying; Gayle, Albert A; Wilder-Smith, Annelies; Rocklöv, Joacim (March 2020). \"The reproductive number of COVID-19 is higher compared to SARS coronavirus\".  Journal of Travel Medicine .  27  (2).  doi : 10.1093/jtm/taaa021 .  PMID   32052846 .  S2CID   211099356 . Semantic Scholar is free to use and unlike similar search engines (i.e.  Google Scholar ) does not search for material that is behind a  paywall . [ 5 ] [ citation needed ] One study compared the index scope of Semantic Scholar to Google Scholar, and found that for the papers cited by secondary studies in computer science, the two indices had comparable coverage, each only missing a handful of the papers. [ 17 ] As of January 2018, following a 2017 project that added biomedical papers and topic summaries, the Semantic Scholar corpus included more than 40 million papers from  computer science  and  biomedicine . [ 18 ]  In March 2018, Doug Raymond, who developed  machine learning  initiatives for the  Amazon Alexa  platform, was hired to lead the Semantic Scholar project. [ 19 ]  As of August 2019 [update] , the number of included papers metadata (not the actual PDFs) had grown to more than 173 million [ 20 ]  after the addition of the  Microsoft Academic Graph  records. [ 21 ]  In 2020, a partnership between Semantic Scholar and the  University of Chicago Press Journals  made all articles published under the University of Chicago Press available in the Semantic Scholar corpus. [ 22 ]  At the end of 2020, Semantic Scholar had indexed 190 million papers. [ 23 ]   In 2020, Semantic Scholar reached seven million users per month. [ 7 ]"
  },
  {
    "id": 192,
    "title": "CiteSeerX",
    "content": "CiteSeer X  (formerly called  CiteSeer ) is a public  search engine  and  digital library  for scientific and academic papers, primarily in the fields of  computer  and  information science . CiteSeer's goal is to improve the dissemination and access of academic and scientific literature. As a non-profit service that can be freely used by anyone, it has been considered part of the  open access  movement that is attempting to change  academic and scientific publishing  to allow greater access to scientific literature. CiteSeer freely provided  Open Archives Initiative   metadata  of all indexed documents and links indexed documents when possible to other sources of metadata such as  DBLP  and the  ACM Portal . To promote  open data ,  CiteSeer X  shares its data for non-commercial purposes under a  Creative Commons license . [ 1 ] CiteSeer is considered a predecessor of academic search tools such as  Google Scholar  and  Microsoft Academic Search . [ 2 ]  CiteSeer-like engines and archives usually only harvest documents from publicly available websites and do not crawl publisher websites. For this reason, authors whose documents are freely available are more likely to be represented in the index. CiteSeer changed its name to ResearchIndex at one point and then changed it back. [ 3 ] CiteSeer was created by researchers  Lee Giles ,  Kurt Bollacker  and  Steve Lawrence  in 1997 while they were at the  NEC Research Institute  (now  NEC Labs ),  Princeton, New Jersey , US. CiteSeer's goal was to actively crawl and harvest academic and scientific documents on the web and use autonomous  citation indexing  to permit querying by citation or by document, ranking them by  citation impact . At one point, it was called ResearchIndex. CiteSeer became public in 1998 and had many new features unavailable in academic search engines at that time. These included: CiteSeer was granted a United States  patent  # 6289342, titled \" Autonomous citation indexing and literature browsing using citation context \", on September 11, 2001. The patent was filed on May 20, 1998, and has priority to January 5, 1998. A continuation patent (US Patent # 6738780) was filed on May 16, 2001, and granted on May 18, 2004. [ citation needed ] After NEC, in 2004 it was hosted as CiteSeer.IST on the  World Wide Web  at the College of Information Sciences and Technology, The  Pennsylvania State University , and had over 700,000 documents. For enhanced access, performance and research, similar versions of CiteSeer were supported at universities such as the  Massachusetts Institute of Technology ,  University of Zürich  and the  National University of Singapore . However, these versions of CiteSeer proved difficult to maintain and are no longer available. Because CiteSeer only indexes freely available papers on the web and does not have access to publisher metadata, it returns fewer citation counts than sites, such as  Google Scholar , that have publisher metadata. CiteSeer had not been comprehensively updated since 2005 due to limitations in its architecture design. It had a representative sampling of research documents in computer and information science but was limited in coverage because it was limited to papers that are publicly available, usually at an author's homepage, or those submitted by an author. To overcome some of these limitations, a modular and open source architecture for CiteSeer was designed – CiteSeer X . CiteSeer X  replaced CiteSeer and all queries to CiteSeer were redirected. CiteSeer X [ 4 ]  is a public  search engine  and  digital library  and  repository  for scientific and academic papers, primarily with a focus on  computer  and  information science . [ 4 ]  However, recently CiteSeer X  has been expanding into other scholarly domains such as economics, physics and others. Released in 2008, it was loosely based on the previous CiteSeer search engine and digital library and is built with a new  open source  infrastructure, SeerSuite, and new algorithms and their implementations. It was developed by researchers Isaac Councill and C.  Lee Giles  at  the College of Information Sciences and Technology ,  Pennsylvania State University . It continues to support the goals outlined by CiteSeer to actively crawl and harvest academic and scientific documents on the public web and to use a citation inquiry by citations and ranking of documents by the impact of citations. Currently, Lee Giles, Prasenjit Mitra, Susan Gauch, Min-Yen Kan, Pradeep Teregowda, Juan Pablo Fernández Ramírez, Pucktada Treeratpituk, Jian Wu, Douglas Jordan, Steve Carman, Jack Carroll, Jim Jansen, and Shuyi Zheng are or have been actively involved in its development. Recently, a table search feature was introduced. [ 5 ]  It has been funded by the  National Science Foundation ,  NASA , and  Microsoft Research . CiteSeer X  continues to be rated as one of the world's top repositories, and was rated number 1 in July 2010. [ 6 ]  It currently has over 6 million documents with nearly 6 million unique authors and 120 million citations. [ timeframe? ] CiteSeer X  also shares its software, data, databases and metadata with other researchers, currently by  Amazon S3  and by  rsync . [ 7 ]  Its new modular open source architecture and software (available previously on  SourceForge  but now on  GitHub ) is built on  Apache Solr  and other  Apache  and open source tools, which allows it to be a testbed for new algorithms in document harvesting, ranking, indexing, and information extraction. CiteSeer X  caches some PDF files that it has scanned.  As such, each page includes a  DMCA  link which can be used to report copyright violations. [ 8 ] CiteSeer X  uses automated  information extraction  tools, usually built on machine learning methods such ParsCit, to extract scholarly document metadata such as title, authors, abstract, citations, etc. As such, there are sometime errors in authors and titles. Other academic search engines have similar errors. CiteSeer X  crawls publicly available scholarly documents primarily from author webpages and other open resources, and does not have access to publisher metadata. As such, citation counts in CiteSeer X  are usually less than those in Google Scholar and Microsoft Academic Search who have access to publisher metadata. CiteSeer X  has nearly one million users worldwide based on unique IP addresses and has millions of hits daily. Annual downloads of document PDFs were nearly 200 million for 2015. CiteSeer X  data is regularly shared under a  Creative Commons BY-NC-SA license  with researchers worldwide and has been and is used in many experiments and competitions. Thanks to its  OAI-PMH  endpoint, [ 9 ]  CiteSeerX is an  open archive  and its content is indexed like an  institutional repository  in  academic search engines , for instance  BASE  and  Unpaywall  consumers. The CiteSeer model had been extended to cover academic documents in business with  SmealSearch  and in e-business with  eBizSearch .  However, these were not maintained by their sponsors. An older version of both of these could be once found at  BizSeer.IST  but is no longer in service. Other Seer-like search and repository systems have been built for chemistry,  Chem X Seer  and for archaeology, ArchSeer. Another had been built for robots.txt file search,  BotSeer . All of these are built on the open source tool  SeerSuite , which uses the open source indexer  Lucene ."
  },
  {
    "id": 193,
    "title": "ISBN",
    "content": "The  International Standard Book Number  ( ISBN ) is a numeric commercial  book   identifier  that is intended to be unique. [ a ] [ b ]  Publishers purchase or receive ISBNs from an affiliate of the International ISBN Agency. [ 2 ] A different ISBN is assigned to each separate edition and variation of a publication, but not to a simple reprinting of an existing item. For example, an  e-book , a  paperback  and a  hardcover  edition of the same  book  must each have a different ISBN, but an unchanged reprint of the hardcover edition keeps the same ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. [ c ]  The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country. The first version of the ISBN identification format was devised in 1967, based upon the 9-digit  Standard Book Numbering  ( SBN ) created in 1966. The 10-digit ISBN format was developed by the  International Organization for Standardization  (ISO) and was published in 1970 as international standard ISO 2108 (any 9-digit SBN can be converted to a 10-digit ISBN by prefixing it with a zero). Privately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns ISBNs to such books on its own initiative. [ 4 ] A separate identifier code of a similar kind, the  International Standard Serial Number  (ISSN), identifies periodical publications such as  magazines  and  newspapers . The  International Standard Music Number  (ISMN) covers  musical scores . The Standard Book Number (SBN) is a commercial system using nine-digit  code  numbers to identify books. In 1965, British bookseller and stationers  WHSmith  announced plans to implement a standard numbering system for its books. [ 1 ]  They hired consultants to work on their behalf, and the system was devised by  Gordon Foster , emeritus professor of statistics at  Trinity College Dublin . [ 5 ]  The  International Organization for Standardization  (ISO) Technical Committee on Documentation sought to adapt the British SBN for international use. The ISBN identification format was conceived in 1967 in the United Kingdom by  David Whitaker [ 6 ] [ 7 ]  (regarded as the \"Father of the ISBN\") [ 8 ]  and in 1968 in the United States by Emery Koltay [ 6 ]  (who later became director of the U.S. ISBN agency  R. R. Bowker ). [ 8 ] [ 9 ] [ 10 ] The 10-digit ISBN format was developed by the ISO and was published in 1970 as international standard ISO 2108. [ 1 ] [ 6 ]  The United Kingdom continued to use the nine-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the  registration authority  for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9  TC 46/SC 9 . The ISO on-line facility only refers back to 1978. [ 11 ] An SBN may be converted to an ISBN by prefixing the digit \"0\". For example, the second edition of  Mr. J. G. Reeder Returns , published by Hodder in 1965, has  \"SBN 340 01381 8\" , where \"340\" indicates the  publisher , \"01381\" is the serial number assigned by the publisher, and \"8\" is the  check digit . By prefixing a zero, this can be converted to  ISBN   0-340-01381-8 ; the check digit does not need to be re-calculated. Some publishers, such as  Ballantine Books , would sometimes use 12-digit SBNs where the last three digits indicated the price of the book; [ 12 ]  for example,  Woodstock Handmade Houses  had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, ISBN: 0-345-24223-8), [ 13 ]  and it cost  US$5.95 . [ 14 ] Since 1 January 2007, ISBNs have contained thirteen digits, a format that is compatible with \" Bookland \"  European Article Numbers , which have 13 digits. [ 3 ]  Since 2016, ISBNs have also been used to identify  mobile games  by China's  Administration of Press and Publication . [ 15 ] The  United States , with 3.9 million registered ISBNs in 2020, was by far the biggest user of the ISBN identifier in 2020, followed by the  Republic of Korea  (329,582),  Germany  (284,000),  China  (263,066), the  UK  (188,553) and  Indonesia  (144,793). Lifetime ISBNs registered in the United States are over 39 million as of 2020. [ 16 ] A separate ISBN is assigned to each edition and variation (except reprintings) of a publication. For example, an ebook,  audiobook , paperback, and hardcover edition of the same book must each have a different ISBN assigned to it. [ 17 ] : 12   The ISBN is thirteen digits long if assigned on or after 1 January 2007, and ten digits long if assigned before 2007. [ c ] [ 3 ]  An International Standard Book Number consists of four parts (if it is a 10-digit ISBN) or five parts (for a 13-digit ISBN). Section 5 of the International ISBN Agency's official user manual [ 17 ] : 11   describes the structure of the 13-digit ISBN, as follows: A 13-digit ISBN can be separated into its parts ( prefix element ,  registration group ,  registrant ,  publication  and  check digit ), and when this is done it is customary to separate the parts with  hyphens  or spaces. Separating the parts ( registration group ,  registrant ,  publication  and  check digit ) of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits. [ e ] ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from the government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded. [ 19 ] A full directory of ISBN agencies is available on the International ISBN Agency website. [ 20 ]  A list for a few countries is given below: The ISBN  registration group element  is a 1-to-5-digit number that is valid within a single prefix element (i.e. one of 978 or 979), [ 17 ] : 11   and can be separated between hyphens, such as  \"978-1-...\" . Registration groups have primarily been allocated within the 978 prefix element. [ 39 ]  The single-digit registration groups within the 978-prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. Example 5-digit registration groups are 99936 and 99980, for Bhutan. The allocated registration groups are: 0–5, 600–631, 65, 7, 80–94, 950–989, 9910–9989, and 99901–99993. [ 40 ]  Books published in rare languages typically have longer group elements. [ 41 ] Within the 979 prefix element, the registration group 0 is reserved for compatibility with  International Standard Music Numbers  (ISMNs), but such material is not actually assigned an ISBN. [ 42 ]  The registration groups within prefix element 979 that have been assigned are 8 for the United States of America, 10 for France, 11 for the Republic of Korea, and 12 for Italy. [ 43 ] The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero to a 9-digit SBN creates a valid 10-digit ISBN. The national ISBN agency assigns the registrant element ( cf.   Category:ISBN agencies ) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not legally required to assign an ISBN, although most large bookstores only handle publications that have ISBNs assigned to them. [ 44 ] [ 45 ] [ 46 ] The International ISBN Agency maintains the details of over one million ISBN prefixes and publishers in the  Global Register of Publishers . [ 47 ]  This database is freely searchable over the internet. Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers. By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. [ 48 ]  Here are some sample ISBN-10 codes, illustrating block length variations. English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows: [ 18 ] A  check digit  is a form of redundancy check used for  error detection , the decimal equivalent of a binary  check bit . It consists of a single digit computed from the other digits in the number. The method for the 10-digit ISBN is an extension of that for SBNs, so the two systems are compatible; an SBN prefixed with a zero (the 10-digit ISBN) will give the same check digit as the SBN without the zero. The check digit is base eleven, and can be an integer between 0 and 9, or an 'X'. The system for 13-digit ISBNs is not compatible with SBNs and will, in general, give a different check digit from the corresponding 10-digit ISBN, so does not provide the same protection against transposition. This is because the 13-digit code was required to be compatible with the  EAN  format, and hence could not contain the letter 'X'. According to the 2001 edition of the International ISBN Agency's official user manual, [ 49 ]  the ISBN-10 check digit (which is the last digit of the 10-digit ISBN) must range from 0 to 10 (the symbol 'X' is used for 10), and must be such that the sum of the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11. That is, if  x i  is the  i th digit, then  x 10  must be chosen such that: For example, for an ISBN-10 of 0-306-40615-2: Formally, using  modular arithmetic , this is rendered It is also true for ISBN-10s that the sum of all ten digits, each multiplied by its weight in  ascending  order from 1 to 10, is a multiple of 11. For this example: Formally, this is rendered The two most common errors in handling an ISBN (e.g. when typing it or writing it down) are a single altered digit or the transposition of adjacent digits. It can be proven mathematically that all pairs of valid ISBN-10s differ in at least two digits. It can also be proven that there are no pairs of valid ISBN-10s with eight identical digits and two transposed digits (these proofs are true because the ISBN is less than eleven digits long and because 11 is a  prime number ). The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e., if either of these types of error has occurred, the result will never be a valid ISBN—the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error were to occur in the publishing house and remain undetected, the book would be issued with an invalid ISBN. [ 50 ] In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely). Each of the first nine digits of the 10-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11. For example, the check digit for an ISBN-10 of 0-306-40615- ?  is calculated as follows: Adding 2 to 130 gives a multiple of 11 (because 132 = 12×11)—this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is ISBN 0-306-40615-2. If the value of  \n \n \n \n \n x \n \n 10 \n \n \n \n \n {\\displaystyle x_{10}} \n \n  required to satisfy this condition is 10, then an 'X' should be used. Alternatively,  modular arithmetic  is convenient for calculating the check digit using modulus 11. The  remainder  of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation, the calculation could result in a check digit value of  11 − 0 = 11 , which is invalid. (Strictly speaking, the  first  \"modulo 11\" is not needed, but it may be considered to simplify the calculation.) For example, the check digit for the ISBN of 0-306-40615- ?  is calculated as follows: Thus the check digit is 2. It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding  t  into  s  computes the necessary multiples: The modular reduction can be done once at the end, as shown above (in which case  s  could hold a value as large as 496, for the invalid ISBN 99999-999-9-X), or  s  and  t  could be reduced by a conditional subtract after each addition. Appendix 1 of the International ISBN Agency's official user manual [ 17 ] : 33   describes how the 13-digit ISBN check digit is calculated. The ISBN-13 check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of  10 . As ISBN-13 is a subset of  EAN-13 , the algorithm for calculating the check digit is exactly the same for both. Formally, using  modular arithmetic , this is rendered: The calculation of an ISBN-13 check digit begins with the first twelve digits of the 13-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed  modulo  10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero replaces a ten, so, in all cases, a single check digit results. For example, the ISBN-13 check digit of 978-0-306-40615- ?  is calculated as follows: Thus, the check digit is 7, and the complete sequence is ISBN 978-0-306-40615-7. In general, the ISBN check digit is calculated as follows. Let Then This check system—similar to the  UPC  check digit formula—does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes  3 × 6 + 1 × 1 = 19  to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be  3 × 1 + 1 × 6 = 9 . However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The ISBN-10 formula uses the  prime  modulus 11 which avoids this blind spot, but requires more than the digits 0–9 to express the check digit. Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0). A 10-digit ISBN is converted to a 13-digit ISBN by prepending \"978\" to the  ISBN-10 and recalculating the final checksum digit using the ISBN-13 algorithm. The reverse process can also be performed, but not for numbers commencing with a prefix other than 978, which have no 10-digit equivalent. Publishers  and  libraries  have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers. [ 51 ]  For example,  ISBN   0-590-76484-5  is shared by two books— Ninja gaiden: a novel based on the best-selling game by Tecmo  (1990) and  Wacky laws  (1997), both published by  Scholastic . Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The  Library of Congress  catalogue contains books published with invalid ISBNs, which it usually tags with the phrase \"Cancelled ISBN\". [ 52 ]  The International Union Library Catalog (a.k.a.,  WorldCat   OCLC —Online Computer Library Center system) often indexes by invalid ISBNs, if the book is indexed in that way by a member library. [ 53 ] Only the term \"ISBN\" should be used; the terms \"eISBN\" and \"e-ISBN\" have historically been sources of confusion and should be avoided. If a book exists in one or more digital ( e-book ) formats, each of those formats must have its own ISBN. In other words, each of the three separate  EPUB ,  Amazon Kindle , and  PDF  formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic \"eISBN\" which encompasses all the e-book formats for a title. [ 54 ] The  barcodes  on a book's back cover (or inside a mass-market  paperback  book's front cover) are  EAN-13 ; they may have a separate barcode encoding five digits called an  EAN-5  for the  currency  and the  recommended retail price . [ 55 ]  For 10-digit ISBNs, the number \"978\", the  Bookland  \"country code\", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN-13 formula (modulo 10, 1× and 3× weighting on alternating digits). Partly because of an expected shortage in certain ISBN categories, the  International Organization for Standardization  (ISO) decided to migrate to a 13-digit ISBN (ISBN-13). The process began on 1 January 2005 and was planned to conclude on 1 January 2007. [ 56 ]  As of 2011 [update] , all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the  Musicland  code for musical scores with an  ISMN . The 10-digit ISMN codes differed visually as they began with an \"M\" letter; the bar code represents the \"M\" as a zero, and for checksum purposes it counted as a 3. All ISMNs are now thirteen digits commencing 979-0; 979-1 to 979-9 will be used by ISBN. Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the 10-digit ISBN check digit generally is not the same as the 13-digit ISBN check digit. Because the GTIN-13 is part of the  Global Trade Item Number  (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range. [ 57 ] Barcode format compatibility is maintained, because (aside from the group breaks) the ISBN-13 barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to  information technology  systems. Hence, many  booksellers  (e.g.,  Barnes & Noble ) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the  UPC   barcode system  to full EAN-13, in 2005, eased migration to the ISBN in North America."
  },
  {
    "id": 194,
    "title": "C. J. van Rijsbergen",
    "content": "C. J. \"Keith\" van Rijsbergen   FREng  ( Cornelis Joost van Rijsbergen ; born 1943) [ 1 ]  is a professor of  computer science  at the  University of Glasgow , where he founded the Glasgow Information Retrieval Group. [ 2 ]  He is one of the founders of modern  Information Retrieval  and the author of the seminal monograph  Information Retrieval  and of the textbook  The Geometry of Information Retrieval . He was born in  Rotterdam , and educated in the  Netherlands ,  Indonesia ,  Namibia  and  Australia .\nHis first degree is in mathematics from the  University of Western Australia , and in 1972 he completed a\nPhD in computer science at the  University of Cambridge . He spent three years lecturing in information retrieval and artificial intelligence at  Monash University [ 1 ]  before returning to  Cambridge  to hold a  Royal Society  Information Research Fellowship. \nIn 1980 he was appointed to the chair of computer science at  University College Dublin ; from there he moved in 1986 to  Glasgow University . He chaired the Scientific Board of the  Information Retrieval Facility  from 2007 to 2012. In 2003 he was inducted as a Fellow of the  Association for Computing Machinery . In 2004 he was awarded the  Tony Kent Strix award .\nIn 2004 he was appointed a  Fellow  of the  Royal Academy of Engineering . [ 3 ]  In 2006, he was awarded the  Gerard Salton Award  for  Quantum haystacks . In 2009, he was made an  honorary professor  at the  University of Edinburgh . [ 3 ] This article about a Dutch scientist is a  stub . You can help Wikipedia by  expanding it . This biographical article relating to a  computer scientist  is a  stub . You can help Wikipedia by  expanding it ."
  }
]